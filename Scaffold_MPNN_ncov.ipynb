{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scaffold_MPNN_ncov.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "15i-bNMdMudRn2oupSkk1K6Bq3XOwDROE",
      "authorship_tag": "ABX9TyOnQu4OOBNpfwDlmxQhJNDX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seyonechithrananda/ncov-ligand-protein/blob/master/Scaffold_MPNN_ncov.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnhZ7HBLDf73",
        "colab_type": "code",
        "outputId": "9d23a0c0-147f-4b0c-d31f-e187862ed0fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!wget -c https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!conda install -q -y -c conda-forge rdkit\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-06 05:03:48--  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.130.3, 104.16.131.3, 2606:4700::6810:8203, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.130.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 85055499 (81M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-latest-Linux-x86_64.sh’\n",
            "\n",
            "\r          Miniconda   0%[                    ]       0  --.-KB/s               \r         Miniconda3  52%[=========>          ]  42.28M   211MB/s               \rMiniconda3-latest-L 100%[===================>]  81.12M   232MB/s    in 0.3s    \n",
            "\n",
            "2020-05-06 05:03:48 (232 MB/s) - ‘Miniconda3-latest-Linux-x86_64.sh’ saved [85055499/85055499]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - _libgcc_mutex==0.1=main\n",
            "    - asn1crypto==1.3.0=py37_0\n",
            "    - ca-certificates==2020.1.1=0\n",
            "    - certifi==2019.11.28=py37_0\n",
            "    - cffi==1.14.0=py37h2e261b9_0\n",
            "    - chardet==3.0.4=py37_1003\n",
            "    - conda-package-handling==1.6.0=py37h7b6447c_0\n",
            "    - conda==4.8.2=py37_0\n",
            "    - cryptography==2.8=py37h1ba5d50_0\n",
            "    - idna==2.8=py37_0\n",
            "    - ld_impl_linux-64==2.33.1=h53a641e_7\n",
            "    - libedit==3.1.20181209=hc058e9b_0\n",
            "    - libffi==3.2.1=hd88cf55_4\n",
            "    - libgcc-ng==9.1.0=hdf63c60_0\n",
            "    - libstdcxx-ng==9.1.0=hdf63c60_0\n",
            "    - ncurses==6.2=he6710b0_0\n",
            "    - openssl==1.1.1d=h7b6447c_4\n",
            "    - pip==20.0.2=py37_1\n",
            "    - pycosat==0.6.3=py37h7b6447c_0\n",
            "    - pycparser==2.19=py37_0\n",
            "    - pyopenssl==19.1.0=py37_0\n",
            "    - pysocks==1.7.1=py37_0\n",
            "    - python==3.7.6=h0371630_2\n",
            "    - readline==7.0=h7b6447c_5\n",
            "    - requests==2.22.0=py37_1\n",
            "    - ruamel_yaml==0.15.87=py37h7b6447c_0\n",
            "    - setuptools==45.2.0=py37_0\n",
            "    - six==1.14.0=py37_0\n",
            "    - sqlite==3.31.1=h7b6447c_0\n",
            "    - tk==8.6.8=hbc83047_0\n",
            "    - tqdm==4.42.1=py_0\n",
            "    - urllib3==1.25.8=py37_0\n",
            "    - wheel==0.34.2=py37_0\n",
            "    - xz==5.2.4=h14c3975_4\n",
            "    - yaml==0.1.7=had09818_2\n",
            "    - zlib==1.2.11=h7b6447c_3\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
            "  asn1crypto         pkgs/main/linux-64::asn1crypto-1.3.0-py37_0\n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2020.1.1-0\n",
            "  certifi            pkgs/main/linux-64::certifi-2019.11.28-py37_0\n",
            "  cffi               pkgs/main/linux-64::cffi-1.14.0-py37h2e261b9_0\n",
            "  chardet            pkgs/main/linux-64::chardet-3.0.4-py37_1003\n",
            "  conda              pkgs/main/linux-64::conda-4.8.2-py37_0\n",
            "  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1.6.0-py37h7b6447c_0\n",
            "  cryptography       pkgs/main/linux-64::cryptography-2.8-py37h1ba5d50_0\n",
            "  idna               pkgs/main/linux-64::idna-2.8-py37_0\n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.33.1-h53a641e_7\n",
            "  libedit            pkgs/main/linux-64::libedit-3.1.20181209-hc058e9b_0\n",
            "  libffi             pkgs/main/linux-64::libffi-3.2.1-hd88cf55_4\n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-9.1.0-hdf63c60_0\n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-9.1.0-hdf63c60_0\n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.2-he6710b0_0\n",
            "  openssl            pkgs/main/linux-64::openssl-1.1.1d-h7b6447c_4\n",
            "  pip                pkgs/main/linux-64::pip-20.0.2-py37_1\n",
            "  pycosat            pkgs/main/linux-64::pycosat-0.6.3-py37h7b6447c_0\n",
            "  pycparser          pkgs/main/linux-64::pycparser-2.19-py37_0\n",
            "  pyopenssl          pkgs/main/linux-64::pyopenssl-19.1.0-py37_0\n",
            "  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py37_0\n",
            "  python             pkgs/main/linux-64::python-3.7.6-h0371630_2\n",
            "  readline           pkgs/main/linux-64::readline-7.0-h7b6447c_5\n",
            "  requests           pkgs/main/linux-64::requests-2.22.0-py37_1\n",
            "  ruamel_yaml        pkgs/main/linux-64::ruamel_yaml-0.15.87-py37h7b6447c_0\n",
            "  setuptools         pkgs/main/linux-64::setuptools-45.2.0-py37_0\n",
            "  six                pkgs/main/linux-64::six-1.14.0-py37_0\n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.31.1-h7b6447c_0\n",
            "  tk                 pkgs/main/linux-64::tk-8.6.8-hbc83047_0\n",
            "  tqdm               pkgs/main/noarch::tqdm-4.42.1-py_0\n",
            "  urllib3            pkgs/main/linux-64::urllib3-1.25.8-py37_0\n",
            "  wheel              pkgs/main/linux-64::wheel-0.34.2-py37_0\n",
            "  xz                 pkgs/main/linux-64::xz-5.2.4-h14c3975_4\n",
            "  yaml               pkgs/main/linux-64::yaml-0.1.7-had09818_2\n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.11-h7b6447c_3\n",
            "\n",
            "\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - rdkit\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    boost-1.72.0               |   py37h9de70de_0         316 KB  conda-forge\n",
            "    boost-cpp-1.72.0           |       h8e57a91_0        21.8 MB  conda-forge\n",
            "    bzip2-1.0.8                |       h516909a_2         396 KB  conda-forge\n",
            "    ca-certificates-2020.4.5.1 |       hecc5488_0         146 KB  conda-forge\n",
            "    cairo-1.16.0               |    hcf35c78_1003         1.5 MB  conda-forge\n",
            "    certifi-2020.4.5.1         |   py37hc8dfbb8_0         151 KB  conda-forge\n",
            "    conda-4.8.3                |   py37hc8dfbb8_1         3.0 MB  conda-forge\n",
            "    fontconfig-2.13.1          |    h86ecdb6_1001         340 KB  conda-forge\n",
            "    freetype-2.10.1            |       he06d7ca_0         877 KB  conda-forge\n",
            "    gettext-0.19.8.1           |    hc5be6a0_1002         3.6 MB  conda-forge\n",
            "    glib-2.64.2                |       h6f030ca_0         3.4 MB  conda-forge\n",
            "    icu-64.2                   |       he1b5a44_1        12.6 MB  conda-forge\n",
            "    jpeg-9c                    |    h14c3975_1001         251 KB  conda-forge\n",
            "    libblas-3.8.0              |      14_openblas          10 KB  conda-forge\n",
            "    libcblas-3.8.0             |      14_openblas          10 KB  conda-forge\n",
            "    libgfortran-ng-7.3.0       |       hdf63c60_5         1.7 MB  conda-forge\n",
            "    libiconv-1.15              |    h516909a_1006         2.0 MB  conda-forge\n",
            "    liblapack-3.8.0            |      14_openblas          10 KB  conda-forge\n",
            "    libopenblas-0.3.7          |       h5ec1e0e_6         7.6 MB  conda-forge\n",
            "    libpng-1.6.37              |       hed695b0_1         308 KB  conda-forge\n",
            "    libtiff-4.1.0              |       hc7e4089_6         668 KB  conda-forge\n",
            "    libuuid-2.32.1             |    h14c3975_1000          26 KB  conda-forge\n",
            "    libwebp-base-1.1.0         |       h516909a_3         845 KB  conda-forge\n",
            "    libxcb-1.13                |    h14c3975_1002         396 KB  conda-forge\n",
            "    libxml2-2.9.10             |       hee79883_0         1.3 MB  conda-forge\n",
            "    lz4-c-1.8.3                |    he1b5a44_1001         187 KB  conda-forge\n",
            "    numpy-1.18.4               |   py37h8960a57_0         5.2 MB  conda-forge\n",
            "    olefile-0.46               |             py_0          31 KB  conda-forge\n",
            "    openssl-1.1.1g             |       h516909a_0         2.1 MB  conda-forge\n",
            "    pandas-1.0.3               |   py37h0da4684_1        11.1 MB  conda-forge\n",
            "    pcre-8.44                  |       he1b5a44_0         261 KB  conda-forge\n",
            "    pillow-7.1.2               |   py37hb39fc2d_0         603 KB\n",
            "    pixman-0.38.0              |    h516909a_1003         594 KB  conda-forge\n",
            "    pthread-stubs-0.4          |    h14c3975_1001           5 KB  conda-forge\n",
            "    pycairo-1.19.1             |   py37h01af8b0_3          77 KB  conda-forge\n",
            "    python-dateutil-2.8.1      |             py_0         220 KB  conda-forge\n",
            "    python_abi-3.7             |          1_cp37m           4 KB  conda-forge\n",
            "    pytz-2020.1                |     pyh9f0ad1d_0         227 KB  conda-forge\n",
            "    rdkit-2020.03.1            |   py37hdd87690_3        24.6 MB  conda-forge\n",
            "    xorg-kbproto-1.0.7         |    h14c3975_1002          26 KB  conda-forge\n",
            "    xorg-libice-1.0.10         |       h516909a_0          57 KB  conda-forge\n",
            "    xorg-libsm-1.2.3           |    h84519dc_1000          25 KB  conda-forge\n",
            "    xorg-libx11-1.6.9          |       h516909a_0         918 KB  conda-forge\n",
            "    xorg-libxau-1.0.9          |       h14c3975_0          13 KB  conda-forge\n",
            "    xorg-libxdmcp-1.1.3        |       h516909a_0          18 KB  conda-forge\n",
            "    xorg-libxext-1.3.4         |       h516909a_0          51 KB  conda-forge\n",
            "    xorg-libxrender-0.9.10     |    h516909a_1002          31 KB  conda-forge\n",
            "    xorg-renderproto-0.11.1    |    h14c3975_1002           8 KB  conda-forge\n",
            "    xorg-xextproto-7.3.0       |    h14c3975_1002          27 KB  conda-forge\n",
            "    xorg-xproto-7.0.31         |    h14c3975_1007          72 KB  conda-forge\n",
            "    zstd-1.4.4                 |       h3b9ef0a_2         982 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       110.7 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  boost              conda-forge/linux-64::boost-1.72.0-py37h9de70de_0\n",
            "  boost-cpp          conda-forge/linux-64::boost-cpp-1.72.0-h8e57a91_0\n",
            "  bzip2              conda-forge/linux-64::bzip2-1.0.8-h516909a_2\n",
            "  cairo              conda-forge/linux-64::cairo-1.16.0-hcf35c78_1003\n",
            "  fontconfig         conda-forge/linux-64::fontconfig-2.13.1-h86ecdb6_1001\n",
            "  freetype           conda-forge/linux-64::freetype-2.10.1-he06d7ca_0\n",
            "  gettext            conda-forge/linux-64::gettext-0.19.8.1-hc5be6a0_1002\n",
            "  glib               conda-forge/linux-64::glib-2.64.2-h6f030ca_0\n",
            "  icu                conda-forge/linux-64::icu-64.2-he1b5a44_1\n",
            "  jpeg               conda-forge/linux-64::jpeg-9c-h14c3975_1001\n",
            "  libblas            conda-forge/linux-64::libblas-3.8.0-14_openblas\n",
            "  libcblas           conda-forge/linux-64::libcblas-3.8.0-14_openblas\n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-7.3.0-hdf63c60_5\n",
            "  libiconv           conda-forge/linux-64::libiconv-1.15-h516909a_1006\n",
            "  liblapack          conda-forge/linux-64::liblapack-3.8.0-14_openblas\n",
            "  libopenblas        conda-forge/linux-64::libopenblas-0.3.7-h5ec1e0e_6\n",
            "  libpng             conda-forge/linux-64::libpng-1.6.37-hed695b0_1\n",
            "  libtiff            conda-forge/linux-64::libtiff-4.1.0-hc7e4089_6\n",
            "  libuuid            conda-forge/linux-64::libuuid-2.32.1-h14c3975_1000\n",
            "  libwebp-base       conda-forge/linux-64::libwebp-base-1.1.0-h516909a_3\n",
            "  libxcb             conda-forge/linux-64::libxcb-1.13-h14c3975_1002\n",
            "  libxml2            conda-forge/linux-64::libxml2-2.9.10-hee79883_0\n",
            "  lz4-c              conda-forge/linux-64::lz4-c-1.8.3-he1b5a44_1001\n",
            "  numpy              conda-forge/linux-64::numpy-1.18.4-py37h8960a57_0\n",
            "  olefile            conda-forge/noarch::olefile-0.46-py_0\n",
            "  pandas             conda-forge/linux-64::pandas-1.0.3-py37h0da4684_1\n",
            "  pcre               conda-forge/linux-64::pcre-8.44-he1b5a44_0\n",
            "  pillow             pkgs/main/linux-64::pillow-7.1.2-py37hb39fc2d_0\n",
            "  pixman             conda-forge/linux-64::pixman-0.38.0-h516909a_1003\n",
            "  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-h14c3975_1001\n",
            "  pycairo            conda-forge/linux-64::pycairo-1.19.1-py37h01af8b0_3\n",
            "  python-dateutil    conda-forge/noarch::python-dateutil-2.8.1-py_0\n",
            "  python_abi         conda-forge/linux-64::python_abi-3.7-1_cp37m\n",
            "  pytz               conda-forge/noarch::pytz-2020.1-pyh9f0ad1d_0\n",
            "  rdkit              conda-forge/linux-64::rdkit-2020.03.1-py37hdd87690_3\n",
            "  xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-h14c3975_1002\n",
            "  xorg-libice        conda-forge/linux-64::xorg-libice-1.0.10-h516909a_0\n",
            "  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.3-h84519dc_1000\n",
            "  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.6.9-h516909a_0\n",
            "  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.9-h14c3975_0\n",
            "  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.3-h516909a_0\n",
            "  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.4-h516909a_0\n",
            "  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.10-h516909a_1002\n",
            "  xorg-renderproto   conda-forge/linux-64::xorg-renderproto-0.11.1-h14c3975_1002\n",
            "  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h14c3975_1002\n",
            "  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h14c3975_1007\n",
            "  zstd               conda-forge/linux-64::zstd-1.4.4-h3b9ef0a_2\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates     pkgs/main::ca-certificates-2020.1.1-0 --> conda-forge::ca-certificates-2020.4.5.1-hecc5488_0\n",
            "  certifi              pkgs/main::certifi-2019.11.28-py37_0 --> conda-forge::certifi-2020.4.5.1-py37hc8dfbb8_0\n",
            "  conda                       pkgs/main::conda-4.8.2-py37_0 --> conda-forge::conda-4.8.3-py37hc8dfbb8_1\n",
            "  openssl              pkgs/main::openssl-1.1.1d-h7b6447c_4 --> conda-forge::openssl-1.1.1g-h516909a_0\n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxPjaPjsGNGb",
        "colab_type": "code",
        "outputId": "71d26596-12d7-4f82-a3e7-77fecc38e187",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        }
      },
      "source": [
        "!conda install -c dglteam dgl-cuda10.1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - dgl-cuda10.1\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    blas-1.0                   |         openblas          46 KB\n",
            "    certifi-2020.4.5.1         |           py37_0         155 KB\n",
            "    decorator-4.4.2            |             py_0          14 KB\n",
            "    dgl-cuda10.1-0.4.3post2    |           py37_0        11.2 MB  dglteam\n",
            "    networkx-2.4               |             py_0         1.2 MB\n",
            "    scipy-1.4.1                |   py37habc2bb6_0        14.6 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        27.1 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  blas               pkgs/main/linux-64::blas-1.0-openblas\n",
            "  decorator          pkgs/main/noarch::decorator-4.4.2-py_0\n",
            "  dgl-cuda10.1       dglteam/linux-64::dgl-cuda10.1-0.4.3post2-py37_0\n",
            "  networkx           pkgs/main/noarch::networkx-2.4-py_0\n",
            "  scipy              pkgs/main/linux-64::scipy-1.4.1-py37habc2bb6_0\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  certifi            conda-forge::certifi-2020.4.5.1-py37h~ --> pkgs/main::certifi-2020.4.5.1-py37_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "decorator-4.4.2      | 14 KB     | : 100% 1.0/1 [00:00<00:00, 10.27it/s]\n",
            "scipy-1.4.1          | 14.6 MB   | : 100% 1.0/1 [00:00<00:00,  2.28it/s]                \n",
            "blas-1.0             | 46 KB     | : 100% 1.0/1 [00:00<00:00, 17.74it/s]\n",
            "networkx-2.4         | 1.2 MB    | : 100% 1.0/1 [00:00<00:00,  3.46it/s]\n",
            "dgl-cuda10.1-0.4.3po | 11.2 MB   | : 100% 1.0/1 [00:07<00:00,  7.18s/it]                \n",
            "certifi-2020.4.5.1   | 155 KB    | : 100% 1.0/1 [00:00<00:00, 13.91it/s]\n",
            "Preparing transaction: \\ \b\b| \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ba7Nw5eGUTr",
        "colab_type": "code",
        "outputId": "e7fc1dc0-4293-42f9-c61b-9b7a7ef69555",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        }
      },
      "source": [
        "!conda install -c dglteam dgllife\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - dgllife\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    dgllife-0.2.1              |           py37_0         132 KB  dglteam\n",
            "    joblib-0.14.1              |             py_0         201 KB\n",
            "    scikit-learn-0.22.1        |   py37h22eb022_0         5.3 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         5.6 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  dgllife            dglteam/linux-64::dgllife-0.2.1-py37_0\n",
            "  joblib             pkgs/main/noarch::joblib-0.14.1-py_0\n",
            "  scikit-learn       pkgs/main/linux-64::scikit-learn-0.22.1-py37h22eb022_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "scikit-learn-0.22.1  | 5.3 MB    | : 100% 1.0/1 [00:00<00:00,  2.88it/s]               \n",
            "joblib-0.14.1        | 201 KB    | : 100% 1.0/1 [00:00<00:00, 18.50it/s]\n",
            "dgllife-0.2.1        | 132 KB    | : 100% 1.0/1 [00:00<00:00,  2.95s/it]                \n",
            "Preparing transaction: \\ \b\bdone\n",
            "Verifying transaction: / \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqRn8KexGdiL",
        "colab_type": "code",
        "outputId": "54e03d0d-22d3-4fbb-e433-f9021dbf9550",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        }
      },
      "source": [
        "!conda install pandas"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - pandas\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    conda-4.8.3                |           py37_0         2.8 MB\n",
            "    openssl-1.1.1g             |       h7b6447c_0         2.5 MB\n",
            "    pandas-1.0.3               |   py37h0573a6f_0         8.6 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        13.9 MB\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  ca-certificates    conda-forge::ca-certificates-2020.4.5~ --> pkgs/main::ca-certificates-2020.1.1-0\n",
            "  conda              conda-forge::conda-4.8.3-py37hc8dfbb8~ --> pkgs/main::conda-4.8.3-py37_0\n",
            "  openssl            conda-forge::openssl-1.1.1g-h516909a_0 --> pkgs/main::openssl-1.1.1g-h7b6447c_0\n",
            "  pandas             conda-forge::pandas-1.0.3-py37h0da468~ --> pkgs/main::pandas-1.0.3-py37h0573a6f_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "pandas-1.0.3         | 8.6 MB    | : 100% 1.0/1 [00:00<00:00,  1.24it/s]                \n",
            "conda-4.8.3          | 2.8 MB    | : 100% 1.0/1 [00:00<00:00,  6.40it/s]\n",
            "openssl-1.1.1g       | 2.5 MB    | : 100% 1.0/1 [00:00<00:00,  5.36it/s]\n",
            "Preparing transaction: \\ \b\b| \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\bdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVKD7kwtHDUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "import sys \n",
        "import pandas as pd\n",
        "\n",
        "# train --> balanced dataset\n",
        "dataset_train_file = \"/content/drive/My Drive/Project De Novo/AID1706_binarized_sars_full_eval_actives_12k_samples.csv\"\n",
        "dataset_eval_file = \"/content/drive/My Drive/Project De Novo/mpro_xchem.csv\"\n",
        "dataset_train = pd.read_csv(dataset_train_file)\n",
        "dataset_eval = pd.read_csv(dataset_eval_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy7_zDz9LCq2",
        "colab_type": "code",
        "outputId": "65d31abe-469c-4796-faf5-654e9e73eb43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "dataset_train.head"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                                   smiles  activity\n",
              "0      C1CC(C1)C(=O)NC2=CC=C(C=C2)N(C(C3=CC(=CC=C3)F)...         1\n",
              "1      CC(C)(C)C1=CC=C(C=C1)N(C(C2=CN=NC=C2)C(=O)NC(C...         1\n",
              "2      CC(C)(C)NC(=O)C(C1=CSC=C1)N(C2=CC=C(C=C2)N)C(=...         1\n",
              "3      CC(C)C(=O)NC1=CC=C(C=C1)N(C(C2=CSC=C2)C(=O)NC(...         1\n",
              "4      CC(C)C(=O)NC1=CC=C(C=C1)N(CC2=CSC=C2)C(=O)CN3C...         1\n",
              "...                                                  ...       ...\n",
              "11994                               C1=CC2=C(C=C1N)NN=C2         0\n",
              "11995  CC(=O)[C@H]1CC[C@@H]2[C@@]1(CC(=O)[C@H]3[C@H]2...         0\n",
              "11996                       C1CN(CCN1CC(CO)O)C2=CC=CC=C2         0\n",
              "11997  CCOC(=O)N1CCC(=C2C3=C(CCC4=C2N=CC=C4)C=C(C=C3)...         0\n",
              "11998                    C1=CC2=C(C=C1OC(F)(F)F)SC(=N2)N         0\n",
              "\n",
              "[11999 rows x 2 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBnPl4isM1FQ",
        "colab_type": "code",
        "outputId": "cb4e5148-483e-41cd-b68b-524f0931ac56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "dataset_eval.head"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                 smiles  activity\n",
              "0      OC=1C=CC=CC1CNC2=NC=3C=CC=CC3N2         1\n",
              "1        CC(=O)NCCC1=CNC=2C=CC(F)=CC12         1\n",
              "2    O=C([C@@H]1[C@H](C2=CSC=C2)CCC1)N         1\n",
              "3       CN1CCCC=2C=CC(=CC12)S(=O)(=O)N         1\n",
              "4     CC(=O)NC=1C=CC(OC=2N=CC=CN2)=CC1         1\n",
              "..                                 ...       ...\n",
              "875   CC(C)C=1C=CC(NC(=O)N2CCOCC2)=CC1         0\n",
              "876        CN(CC(=O)O)C(=O)C=1C=CC=CN1         0\n",
              "877  CN1CCN(CC1)C(=O)C=2C=CC(F)=C(F)C2         0\n",
              "878      FC=1C=CC=C(F)C1C(=O)N2CCCCCC2         0\n",
              "879             FC=1C=CC=NC1NCC2CCOCC2         0\n",
              "\n",
              "[880 rows x 2 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVrUoqtTHGy8",
        "colab_type": "code",
        "outputId": "e29f8ba9-6855-41d8-89a2-b86e21f6ad60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "from dgllife.data import MoleculeCSVDataset\n",
        "from dgllife.data.csv_dataset import *\n",
        "from dgllife.utils.featurizers import *\n",
        "from dgllife.utils.mol_to_graph import *\n",
        "\n",
        "# featurize bigraph/molecular graph set for train (SARS-COV-1) set\n",
        "train_set = MoleculeCSVDataset(dataset_train, smiles_to_graph=smiles_to_bigraph, node_featurizer=CanonicalAtomFeaturizer(),\n",
        "                               edge_featurizer=CanonicalBondFeaturizer(), smiles_column='smiles', cache_file_path='/content/drive/My Drive/Project De Novo/graph_featurizer/train.bin', task_names=['activity'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Using backend: pytorch\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "Loading previously saved dgl graphs...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofwpNYUXJASi",
        "colab_type": "code",
        "outputId": "b0e9dc0f-517f-4530-e640-fb9c46934a91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# featurize bigraph/molecular graph set for test (SARS-COV-2) set\n",
        "test_set = MoleculeCSVDataset(dataset_eval, smiles_to_graph=smiles_to_bigraph, node_featurizer=CanonicalAtomFeaturizer(),\n",
        "                               edge_featurizer=CanonicalBondFeaturizer(), smiles_column='smiles', cache_file_path='/content/drive/My Drive/Project De Novo/graph_featurizer/test.bin', task_names=['activity'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading previously saved dgl graphs...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1TtgCu8U26K",
        "colab_type": "code",
        "outputId": "f516feb6-2171-489c-fdf6-21d04dc1d69b",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5c0de9ef-b50e-45ef-acae-644387e184ca\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-5c0de9ef-b50e-45ef-acae-644387e184ca\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving utils.py to utils.py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'utils.py': b'import dgl\\nimport numpy as np\\nimport random\\nimport torch\\n\\nfrom dgllife.utils.featurizers import one_hot_encoding\\nfrom dgllife.utils.splitters import RandomSplitter\\n\\ndef set_random_seed(seed=0):\\n    \"\"\"Set random seed.\\n    Parameters\\n    ----------\\n    seed : int\\n        Random seed to use\\n    \"\"\"\\n    random.seed(seed)\\n    np.random.seed(seed)\\n    torch.manual_seed(seed)\\n    if torch.cuda.is_available():\\n        torch.cuda.manual_seed(seed)\\n\\n\\ndef load_dataset_for_classification(args):\\n    \"\"\"Load dataset for classification tasks.\\n    Parameters\\n    ----------\\n    args : dict\\n        Configurations.\\n    Returns\\n    -------\\n    dataset\\n        The whole dataset.\\n    train_set\\n        Subset for training.\\n    val_set\\n        Subset for validation.\\n    test_set\\n        Subset for test.\\n    \"\"\"\\n    assert args[\\'dataset\\'] in [\\'Tox21\\']\\n    if args[\\'dataset\\'] == \\'Tox21\\':\\n        from dgllife.data import Tox21\\n        dataset = Tox21(smiles_to_graph=args[\\'smiles_to_graph\\'],\\n                        node_featurizer=args.get(\\'node_featurizer\\', None),\\n                        edge_featurizer=args.get(\\'edge_featurizer\\', None))\\n        train_set, val_set, test_set = RandomSplitter.train_val_test_split(\\n            dataset, frac_train=args[\\'frac_train\\'], frac_val=args[\\'frac_val\\'],\\n            frac_test=args[\\'frac_test\\'], random_state=args[\\'random_seed\\'])\\n\\n    return dataset, train_set, val_set, test_set\\n\\n\\ndef load_dataset_for_regression(args):\\n    \"\"\"Load dataset for regression tasks.\\n    Parameters\\n    ----------\\n    args : dict\\n        Configurations.\\n    Returns\\n    -------\\n    train_set\\n        Subset for training.\\n    val_set\\n        Subset for validation.\\n    test_set\\n        Subset for test.\\n    \"\"\"\\n    assert args[\\'dataset\\'] in [\\'Alchemy\\', \\'Aromaticity\\']\\n\\n    if args[\\'dataset\\'] == \\'Alchemy\\':\\n        from dgllife.data import TencentAlchemyDataset\\n        train_set = TencentAlchemyDataset(mode=\\'dev\\')\\n        val_set = TencentAlchemyDataset(mode=\\'valid\\')\\n        test_set = None\\n\\n    if args[\\'dataset\\'] == \\'Aromaticity\\':\\n        from dgllife.data import PubChemBioAssayAromaticity\\n        dataset = PubChemBioAssayAromaticity(smiles_to_graph=args[\\'smiles_to_graph\\'],\\n                                             node_featurizer=args.get(\\'node_featurizer\\', None),\\n                                             edge_featurizer=args.get(\\'edge_featurizer\\', None))\\n        train_set, val_set, test_set = RandomSplitter.train_val_test_split(\\n            dataset, frac_train=args[\\'frac_train\\'], frac_val=args[\\'frac_val\\'],\\n            frac_test=args[\\'frac_test\\'], random_state=args[\\'random_seed\\'])\\n\\n    return train_set, val_set, test_set\\n\\n\\ndef collate_molgraphs(data):\\n    \"\"\"Batching a list of datapoints for dataloader.\\n    Parameters\\n    ----------\\n    data : list of 3-tuples or 4-tuples.\\n        Each tuple is for a single datapoint, consisting of\\n        a SMILES, a DGLGraph, all-task labels and optionally\\n        a binary mask indicating the existence of labels.\\n    Returns\\n    -------\\n    smiles : list\\n        List of smiles\\n    bg : DGLGraph\\n        The batched DGLGraph.\\n    labels : Tensor of dtype float32 and shape (B, T)\\n        Batched datapoint labels. B is len(data) and\\n        T is the number of total tasks.\\n    masks : Tensor of dtype float32 and shape (B, T)\\n        Batched datapoint binary mask, indicating the\\n        existence of labels. If binary masks are not\\n        provided, return a tensor with ones.\\n    \"\"\"\\n    assert len(data[0]) in [3, 4], \\\\\\n        \\'Expect the tuple to be of length 3 or 4, got {:d}\\'.format(len(data[0]))\\n    if len(data[0]) == 3:\\n        smiles, graphs, labels = map(list, zip(*data))\\n        masks = None\\n    else:\\n        smiles, graphs, labels, masks = map(list, zip(*data))\\n\\n    bg = dgl.batch(graphs)\\n    bg.set_n_initializer(dgl.init.zero_initializer)\\n    bg.set_e_initializer(dgl.init.zero_initializer)\\n    labels = torch.stack(labels, dim=0)\\n\\n    if masks is None:\\n        masks = torch.ones(labels.shape)\\n    else:\\n        masks = torch.stack(masks, dim=0)\\n    return smiles, bg, labels, masks\\n\\n\\ndef load_model(args):\\n    if args[\\'model\\'] == \\'GCN\\':\\n        from dgllife.model import GCNPredictor\\n        model = GCNPredictor(in_feats=args[\\'node_featurizer\\'].feat_size(),\\n                             hidden_feats=args[\\'gcn_hidden_feats\\'],\\n                             classifier_hidden_feats=args[\\'classifier_hidden_feats\\'],\\n                             n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'GAT\\':\\n        from dgllife.model import GATPredictor\\n        model = GATPredictor(in_feats=args[\\'node_featurizer\\'].feat_size(),\\n                             hidden_feats=args[\\'gat_hidden_feats\\'],\\n                             num_heads=args[\\'num_heads\\'],\\n                             classifier_hidden_feats=args[\\'classifier_hidden_feats\\'],\\n                             n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'Weave\\':\\n        from dgllife.model import WeavePredictor\\n        model = WeavePredictor(node_in_feats=args[\\'node_featurizer\\'].feat_size(),\\n                               edge_in_feats=args[\\'edge_featurizer\\'].feat_size(),\\n                               num_gnn_layers=args[\\'num_gnn_layers\\'],\\n                               gnn_hidden_feats=args[\\'gnn_hidden_feats\\'],\\n                               graph_feats=args[\\'graph_feats\\'],\\n                               n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'AttentiveFP\\':\\n        from dgllife.model import AttentiveFPPredictor\\n        model = AttentiveFPPredictor(node_feat_size=args[\\'node_featurizer\\'].feat_size(),\\n                                     edge_feat_size=args[\\'edge_featurizer\\'].feat_size(),\\n                                     num_layers=args[\\'num_layers\\'],\\n                                     num_timesteps=args[\\'num_timesteps\\'],\\n                                     graph_feat_size=args[\\'graph_feat_size\\'],\\n                                     n_tasks=args[\\'n_tasks\\'],\\n                                     dropout=args[\\'dropout\\'])\\n\\n    if args[\\'model\\'] == \\'SchNet\\':\\n        from dgllife.model import SchNetPredictor\\n        model = SchNetPredictor(node_feats=args[\\'node_feats\\'],\\n                                hidden_feats=args[\\'hidden_feats\\'],\\n                                classifier_hidden_feats=args[\\'classifier_hidden_feats\\'],\\n                                n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'MGCN\\':\\n        from dgllife.model import MGCNPredictor\\n        model = MGCNPredictor(feats=args[\\'feats\\'],\\n                              n_layers=args[\\'n_layers\\'],\\n                              classifier_hidden_feats=args[\\'classifier_hidden_feats\\'],\\n                              n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'MPNN\\':\\n        from dgllife.model import MPNNPredictor\\n        model = MPNNPredictor(node_in_feats=args[\\'node_in_feats\\'],\\n                              edge_in_feats=args[\\'edge_in_feats\\'],\\n                              node_out_feats=args[\\'node_out_feats\\'],\\n                              edge_hidden_feats=args[\\'edge_hidden_feats\\'],\\n                              n_tasks=args[\\'n_tasks\\'])\\n\\n    return model\\n\\n\\ndef chirality(atom):\\n    try:\\n        return one_hot_encoding(atom.GetProp(\\'_CIPCode\\'), [\\'R\\', \\'S\\']) + \\\\\\n               [atom.HasProp(\\'_ChiralityPossible\\')]\\n    except:\\n        return [False, False] + [atom.HasProp(\\'_ChiralityPossible\\')]\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKMgchzflhiW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = {\n",
        "    'random_seed': 2,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-3,\n",
        "    'num_epochs': 100,\n",
        "    'node_data_field': 'h',\n",
        "    'edge_data_field': 'e',\n",
        "    'frac_train': 0.8,\n",
        "    'frac_val': 0.1,\n",
        "    'frac_test': 0.1,\n",
        "    'in_feats': 74,\n",
        "    'gat_hidden_feats': [32, 32],\n",
        "    'classifier_hidden_feats': 64,\n",
        "    'num_heads': [4, 4],\n",
        "    'patience': 10,\n",
        "    'smiles_to_graph': smiles_to_bigraph,\n",
        "    'node_featurizer': CanonicalAtomFeaturizer(),\n",
        "    'edge_featurizer': CanonicalBondFeaturizer(),\n",
        "    'metric_name': 'roc_auc_score',\n",
        "    'model': 'MPNNPredictor'\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuZkFAz-PDvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import utils\n",
        "\n",
        "from dgllife.model import load_pretrained\n",
        "from dgllife.utils import EarlyStopping, Meter\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "from utils import set_random_seed, load_dataset_for_classification, collate_molgraphs, load_model\n",
        "\n",
        "from dgllife.model import MPNNPredictor\n",
        "\n",
        "args['device'] = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "set_random_seed(args['random_seed'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJf2RePJHUcx",
        "colab_type": "code",
        "outputId": "6f4e102f-12a2-4eb1-e1b8-a7a30e8b4513",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        }
      },
      "source": [
        "from dgllife.utils.splitters import ScaffoldSplitter\n",
        "\n",
        "train_scaffold_set, val_set, test_scaffold_set = ScaffoldSplitter.train_val_test_split(train_set, frac_train=0.8, frac_val=0.2,frac_test=0.0)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start initializing RDKit molecule instances...\n",
            "Creating RDKit molecule instance 1000/11999\n",
            "Creating RDKit molecule instance 2000/11999\n",
            "Creating RDKit molecule instance 3000/11999\n",
            "Creating RDKit molecule instance 4000/11999\n",
            "Creating RDKit molecule instance 5000/11999\n",
            "Creating RDKit molecule instance 6000/11999\n",
            "Creating RDKit molecule instance 7000/11999\n",
            "Creating RDKit molecule instance 8000/11999\n",
            "Creating RDKit molecule instance 9000/11999\n",
            "Creating RDKit molecule instance 10000/11999\n",
            "Creating RDKit molecule instance 11000/11999\n",
            "Start computing Bemis-Murcko scaffolds.\n",
            "Computing Bemis-Murcko for compound 1000/11999\n",
            "Computing Bemis-Murcko for compound 2000/11999\n",
            "Computing Bemis-Murcko for compound 3000/11999\n",
            "Computing Bemis-Murcko for compound 4000/11999\n",
            "Computing Bemis-Murcko for compound 5000/11999\n",
            "Computing Bemis-Murcko for compound 6000/11999\n",
            "Computing Bemis-Murcko for compound 7000/11999\n",
            "Computing Bemis-Murcko for compound 8000/11999\n",
            "Computing Bemis-Murcko for compound 9000/11999\n",
            "Computing Bemis-Murcko for compound 10000/11999\n",
            "Computing Bemis-Murcko for compound 11000/11999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCmuQegvJMcq",
        "colab_type": "code",
        "outputId": "c68c58a4-e867-4410-b170-4074b145cb14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "print (len(train_set))\n",
        "print(len(train_scaffold_set))\n",
        "print (len(val_set))\n",
        "print (len(test_set))\n",
        "print(train_set[1])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11999\n",
            "9599\n",
            "2400\n",
            "880\n",
            "('CC(C)(C)C1=CC=C(C=C1)N(C(C2=CN=NC=C2)C(=O)NC(C)(C)C)C(=O)C3=CC=CO3', DGLGraph(num_nodes=32, num_edges=68,\n",
            "         ndata_schemes={'h': Scheme(shape=(74,), dtype=torch.float32)}\n",
            "         edata_schemes={'e': Scheme(shape=(12,), dtype=torch.float32)}), tensor([1.]), tensor([1.]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YjAjoUyLEjT",
        "colab_type": "code",
        "outputId": "a18bd4d6-5f46-435b-b30f-66f61fb6cfd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(type(train_set))\n",
        "print(type(train_scaffold_set))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'dgllife.data.csv_dataset.MoleculeCSVDataset'>\n",
            "<class 'dgl.data.utils.Subset'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYfHiYl0T8V4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_scaffold_set,  batch_size=args['batch_size'],\n",
        "                              collate_fn=collate_molgraphs)\n",
        "val_loader = DataLoader(val_set,  batch_size=args['batch_size'],\n",
        "                              collate_fn=collate_molgraphs)\n",
        "\n",
        "test_loader = DataLoader(test_set,  batch_size=args['batch_size'],\n",
        "                          collate_fn=collate_molgraphs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzUMvJO8Qx85",
        "colab_type": "code",
        "outputId": "f4ae39b0-2e66-473b-823e-c3102c83511f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(len(test_loader))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGAUIdpRUAUu",
        "colab_type": "code",
        "outputId": "d711d0be-7d87-4a5c-d12f-1ef400fdb132",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "args['n_tasks'] = 1\n",
        "\n",
        "model = MPNNPredictor(node_in_feats=args['node_featurizer'].feat_size('h'),\n",
        "                      edge_in_feats=args['edge_featurizer'].feat_size('e'))\n",
        "                      \n",
        "'''\n",
        "model = GATPredictor(in_feats=args['node_featurizer'].feat_size('h'),\n",
        "                             hidden_feats=args['gat_hidden_feats'],\n",
        "                             num_heads=args['num_heads'],\n",
        "                             classifier_hidden_feats=args['classifier_hidden_feats'],\n",
        "                             n_tasks=args['n_tasks'])\n",
        "'''\n",
        "\n",
        "import dgl.backend as F\n",
        "\n",
        "train_num_pos = F.sum(train_set.labels, dim=0)\n",
        "train_num_indices = F.sum(train_set.mask, dim=0)\n",
        "train_task_pos_weights = (train_num_indices - train_num_pos) / train_num_pos\n",
        "\n",
        "loss_criterion = BCEWithLogitsLoss(pos_weight=train_task_pos_weights.to(args['device']),\n",
        "                                    reduction='none')\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=args['lr'])\n",
        "stopper = EarlyStopping(patience=args['patience'], mode='higher', filename='/content/drive/My Drive/Project De Novo/MPNN/train.pth')\n",
        "model.to(args['device'])\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MPNNPredictor(\n",
              "  (gnn): MPNNGNN(\n",
              "    (project_node_feats): Sequential(\n",
              "      (0): Linear(in_features=74, out_features=64, bias=True)\n",
              "      (1): ReLU()\n",
              "    )\n",
              "    (gnn_layer): NNConv(\n",
              "      (edge_nn): Sequential(\n",
              "        (0): Linear(in_features=12, out_features=128, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=128, out_features=4096, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (gru): GRU(64, 64)\n",
              "  )\n",
              "  (readout): Set2Set(\n",
              "    n_iters=6\n",
              "    (lstm): LSTM(128, 64, num_layers=3)\n",
              "  )\n",
              "  (predict): Sequential(\n",
              "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7Fdo_NxLPWz",
        "colab_type": "code",
        "outputId": "c11b0150-2cfc-4755-abed-583912ee8650",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(train_task_pos_weights)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([25.9036])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C941OCzRuXH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def predict(args, model, bg):\n",
        "    node_feats = bg.ndata.pop(args['node_data_field']).to(args['device'])\n",
        "    if args.get('edge_featurizer', None) is not None:\n",
        "        edge_feats = bg.edata.pop(args['edge_data_field']).to(args['device'])\n",
        "        return model(bg, node_feats, edge_feats)\n",
        "    else:\n",
        "        return model(bg, node_feats)\n",
        "\n",
        "def run_a_train_epoch(args, epoch, model, data_loader, loss_criterion, optimizer):\n",
        "    model.train()\n",
        "    train_meter = Meter()\n",
        "    for batch_id, batch_data in enumerate(data_loader):\n",
        "        smiles, bg, labels, masks = batch_data\n",
        "        labels, masks = labels.to(args['device']), masks.to(args['device'])\n",
        "        logits = predict(args, model, bg)\n",
        "        # Mask non-existing labels\n",
        "        loss = (loss_criterion(logits, labels) * (masks != 0).float()).mean()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print('epoch {:d}/{:d}, batch {:d}/{:d}, loss {:.4f}'.format(\n",
        "            epoch + 1, args['num_epochs'], batch_id + 1, len(data_loader), loss.item()))\n",
        "        train_meter.update(logits, labels, masks)\n",
        "    train_score = np.mean(train_meter.compute_metric(args['metric_name']))\n",
        "    print('epoch {:d}/{:d}, training {} {:.4f}'.format(\n",
        "        epoch + 1, args['num_epochs'], args['metric_name'], train_score))\n",
        "\n",
        "def run_an_eval_epoch(args, model, data_loader):\n",
        "    model.eval()\n",
        "    eval_meter = Meter()\n",
        "    with torch.no_grad():\n",
        "        for batch_id, batch_data in enumerate(data_loader):\n",
        "            smiles, bg, labels, masks = batch_data\n",
        "            labels = labels.to(args['device'])\n",
        "            logits = predict(args, model, bg)\n",
        "            eval_meter.update(logits, labels, masks)\n",
        "    return np.mean(eval_meter.compute_metric(args['metric_name']))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73ZBLfQxuHQA",
        "colab_type": "code",
        "outputId": "d8b693df-e2b5-430c-a961-d1558f13fd4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(args['num_epochs']):\n",
        "        # Train\n",
        "        run_a_train_epoch(args, epoch, model, train_loader, loss_criterion, optimizer)\n",
        "\n",
        "        # Validation and early stop\n",
        "        val_score = run_an_eval_epoch(args, model, val_loader)\n",
        "        early_stop = stopper.step(val_score, model)\n",
        "        print('epoch {:d}/{:d}, validation {} {:.4f}, best validation {} {:.4f}'.format(\n",
        "            epoch + 1, args['num_epochs'], args['metric_name'],\n",
        "            val_score, args['metric_name'], stopper.best_score))\n",
        "        if early_stop:\n",
        "            break"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1/100, batch 1/75, loss 1.2289\n",
            "epoch 1/100, batch 2/75, loss 0.9389\n",
            "epoch 1/100, batch 3/75, loss 0.7569\n",
            "epoch 1/100, batch 4/75, loss 0.7035\n",
            "epoch 1/100, batch 5/75, loss 0.4630\n",
            "epoch 1/100, batch 6/75, loss 0.4110\n",
            "epoch 1/100, batch 7/75, loss 0.3663\n",
            "epoch 1/100, batch 8/75, loss 1.6085\n",
            "epoch 1/100, batch 9/75, loss 0.3249\n",
            "epoch 1/100, batch 10/75, loss 2.9578\n",
            "epoch 1/100, batch 11/75, loss 0.8396\n",
            "epoch 1/100, batch 12/75, loss 0.3444\n",
            "epoch 1/100, batch 13/75, loss 0.5966\n",
            "epoch 1/100, batch 14/75, loss 0.3562\n",
            "epoch 1/100, batch 15/75, loss 4.9490\n",
            "epoch 1/100, batch 16/75, loss 0.3751\n",
            "epoch 1/100, batch 17/75, loss 0.3892\n",
            "epoch 1/100, batch 18/75, loss 0.3990\n",
            "epoch 1/100, batch 19/75, loss 0.4045\n",
            "epoch 1/100, batch 20/75, loss 3.2532\n",
            "epoch 1/100, batch 21/75, loss 0.4184\n",
            "epoch 1/100, batch 22/75, loss 0.4271\n",
            "epoch 1/100, batch 23/75, loss 0.4326\n",
            "epoch 1/100, batch 24/75, loss 0.4338\n",
            "epoch 1/100, batch 25/75, loss 0.4330\n",
            "epoch 1/100, batch 26/75, loss 5.4526\n",
            "epoch 1/100, batch 27/75, loss 0.4429\n",
            "epoch 1/100, batch 28/75, loss 0.4523\n",
            "epoch 1/100, batch 29/75, loss 0.4593\n",
            "epoch 1/100, batch 30/75, loss 0.4644\n",
            "epoch 1/100, batch 31/75, loss 0.4659\n",
            "epoch 1/100, batch 32/75, loss 0.4661\n",
            "epoch 1/100, batch 33/75, loss 0.4636\n",
            "epoch 1/100, batch 34/75, loss 0.4603\n",
            "epoch 1/100, batch 35/75, loss 0.4551\n",
            "epoch 1/100, batch 36/75, loss 5.2980\n",
            "epoch 1/100, batch 37/75, loss 0.6561\n",
            "epoch 1/100, batch 38/75, loss 0.4612\n",
            "epoch 1/100, batch 39/75, loss 0.4646\n",
            "epoch 1/100, batch 40/75, loss 0.4661\n",
            "epoch 1/100, batch 41/75, loss 0.4663\n",
            "epoch 1/100, batch 42/75, loss 0.4640\n",
            "epoch 1/100, batch 43/75, loss 0.4603\n",
            "epoch 1/100, batch 44/75, loss 0.4553\n",
            "epoch 1/100, batch 45/75, loss 0.4490\n",
            "epoch 1/100, batch 46/75, loss 0.4415\n",
            "epoch 1/100, batch 47/75, loss 0.4335\n",
            "epoch 1/100, batch 48/75, loss 0.4241\n",
            "epoch 1/100, batch 49/75, loss 0.4140\n",
            "epoch 1/100, batch 50/75, loss 0.4035\n",
            "epoch 1/100, batch 51/75, loss 0.3928\n",
            "epoch 1/100, batch 52/75, loss 0.3806\n",
            "epoch 1/100, batch 53/75, loss 17.2407\n",
            "epoch 1/100, batch 54/75, loss 1.5206\n",
            "epoch 1/100, batch 55/75, loss 0.4032\n",
            "epoch 1/100, batch 56/75, loss 0.4180\n",
            "epoch 1/100, batch 57/75, loss 0.4286\n",
            "epoch 1/100, batch 58/75, loss 0.4379\n",
            "epoch 1/100, batch 59/75, loss 0.4449\n",
            "epoch 1/100, batch 60/75, loss 0.4491\n",
            "epoch 1/100, batch 61/75, loss 0.4523\n",
            "epoch 1/100, batch 62/75, loss 0.4537\n",
            "epoch 1/100, batch 63/75, loss 0.4540\n",
            "epoch 1/100, batch 64/75, loss 0.4533\n",
            "epoch 1/100, batch 65/75, loss 0.4517\n",
            "epoch 1/100, batch 66/75, loss 0.4492\n",
            "epoch 1/100, batch 67/75, loss 0.4468\n",
            "epoch 1/100, batch 68/75, loss 0.4425\n",
            "epoch 1/100, batch 69/75, loss 0.4384\n",
            "epoch 1/100, batch 70/75, loss 0.4325\n",
            "epoch 1/100, batch 71/75, loss 0.4276\n",
            "epoch 1/100, batch 72/75, loss 0.4213\n",
            "epoch 1/100, batch 73/75, loss 0.4138\n",
            "epoch 1/100, batch 74/75, loss 0.4055\n",
            "epoch 1/100, batch 75/75, loss 0.3984\n",
            "epoch 1/100, training roc_auc_score 0.2665\n",
            "epoch 1/100, validation roc_auc_score 0.6550, best validation roc_auc_score 0.6550\n",
            "epoch 2/100, batch 1/75, loss 1.2910\n",
            "epoch 2/100, batch 2/75, loss 0.8402\n",
            "epoch 2/100, batch 3/75, loss 0.6096\n",
            "epoch 2/100, batch 4/75, loss 0.6053\n",
            "epoch 2/100, batch 5/75, loss 0.3679\n",
            "epoch 2/100, batch 6/75, loss 0.3598\n",
            "epoch 2/100, batch 7/75, loss 0.3529\n",
            "epoch 2/100, batch 8/75, loss 1.5751\n",
            "epoch 2/100, batch 9/75, loss 0.3408\n",
            "epoch 2/100, batch 10/75, loss 2.8439\n",
            "epoch 2/100, batch 11/75, loss 0.8398\n",
            "epoch 2/100, batch 12/75, loss 0.3354\n",
            "epoch 2/100, batch 13/75, loss 0.5869\n",
            "epoch 2/100, batch 14/75, loss 0.3335\n",
            "epoch 2/100, batch 15/75, loss 5.1302\n",
            "epoch 2/100, batch 16/75, loss 0.3373\n",
            "epoch 2/100, batch 17/75, loss 0.3416\n",
            "epoch 2/100, batch 18/75, loss 0.3448\n",
            "epoch 2/100, batch 19/75, loss 0.3467\n",
            "epoch 2/100, batch 20/75, loss 3.5302\n",
            "epoch 2/100, batch 21/75, loss 0.3520\n",
            "epoch 2/100, batch 22/75, loss 0.3561\n",
            "epoch 2/100, batch 23/75, loss 0.3591\n",
            "epoch 2/100, batch 24/75, loss 0.3601\n",
            "epoch 2/100, batch 25/75, loss 0.3612\n",
            "epoch 2/100, batch 26/75, loss 6.0767\n",
            "epoch 2/100, batch 27/75, loss 0.3681\n",
            "epoch 2/100, batch 28/75, loss 0.3741\n",
            "epoch 2/100, batch 29/75, loss 0.3782\n",
            "epoch 2/100, batch 30/75, loss 0.3827\n",
            "epoch 2/100, batch 31/75, loss 0.3845\n",
            "epoch 2/100, batch 32/75, loss 0.3864\n",
            "epoch 2/100, batch 33/75, loss 0.3858\n",
            "epoch 2/100, batch 34/75, loss 0.3860\n",
            "epoch 2/100, batch 35/75, loss 0.3846\n",
            "epoch 2/100, batch 36/75, loss 5.8556\n",
            "epoch 2/100, batch 37/75, loss 0.6161\n",
            "epoch 2/100, batch 38/75, loss 0.3928\n",
            "epoch 2/100, batch 39/75, loss 0.3962\n",
            "epoch 2/100, batch 40/75, loss 0.3985\n",
            "epoch 2/100, batch 41/75, loss 0.4005\n",
            "epoch 2/100, batch 42/75, loss 0.4004\n",
            "epoch 2/100, batch 43/75, loss 0.3999\n",
            "epoch 2/100, batch 44/75, loss 0.3987\n",
            "epoch 2/100, batch 45/75, loss 0.3967\n",
            "epoch 2/100, batch 46/75, loss 0.3940\n",
            "epoch 2/100, batch 47/75, loss 0.3915\n",
            "epoch 2/100, batch 48/75, loss 0.3870\n",
            "epoch 2/100, batch 49/75, loss 0.3830\n",
            "epoch 2/100, batch 50/75, loss 0.3784\n",
            "epoch 2/100, batch 51/75, loss 0.3739\n",
            "epoch 2/100, batch 52/75, loss 0.3674\n",
            "epoch 2/100, batch 53/75, loss 17.3456\n",
            "epoch 2/100, batch 54/75, loss 1.5242\n",
            "epoch 2/100, batch 55/75, loss 0.3924\n",
            "epoch 2/100, batch 56/75, loss 0.4066\n",
            "epoch 2/100, batch 57/75, loss 0.4162\n",
            "epoch 2/100, batch 58/75, loss 0.4262\n",
            "epoch 2/100, batch 59/75, loss 0.4346\n",
            "epoch 2/100, batch 60/75, loss 0.4402\n",
            "epoch 2/100, batch 61/75, loss 0.4450\n",
            "epoch 2/100, batch 62/75, loss 0.4488\n",
            "epoch 2/100, batch 63/75, loss 0.4496\n",
            "epoch 2/100, batch 64/75, loss 0.4488\n",
            "epoch 2/100, batch 65/75, loss 0.4473\n",
            "epoch 2/100, batch 66/75, loss 0.4443\n",
            "epoch 2/100, batch 67/75, loss 0.4436\n",
            "epoch 2/100, batch 68/75, loss 0.4398\n",
            "epoch 2/100, batch 69/75, loss 0.4369\n",
            "epoch 2/100, batch 70/75, loss 0.4313\n",
            "epoch 2/100, batch 71/75, loss 0.4291\n",
            "epoch 2/100, batch 72/75, loss 0.4249\n",
            "epoch 2/100, batch 73/75, loss 0.4179\n",
            "epoch 2/100, batch 74/75, loss 0.4097\n",
            "epoch 2/100, batch 75/75, loss 0.4033\n",
            "epoch 2/100, training roc_auc_score 0.2755\n",
            "epoch 2/100, validation roc_auc_score 0.7181, best validation roc_auc_score 0.7181\n",
            "epoch 3/100, batch 1/75, loss 1.2761\n",
            "epoch 3/100, batch 2/75, loss 0.8435\n",
            "epoch 3/100, batch 3/75, loss 0.6153\n",
            "epoch 3/100, batch 4/75, loss 0.6110\n",
            "epoch 3/100, batch 5/75, loss 0.3829\n",
            "epoch 3/100, batch 6/75, loss 0.3710\n",
            "epoch 3/100, batch 7/75, loss 0.3649\n",
            "epoch 3/100, batch 8/75, loss 1.5418\n",
            "epoch 3/100, batch 9/75, loss 0.3550\n",
            "epoch 3/100, batch 10/75, loss 2.7468\n",
            "epoch 3/100, batch 11/75, loss 0.8457\n",
            "epoch 3/100, batch 12/75, loss 0.3488\n",
            "epoch 3/100, batch 13/75, loss 0.5926\n",
            "epoch 3/100, batch 14/75, loss 0.3452\n",
            "epoch 3/100, batch 15/75, loss 4.9736\n",
            "epoch 3/100, batch 16/75, loss 0.3451\n",
            "epoch 3/100, batch 17/75, loss 0.3506\n",
            "epoch 3/100, batch 18/75, loss 0.3514\n",
            "epoch 3/100, batch 19/75, loss 0.3539\n",
            "epoch 3/100, batch 20/75, loss 3.4792\n",
            "epoch 3/100, batch 21/75, loss 0.3566\n",
            "epoch 3/100, batch 22/75, loss 0.3602\n",
            "epoch 3/100, batch 23/75, loss 0.3631\n",
            "epoch 3/100, batch 24/75, loss 0.3634\n",
            "epoch 3/100, batch 25/75, loss 0.3657\n",
            "epoch 3/100, batch 26/75, loss 5.9548\n",
            "epoch 3/100, batch 27/75, loss 0.3669\n",
            "epoch 3/100, batch 28/75, loss 0.3755\n",
            "epoch 3/100, batch 29/75, loss 0.3754\n",
            "epoch 3/100, batch 30/75, loss 0.3822\n",
            "epoch 3/100, batch 31/75, loss 0.3817\n",
            "epoch 3/100, batch 32/75, loss 0.3837\n",
            "epoch 3/100, batch 33/75, loss 0.3780\n",
            "epoch 3/100, batch 34/75, loss 0.3799\n",
            "epoch 3/100, batch 35/75, loss 0.3747\n",
            "epoch 3/100, batch 36/75, loss 5.8722\n",
            "epoch 3/100, batch 37/75, loss 0.6185\n",
            "epoch 3/100, batch 38/75, loss 0.3794\n",
            "epoch 3/100, batch 39/75, loss 0.3862\n",
            "epoch 3/100, batch 40/75, loss 0.3869\n",
            "epoch 3/100, batch 41/75, loss 0.3930\n",
            "epoch 3/100, batch 42/75, loss 0.3902\n",
            "epoch 3/100, batch 43/75, loss 0.3897\n",
            "epoch 3/100, batch 44/75, loss 0.3882\n",
            "epoch 3/100, batch 45/75, loss 0.3833\n",
            "epoch 3/100, batch 46/75, loss 0.3812\n",
            "epoch 3/100, batch 47/75, loss 0.3805\n",
            "epoch 3/100, batch 48/75, loss 0.3683\n",
            "epoch 3/100, batch 49/75, loss 0.3667\n",
            "epoch 3/100, batch 50/75, loss 0.3571\n",
            "epoch 3/100, batch 51/75, loss 0.3523\n",
            "epoch 3/100, batch 52/75, loss 0.3362\n",
            "epoch 3/100, batch 53/75, loss 17.3613\n",
            "epoch 3/100, batch 54/75, loss 1.4951\n",
            "epoch 3/100, batch 55/75, loss 0.3704\n",
            "epoch 3/100, batch 56/75, loss 0.3937\n",
            "epoch 3/100, batch 57/75, loss 0.4000\n",
            "epoch 3/100, batch 58/75, loss 0.4144\n",
            "epoch 3/100, batch 59/75, loss 0.4273\n",
            "epoch 3/100, batch 60/75, loss 0.4339\n",
            "epoch 3/100, batch 61/75, loss 0.4389\n",
            "epoch 3/100, batch 62/75, loss 0.4456\n",
            "epoch 3/100, batch 63/75, loss 0.4479\n",
            "epoch 3/100, batch 64/75, loss 0.4504\n",
            "epoch 3/100, batch 65/75, loss 0.4528\n",
            "epoch 3/100, batch 66/75, loss 0.4482\n",
            "epoch 3/100, batch 67/75, loss 0.4529\n",
            "epoch 3/100, batch 68/75, loss 0.4465\n",
            "epoch 3/100, batch 69/75, loss 0.4445\n",
            "epoch 3/100, batch 70/75, loss 0.4358\n",
            "epoch 3/100, batch 71/75, loss 0.4398\n",
            "epoch 3/100, batch 72/75, loss 0.4386\n",
            "epoch 3/100, batch 73/75, loss 0.4277\n",
            "epoch 3/100, batch 74/75, loss 0.4173\n",
            "epoch 3/100, batch 75/75, loss 0.4115\n",
            "epoch 3/100, training roc_auc_score 0.3852\n",
            "epoch 3/100, validation roc_auc_score 0.7379, best validation roc_auc_score 0.7379\n",
            "epoch 4/100, batch 1/75, loss 1.2359\n",
            "epoch 4/100, batch 2/75, loss 0.8536\n",
            "epoch 4/100, batch 3/75, loss 0.6220\n",
            "epoch 4/100, batch 4/75, loss 0.6192\n",
            "epoch 4/100, batch 5/75, loss 0.4092\n",
            "epoch 4/100, batch 6/75, loss 0.3827\n",
            "epoch 4/100, batch 7/75, loss 0.3727\n",
            "epoch 4/100, batch 8/75, loss 1.4884\n",
            "epoch 4/100, batch 9/75, loss 0.3620\n",
            "epoch 4/100, batch 10/75, loss 2.5760\n",
            "epoch 4/100, batch 11/75, loss 0.8642\n",
            "epoch 4/100, batch 12/75, loss 0.3571\n",
            "epoch 4/100, batch 13/75, loss 0.5942\n",
            "epoch 4/100, batch 14/75, loss 0.3435\n",
            "epoch 4/100, batch 15/75, loss 4.8938\n",
            "epoch 4/100, batch 16/75, loss 0.3417\n",
            "epoch 4/100, batch 17/75, loss 0.3549\n",
            "epoch 4/100, batch 18/75, loss 0.3551\n",
            "epoch 4/100, batch 19/75, loss 0.3637\n",
            "epoch 4/100, batch 20/75, loss 3.3476\n",
            "epoch 4/100, batch 21/75, loss 0.3645\n",
            "epoch 4/100, batch 22/75, loss 0.3711\n",
            "epoch 4/100, batch 23/75, loss 0.3712\n",
            "epoch 4/100, batch 24/75, loss 0.3710\n",
            "epoch 4/100, batch 25/75, loss 0.3749\n",
            "epoch 4/100, batch 26/75, loss 5.6718\n",
            "epoch 4/100, batch 27/75, loss 0.3680\n",
            "epoch 4/100, batch 28/75, loss 0.3827\n",
            "epoch 4/100, batch 29/75, loss 0.3756\n",
            "epoch 4/100, batch 30/75, loss 0.3893\n",
            "epoch 4/100, batch 31/75, loss 0.3863\n",
            "epoch 4/100, batch 32/75, loss 0.3895\n",
            "epoch 4/100, batch 33/75, loss 0.3798\n",
            "epoch 4/100, batch 34/75, loss 0.3843\n",
            "epoch 4/100, batch 35/75, loss 0.3773\n",
            "epoch 4/100, batch 36/75, loss 5.6758\n",
            "epoch 4/100, batch 37/75, loss 0.6351\n",
            "epoch 4/100, batch 38/75, loss 0.3733\n",
            "epoch 4/100, batch 39/75, loss 0.3878\n",
            "epoch 4/100, batch 40/75, loss 0.3821\n",
            "epoch 4/100, batch 41/75, loss 0.3974\n",
            "epoch 4/100, batch 42/75, loss 0.3874\n",
            "epoch 4/100, batch 43/75, loss 0.3874\n",
            "epoch 4/100, batch 44/75, loss 0.3863\n",
            "epoch 4/100, batch 45/75, loss 0.3797\n",
            "epoch 4/100, batch 46/75, loss 0.3775\n",
            "epoch 4/100, batch 47/75, loss 0.3808\n",
            "epoch 4/100, batch 48/75, loss 0.3602\n",
            "epoch 4/100, batch 49/75, loss 0.3653\n",
            "epoch 4/100, batch 50/75, loss 0.3499\n",
            "epoch 4/100, batch 51/75, loss 0.3478\n",
            "epoch 4/100, batch 52/75, loss 0.3269\n",
            "epoch 4/100, batch 53/75, loss 16.4763\n",
            "epoch 4/100, batch 54/75, loss 1.4130\n",
            "epoch 4/100, batch 55/75, loss 0.3611\n",
            "epoch 4/100, batch 56/75, loss 0.3896\n",
            "epoch 4/100, batch 57/75, loss 0.3865\n",
            "epoch 4/100, batch 58/75, loss 0.4083\n",
            "epoch 4/100, batch 59/75, loss 0.4252\n",
            "epoch 4/100, batch 60/75, loss 0.4314\n",
            "epoch 4/100, batch 61/75, loss 0.4335\n",
            "epoch 4/100, batch 62/75, loss 0.4423\n",
            "epoch 4/100, batch 63/75, loss 0.4450\n",
            "epoch 4/100, batch 64/75, loss 0.4474\n",
            "epoch 4/100, batch 65/75, loss 0.4506\n",
            "epoch 4/100, batch 66/75, loss 0.4374\n",
            "epoch 4/100, batch 67/75, loss 0.4518\n",
            "epoch 4/100, batch 68/75, loss 0.4332\n",
            "epoch 4/100, batch 69/75, loss 0.4341\n",
            "epoch 4/100, batch 70/75, loss 0.4184\n",
            "epoch 4/100, batch 71/75, loss 0.4300\n",
            "epoch 4/100, batch 72/75, loss 0.4364\n",
            "epoch 4/100, batch 73/75, loss 0.4173\n",
            "epoch 4/100, batch 74/75, loss 0.4010\n",
            "epoch 4/100, batch 75/75, loss 0.3985\n",
            "epoch 4/100, training roc_auc_score 0.5769\n",
            "epoch 4/100, validation roc_auc_score 0.7468, best validation roc_auc_score 0.7468\n",
            "epoch 5/100, batch 1/75, loss 1.1792\n",
            "epoch 5/100, batch 2/75, loss 0.8552\n",
            "epoch 5/100, batch 3/75, loss 0.6163\n",
            "epoch 5/100, batch 4/75, loss 0.6192\n",
            "epoch 5/100, batch 5/75, loss 0.4235\n",
            "epoch 5/100, batch 6/75, loss 0.3848\n",
            "epoch 5/100, batch 7/75, loss 0.3664\n",
            "epoch 5/100, batch 8/75, loss 1.4274\n",
            "epoch 5/100, batch 9/75, loss 0.3618\n",
            "epoch 5/100, batch 10/75, loss 2.3114\n",
            "epoch 5/100, batch 11/75, loss 0.8589\n",
            "epoch 5/100, batch 12/75, loss 0.3507\n",
            "epoch 5/100, batch 13/75, loss 0.5739\n",
            "epoch 5/100, batch 14/75, loss 0.3263\n",
            "epoch 5/100, batch 15/75, loss 5.0400\n",
            "epoch 5/100, batch 16/75, loss 0.3063\n",
            "epoch 5/100, batch 17/75, loss 0.3268\n",
            "epoch 5/100, batch 18/75, loss 0.3304\n",
            "epoch 5/100, batch 19/75, loss 0.3461\n",
            "epoch 5/100, batch 20/75, loss 3.2957\n",
            "epoch 5/100, batch 21/75, loss 0.3402\n",
            "epoch 5/100, batch 22/75, loss 0.3617\n",
            "epoch 5/100, batch 23/75, loss 0.3504\n",
            "epoch 5/100, batch 24/75, loss 0.3561\n",
            "epoch 5/100, batch 25/75, loss 0.3649\n",
            "epoch 5/100, batch 26/75, loss 5.6475\n",
            "epoch 5/100, batch 27/75, loss 0.3504\n",
            "epoch 5/100, batch 28/75, loss 0.3785\n",
            "epoch 5/100, batch 29/75, loss 0.3544\n",
            "epoch 5/100, batch 30/75, loss 0.3734\n",
            "epoch 5/100, batch 31/75, loss 0.3770\n",
            "epoch 5/100, batch 32/75, loss 0.3690\n",
            "epoch 5/100, batch 33/75, loss 0.3698\n",
            "epoch 5/100, batch 34/75, loss 0.3670\n",
            "epoch 5/100, batch 35/75, loss 0.3612\n",
            "epoch 5/100, batch 36/75, loss 5.6234\n",
            "epoch 5/100, batch 37/75, loss 0.6551\n",
            "epoch 5/100, batch 38/75, loss 0.3447\n",
            "epoch 5/100, batch 39/75, loss 0.3735\n",
            "epoch 5/100, batch 40/75, loss 0.3596\n",
            "epoch 5/100, batch 41/75, loss 0.3873\n",
            "epoch 5/100, batch 42/75, loss 0.3664\n",
            "epoch 5/100, batch 43/75, loss 0.3661\n",
            "epoch 5/100, batch 44/75, loss 0.3644\n",
            "epoch 5/100, batch 45/75, loss 0.3609\n",
            "epoch 5/100, batch 46/75, loss 0.3501\n",
            "epoch 5/100, batch 47/75, loss 0.3606\n",
            "epoch 5/100, batch 48/75, loss 0.3350\n",
            "epoch 5/100, batch 49/75, loss 0.3485\n",
            "epoch 5/100, batch 50/75, loss 0.3256\n",
            "epoch 5/100, batch 51/75, loss 0.3271\n",
            "epoch 5/100, batch 52/75, loss 0.2945\n",
            "epoch 5/100, batch 53/75, loss 15.8259\n",
            "epoch 5/100, batch 54/75, loss 1.3085\n",
            "epoch 5/100, batch 55/75, loss 0.3473\n",
            "epoch 5/100, batch 56/75, loss 0.3850\n",
            "epoch 5/100, batch 57/75, loss 0.3745\n",
            "epoch 5/100, batch 58/75, loss 0.4173\n",
            "epoch 5/100, batch 59/75, loss 0.4383\n",
            "epoch 5/100, batch 60/75, loss 0.4466\n",
            "epoch 5/100, batch 61/75, loss 0.4421\n",
            "epoch 5/100, batch 62/75, loss 0.4590\n",
            "epoch 5/100, batch 63/75, loss 0.4575\n",
            "epoch 5/100, batch 64/75, loss 0.4572\n",
            "epoch 5/100, batch 65/75, loss 0.4588\n",
            "epoch 5/100, batch 66/75, loss 0.4347\n",
            "epoch 5/100, batch 67/75, loss 0.4513\n",
            "epoch 5/100, batch 68/75, loss 0.4194\n",
            "epoch 5/100, batch 69/75, loss 0.4153\n",
            "epoch 5/100, batch 70/75, loss 0.3896\n",
            "epoch 5/100, batch 71/75, loss 0.4011\n",
            "epoch 5/100, batch 72/75, loss 0.4049\n",
            "epoch 5/100, batch 73/75, loss 0.3733\n",
            "epoch 5/100, batch 74/75, loss 0.3480\n",
            "epoch 5/100, batch 75/75, loss 0.3379\n",
            "epoch 5/100, training roc_auc_score 0.6366\n",
            "epoch 5/100, validation roc_auc_score 0.7578, best validation roc_auc_score 0.7578\n",
            "epoch 6/100, batch 1/75, loss 1.2121\n",
            "epoch 6/100, batch 2/75, loss 0.8381\n",
            "epoch 6/100, batch 3/75, loss 0.5660\n",
            "epoch 6/100, batch 4/75, loss 0.5703\n",
            "epoch 6/100, batch 5/75, loss 0.3328\n",
            "epoch 6/100, batch 6/75, loss 0.2942\n",
            "epoch 6/100, batch 7/75, loss 0.2766\n",
            "epoch 6/100, batch 8/75, loss 1.5118\n",
            "epoch 6/100, batch 9/75, loss 0.2627\n",
            "epoch 6/100, batch 10/75, loss 2.6291\n",
            "epoch 6/100, batch 11/75, loss 0.8688\n",
            "epoch 6/100, batch 12/75, loss 0.2611\n",
            "epoch 6/100, batch 13/75, loss 0.5188\n",
            "epoch 6/100, batch 14/75, loss 0.2532\n",
            "epoch 6/100, batch 15/75, loss 5.5393\n",
            "epoch 6/100, batch 16/75, loss 0.2242\n",
            "epoch 6/100, batch 17/75, loss 0.2408\n",
            "epoch 6/100, batch 18/75, loss 0.2481\n",
            "epoch 6/100, batch 19/75, loss 0.2665\n",
            "epoch 6/100, batch 20/75, loss 3.8087\n",
            "epoch 6/100, batch 21/75, loss 0.2588\n",
            "epoch 6/100, batch 22/75, loss 0.2828\n",
            "epoch 6/100, batch 23/75, loss 0.2730\n",
            "epoch 6/100, batch 24/75, loss 0.2865\n",
            "epoch 6/100, batch 25/75, loss 0.3011\n",
            "epoch 6/100, batch 26/75, loss 6.3756\n",
            "epoch 6/100, batch 27/75, loss 0.2905\n",
            "epoch 6/100, batch 28/75, loss 0.3385\n",
            "epoch 6/100, batch 29/75, loss 0.3165\n",
            "epoch 6/100, batch 30/75, loss 0.3422\n",
            "epoch 6/100, batch 31/75, loss 0.3500\n",
            "epoch 6/100, batch 32/75, loss 0.3380\n",
            "epoch 6/100, batch 33/75, loss 0.3466\n",
            "epoch 6/100, batch 34/75, loss 0.3349\n",
            "epoch 6/100, batch 35/75, loss 0.3429\n",
            "epoch 6/100, batch 36/75, loss 5.7810\n",
            "epoch 6/100, batch 37/75, loss 0.6880\n",
            "epoch 6/100, batch 38/75, loss 0.3261\n",
            "epoch 6/100, batch 39/75, loss 0.3505\n",
            "epoch 6/100, batch 40/75, loss 0.3390\n",
            "epoch 6/100, batch 41/75, loss 0.3730\n",
            "epoch 6/100, batch 42/75, loss 0.3551\n",
            "epoch 6/100, batch 43/75, loss 0.3570\n",
            "epoch 6/100, batch 44/75, loss 0.3587\n",
            "epoch 6/100, batch 45/75, loss 0.3484\n",
            "epoch 6/100, batch 46/75, loss 0.3420\n",
            "epoch 6/100, batch 47/75, loss 0.3705\n",
            "epoch 6/100, batch 48/75, loss 0.3260\n",
            "epoch 6/100, batch 49/75, loss 0.3454\n",
            "epoch 6/100, batch 50/75, loss 0.3167\n",
            "epoch 6/100, batch 51/75, loss 0.3228\n",
            "epoch 6/100, batch 52/75, loss 0.2938\n",
            "epoch 6/100, batch 53/75, loss 14.7868\n",
            "epoch 6/100, batch 54/75, loss 1.2492\n",
            "epoch 6/100, batch 55/75, loss 0.3135\n",
            "epoch 6/100, batch 56/75, loss 0.3441\n",
            "epoch 6/100, batch 57/75, loss 0.3223\n",
            "epoch 6/100, batch 58/75, loss 0.3682\n",
            "epoch 6/100, batch 59/75, loss 0.3765\n",
            "epoch 6/100, batch 60/75, loss 0.3833\n",
            "epoch 6/100, batch 61/75, loss 0.3619\n",
            "epoch 6/100, batch 62/75, loss 0.3876\n",
            "epoch 6/100, batch 63/75, loss 0.3756\n",
            "epoch 6/100, batch 64/75, loss 0.3718\n",
            "epoch 6/100, batch 65/75, loss 0.3740\n",
            "epoch 6/100, batch 66/75, loss 0.3364\n",
            "epoch 6/100, batch 67/75, loss 0.3610\n",
            "epoch 6/100, batch 68/75, loss 0.3026\n",
            "epoch 6/100, batch 69/75, loss 0.3057\n",
            "epoch 6/100, batch 70/75, loss 0.2640\n",
            "epoch 6/100, batch 71/75, loss 0.2568\n",
            "epoch 6/100, batch 72/75, loss 0.2825\n",
            "epoch 6/100, batch 73/75, loss 0.1662\n",
            "epoch 6/100, batch 74/75, loss 0.1498\n",
            "epoch 6/100, batch 75/75, loss 0.1349\n",
            "epoch 6/100, training roc_auc_score 0.7030\n",
            "EarlyStopping counter: 1 out of 10\n",
            "epoch 6/100, validation roc_auc_score 0.6698, best validation roc_auc_score 0.7578\n",
            "epoch 7/100, batch 1/75, loss 1.8106\n",
            "epoch 7/100, batch 2/75, loss 0.9901\n",
            "epoch 7/100, batch 3/75, loss 0.5531\n",
            "epoch 7/100, batch 4/75, loss 0.5473\n",
            "epoch 7/100, batch 5/75, loss 0.1665\n",
            "epoch 7/100, batch 6/75, loss 0.1546\n",
            "epoch 7/100, batch 7/75, loss 0.1541\n",
            "epoch 7/100, batch 8/75, loss 1.9994\n",
            "epoch 7/100, batch 9/75, loss 0.1613\n",
            "epoch 7/100, batch 10/75, loss 3.3263\n",
            "epoch 7/100, batch 11/75, loss 0.8640\n",
            "epoch 7/100, batch 12/75, loss 0.2805\n",
            "epoch 7/100, batch 13/75, loss 0.5353\n",
            "epoch 7/100, batch 14/75, loss 0.2851\n",
            "epoch 7/100, batch 15/75, loss 5.7375\n",
            "epoch 7/100, batch 16/75, loss 0.3502\n",
            "epoch 7/100, batch 17/75, loss 0.4665\n",
            "epoch 7/100, batch 18/75, loss 0.4989\n",
            "epoch 7/100, batch 19/75, loss 0.5173\n",
            "epoch 7/100, batch 20/75, loss 2.9851\n",
            "epoch 7/100, batch 21/75, loss 0.4775\n",
            "epoch 7/100, batch 22/75, loss 0.5197\n",
            "epoch 7/100, batch 23/75, loss 0.4799\n",
            "epoch 7/100, batch 24/75, loss 0.4770\n",
            "epoch 7/100, batch 25/75, loss 0.5083\n",
            "epoch 7/100, batch 26/75, loss 4.9335\n",
            "epoch 7/100, batch 27/75, loss 0.4313\n",
            "epoch 7/100, batch 28/75, loss 0.4881\n",
            "epoch 7/100, batch 29/75, loss 0.4605\n",
            "epoch 7/100, batch 30/75, loss 0.4802\n",
            "epoch 7/100, batch 31/75, loss 0.4941\n",
            "epoch 7/100, batch 32/75, loss 0.4694\n",
            "epoch 7/100, batch 33/75, loss 0.4830\n",
            "epoch 7/100, batch 34/75, loss 0.4478\n",
            "epoch 7/100, batch 35/75, loss 0.4498\n",
            "epoch 7/100, batch 36/75, loss 4.4956\n",
            "epoch 7/100, batch 37/75, loss 0.7592\n",
            "epoch 7/100, batch 38/75, loss 0.4477\n",
            "epoch 7/100, batch 39/75, loss 0.4544\n",
            "epoch 7/100, batch 40/75, loss 0.4551\n",
            "epoch 7/100, batch 41/75, loss 0.4917\n",
            "epoch 7/100, batch 42/75, loss 0.4782\n",
            "epoch 7/100, batch 43/75, loss 0.4646\n",
            "epoch 7/100, batch 44/75, loss 0.4498\n",
            "epoch 7/100, batch 45/75, loss 0.4581\n",
            "epoch 7/100, batch 46/75, loss 0.4426\n",
            "epoch 7/100, batch 47/75, loss 0.4658\n",
            "epoch 7/100, batch 48/75, loss 0.3985\n",
            "epoch 7/100, batch 49/75, loss 0.4523\n",
            "epoch 7/100, batch 50/75, loss 0.3932\n",
            "epoch 7/100, batch 51/75, loss 0.4047\n",
            "epoch 7/100, batch 52/75, loss 0.3759\n",
            "epoch 7/100, batch 53/75, loss 12.6146\n",
            "epoch 7/100, batch 54/75, loss 1.2931\n",
            "epoch 7/100, batch 55/75, loss 0.3762\n",
            "epoch 7/100, batch 56/75, loss 0.4191\n",
            "epoch 7/100, batch 57/75, loss 0.3606\n",
            "epoch 7/100, batch 58/75, loss 0.4145\n",
            "epoch 7/100, batch 59/75, loss 0.4161\n",
            "epoch 7/100, batch 60/75, loss 0.4211\n",
            "epoch 7/100, batch 61/75, loss 0.3904\n",
            "epoch 7/100, batch 62/75, loss 0.4222\n",
            "epoch 7/100, batch 63/75, loss 0.4160\n",
            "epoch 7/100, batch 64/75, loss 0.4158\n",
            "epoch 7/100, batch 65/75, loss 0.4255\n",
            "epoch 7/100, batch 66/75, loss 0.3744\n",
            "epoch 7/100, batch 67/75, loss 0.4163\n",
            "epoch 7/100, batch 68/75, loss 0.3390\n",
            "epoch 7/100, batch 69/75, loss 0.3476\n",
            "epoch 7/100, batch 70/75, loss 0.3371\n",
            "epoch 7/100, batch 71/75, loss 0.3639\n",
            "epoch 7/100, batch 72/75, loss 0.3988\n",
            "epoch 7/100, batch 73/75, loss 0.3679\n",
            "epoch 7/100, batch 74/75, loss 0.3131\n",
            "epoch 7/100, batch 75/75, loss 0.3430\n",
            "epoch 7/100, training roc_auc_score 0.5976\n",
            "EarlyStopping counter: 2 out of 10\n",
            "epoch 7/100, validation roc_auc_score 0.6939, best validation roc_auc_score 0.7578\n",
            "epoch 8/100, batch 1/75, loss 1.2179\n",
            "epoch 8/100, batch 2/75, loss 0.8535\n",
            "epoch 8/100, batch 3/75, loss 0.5299\n",
            "epoch 8/100, batch 4/75, loss 0.5646\n",
            "epoch 8/100, batch 5/75, loss 0.3793\n",
            "epoch 8/100, batch 6/75, loss 0.3281\n",
            "epoch 8/100, batch 7/75, loss 0.3078\n",
            "epoch 8/100, batch 8/75, loss 1.5803\n",
            "epoch 8/100, batch 9/75, loss 0.2871\n",
            "epoch 8/100, batch 10/75, loss 1.9533\n",
            "epoch 8/100, batch 11/75, loss 0.9397\n",
            "epoch 8/100, batch 12/75, loss 0.3763\n",
            "epoch 8/100, batch 13/75, loss 0.6066\n",
            "epoch 8/100, batch 14/75, loss 0.3662\n",
            "epoch 8/100, batch 15/75, loss 4.7400\n",
            "epoch 8/100, batch 16/75, loss 0.2846\n",
            "epoch 8/100, batch 17/75, loss 0.3145\n",
            "epoch 8/100, batch 18/75, loss 0.3314\n",
            "epoch 8/100, batch 19/75, loss 0.3627\n",
            "epoch 8/100, batch 20/75, loss 3.5244\n",
            "epoch 8/100, batch 21/75, loss 0.3212\n",
            "epoch 8/100, batch 22/75, loss 0.3693\n",
            "epoch 8/100, batch 23/75, loss 0.3121\n",
            "epoch 8/100, batch 24/75, loss 0.3282\n",
            "epoch 8/100, batch 25/75, loss 0.3687\n",
            "epoch 8/100, batch 26/75, loss 6.0727\n",
            "epoch 8/100, batch 27/75, loss 0.2960\n",
            "epoch 8/100, batch 28/75, loss 0.3777\n",
            "epoch 8/100, batch 29/75, loss 0.3273\n",
            "epoch 8/100, batch 30/75, loss 0.3600\n",
            "epoch 8/100, batch 31/75, loss 0.3801\n",
            "epoch 8/100, batch 32/75, loss 0.3563\n",
            "epoch 8/100, batch 33/75, loss 0.4085\n",
            "epoch 8/100, batch 34/75, loss 0.3587\n",
            "epoch 8/100, batch 35/75, loss 0.3769\n",
            "epoch 8/100, batch 36/75, loss 4.9696\n",
            "epoch 8/100, batch 37/75, loss 0.7709\n",
            "epoch 8/100, batch 38/75, loss 0.3533\n",
            "epoch 8/100, batch 39/75, loss 0.3579\n",
            "epoch 8/100, batch 40/75, loss 0.3517\n",
            "epoch 8/100, batch 41/75, loss 0.4101\n",
            "epoch 8/100, batch 42/75, loss 0.3644\n",
            "epoch 8/100, batch 43/75, loss 0.3513\n",
            "epoch 8/100, batch 44/75, loss 0.3546\n",
            "epoch 8/100, batch 45/75, loss 0.3721\n",
            "epoch 8/100, batch 46/75, loss 0.3417\n",
            "epoch 8/100, batch 47/75, loss 0.3776\n",
            "epoch 8/100, batch 48/75, loss 0.3231\n",
            "epoch 8/100, batch 49/75, loss 0.3863\n",
            "epoch 8/100, batch 50/75, loss 0.3282\n",
            "epoch 8/100, batch 51/75, loss 0.3414\n",
            "epoch 8/100, batch 52/75, loss 0.2923\n",
            "epoch 8/100, batch 53/75, loss 13.0743\n",
            "epoch 8/100, batch 54/75, loss 1.2811\n",
            "epoch 8/100, batch 55/75, loss 0.3223\n",
            "epoch 8/100, batch 56/75, loss 0.3401\n",
            "epoch 8/100, batch 57/75, loss 0.2982\n",
            "epoch 8/100, batch 58/75, loss 0.3625\n",
            "epoch 8/100, batch 59/75, loss 0.3656\n",
            "epoch 8/100, batch 60/75, loss 0.3613\n",
            "epoch 8/100, batch 61/75, loss 0.3422\n",
            "epoch 8/100, batch 62/75, loss 0.3866\n",
            "epoch 8/100, batch 63/75, loss 0.3652\n",
            "epoch 8/100, batch 64/75, loss 0.3686\n",
            "epoch 8/100, batch 65/75, loss 0.4038\n",
            "epoch 8/100, batch 66/75, loss 0.3460\n",
            "epoch 8/100, batch 67/75, loss 0.3878\n",
            "epoch 8/100, batch 68/75, loss 0.3171\n",
            "epoch 8/100, batch 69/75, loss 0.3351\n",
            "epoch 8/100, batch 70/75, loss 0.3265\n",
            "epoch 8/100, batch 71/75, loss 0.3474\n",
            "epoch 8/100, batch 72/75, loss 0.3957\n",
            "epoch 8/100, batch 73/75, loss 0.3558\n",
            "epoch 8/100, batch 74/75, loss 0.2954\n",
            "epoch 8/100, batch 75/75, loss 0.3385\n",
            "epoch 8/100, training roc_auc_score 0.6234\n",
            "EarlyStopping counter: 3 out of 10\n",
            "epoch 8/100, validation roc_auc_score 0.7009, best validation roc_auc_score 0.7578\n",
            "epoch 9/100, batch 1/75, loss 1.2220\n",
            "epoch 9/100, batch 2/75, loss 0.8951\n",
            "epoch 9/100, batch 3/75, loss 0.5467\n",
            "epoch 9/100, batch 4/75, loss 0.6236\n",
            "epoch 9/100, batch 5/75, loss 0.3974\n",
            "epoch 9/100, batch 6/75, loss 0.3548\n",
            "epoch 9/100, batch 7/75, loss 0.3494\n",
            "epoch 9/100, batch 8/75, loss 1.5713\n",
            "epoch 9/100, batch 9/75, loss 0.3203\n",
            "epoch 9/100, batch 10/75, loss 1.7643\n",
            "epoch 9/100, batch 11/75, loss 0.9670\n",
            "epoch 9/100, batch 12/75, loss 0.4116\n",
            "epoch 9/100, batch 13/75, loss 0.6085\n",
            "epoch 9/100, batch 14/75, loss 0.3930\n",
            "epoch 9/100, batch 15/75, loss 6.0954\n",
            "epoch 9/100, batch 16/75, loss 0.2593\n",
            "epoch 9/100, batch 17/75, loss 0.3326\n",
            "epoch 9/100, batch 18/75, loss 0.3544\n",
            "epoch 9/100, batch 19/75, loss 0.3897\n",
            "epoch 9/100, batch 20/75, loss 3.3998\n",
            "epoch 9/100, batch 21/75, loss 0.3435\n",
            "epoch 9/100, batch 22/75, loss 0.3943\n",
            "epoch 9/100, batch 23/75, loss 0.3357\n",
            "epoch 9/100, batch 24/75, loss 0.3488\n",
            "epoch 9/100, batch 25/75, loss 0.3919\n",
            "epoch 9/100, batch 26/75, loss 5.7826\n",
            "epoch 9/100, batch 27/75, loss 0.3120\n",
            "epoch 9/100, batch 28/75, loss 0.3899\n",
            "epoch 9/100, batch 29/75, loss 0.3515\n",
            "epoch 9/100, batch 30/75, loss 0.3786\n",
            "epoch 9/100, batch 31/75, loss 0.4021\n",
            "epoch 9/100, batch 32/75, loss 0.3631\n",
            "epoch 9/100, batch 33/75, loss 0.4184\n",
            "epoch 9/100, batch 34/75, loss 0.3731\n",
            "epoch 9/100, batch 35/75, loss 0.3953\n",
            "epoch 9/100, batch 36/75, loss 4.7651\n",
            "epoch 9/100, batch 37/75, loss 0.7544\n",
            "epoch 9/100, batch 38/75, loss 0.3528\n",
            "epoch 9/100, batch 39/75, loss 0.3689\n",
            "epoch 9/100, batch 40/75, loss 0.3525\n",
            "epoch 9/100, batch 41/75, loss 0.4228\n",
            "epoch 9/100, batch 42/75, loss 0.3659\n",
            "epoch 9/100, batch 43/75, loss 0.3606\n",
            "epoch 9/100, batch 44/75, loss 0.3543\n",
            "epoch 9/100, batch 45/75, loss 0.3691\n",
            "epoch 9/100, batch 46/75, loss 0.3472\n",
            "epoch 9/100, batch 47/75, loss 0.3585\n",
            "epoch 9/100, batch 48/75, loss 0.3382\n",
            "epoch 9/100, batch 49/75, loss 0.3910\n",
            "epoch 9/100, batch 50/75, loss 0.3351\n",
            "epoch 9/100, batch 51/75, loss 0.3473\n",
            "epoch 9/100, batch 52/75, loss 0.3029\n",
            "epoch 9/100, batch 53/75, loss 12.9143\n",
            "epoch 9/100, batch 54/75, loss 1.2354\n",
            "epoch 9/100, batch 55/75, loss 0.3178\n",
            "epoch 9/100, batch 56/75, loss 0.3435\n",
            "epoch 9/100, batch 57/75, loss 0.2929\n",
            "epoch 9/100, batch 58/75, loss 0.3638\n",
            "epoch 9/100, batch 59/75, loss 0.3813\n",
            "epoch 9/100, batch 60/75, loss 0.3715\n",
            "epoch 9/100, batch 61/75, loss 0.3556\n",
            "epoch 9/100, batch 62/75, loss 0.3871\n",
            "epoch 9/100, batch 63/75, loss 0.3770\n",
            "epoch 9/100, batch 64/75, loss 0.3722\n",
            "epoch 9/100, batch 65/75, loss 0.4001\n",
            "epoch 9/100, batch 66/75, loss 0.3518\n",
            "epoch 9/100, batch 67/75, loss 0.3973\n",
            "epoch 9/100, batch 68/75, loss 0.3250\n",
            "epoch 9/100, batch 69/75, loss 0.3462\n",
            "epoch 9/100, batch 70/75, loss 0.3265\n",
            "epoch 9/100, batch 71/75, loss 0.3597\n",
            "epoch 9/100, batch 72/75, loss 0.4025\n",
            "epoch 9/100, batch 73/75, loss 0.3628\n",
            "epoch 9/100, batch 74/75, loss 0.3040\n",
            "epoch 9/100, batch 75/75, loss 0.3361\n",
            "epoch 9/100, training roc_auc_score 0.6403\n",
            "EarlyStopping counter: 4 out of 10\n",
            "epoch 9/100, validation roc_auc_score 0.7118, best validation roc_auc_score 0.7578\n",
            "epoch 10/100, batch 1/75, loss 1.1906\n",
            "epoch 10/100, batch 2/75, loss 0.8441\n",
            "epoch 10/100, batch 3/75, loss 0.5309\n",
            "epoch 10/100, batch 4/75, loss 0.6185\n",
            "epoch 10/100, batch 5/75, loss 0.4075\n",
            "epoch 10/100, batch 6/75, loss 0.3653\n",
            "epoch 10/100, batch 7/75, loss 0.3453\n",
            "epoch 10/100, batch 8/75, loss 1.5323\n",
            "epoch 10/100, batch 9/75, loss 0.3108\n",
            "epoch 10/100, batch 10/75, loss 1.7074\n",
            "epoch 10/100, batch 11/75, loss 0.9480\n",
            "epoch 10/100, batch 12/75, loss 0.3970\n",
            "epoch 10/100, batch 13/75, loss 0.5753\n",
            "epoch 10/100, batch 14/75, loss 0.3608\n",
            "epoch 10/100, batch 15/75, loss 5.7798\n",
            "epoch 10/100, batch 16/75, loss 0.2652\n",
            "epoch 10/100, batch 17/75, loss 0.3162\n",
            "epoch 10/100, batch 18/75, loss 0.3488\n",
            "epoch 10/100, batch 19/75, loss 0.3496\n",
            "epoch 10/100, batch 20/75, loss 3.4047\n",
            "epoch 10/100, batch 21/75, loss 0.3477\n",
            "epoch 10/100, batch 22/75, loss 0.3965\n",
            "epoch 10/100, batch 23/75, loss 0.3474\n",
            "epoch 10/100, batch 24/75, loss 0.3563\n",
            "epoch 10/100, batch 25/75, loss 0.4018\n",
            "epoch 10/100, batch 26/75, loss 5.6383\n",
            "epoch 10/100, batch 27/75, loss 0.3557\n",
            "epoch 10/100, batch 28/75, loss 0.4651\n",
            "epoch 10/100, batch 29/75, loss 0.4129\n",
            "epoch 10/100, batch 30/75, loss 0.4376\n",
            "epoch 10/100, batch 31/75, loss 0.4592\n",
            "epoch 10/100, batch 32/75, loss 0.4727\n",
            "epoch 10/100, batch 33/75, loss 0.4695\n",
            "epoch 10/100, batch 34/75, loss 0.4244\n",
            "epoch 10/100, batch 35/75, loss 0.4469\n",
            "epoch 10/100, batch 36/75, loss 4.5885\n",
            "epoch 10/100, batch 37/75, loss 0.7988\n",
            "epoch 10/100, batch 38/75, loss 0.4377\n",
            "epoch 10/100, batch 39/75, loss 0.4325\n",
            "epoch 10/100, batch 40/75, loss 0.4252\n",
            "epoch 10/100, batch 41/75, loss 0.4589\n",
            "epoch 10/100, batch 42/75, loss 0.4365\n",
            "epoch 10/100, batch 43/75, loss 0.4064\n",
            "epoch 10/100, batch 44/75, loss 0.4020\n",
            "epoch 10/100, batch 45/75, loss 0.4113\n",
            "epoch 10/100, batch 46/75, loss 0.4043\n",
            "epoch 10/100, batch 47/75, loss 0.4296\n",
            "epoch 10/100, batch 48/75, loss 0.3679\n",
            "epoch 10/100, batch 49/75, loss 0.4270\n",
            "epoch 10/100, batch 50/75, loss 0.3703\n",
            "epoch 10/100, batch 51/75, loss 0.3689\n",
            "epoch 10/100, batch 52/75, loss 0.3308\n",
            "epoch 10/100, batch 53/75, loss 11.9854\n",
            "epoch 10/100, batch 54/75, loss 1.2287\n",
            "epoch 10/100, batch 55/75, loss 0.3527\n",
            "epoch 10/100, batch 56/75, loss 0.3710\n",
            "epoch 10/100, batch 57/75, loss 0.3480\n",
            "epoch 10/100, batch 58/75, loss 0.4030\n",
            "epoch 10/100, batch 59/75, loss 0.4076\n",
            "epoch 10/100, batch 60/75, loss 0.4101\n",
            "epoch 10/100, batch 61/75, loss 0.3783\n",
            "epoch 10/100, batch 62/75, loss 0.4146\n",
            "epoch 10/100, batch 63/75, loss 0.4132\n",
            "epoch 10/100, batch 64/75, loss 0.4080\n",
            "epoch 10/100, batch 65/75, loss 0.4268\n",
            "epoch 10/100, batch 66/75, loss 0.3845\n",
            "epoch 10/100, batch 67/75, loss 0.4333\n",
            "epoch 10/100, batch 68/75, loss 0.3588\n",
            "epoch 10/100, batch 69/75, loss 0.3808\n",
            "epoch 10/100, batch 70/75, loss 0.3501\n",
            "epoch 10/100, batch 71/75, loss 0.3886\n",
            "epoch 10/100, batch 72/75, loss 0.4420\n",
            "epoch 10/100, batch 73/75, loss 0.3931\n",
            "epoch 10/100, batch 74/75, loss 0.3604\n",
            "epoch 10/100, batch 75/75, loss 0.3672\n",
            "epoch 10/100, training roc_auc_score 0.6503\n",
            "EarlyStopping counter: 5 out of 10\n",
            "epoch 10/100, validation roc_auc_score 0.7342, best validation roc_auc_score 0.7578\n",
            "epoch 11/100, batch 1/75, loss 1.2588\n",
            "epoch 11/100, batch 2/75, loss 0.8914\n",
            "epoch 11/100, batch 3/75, loss 0.5828\n",
            "epoch 11/100, batch 4/75, loss 0.6394\n",
            "epoch 11/100, batch 5/75, loss 0.4303\n",
            "epoch 11/100, batch 6/75, loss 0.4170\n",
            "epoch 11/100, batch 7/75, loss 0.3861\n",
            "epoch 11/100, batch 8/75, loss 1.5332\n",
            "epoch 11/100, batch 9/75, loss 0.3786\n",
            "epoch 11/100, batch 10/75, loss 1.7675\n",
            "epoch 11/100, batch 11/75, loss 0.9702\n",
            "epoch 11/100, batch 12/75, loss 0.4098\n",
            "epoch 11/100, batch 13/75, loss 0.6119\n",
            "epoch 11/100, batch 14/75, loss 0.4126\n",
            "epoch 11/100, batch 15/75, loss 4.5738\n",
            "epoch 11/100, batch 16/75, loss 0.3008\n",
            "epoch 11/100, batch 17/75, loss 0.3296\n",
            "epoch 11/100, batch 18/75, loss 0.3453\n",
            "epoch 11/100, batch 19/75, loss 0.3749\n",
            "epoch 11/100, batch 20/75, loss 3.4392\n",
            "epoch 11/100, batch 21/75, loss 0.3280\n",
            "epoch 11/100, batch 22/75, loss 0.3745\n",
            "epoch 11/100, batch 23/75, loss 0.3170\n",
            "epoch 11/100, batch 24/75, loss 0.3176\n",
            "epoch 11/100, batch 25/75, loss 0.3452\n",
            "epoch 11/100, batch 26/75, loss 5.9710\n",
            "epoch 11/100, batch 27/75, loss 0.2871\n",
            "epoch 11/100, batch 28/75, loss 0.3481\n",
            "epoch 11/100, batch 29/75, loss 0.3206\n",
            "epoch 11/100, batch 30/75, loss 0.3551\n",
            "epoch 11/100, batch 31/75, loss 0.3686\n",
            "epoch 11/100, batch 32/75, loss 0.3380\n",
            "epoch 11/100, batch 33/75, loss 0.3761\n",
            "epoch 11/100, batch 34/75, loss 0.3465\n",
            "epoch 11/100, batch 35/75, loss 0.3604\n",
            "epoch 11/100, batch 36/75, loss 5.0141\n",
            "epoch 11/100, batch 37/75, loss 0.7435\n",
            "epoch 11/100, batch 38/75, loss 0.3089\n",
            "epoch 11/100, batch 39/75, loss 0.3436\n",
            "epoch 11/100, batch 40/75, loss 0.3247\n",
            "epoch 11/100, batch 41/75, loss 0.3985\n",
            "epoch 11/100, batch 42/75, loss 0.3305\n",
            "epoch 11/100, batch 43/75, loss 0.3339\n",
            "epoch 11/100, batch 44/75, loss 0.3246\n",
            "epoch 11/100, batch 45/75, loss 0.3425\n",
            "epoch 11/100, batch 46/75, loss 0.3077\n",
            "epoch 11/100, batch 47/75, loss 0.3317\n",
            "epoch 11/100, batch 48/75, loss 0.3115\n",
            "epoch 11/100, batch 49/75, loss 0.3627\n",
            "epoch 11/100, batch 50/75, loss 0.3114\n",
            "epoch 11/100, batch 51/75, loss 0.3219\n",
            "epoch 11/100, batch 52/75, loss 0.2621\n",
            "epoch 11/100, batch 53/75, loss 13.4528\n",
            "epoch 11/100, batch 54/75, loss 1.2336\n",
            "epoch 11/100, batch 55/75, loss 0.3041\n",
            "epoch 11/100, batch 56/75, loss 0.3290\n",
            "epoch 11/100, batch 57/75, loss 0.2787\n",
            "epoch 11/100, batch 58/75, loss 0.3505\n",
            "epoch 11/100, batch 59/75, loss 0.3618\n",
            "epoch 11/100, batch 60/75, loss 0.3617\n",
            "epoch 11/100, batch 61/75, loss 0.3429\n",
            "epoch 11/100, batch 62/75, loss 0.3750\n",
            "epoch 11/100, batch 63/75, loss 0.3696\n",
            "epoch 11/100, batch 64/75, loss 0.3680\n",
            "epoch 11/100, batch 65/75, loss 0.3943\n",
            "epoch 11/100, batch 66/75, loss 0.3423\n",
            "epoch 11/100, batch 67/75, loss 0.3938\n",
            "epoch 11/100, batch 68/75, loss 0.3202\n",
            "epoch 11/100, batch 69/75, loss 0.3429\n",
            "epoch 11/100, batch 70/75, loss 0.3186\n",
            "epoch 11/100, batch 71/75, loss 0.3573\n",
            "epoch 11/100, batch 72/75, loss 0.3956\n",
            "epoch 11/100, batch 73/75, loss 0.3600\n",
            "epoch 11/100, batch 74/75, loss 0.3020\n",
            "epoch 11/100, batch 75/75, loss 0.3417\n",
            "epoch 11/100, training roc_auc_score 0.6619\n",
            "EarlyStopping counter: 6 out of 10\n",
            "epoch 11/100, validation roc_auc_score 0.7245, best validation roc_auc_score 0.7578\n",
            "epoch 12/100, batch 1/75, loss 1.2032\n",
            "epoch 12/100, batch 2/75, loss 0.8588\n",
            "epoch 12/100, batch 3/75, loss 0.5468\n",
            "epoch 12/100, batch 4/75, loss 0.6217\n",
            "epoch 12/100, batch 5/75, loss 0.4013\n",
            "epoch 12/100, batch 6/75, loss 0.3600\n",
            "epoch 12/100, batch 7/75, loss 0.3316\n",
            "epoch 12/100, batch 8/75, loss 1.5284\n",
            "epoch 12/100, batch 9/75, loss 0.3259\n",
            "epoch 12/100, batch 10/75, loss 1.7355\n",
            "epoch 12/100, batch 11/75, loss 0.9476\n",
            "epoch 12/100, batch 12/75, loss 0.4036\n",
            "epoch 12/100, batch 13/75, loss 0.5931\n",
            "epoch 12/100, batch 14/75, loss 0.3598\n",
            "epoch 12/100, batch 15/75, loss 5.4863\n",
            "epoch 12/100, batch 16/75, loss 0.3070\n",
            "epoch 12/100, batch 17/75, loss 0.3441\n",
            "epoch 12/100, batch 18/75, loss 0.3651\n",
            "epoch 12/100, batch 19/75, loss 0.4131\n",
            "epoch 12/100, batch 20/75, loss 3.3243\n",
            "epoch 12/100, batch 21/75, loss 0.3711\n",
            "epoch 12/100, batch 22/75, loss 0.4376\n",
            "epoch 12/100, batch 23/75, loss 0.3893\n",
            "epoch 12/100, batch 24/75, loss 0.4324\n",
            "epoch 12/100, batch 25/75, loss 0.4751\n",
            "epoch 12/100, batch 26/75, loss 5.1705\n",
            "epoch 12/100, batch 27/75, loss 0.4380\n",
            "epoch 12/100, batch 28/75, loss 0.5267\n",
            "epoch 12/100, batch 29/75, loss 0.4922\n",
            "epoch 12/100, batch 30/75, loss 0.5413\n",
            "epoch 12/100, batch 31/75, loss 0.5625\n",
            "epoch 12/100, batch 32/75, loss 0.5634\n",
            "epoch 12/100, batch 33/75, loss 0.5439\n",
            "epoch 12/100, batch 34/75, loss 0.5271\n",
            "epoch 12/100, batch 35/75, loss 0.5331\n",
            "epoch 12/100, batch 36/75, loss 4.4689\n",
            "epoch 12/100, batch 37/75, loss 0.8033\n",
            "epoch 12/100, batch 38/75, loss 0.5238\n",
            "epoch 12/100, batch 39/75, loss 0.5066\n",
            "epoch 12/100, batch 40/75, loss 0.5020\n",
            "epoch 12/100, batch 41/75, loss 0.5402\n",
            "epoch 12/100, batch 42/75, loss 0.5165\n",
            "epoch 12/100, batch 43/75, loss 0.5005\n",
            "epoch 12/100, batch 44/75, loss 0.4697\n",
            "epoch 12/100, batch 45/75, loss 0.4779\n",
            "epoch 12/100, batch 46/75, loss 0.4586\n",
            "epoch 12/100, batch 47/75, loss 0.4866\n",
            "epoch 12/100, batch 48/75, loss 0.4200\n",
            "epoch 12/100, batch 49/75, loss 0.4514\n",
            "epoch 12/100, batch 50/75, loss 0.3964\n",
            "epoch 12/100, batch 51/75, loss 0.3992\n",
            "epoch 12/100, batch 52/75, loss 0.3902\n",
            "epoch 12/100, batch 53/75, loss 12.5107\n",
            "epoch 12/100, batch 54/75, loss 1.2643\n",
            "epoch 12/100, batch 55/75, loss 0.3662\n",
            "epoch 12/100, batch 56/75, loss 0.4079\n",
            "epoch 12/100, batch 57/75, loss 0.3439\n",
            "epoch 12/100, batch 58/75, loss 0.4050\n",
            "epoch 12/100, batch 59/75, loss 0.4054\n",
            "epoch 12/100, batch 60/75, loss 0.4086\n",
            "epoch 12/100, batch 61/75, loss 0.3858\n",
            "epoch 12/100, batch 62/75, loss 0.4128\n",
            "epoch 12/100, batch 63/75, loss 0.4041\n",
            "epoch 12/100, batch 64/75, loss 0.3956\n",
            "epoch 12/100, batch 65/75, loss 0.4145\n",
            "epoch 12/100, batch 66/75, loss 0.3669\n",
            "epoch 12/100, batch 67/75, loss 0.4212\n",
            "epoch 12/100, batch 68/75, loss 0.3473\n",
            "epoch 12/100, batch 69/75, loss 0.3685\n",
            "epoch 12/100, batch 70/75, loss 0.3430\n",
            "epoch 12/100, batch 71/75, loss 0.3794\n",
            "epoch 12/100, batch 72/75, loss 0.4177\n",
            "epoch 12/100, batch 73/75, loss 0.3699\n",
            "epoch 12/100, batch 74/75, loss 0.3404\n",
            "epoch 12/100, batch 75/75, loss 0.3483\n",
            "epoch 12/100, training roc_auc_score 0.6326\n",
            "EarlyStopping counter: 7 out of 10\n",
            "epoch 12/100, validation roc_auc_score 0.7212, best validation roc_auc_score 0.7578\n",
            "epoch 13/100, batch 1/75, loss 1.2907\n",
            "epoch 13/100, batch 2/75, loss 0.7318\n",
            "epoch 13/100, batch 3/75, loss 0.5853\n",
            "epoch 13/100, batch 4/75, loss 0.6317\n",
            "epoch 13/100, batch 5/75, loss 0.4227\n",
            "epoch 13/100, batch 6/75, loss 0.3981\n",
            "epoch 13/100, batch 7/75, loss 0.3363\n",
            "epoch 13/100, batch 8/75, loss 1.3073\n",
            "epoch 13/100, batch 9/75, loss 0.3587\n",
            "epoch 13/100, batch 10/75, loss 1.8277\n",
            "epoch 13/100, batch 11/75, loss 0.9804\n",
            "epoch 13/100, batch 12/75, loss 0.4016\n",
            "epoch 13/100, batch 13/75, loss 0.5952\n",
            "epoch 13/100, batch 14/75, loss 0.3703\n",
            "epoch 13/100, batch 15/75, loss 4.6916\n",
            "epoch 13/100, batch 16/75, loss 0.2872\n",
            "epoch 13/100, batch 17/75, loss 0.3082\n",
            "epoch 13/100, batch 18/75, loss 0.2873\n",
            "epoch 13/100, batch 19/75, loss 0.3498\n",
            "epoch 13/100, batch 20/75, loss 3.7146\n",
            "epoch 13/100, batch 21/75, loss 0.2723\n",
            "epoch 13/100, batch 22/75, loss 0.3342\n",
            "epoch 13/100, batch 23/75, loss 0.2956\n",
            "epoch 13/100, batch 24/75, loss 0.2811\n",
            "epoch 13/100, batch 25/75, loss 0.3038\n",
            "epoch 13/100, batch 26/75, loss 6.2693\n",
            "epoch 13/100, batch 27/75, loss 0.2604\n",
            "epoch 13/100, batch 28/75, loss 0.3462\n",
            "epoch 13/100, batch 29/75, loss 0.2970\n",
            "epoch 13/100, batch 30/75, loss 0.3489\n",
            "epoch 13/100, batch 31/75, loss 0.3446\n",
            "epoch 13/100, batch 32/75, loss 0.3287\n",
            "epoch 13/100, batch 33/75, loss 0.3618\n",
            "epoch 13/100, batch 34/75, loss 0.3249\n",
            "epoch 13/100, batch 35/75, loss 0.3500\n",
            "epoch 13/100, batch 36/75, loss 5.4945\n",
            "epoch 13/100, batch 37/75, loss 0.7620\n",
            "epoch 13/100, batch 38/75, loss 0.3529\n",
            "epoch 13/100, batch 39/75, loss 0.3731\n",
            "epoch 13/100, batch 40/75, loss 0.3669\n",
            "epoch 13/100, batch 41/75, loss 0.4178\n",
            "epoch 13/100, batch 42/75, loss 0.4080\n",
            "epoch 13/100, batch 43/75, loss 0.4101\n",
            "epoch 13/100, batch 44/75, loss 0.3880\n",
            "epoch 13/100, batch 45/75, loss 0.4074\n",
            "epoch 13/100, batch 46/75, loss 0.3964\n",
            "epoch 13/100, batch 47/75, loss 0.4457\n",
            "epoch 13/100, batch 48/75, loss 0.3886\n",
            "epoch 13/100, batch 49/75, loss 0.4231\n",
            "epoch 13/100, batch 50/75, loss 0.3740\n",
            "epoch 13/100, batch 51/75, loss 0.3740\n",
            "epoch 13/100, batch 52/75, loss 0.3556\n",
            "epoch 13/100, batch 53/75, loss 13.1790\n",
            "epoch 13/100, batch 54/75, loss 1.3660\n",
            "epoch 13/100, batch 55/75, loss 0.3734\n",
            "epoch 13/100, batch 56/75, loss 0.4090\n",
            "epoch 13/100, batch 57/75, loss 0.3451\n",
            "epoch 13/100, batch 58/75, loss 0.4063\n",
            "epoch 13/100, batch 59/75, loss 0.4089\n",
            "epoch 13/100, batch 60/75, loss 0.4252\n",
            "epoch 13/100, batch 61/75, loss 0.3985\n",
            "epoch 13/100, batch 62/75, loss 0.4353\n",
            "epoch 13/100, batch 63/75, loss 0.4221\n",
            "epoch 13/100, batch 64/75, loss 0.4147\n",
            "epoch 13/100, batch 65/75, loss 0.4268\n",
            "epoch 13/100, batch 66/75, loss 0.3835\n",
            "epoch 13/100, batch 67/75, loss 0.4190\n",
            "epoch 13/100, batch 68/75, loss 0.3975\n",
            "epoch 13/100, batch 69/75, loss 0.3862\n",
            "epoch 13/100, batch 70/75, loss 0.3651\n",
            "epoch 13/100, batch 71/75, loss 0.3966\n",
            "epoch 13/100, batch 72/75, loss 0.4290\n",
            "epoch 13/100, batch 73/75, loss 0.3855\n",
            "epoch 13/100, batch 74/75, loss 0.3602\n",
            "epoch 13/100, batch 75/75, loss 0.3625\n",
            "epoch 13/100, training roc_auc_score 0.6361\n",
            "EarlyStopping counter: 8 out of 10\n",
            "epoch 13/100, validation roc_auc_score 0.7294, best validation roc_auc_score 0.7578\n",
            "epoch 14/100, batch 1/75, loss 1.3225\n",
            "epoch 14/100, batch 2/75, loss 0.7938\n",
            "epoch 14/100, batch 3/75, loss 0.5998\n",
            "epoch 14/100, batch 4/75, loss 0.6359\n",
            "epoch 14/100, batch 5/75, loss 0.4180\n",
            "epoch 14/100, batch 6/75, loss 0.3862\n",
            "epoch 14/100, batch 7/75, loss 0.3908\n",
            "epoch 14/100, batch 8/75, loss 1.3251\n",
            "epoch 14/100, batch 9/75, loss 0.3595\n",
            "epoch 14/100, batch 10/75, loss 1.8721\n",
            "epoch 14/100, batch 11/75, loss 0.9592\n",
            "epoch 14/100, batch 12/75, loss 0.3773\n",
            "epoch 14/100, batch 13/75, loss 0.6322\n",
            "epoch 14/100, batch 14/75, loss 0.3865\n",
            "epoch 14/100, batch 15/75, loss 4.8072\n",
            "epoch 14/100, batch 16/75, loss 0.2760\n",
            "epoch 14/100, batch 17/75, loss 0.3215\n",
            "epoch 14/100, batch 18/75, loss 0.3117\n",
            "epoch 14/100, batch 19/75, loss 0.3492\n",
            "epoch 14/100, batch 20/75, loss 3.5856\n",
            "epoch 14/100, batch 21/75, loss 0.3027\n",
            "epoch 14/100, batch 22/75, loss 0.3552\n",
            "epoch 14/100, batch 23/75, loss 0.2987\n",
            "epoch 14/100, batch 24/75, loss 0.3329\n",
            "epoch 14/100, batch 25/75, loss 0.3440\n",
            "epoch 14/100, batch 26/75, loss 6.1762\n",
            "epoch 14/100, batch 27/75, loss 0.2830\n",
            "epoch 14/100, batch 28/75, loss 0.3792\n",
            "epoch 14/100, batch 29/75, loss 0.3315\n",
            "epoch 14/100, batch 30/75, loss 0.3649\n",
            "epoch 14/100, batch 31/75, loss 0.3729\n",
            "epoch 14/100, batch 32/75, loss 0.3798\n",
            "epoch 14/100, batch 33/75, loss 0.3830\n",
            "epoch 14/100, batch 34/75, loss 0.3504\n",
            "epoch 14/100, batch 35/75, loss 0.3753\n",
            "epoch 14/100, batch 36/75, loss 5.1985\n",
            "epoch 14/100, batch 37/75, loss 0.7669\n",
            "epoch 14/100, batch 38/75, loss 0.3497\n",
            "epoch 14/100, batch 39/75, loss 0.3616\n",
            "epoch 14/100, batch 40/75, loss 0.3451\n",
            "epoch 14/100, batch 41/75, loss 0.3943\n",
            "epoch 14/100, batch 42/75, loss 0.3681\n",
            "epoch 14/100, batch 43/75, loss 0.3496\n",
            "epoch 14/100, batch 44/75, loss 0.3492\n",
            "epoch 14/100, batch 45/75, loss 0.3567\n",
            "epoch 14/100, batch 46/75, loss 0.3332\n",
            "epoch 14/100, batch 47/75, loss 0.3807\n",
            "epoch 14/100, batch 48/75, loss 0.3222\n",
            "epoch 14/100, batch 49/75, loss 0.3633\n",
            "epoch 14/100, batch 50/75, loss 0.3190\n",
            "epoch 14/100, batch 51/75, loss 0.3318\n",
            "epoch 14/100, batch 52/75, loss 0.2897\n",
            "epoch 14/100, batch 53/75, loss 13.1318\n",
            "epoch 14/100, batch 54/75, loss 1.3272\n",
            "epoch 14/100, batch 55/75, loss 0.3290\n",
            "epoch 14/100, batch 56/75, loss 0.3547\n",
            "epoch 14/100, batch 57/75, loss 0.3096\n",
            "epoch 14/100, batch 58/75, loss 0.3695\n",
            "epoch 14/100, batch 59/75, loss 0.3714\n",
            "epoch 14/100, batch 60/75, loss 0.3819\n",
            "epoch 14/100, batch 61/75, loss 0.3541\n",
            "epoch 14/100, batch 62/75, loss 0.3965\n",
            "epoch 14/100, batch 63/75, loss 0.3823\n",
            "epoch 14/100, batch 64/75, loss 0.3822\n",
            "epoch 14/100, batch 65/75, loss 0.4081\n",
            "epoch 14/100, batch 66/75, loss 0.3591\n",
            "epoch 14/100, batch 67/75, loss 0.4027\n",
            "epoch 14/100, batch 68/75, loss 0.3312\n",
            "epoch 14/100, batch 69/75, loss 0.3496\n",
            "epoch 14/100, batch 70/75, loss 0.3141\n",
            "epoch 14/100, batch 71/75, loss 0.3524\n",
            "epoch 14/100, batch 72/75, loss 0.3980\n",
            "epoch 14/100, batch 73/75, loss 0.3462\n",
            "epoch 14/100, batch 74/75, loss 0.3061\n",
            "epoch 14/100, batch 75/75, loss 0.3385\n",
            "epoch 14/100, training roc_auc_score 0.6716\n",
            "EarlyStopping counter: 9 out of 10\n",
            "epoch 14/100, validation roc_auc_score 0.7266, best validation roc_auc_score 0.7578\n",
            "epoch 15/100, batch 1/75, loss 1.2714\n",
            "epoch 15/100, batch 2/75, loss 0.8039\n",
            "epoch 15/100, batch 3/75, loss 0.5366\n",
            "epoch 15/100, batch 4/75, loss 0.6091\n",
            "epoch 15/100, batch 5/75, loss 0.3864\n",
            "epoch 15/100, batch 6/75, loss 0.3507\n",
            "epoch 15/100, batch 7/75, loss 0.3376\n",
            "epoch 15/100, batch 8/75, loss 1.3231\n",
            "epoch 15/100, batch 9/75, loss 0.3257\n",
            "epoch 15/100, batch 10/75, loss 1.7359\n",
            "epoch 15/100, batch 11/75, loss 0.9486\n",
            "epoch 15/100, batch 12/75, loss 0.3614\n",
            "epoch 15/100, batch 13/75, loss 0.5720\n",
            "epoch 15/100, batch 14/75, loss 0.3591\n",
            "epoch 15/100, batch 15/75, loss 4.7031\n",
            "epoch 15/100, batch 16/75, loss 0.2741\n",
            "epoch 15/100, batch 17/75, loss 0.2978\n",
            "epoch 15/100, batch 18/75, loss 0.3015\n",
            "epoch 15/100, batch 19/75, loss 0.3321\n",
            "epoch 15/100, batch 20/75, loss 3.6339\n",
            "epoch 15/100, batch 21/75, loss 0.2865\n",
            "epoch 15/100, batch 22/75, loss 0.3236\n",
            "epoch 15/100, batch 23/75, loss 0.2888\n",
            "epoch 15/100, batch 24/75, loss 0.2928\n",
            "epoch 15/100, batch 25/75, loss 0.3017\n",
            "epoch 15/100, batch 26/75, loss 6.2896\n",
            "epoch 15/100, batch 27/75, loss 0.2626\n",
            "epoch 15/100, batch 28/75, loss 0.3502\n",
            "epoch 15/100, batch 29/75, loss 0.2934\n",
            "epoch 15/100, batch 30/75, loss 0.3473\n",
            "epoch 15/100, batch 31/75, loss 0.3467\n",
            "epoch 15/100, batch 32/75, loss 0.3205\n",
            "epoch 15/100, batch 33/75, loss 0.3611\n",
            "epoch 15/100, batch 34/75, loss 0.3296\n",
            "epoch 15/100, batch 35/75, loss 0.3564\n",
            "epoch 15/100, batch 36/75, loss 5.5380\n",
            "epoch 15/100, batch 37/75, loss 0.7636\n",
            "epoch 15/100, batch 38/75, loss 0.3145\n",
            "epoch 15/100, batch 39/75, loss 0.3386\n",
            "epoch 15/100, batch 40/75, loss 0.3145\n",
            "epoch 15/100, batch 41/75, loss 0.3827\n",
            "epoch 15/100, batch 42/75, loss 0.3468\n",
            "epoch 15/100, batch 43/75, loss 0.3266\n",
            "epoch 15/100, batch 44/75, loss 0.3453\n",
            "epoch 15/100, batch 45/75, loss 0.3521\n",
            "epoch 15/100, batch 46/75, loss 0.3199\n",
            "epoch 15/100, batch 47/75, loss 0.3688\n",
            "epoch 15/100, batch 48/75, loss 0.3156\n",
            "epoch 15/100, batch 49/75, loss 0.3581\n",
            "epoch 15/100, batch 50/75, loss 0.3076\n",
            "epoch 15/100, batch 51/75, loss 0.3169\n",
            "epoch 15/100, batch 52/75, loss 0.2584\n",
            "epoch 15/100, batch 53/75, loss 13.0286\n",
            "epoch 15/100, batch 54/75, loss 1.3154\n",
            "epoch 15/100, batch 55/75, loss 0.3001\n",
            "epoch 15/100, batch 56/75, loss 0.3302\n",
            "epoch 15/100, batch 57/75, loss 0.2851\n",
            "epoch 15/100, batch 58/75, loss 0.3500\n",
            "epoch 15/100, batch 59/75, loss 0.3557\n",
            "epoch 15/100, batch 60/75, loss 0.3600\n",
            "epoch 15/100, batch 61/75, loss 0.3342\n",
            "epoch 15/100, batch 62/75, loss 0.3714\n",
            "epoch 15/100, batch 63/75, loss 0.3694\n",
            "epoch 15/100, batch 64/75, loss 0.3692\n",
            "epoch 15/100, batch 65/75, loss 0.3953\n",
            "epoch 15/100, batch 66/75, loss 0.3515\n",
            "epoch 15/100, batch 67/75, loss 0.3957\n",
            "epoch 15/100, batch 68/75, loss 0.3147\n",
            "epoch 15/100, batch 69/75, loss 0.3315\n",
            "epoch 15/100, batch 70/75, loss 0.2971\n",
            "epoch 15/100, batch 71/75, loss 0.3397\n",
            "epoch 15/100, batch 72/75, loss 0.3861\n",
            "epoch 15/100, batch 73/75, loss 0.3333\n",
            "epoch 15/100, batch 74/75, loss 0.2839\n",
            "epoch 15/100, batch 75/75, loss 0.3315\n",
            "epoch 15/100, training roc_auc_score 0.6865\n",
            "EarlyStopping counter: 10 out of 10\n",
            "epoch 15/100, validation roc_auc_score 0.7203, best validation roc_auc_score 0.7578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YaWpHl2Gnr-",
        "colab_type": "code",
        "outputId": "d6bb2f9e-22e5-422d-9f66-da50ad5f8343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "stopper.load_checkpoint(model)\n",
        "test_score = run_an_eval_epoch(args, model, test_loader)\n",
        "print('test {} {:.4f}'.format(args['metric_name'], test_score))\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test roc_auc_score 0.4889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLkl062SLtaF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}