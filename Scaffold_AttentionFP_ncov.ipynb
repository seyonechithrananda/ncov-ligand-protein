{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scaffold_AttentionFP_ncov.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "12r-Xf-8S1oVnyF6f-pEkOpfxotNEeSyp",
      "authorship_tag": "ABX9TyNv6S7TrV2h1yv+sGXnSm5C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seyonechithrananda/ncov-ligand-protein/blob/master/Scaffold_AttentionFP_ncov.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnhZ7HBLDf73",
        "colab_type": "code",
        "outputId": "05ece7d5-16ce-4121-9098-42413e84d776",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!wget -c https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!conda install -q -y -c conda-forge rdkit\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-06 06:02:20--  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.130.3, 104.16.131.3, 2606:4700::6810:8203, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.130.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 85055499 (81M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-latest-Linux-x86_64.sh’\n",
            "\n",
            "\r          Miniconda   0%[                    ]       0  --.-KB/s               \r         Miniconda3  47%[========>           ]  38.70M   190MB/s               \r        Miniconda3-  98%[==================> ]  79.76M   198MB/s               \rMiniconda3-latest-L 100%[===================>]  81.12M   198MB/s    in 0.4s    \n",
            "\n",
            "2020-05-06 06:02:21 (198 MB/s) - ‘Miniconda3-latest-Linux-x86_64.sh’ saved [85055499/85055499]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - _libgcc_mutex==0.1=main\n",
            "    - asn1crypto==1.3.0=py37_0\n",
            "    - ca-certificates==2020.1.1=0\n",
            "    - certifi==2019.11.28=py37_0\n",
            "    - cffi==1.14.0=py37h2e261b9_0\n",
            "    - chardet==3.0.4=py37_1003\n",
            "    - conda-package-handling==1.6.0=py37h7b6447c_0\n",
            "    - conda==4.8.2=py37_0\n",
            "    - cryptography==2.8=py37h1ba5d50_0\n",
            "    - idna==2.8=py37_0\n",
            "    - ld_impl_linux-64==2.33.1=h53a641e_7\n",
            "    - libedit==3.1.20181209=hc058e9b_0\n",
            "    - libffi==3.2.1=hd88cf55_4\n",
            "    - libgcc-ng==9.1.0=hdf63c60_0\n",
            "    - libstdcxx-ng==9.1.0=hdf63c60_0\n",
            "    - ncurses==6.2=he6710b0_0\n",
            "    - openssl==1.1.1d=h7b6447c_4\n",
            "    - pip==20.0.2=py37_1\n",
            "    - pycosat==0.6.3=py37h7b6447c_0\n",
            "    - pycparser==2.19=py37_0\n",
            "    - pyopenssl==19.1.0=py37_0\n",
            "    - pysocks==1.7.1=py37_0\n",
            "    - python==3.7.6=h0371630_2\n",
            "    - readline==7.0=h7b6447c_5\n",
            "    - requests==2.22.0=py37_1\n",
            "    - ruamel_yaml==0.15.87=py37h7b6447c_0\n",
            "    - setuptools==45.2.0=py37_0\n",
            "    - six==1.14.0=py37_0\n",
            "    - sqlite==3.31.1=h7b6447c_0\n",
            "    - tk==8.6.8=hbc83047_0\n",
            "    - tqdm==4.42.1=py_0\n",
            "    - urllib3==1.25.8=py37_0\n",
            "    - wheel==0.34.2=py37_0\n",
            "    - xz==5.2.4=h14c3975_4\n",
            "    - yaml==0.1.7=had09818_2\n",
            "    - zlib==1.2.11=h7b6447c_3\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
            "  asn1crypto         pkgs/main/linux-64::asn1crypto-1.3.0-py37_0\n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2020.1.1-0\n",
            "  certifi            pkgs/main/linux-64::certifi-2019.11.28-py37_0\n",
            "  cffi               pkgs/main/linux-64::cffi-1.14.0-py37h2e261b9_0\n",
            "  chardet            pkgs/main/linux-64::chardet-3.0.4-py37_1003\n",
            "  conda              pkgs/main/linux-64::conda-4.8.2-py37_0\n",
            "  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1.6.0-py37h7b6447c_0\n",
            "  cryptography       pkgs/main/linux-64::cryptography-2.8-py37h1ba5d50_0\n",
            "  idna               pkgs/main/linux-64::idna-2.8-py37_0\n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.33.1-h53a641e_7\n",
            "  libedit            pkgs/main/linux-64::libedit-3.1.20181209-hc058e9b_0\n",
            "  libffi             pkgs/main/linux-64::libffi-3.2.1-hd88cf55_4\n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-9.1.0-hdf63c60_0\n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-9.1.0-hdf63c60_0\n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.2-he6710b0_0\n",
            "  openssl            pkgs/main/linux-64::openssl-1.1.1d-h7b6447c_4\n",
            "  pip                pkgs/main/linux-64::pip-20.0.2-py37_1\n",
            "  pycosat            pkgs/main/linux-64::pycosat-0.6.3-py37h7b6447c_0\n",
            "  pycparser          pkgs/main/linux-64::pycparser-2.19-py37_0\n",
            "  pyopenssl          pkgs/main/linux-64::pyopenssl-19.1.0-py37_0\n",
            "  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py37_0\n",
            "  python             pkgs/main/linux-64::python-3.7.6-h0371630_2\n",
            "  readline           pkgs/main/linux-64::readline-7.0-h7b6447c_5\n",
            "  requests           pkgs/main/linux-64::requests-2.22.0-py37_1\n",
            "  ruamel_yaml        pkgs/main/linux-64::ruamel_yaml-0.15.87-py37h7b6447c_0\n",
            "  setuptools         pkgs/main/linux-64::setuptools-45.2.0-py37_0\n",
            "  six                pkgs/main/linux-64::six-1.14.0-py37_0\n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.31.1-h7b6447c_0\n",
            "  tk                 pkgs/main/linux-64::tk-8.6.8-hbc83047_0\n",
            "  tqdm               pkgs/main/noarch::tqdm-4.42.1-py_0\n",
            "  urllib3            pkgs/main/linux-64::urllib3-1.25.8-py37_0\n",
            "  wheel              pkgs/main/linux-64::wheel-0.34.2-py37_0\n",
            "  xz                 pkgs/main/linux-64::xz-5.2.4-h14c3975_4\n",
            "  yaml               pkgs/main/linux-64::yaml-0.1.7-had09818_2\n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.11-h7b6447c_3\n",
            "\n",
            "\n",
            "Preparing transaction: | \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - rdkit\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    boost-1.72.0               |   py37h9de70de_0         316 KB  conda-forge\n",
            "    boost-cpp-1.72.0           |       h8e57a91_0        21.8 MB  conda-forge\n",
            "    bzip2-1.0.8                |       h516909a_2         396 KB  conda-forge\n",
            "    ca-certificates-2020.4.5.1 |       hecc5488_0         146 KB  conda-forge\n",
            "    cairo-1.16.0               |    hcf35c78_1003         1.5 MB  conda-forge\n",
            "    certifi-2020.4.5.1         |   py37hc8dfbb8_0         151 KB  conda-forge\n",
            "    conda-4.8.3                |   py37hc8dfbb8_1         3.0 MB  conda-forge\n",
            "    fontconfig-2.13.1          |    h86ecdb6_1001         340 KB  conda-forge\n",
            "    freetype-2.10.1            |       he06d7ca_0         877 KB  conda-forge\n",
            "    gettext-0.19.8.1           |    hc5be6a0_1002         3.6 MB  conda-forge\n",
            "    glib-2.64.2                |       h6f030ca_0         3.4 MB  conda-forge\n",
            "    icu-64.2                   |       he1b5a44_1        12.6 MB  conda-forge\n",
            "    jpeg-9c                    |    h14c3975_1001         251 KB  conda-forge\n",
            "    libblas-3.8.0              |      14_openblas          10 KB  conda-forge\n",
            "    libcblas-3.8.0             |      14_openblas          10 KB  conda-forge\n",
            "    libgfortran-ng-7.3.0       |       hdf63c60_5         1.7 MB  conda-forge\n",
            "    libiconv-1.15              |    h516909a_1006         2.0 MB  conda-forge\n",
            "    liblapack-3.8.0            |      14_openblas          10 KB  conda-forge\n",
            "    libopenblas-0.3.7          |       h5ec1e0e_6         7.6 MB  conda-forge\n",
            "    libpng-1.6.37              |       hed695b0_1         308 KB  conda-forge\n",
            "    libtiff-4.1.0              |       hc7e4089_6         668 KB  conda-forge\n",
            "    libuuid-2.32.1             |    h14c3975_1000          26 KB  conda-forge\n",
            "    libwebp-base-1.1.0         |       h516909a_3         845 KB  conda-forge\n",
            "    libxcb-1.13                |    h14c3975_1002         396 KB  conda-forge\n",
            "    libxml2-2.9.10             |       hee79883_0         1.3 MB  conda-forge\n",
            "    lz4-c-1.8.3                |    he1b5a44_1001         187 KB  conda-forge\n",
            "    numpy-1.18.4               |   py37h8960a57_0         5.2 MB  conda-forge\n",
            "    olefile-0.46               |             py_0          31 KB  conda-forge\n",
            "    openssl-1.1.1g             |       h516909a_0         2.1 MB  conda-forge\n",
            "    pandas-1.0.3               |   py37h0da4684_1        11.1 MB  conda-forge\n",
            "    pcre-8.44                  |       he1b5a44_0         261 KB  conda-forge\n",
            "    pillow-7.1.2               |   py37hb39fc2d_0         603 KB\n",
            "    pixman-0.38.0              |    h516909a_1003         594 KB  conda-forge\n",
            "    pthread-stubs-0.4          |    h14c3975_1001           5 KB  conda-forge\n",
            "    pycairo-1.19.1             |   py37h01af8b0_3          77 KB  conda-forge\n",
            "    python-dateutil-2.8.1      |             py_0         220 KB  conda-forge\n",
            "    python_abi-3.7             |          1_cp37m           4 KB  conda-forge\n",
            "    pytz-2020.1                |     pyh9f0ad1d_0         227 KB  conda-forge\n",
            "    rdkit-2020.03.1            |   py37hdd87690_3        24.6 MB  conda-forge\n",
            "    xorg-kbproto-1.0.7         |    h14c3975_1002          26 KB  conda-forge\n",
            "    xorg-libice-1.0.10         |       h516909a_0          57 KB  conda-forge\n",
            "    xorg-libsm-1.2.3           |    h84519dc_1000          25 KB  conda-forge\n",
            "    xorg-libx11-1.6.9          |       h516909a_0         918 KB  conda-forge\n",
            "    xorg-libxau-1.0.9          |       h14c3975_0          13 KB  conda-forge\n",
            "    xorg-libxdmcp-1.1.3        |       h516909a_0          18 KB  conda-forge\n",
            "    xorg-libxext-1.3.4         |       h516909a_0          51 KB  conda-forge\n",
            "    xorg-libxrender-0.9.10     |    h516909a_1002          31 KB  conda-forge\n",
            "    xorg-renderproto-0.11.1    |    h14c3975_1002           8 KB  conda-forge\n",
            "    xorg-xextproto-7.3.0       |    h14c3975_1002          27 KB  conda-forge\n",
            "    xorg-xproto-7.0.31         |    h14c3975_1007          72 KB  conda-forge\n",
            "    zstd-1.4.4                 |       h3b9ef0a_2         982 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       110.7 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  boost              conda-forge/linux-64::boost-1.72.0-py37h9de70de_0\n",
            "  boost-cpp          conda-forge/linux-64::boost-cpp-1.72.0-h8e57a91_0\n",
            "  bzip2              conda-forge/linux-64::bzip2-1.0.8-h516909a_2\n",
            "  cairo              conda-forge/linux-64::cairo-1.16.0-hcf35c78_1003\n",
            "  fontconfig         conda-forge/linux-64::fontconfig-2.13.1-h86ecdb6_1001\n",
            "  freetype           conda-forge/linux-64::freetype-2.10.1-he06d7ca_0\n",
            "  gettext            conda-forge/linux-64::gettext-0.19.8.1-hc5be6a0_1002\n",
            "  glib               conda-forge/linux-64::glib-2.64.2-h6f030ca_0\n",
            "  icu                conda-forge/linux-64::icu-64.2-he1b5a44_1\n",
            "  jpeg               conda-forge/linux-64::jpeg-9c-h14c3975_1001\n",
            "  libblas            conda-forge/linux-64::libblas-3.8.0-14_openblas\n",
            "  libcblas           conda-forge/linux-64::libcblas-3.8.0-14_openblas\n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-7.3.0-hdf63c60_5\n",
            "  libiconv           conda-forge/linux-64::libiconv-1.15-h516909a_1006\n",
            "  liblapack          conda-forge/linux-64::liblapack-3.8.0-14_openblas\n",
            "  libopenblas        conda-forge/linux-64::libopenblas-0.3.7-h5ec1e0e_6\n",
            "  libpng             conda-forge/linux-64::libpng-1.6.37-hed695b0_1\n",
            "  libtiff            conda-forge/linux-64::libtiff-4.1.0-hc7e4089_6\n",
            "  libuuid            conda-forge/linux-64::libuuid-2.32.1-h14c3975_1000\n",
            "  libwebp-base       conda-forge/linux-64::libwebp-base-1.1.0-h516909a_3\n",
            "  libxcb             conda-forge/linux-64::libxcb-1.13-h14c3975_1002\n",
            "  libxml2            conda-forge/linux-64::libxml2-2.9.10-hee79883_0\n",
            "  lz4-c              conda-forge/linux-64::lz4-c-1.8.3-he1b5a44_1001\n",
            "  numpy              conda-forge/linux-64::numpy-1.18.4-py37h8960a57_0\n",
            "  olefile            conda-forge/noarch::olefile-0.46-py_0\n",
            "  pandas             conda-forge/linux-64::pandas-1.0.3-py37h0da4684_1\n",
            "  pcre               conda-forge/linux-64::pcre-8.44-he1b5a44_0\n",
            "  pillow             pkgs/main/linux-64::pillow-7.1.2-py37hb39fc2d_0\n",
            "  pixman             conda-forge/linux-64::pixman-0.38.0-h516909a_1003\n",
            "  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-h14c3975_1001\n",
            "  pycairo            conda-forge/linux-64::pycairo-1.19.1-py37h01af8b0_3\n",
            "  python-dateutil    conda-forge/noarch::python-dateutil-2.8.1-py_0\n",
            "  python_abi         conda-forge/linux-64::python_abi-3.7-1_cp37m\n",
            "  pytz               conda-forge/noarch::pytz-2020.1-pyh9f0ad1d_0\n",
            "  rdkit              conda-forge/linux-64::rdkit-2020.03.1-py37hdd87690_3\n",
            "  xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-h14c3975_1002\n",
            "  xorg-libice        conda-forge/linux-64::xorg-libice-1.0.10-h516909a_0\n",
            "  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.3-h84519dc_1000\n",
            "  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.6.9-h516909a_0\n",
            "  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.9-h14c3975_0\n",
            "  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.3-h516909a_0\n",
            "  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.4-h516909a_0\n",
            "  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.10-h516909a_1002\n",
            "  xorg-renderproto   conda-forge/linux-64::xorg-renderproto-0.11.1-h14c3975_1002\n",
            "  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h14c3975_1002\n",
            "  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h14c3975_1007\n",
            "  zstd               conda-forge/linux-64::zstd-1.4.4-h3b9ef0a_2\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates     pkgs/main::ca-certificates-2020.1.1-0 --> conda-forge::ca-certificates-2020.4.5.1-hecc5488_0\n",
            "  certifi              pkgs/main::certifi-2019.11.28-py37_0 --> conda-forge::certifi-2020.4.5.1-py37hc8dfbb8_0\n",
            "  conda                       pkgs/main::conda-4.8.2-py37_0 --> conda-forge::conda-4.8.3-py37hc8dfbb8_1\n",
            "  openssl              pkgs/main::openssl-1.1.1d-h7b6447c_4 --> conda-forge::openssl-1.1.1g-h516909a_0\n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxPjaPjsGNGb",
        "colab_type": "code",
        "outputId": "e820c03f-55a5-4333-ad51-9ff27c5bdd27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        }
      },
      "source": [
        "!conda install -c dglteam dgl-cuda10.1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - dgl-cuda10.1\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    blas-1.0                   |         openblas          46 KB\n",
            "    certifi-2020.4.5.1         |           py37_0         155 KB\n",
            "    decorator-4.4.2            |             py_0          14 KB\n",
            "    dgl-cuda10.1-0.4.3post2    |           py37_0        11.2 MB  dglteam\n",
            "    networkx-2.4               |             py_0         1.2 MB\n",
            "    scipy-1.4.1                |   py37habc2bb6_0        14.6 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        27.1 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  blas               pkgs/main/linux-64::blas-1.0-openblas\n",
            "  decorator          pkgs/main/noarch::decorator-4.4.2-py_0\n",
            "  dgl-cuda10.1       dglteam/linux-64::dgl-cuda10.1-0.4.3post2-py37_0\n",
            "  networkx           pkgs/main/noarch::networkx-2.4-py_0\n",
            "  scipy              pkgs/main/linux-64::scipy-1.4.1-py37habc2bb6_0\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  certifi            conda-forge::certifi-2020.4.5.1-py37h~ --> pkgs/main::certifi-2020.4.5.1-py37_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "decorator-4.4.2      | 14 KB     | : 100% 1.0/1 [00:00<00:00, 10.78it/s]\n",
            "blas-1.0             | 46 KB     | : 100% 1.0/1 [00:00<00:00, 23.20it/s]\n",
            "dgl-cuda10.1-0.4.3po | 11.2 MB   | : 100% 1.0/1 [00:06<00:00, 57.64s/it]               \n",
            "scipy-1.4.1          | 14.6 MB   | : 100% 1.0/1 [00:00<00:00,  2.26it/s]                \n",
            "certifi-2020.4.5.1   | 155 KB    | : 100% 1.0/1 [00:00<00:00, 18.05it/s]\n",
            "networkx-2.4         | 1.2 MB    | : 100% 1.0/1 [00:00<00:00,  3.53it/s]\n",
            "Preparing transaction: - \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ba7Nw5eGUTr",
        "colab_type": "code",
        "outputId": "816eaebb-12c4-4f6d-b292-5eac32d0e234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        }
      },
      "source": [
        "!conda install -c dglteam dgllife\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Solving environment: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - dgllife\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    dgllife-0.2.1              |           py37_0         132 KB  dglteam\n",
            "    joblib-0.14.1              |             py_0         201 KB\n",
            "    scikit-learn-0.22.1        |   py37h22eb022_0         5.3 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         5.6 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  dgllife            dglteam/linux-64::dgllife-0.2.1-py37_0\n",
            "  joblib             pkgs/main/noarch::joblib-0.14.1-py_0\n",
            "  scikit-learn       pkgs/main/linux-64::scikit-learn-0.22.1-py37h22eb022_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "scikit-learn-0.22.1  | 5.3 MB    | : 100% 1.0/1 [00:00<00:00,  1.15it/s]                \n",
            "dgllife-0.2.1        | 132 KB    | : 100% 1.0/1 [00:01<00:00,  3.40s/it]                \n",
            "joblib-0.14.1        | 201 KB    | : 100% 1.0/1 [00:00<00:00, 19.57it/s]\n",
            "Preparing transaction: - \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\bdone\n",
            "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqRn8KexGdiL",
        "colab_type": "code",
        "outputId": "5298234c-0de5-4c4f-a4d5-cf77a46b2e07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        }
      },
      "source": [
        "!conda install pandas"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Solving environment: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - pandas\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    conda-4.8.3                |           py37_0         2.8 MB\n",
            "    openssl-1.1.1g             |       h7b6447c_0         2.5 MB\n",
            "    pandas-1.0.3               |   py37h0573a6f_0         8.6 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        13.9 MB\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  ca-certificates    conda-forge::ca-certificates-2020.4.5~ --> pkgs/main::ca-certificates-2020.1.1-0\n",
            "  conda              conda-forge::conda-4.8.3-py37hc8dfbb8~ --> pkgs/main::conda-4.8.3-py37_0\n",
            "  openssl            conda-forge::openssl-1.1.1g-h516909a_0 --> pkgs/main::openssl-1.1.1g-h7b6447c_0\n",
            "  pandas             conda-forge::pandas-1.0.3-py37h0da468~ --> pkgs/main::pandas-1.0.3-py37h0573a6f_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "openssl-1.1.1g       | 2.5 MB    | : 100% 1.0/1 [00:00<00:00,  4.97it/s]                \n",
            "pandas-1.0.3         | 8.6 MB    | : 100% 1.0/1 [00:00<00:00,  2.42it/s]               \n",
            "conda-4.8.3          | 2.8 MB    | : 100% 1.0/1 [00:00<00:00,  6.45it/s]\n",
            "Preparing transaction: - \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\bdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVKD7kwtHDUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "import sys \n",
        "import pandas as pd\n",
        "\n",
        "# train --> balanced dataset\n",
        "dataset_train_file = \"/content/drive/My Drive/Project De Novo/AID1706_binarized_sars_full_eval_actives_12k_samples.csv\"\n",
        "dataset_eval_file = \"/content/drive/My Drive/Project De Novo/mpro_xchem.csv\"\n",
        "dataset_train = pd.read_csv(dataset_train_file)\n",
        "dataset_eval = pd.read_csv(dataset_eval_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy7_zDz9LCq2",
        "colab_type": "code",
        "outputId": "af1cf14e-c143-4d16-834d-71ee02a2befe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "dataset_train.head"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                                   smiles  activity\n",
              "0      C1CC(C1)C(=O)NC2=CC=C(C=C2)N(C(C3=CC(=CC=C3)F)...         1\n",
              "1      CC(C)(C)C1=CC=C(C=C1)N(C(C2=CN=NC=C2)C(=O)NC(C...         1\n",
              "2      CC(C)(C)NC(=O)C(C1=CSC=C1)N(C2=CC=C(C=C2)N)C(=...         1\n",
              "3      CC(C)C(=O)NC1=CC=C(C=C1)N(C(C2=CSC=C2)C(=O)NC(...         1\n",
              "4      CC(C)C(=O)NC1=CC=C(C=C1)N(CC2=CSC=C2)C(=O)CN3C...         1\n",
              "...                                                  ...       ...\n",
              "11994                               C1=CC2=C(C=C1N)NN=C2         0\n",
              "11995  CC(=O)[C@H]1CC[C@@H]2[C@@]1(CC(=O)[C@H]3[C@H]2...         0\n",
              "11996                       C1CN(CCN1CC(CO)O)C2=CC=CC=C2         0\n",
              "11997  CCOC(=O)N1CCC(=C2C3=C(CCC4=C2N=CC=C4)C=C(C=C3)...         0\n",
              "11998                    C1=CC2=C(C=C1OC(F)(F)F)SC(=N2)N         0\n",
              "\n",
              "[11999 rows x 2 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBnPl4isM1FQ",
        "colab_type": "code",
        "outputId": "01f52c7a-a46f-4adc-fad4-e63e0fa2ccdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "dataset_eval.head"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                 smiles  activity\n",
              "0      OC=1C=CC=CC1CNC2=NC=3C=CC=CC3N2         1\n",
              "1        CC(=O)NCCC1=CNC=2C=CC(F)=CC12         1\n",
              "2    O=C([C@@H]1[C@H](C2=CSC=C2)CCC1)N         1\n",
              "3       CN1CCCC=2C=CC(=CC12)S(=O)(=O)N         1\n",
              "4     CC(=O)NC=1C=CC(OC=2N=CC=CN2)=CC1         1\n",
              "..                                 ...       ...\n",
              "875   CC(C)C=1C=CC(NC(=O)N2CCOCC2)=CC1         0\n",
              "876        CN(CC(=O)O)C(=O)C=1C=CC=CN1         0\n",
              "877  CN1CCN(CC1)C(=O)C=2C=CC(F)=C(F)C2         0\n",
              "878      FC=1C=CC=C(F)C1C(=O)N2CCCCCC2         0\n",
              "879             FC=1C=CC=NC1NCC2CCOCC2         0\n",
              "\n",
              "[880 rows x 2 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1TtgCu8U26K",
        "colab_type": "code",
        "outputId": "added698-f3d5-461f-9a57-2dcad31145cf",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-24d80d97-91f3-4344-8131-649b28d72a93\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-24d80d97-91f3-4344-8131-649b28d72a93\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving utils.py to utils.py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'utils.py': b'import dgl\\nimport numpy as np\\nimport random\\nimport torch\\n\\nfrom dgllife.utils.featurizers import one_hot_encoding\\nfrom dgllife.utils.splitters import RandomSplitter\\n\\ndef set_random_seed(seed=0):\\n    \"\"\"Set random seed.\\n    Parameters\\n    ----------\\n    seed : int\\n        Random seed to use\\n    \"\"\"\\n    random.seed(seed)\\n    np.random.seed(seed)\\n    torch.manual_seed(seed)\\n    if torch.cuda.is_available():\\n        torch.cuda.manual_seed(seed)\\n\\n\\ndef load_dataset_for_classification(args):\\n    \"\"\"Load dataset for classification tasks.\\n    Parameters\\n    ----------\\n    args : dict\\n        Configurations.\\n    Returns\\n    -------\\n    dataset\\n        The whole dataset.\\n    train_set\\n        Subset for training.\\n    val_set\\n        Subset for validation.\\n    test_set\\n        Subset for test.\\n    \"\"\"\\n    assert args[\\'dataset\\'] in [\\'Tox21\\']\\n    if args[\\'dataset\\'] == \\'Tox21\\':\\n        from dgllife.data import Tox21\\n        dataset = Tox21(smiles_to_graph=args[\\'smiles_to_graph\\'],\\n                        node_featurizer=args.get(\\'node_featurizer\\', None),\\n                        edge_featurizer=args.get(\\'edge_featurizer\\', None))\\n        train_set, val_set, test_set = RandomSplitter.train_val_test_split(\\n            dataset, frac_train=args[\\'frac_train\\'], frac_val=args[\\'frac_val\\'],\\n            frac_test=args[\\'frac_test\\'], random_state=args[\\'random_seed\\'])\\n\\n    return dataset, train_set, val_set, test_set\\n\\n\\ndef load_dataset_for_regression(args):\\n    \"\"\"Load dataset for regression tasks.\\n    Parameters\\n    ----------\\n    args : dict\\n        Configurations.\\n    Returns\\n    -------\\n    train_set\\n        Subset for training.\\n    val_set\\n        Subset for validation.\\n    test_set\\n        Subset for test.\\n    \"\"\"\\n    assert args[\\'dataset\\'] in [\\'Alchemy\\', \\'Aromaticity\\']\\n\\n    if args[\\'dataset\\'] == \\'Alchemy\\':\\n        from dgllife.data import TencentAlchemyDataset\\n        train_set = TencentAlchemyDataset(mode=\\'dev\\')\\n        val_set = TencentAlchemyDataset(mode=\\'valid\\')\\n        test_set = None\\n\\n    if args[\\'dataset\\'] == \\'Aromaticity\\':\\n        from dgllife.data import PubChemBioAssayAromaticity\\n        dataset = PubChemBioAssayAromaticity(smiles_to_graph=args[\\'smiles_to_graph\\'],\\n                                             node_featurizer=args.get(\\'node_featurizer\\', None),\\n                                             edge_featurizer=args.get(\\'edge_featurizer\\', None))\\n        train_set, val_set, test_set = RandomSplitter.train_val_test_split(\\n            dataset, frac_train=args[\\'frac_train\\'], frac_val=args[\\'frac_val\\'],\\n            frac_test=args[\\'frac_test\\'], random_state=args[\\'random_seed\\'])\\n\\n    return train_set, val_set, test_set\\n\\n\\ndef collate_molgraphs(data):\\n    \"\"\"Batching a list of datapoints for dataloader.\\n    Parameters\\n    ----------\\n    data : list of 3-tuples or 4-tuples.\\n        Each tuple is for a single datapoint, consisting of\\n        a SMILES, a DGLGraph, all-task labels and optionally\\n        a binary mask indicating the existence of labels.\\n    Returns\\n    -------\\n    smiles : list\\n        List of smiles\\n    bg : DGLGraph\\n        The batched DGLGraph.\\n    labels : Tensor of dtype float32 and shape (B, T)\\n        Batched datapoint labels. B is len(data) and\\n        T is the number of total tasks.\\n    masks : Tensor of dtype float32 and shape (B, T)\\n        Batched datapoint binary mask, indicating the\\n        existence of labels. If binary masks are not\\n        provided, return a tensor with ones.\\n    \"\"\"\\n    assert len(data[0]) in [3, 4], \\\\\\n        \\'Expect the tuple to be of length 3 or 4, got {:d}\\'.format(len(data[0]))\\n    if len(data[0]) == 3:\\n        smiles, graphs, labels = map(list, zip(*data))\\n        masks = None\\n    else:\\n        smiles, graphs, labels, masks = map(list, zip(*data))\\n\\n    bg = dgl.batch(graphs)\\n    bg.set_n_initializer(dgl.init.zero_initializer)\\n    bg.set_e_initializer(dgl.init.zero_initializer)\\n    labels = torch.stack(labels, dim=0)\\n\\n    if masks is None:\\n        masks = torch.ones(labels.shape)\\n    else:\\n        masks = torch.stack(masks, dim=0)\\n    return smiles, bg, labels, masks\\n\\n\\ndef load_model(args):\\n    if args[\\'model\\'] == \\'GCN\\':\\n        from dgllife.model import GCNPredictor\\n        model = GCNPredictor(in_feats=args[\\'node_featurizer\\'].feat_size(),\\n                             hidden_feats=args[\\'gcn_hidden_feats\\'],\\n                             classifier_hidden_feats=args[\\'classifier_hidden_feats\\'],\\n                             n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'GAT\\':\\n        from dgllife.model import GATPredictor\\n        model = GATPredictor(in_feats=args[\\'node_featurizer\\'].feat_size(),\\n                             hidden_feats=args[\\'gat_hidden_feats\\'],\\n                             num_heads=args[\\'num_heads\\'],\\n                             classifier_hidden_feats=args[\\'classifier_hidden_feats\\'],\\n                             n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'Weave\\':\\n        from dgllife.model import WeavePredictor\\n        model = WeavePredictor(node_in_feats=args[\\'node_featurizer\\'].feat_size(),\\n                               edge_in_feats=args[\\'edge_featurizer\\'].feat_size(),\\n                               num_gnn_layers=args[\\'num_gnn_layers\\'],\\n                               gnn_hidden_feats=args[\\'gnn_hidden_feats\\'],\\n                               graph_feats=args[\\'graph_feats\\'],\\n                               n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'AttentiveFP\\':\\n        from dgllife.model import AttentiveFPPredictor\\n        model = AttentiveFPPredictor(node_feat_size=args[\\'node_featurizer\\'].feat_size(),\\n                                     edge_feat_size=args[\\'edge_featurizer\\'].feat_size(),\\n                                     num_layers=args[\\'num_layers\\'],\\n                                     num_timesteps=args[\\'num_timesteps\\'],\\n                                     graph_feat_size=args[\\'graph_feat_size\\'],\\n                                     n_tasks=args[\\'n_tasks\\'],\\n                                     dropout=args[\\'dropout\\'])\\n\\n    if args[\\'model\\'] == \\'SchNet\\':\\n        from dgllife.model import SchNetPredictor\\n        model = SchNetPredictor(node_feats=args[\\'node_feats\\'],\\n                                hidden_feats=args[\\'hidden_feats\\'],\\n                                classifier_hidden_feats=args[\\'classifier_hidden_feats\\'],\\n                                n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'MGCN\\':\\n        from dgllife.model import MGCNPredictor\\n        model = MGCNPredictor(feats=args[\\'feats\\'],\\n                              n_layers=args[\\'n_layers\\'],\\n                              classifier_hidden_feats=args[\\'classifier_hidden_feats\\'],\\n                              n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'MPNN\\':\\n        from dgllife.model import MPNNPredictor\\n        model = MPNNPredictor(node_in_feats=args[\\'node_in_feats\\'],\\n                              edge_in_feats=args[\\'edge_in_feats\\'],\\n                              node_out_feats=args[\\'node_out_feats\\'],\\n                              edge_hidden_feats=args[\\'edge_hidden_feats\\'],\\n                              n_tasks=args[\\'n_tasks\\'])\\n\\n    return model\\n\\n\\ndef chirality(atom):\\n    try:\\n        return one_hot_encoding(atom.GetProp(\\'_CIPCode\\'), [\\'R\\', \\'S\\']) + \\\\\\n               [atom.HasProp(\\'_ChiralityPossible\\')]\\n    except:\\n        return [False, False] + [atom.HasProp(\\'_ChiralityPossible\\')]\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVrUoqtTHGy8",
        "colab_type": "code",
        "outputId": "56dbbd6b-2e01-41fc-b249-6d0274477a8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "from dgllife.data import MoleculeCSVDataset\n",
        "from dgllife.data.csv_dataset import *\n",
        "from dgllife.utils.featurizers import *\n",
        "from dgllife.utils.mol_to_graph import *\n",
        "\n",
        "from dgllife.utils import ConcatFeaturizer\n",
        "# node featurization\n",
        "from dgllife.utils import CanonicalAtomFeaturizer, BaseAtomFeaturizer, atom_type_one_hot, atom_degree_one_hot, atom_formal_charge, atom_num_radical_electrons, atom_hybridization_one_hot, atom_total_num_H_one_hot\n",
        "# edge featurization\n",
        "from dgllife.utils.featurizers import BaseBondFeaturizer\n",
        "from functools import partial\n",
        "from utils import chirality\n",
        "\n",
        "\n",
        "# featurize bigraph/molecular graph set for train (SARS-COV-1) set\n",
        "train_set = MoleculeCSVDataset(dataset_train, smiles_to_graph=smiles_to_bigraph, node_featurizer=BaseAtomFeaturizer(featurizer_funcs={'hv': ConcatFeaturizer([\n",
        "            partial(atom_type_one_hot, allowable_set=[\n",
        "                'B', 'C', 'N', 'O', 'F', 'Si', 'P', 'S', 'Cl', 'As', 'Se', 'Br', 'Te', 'I', 'At'],\n",
        "                    encode_unknown=True),\n",
        "            partial(atom_degree_one_hot, allowable_set=list(range(6))),\n",
        "            atom_formal_charge, atom_num_radical_electrons,\n",
        "            partial(atom_hybridization_one_hot, encode_unknown=True),\n",
        "            lambda atom: [0], # A placeholder for aromatic information,\n",
        "            atom_total_num_H_one_hot, chirality],)}),\n",
        "            edge_featurizer=BaseBondFeaturizer({'he': lambda bond: [0 for _ in range(10)]}), \n",
        "            smiles_column='smiles', cache_file_path='/content/drive/My Drive/Project De Novo/AttentionFP/train.bin', task_names=['activity'])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing dgl graphs from scratch...\n",
            "Processing molecule 1000/11999\n",
            "Processing molecule 2000/11999\n",
            "Processing molecule 3000/11999\n",
            "Processing molecule 4000/11999\n",
            "Processing molecule 5000/11999\n",
            "Processing molecule 6000/11999\n",
            "Processing molecule 7000/11999\n",
            "Processing molecule 8000/11999\n",
            "Processing molecule 9000/11999\n",
            "Processing molecule 10000/11999\n",
            "Processing molecule 11000/11999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofwpNYUXJASi",
        "colab_type": "code",
        "outputId": "bf026dd3-6218-4a18-feed-abdeed39d59d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# featurize bigraph/molecular graph set for test (SARS-COV-2) set\n",
        "test_set = MoleculeCSVDataset(dataset_eval, smiles_to_graph=smiles_to_bigraph, node_featurizer=BaseAtomFeaturizer(featurizer_funcs={'hv': ConcatFeaturizer([\n",
        "            partial(atom_type_one_hot, allowable_set=[\n",
        "                'B', 'C', 'N', 'O', 'F', 'Si', 'P', 'S', 'Cl', 'As', 'Se', 'Br', 'Te', 'I', 'At'],\n",
        "                    encode_unknown=True),\n",
        "            partial(atom_degree_one_hot, allowable_set=list(range(6))),\n",
        "            atom_formal_charge, atom_num_radical_electrons,\n",
        "            partial(atom_hybridization_one_hot, encode_unknown=True),\n",
        "            lambda atom: [0], # A placeholder for aromatic information,\n",
        "            atom_total_num_H_one_hot, chirality],)}),\n",
        "            edge_featurizer=BaseBondFeaturizer({'he': lambda bond: [0 for _ in range(10)]}), smiles_column='smiles', cache_file_path='/content/drive/My Drive/Project De Novo/AttentionFP/test.bin', task_names=['activity'])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing dgl graphs from scratch...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKMgchzflhiW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = {\n",
        "    'random_seed': 8,\n",
        "    'graph_feat_size': 200,\n",
        "    'num_layers': 2,\n",
        "    'num_timesteps': 2,\n",
        "    'node_feat_size': 39,\n",
        "    'edge_feat_size': 10,\n",
        "    'node_data_field':'hv',\n",
        "    'edge_data_field':'he',\n",
        "    'n_tasks': 1,\n",
        "    'dropout': 0.2,\n",
        "    'weight_decay': 10 ** (-5.0),\n",
        "    'lr': 10 ** (-2.5),\n",
        "    'batch_size': 128,\n",
        "    'num_epochs': 800,\n",
        "    'frac_train': 0.8,\n",
        "    'frac_val': 0.1,\n",
        "    'frac_test': 0.1,\n",
        "    'patience': 80,\n",
        "    'model' : 'AttentiveFPPredictor',\n",
        "    'metric_name': 'roc_auc_score',\n",
        "    'smiles_to_graph': smiles_to_bigraph,\n",
        "    # Follow the atom featurization in the original work\n",
        "    'node_featurizer': BaseAtomFeaturizer(\n",
        "        featurizer_funcs={'hv': ConcatFeaturizer([\n",
        "            partial(atom_type_one_hot, allowable_set=[\n",
        "                'B', 'C', 'N', 'O', 'F', 'Si', 'P', 'S', 'Cl', 'As', 'Se', 'Br', 'Te', 'I', 'At'],\n",
        "                    encode_unknown=True),\n",
        "            partial(atom_degree_one_hot, allowable_set=list(range(6))),\n",
        "            atom_formal_charge, atom_num_radical_electrons,\n",
        "            partial(atom_hybridization_one_hot, encode_unknown=True),\n",
        "            lambda atom: [0], # A placeholder for aromatic information,\n",
        "            atom_total_num_H_one_hot, chirality\n",
        "        ],\n",
        "        )}\n",
        "    ),\n",
        "    'edge_featurizer': BaseBondFeaturizer({\n",
        "        'he': lambda bond: [0 for _ in range(10)]\n",
        "    })\n",
        "}\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuZkFAz-PDvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import utils\n",
        "\n",
        "from dgllife.model import load_pretrained\n",
        "from dgllife.utils import EarlyStopping, Meter\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "from utils import set_random_seed, load_dataset_for_classification, collate_molgraphs, load_model\n",
        "\n",
        "from dgllife.model import MPNNPredictor\n",
        "\n",
        "args['device'] = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "set_random_seed(args['random_seed'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJf2RePJHUcx",
        "colab_type": "code",
        "outputId": "cbc56312-83be-465b-cd7b-55732659ecc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        }
      },
      "source": [
        "from dgllife.utils.splitters import ScaffoldSplitter\n",
        "\n",
        "train_scaffold_set, val_set, test_scaffold_set = ScaffoldSplitter.train_val_test_split(train_set, frac_train=0.8, frac_val=0.2,frac_test=0.0)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start initializing RDKit molecule instances...\n",
            "Creating RDKit molecule instance 1000/11999\n",
            "Creating RDKit molecule instance 2000/11999\n",
            "Creating RDKit molecule instance 3000/11999\n",
            "Creating RDKit molecule instance 4000/11999\n",
            "Creating RDKit molecule instance 5000/11999\n",
            "Creating RDKit molecule instance 6000/11999\n",
            "Creating RDKit molecule instance 7000/11999\n",
            "Creating RDKit molecule instance 8000/11999\n",
            "Creating RDKit molecule instance 9000/11999\n",
            "Creating RDKit molecule instance 10000/11999\n",
            "Creating RDKit molecule instance 11000/11999\n",
            "Start computing Bemis-Murcko scaffolds.\n",
            "Computing Bemis-Murcko for compound 1000/11999\n",
            "Computing Bemis-Murcko for compound 2000/11999\n",
            "Computing Bemis-Murcko for compound 3000/11999\n",
            "Computing Bemis-Murcko for compound 4000/11999\n",
            "Computing Bemis-Murcko for compound 5000/11999\n",
            "Computing Bemis-Murcko for compound 6000/11999\n",
            "Computing Bemis-Murcko for compound 7000/11999\n",
            "Computing Bemis-Murcko for compound 8000/11999\n",
            "Computing Bemis-Murcko for compound 9000/11999\n",
            "Computing Bemis-Murcko for compound 10000/11999\n",
            "Computing Bemis-Murcko for compound 11000/11999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCmuQegvJMcq",
        "colab_type": "code",
        "outputId": "a8281cdc-8f0e-445d-f83f-cb44bc6c3c1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "print (len(train_set))\n",
        "print(len(train_scaffold_set))\n",
        "print (len(val_set))\n",
        "print (len(test_set))\n",
        "print(train_set[1])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11999\n",
            "9599\n",
            "2400\n",
            "880\n",
            "('CC(C)(C)C1=CC=C(C=C1)N(C(C2=CN=NC=C2)C(=O)NC(C)(C)C)C(=O)C3=CC=CO3', DGLGraph(num_nodes=32, num_edges=68,\n",
            "         ndata_schemes={'hv': Scheme(shape=(39,), dtype=torch.float32)}\n",
            "         edata_schemes={'he': Scheme(shape=(10,), dtype=torch.float32)}), tensor([1.]), tensor([1.]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YjAjoUyLEjT",
        "colab_type": "code",
        "outputId": "c16b2e49-40ef-4417-a739-958227439964",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(type(train_set))\n",
        "print(type(train_scaffold_set))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'dgllife.data.csv_dataset.MoleculeCSVDataset'>\n",
            "<class 'dgl.data.utils.Subset'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYfHiYl0T8V4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_scaffold_set,  batch_size=args['batch_size'],\n",
        "                              collate_fn=collate_molgraphs)\n",
        "val_loader = DataLoader(val_set,  batch_size=args['batch_size'],\n",
        "                              collate_fn=collate_molgraphs)\n",
        "\n",
        "test_loader = DataLoader(test_set,  batch_size=args['batch_size'],\n",
        "                          collate_fn=collate_molgraphs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzUMvJO8Qx85",
        "colab_type": "code",
        "outputId": "17d08832-5b1e-4038-dedf-23a2a924af49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(len(test_loader))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jV6QrE0ltd7M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cc4eba02-1870-4678-a42f-8af2601f582d"
      },
      "source": [
        "print(args['node_featurizer'].feat_size('hv'))\n",
        "print(args['edge_featurizer'].feat_size('he'))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39\n",
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGAUIdpRUAUu",
        "colab_type": "code",
        "outputId": "eeba6934-faf4-4cc6-ef44-2e22d9eb2414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "args['n_tasks'] = 1\n",
        "\n",
        "from dgllife.model import AttentiveFPPredictor\n",
        "\n",
        "model = AttentiveFPPredictor(node_feat_size=args['node_featurizer'].feat_size('hv'),\n",
        "                              edge_feat_size=args['edge_featurizer'].feat_size('he'),\n",
        "                              num_layers=args['num_layers'],\n",
        "                              num_timesteps=args['num_timesteps'],\n",
        "                              graph_feat_size=args['graph_feat_size'],\n",
        "                              n_tasks=args['n_tasks'],\n",
        "                              dropout=args['dropout'])\n",
        "\n",
        "'''\n",
        "\n",
        "model = GATPredictor(in_feats=args['node_featurizer'].feat_size('h'),\n",
        "                             hidden_feats=args['gat_hidden_feats'],\n",
        "                             num_heads=args['num_heads'],\n",
        "                             classifier_hidden_feats=args['classifier_hidden_feats'],\n",
        "                             n_tasks=args['n_tasks'])\n",
        "\n",
        "'''\n",
        "\n",
        "import dgl.backend as F\n",
        "\n",
        "train_num_pos = F.sum(train_set.labels, dim=0)\n",
        "train_num_indices = F.sum(train_set.mask, dim=0)\n",
        "train_task_pos_weights = (train_num_indices - train_num_pos) / train_num_pos\n",
        "\n",
        "loss_criterion = BCEWithLogitsLoss(pos_weight=train_task_pos_weights.to(args['device']),\n",
        "                                    reduction='none')\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=args['lr'])\n",
        "stopper = EarlyStopping(patience=args['patience'], mode='higher', filename='/content/drive/My Drive/Project De Novo/AttentionFP/train.pth')\n",
        "model.to(args['device'])\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AttentiveFPPredictor(\n",
              "  (gnn): AttentiveFPGNN(\n",
              "    (init_context): GetContext(\n",
              "      (project_node): Sequential(\n",
              "        (0): Linear(in_features=39, out_features=200, bias=True)\n",
              "        (1): LeakyReLU(negative_slope=0.01)\n",
              "      )\n",
              "      (project_edge1): Sequential(\n",
              "        (0): Linear(in_features=49, out_features=200, bias=True)\n",
              "        (1): LeakyReLU(negative_slope=0.01)\n",
              "      )\n",
              "      (project_edge2): Sequential(\n",
              "        (0): Dropout(p=0.2, inplace=False)\n",
              "        (1): Linear(in_features=400, out_features=1, bias=True)\n",
              "        (2): LeakyReLU(negative_slope=0.01)\n",
              "      )\n",
              "      (attentive_gru): AttentiveGRU1(\n",
              "        (edge_transform): Sequential(\n",
              "          (0): Dropout(p=0.2, inplace=False)\n",
              "          (1): Linear(in_features=200, out_features=200, bias=True)\n",
              "        )\n",
              "        (gru): GRUCell(200, 200)\n",
              "      )\n",
              "    )\n",
              "    (gnn_layers): ModuleList(\n",
              "      (0): GNNLayer(\n",
              "        (project_edge): Sequential(\n",
              "          (0): Dropout(p=0.2, inplace=False)\n",
              "          (1): Linear(in_features=400, out_features=1, bias=True)\n",
              "          (2): LeakyReLU(negative_slope=0.01)\n",
              "        )\n",
              "        (attentive_gru): AttentiveGRU2(\n",
              "          (project_node): Sequential(\n",
              "            (0): Dropout(p=0.2, inplace=False)\n",
              "            (1): Linear(in_features=200, out_features=200, bias=True)\n",
              "          )\n",
              "          (gru): GRUCell(200, 200)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (readout): AttentiveFPReadout(\n",
              "    (readouts): ModuleList(\n",
              "      (0): GlobalPool(\n",
              "        (compute_logits): Sequential(\n",
              "          (0): Linear(in_features=400, out_features=1, bias=True)\n",
              "          (1): LeakyReLU(negative_slope=0.01)\n",
              "        )\n",
              "        (project_nodes): Sequential(\n",
              "          (0): Dropout(p=0.2, inplace=False)\n",
              "          (1): Linear(in_features=200, out_features=200, bias=True)\n",
              "        )\n",
              "        (gru): GRUCell(200, 200)\n",
              "      )\n",
              "      (1): GlobalPool(\n",
              "        (compute_logits): Sequential(\n",
              "          (0): Linear(in_features=400, out_features=1, bias=True)\n",
              "          (1): LeakyReLU(negative_slope=0.01)\n",
              "        )\n",
              "        (project_nodes): Sequential(\n",
              "          (0): Dropout(p=0.2, inplace=False)\n",
              "          (1): Linear(in_features=200, out_features=200, bias=True)\n",
              "        )\n",
              "        (gru): GRUCell(200, 200)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (predict): Sequential(\n",
              "    (0): Dropout(p=0.2, inplace=False)\n",
              "    (1): Linear(in_features=200, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7Fdo_NxLPWz",
        "colab_type": "code",
        "outputId": "c398ad3a-0c4e-4f35-c2f9-58d768e98355",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(train_task_pos_weights)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([25.9036])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C941OCzRuXH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def predict(args, model, bg):\n",
        "    node_feats = bg.ndata.pop(args['node_data_field']).to(args['device'])\n",
        "    if args.get('edge_featurizer', None) is not None:\n",
        "        edge_feats = bg.edata.pop(args['edge_data_field']).to(args['device'])\n",
        "        return model(bg, node_feats, edge_feats)\n",
        "    else:\n",
        "        return model(bg, node_feats)\n",
        "\n",
        "def run_a_train_epoch(args, epoch, model, data_loader, loss_criterion, optimizer):\n",
        "    model.train()\n",
        "    train_meter = Meter()\n",
        "    for batch_id, batch_data in enumerate(data_loader):\n",
        "        smiles, bg, labels, masks = batch_data\n",
        "        labels, masks = labels.to(args['device']), masks.to(args['device'])\n",
        "        logits = predict(args, model, bg)\n",
        "        # Mask non-existing labels\n",
        "        loss = (loss_criterion(logits, labels) * (masks != 0).float()).mean()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print('epoch {:d}/{:d}, batch {:d}/{:d}, loss {:.4f}'.format(\n",
        "            epoch + 1, args['num_epochs'], batch_id + 1, len(data_loader), loss.item()))\n",
        "        train_meter.update(logits, labels, masks)\n",
        "    train_score = np.mean(train_meter.compute_metric(args['metric_name']))\n",
        "    print('epoch {:d}/{:d}, training {} {:.4f}'.format(\n",
        "        epoch + 1, args['num_epochs'], args['metric_name'], train_score))\n",
        "\n",
        "def run_an_eval_epoch(args, model, data_loader):\n",
        "    model.eval()\n",
        "    eval_meter = Meter()\n",
        "    with torch.no_grad():\n",
        "        for batch_id, batch_data in enumerate(data_loader):\n",
        "            smiles, bg, labels, masks = batch_data\n",
        "            labels = labels.to(args['device'])\n",
        "            logits = predict(args, model, bg)\n",
        "            eval_meter.update(logits, labels, masks)\n",
        "    return np.mean(eval_meter.compute_metric(args['metric_name']))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLkl062SLtaF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "6263f922-3531-4706-fe10-89227bec52b6"
      },
      "source": [
        "args['device'] = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "args"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 128,\n",
              " 'device': device(type='cuda'),\n",
              " 'dropout': 0.2,\n",
              " 'edge_data_field': 'he',\n",
              " 'edge_feat_size': 10,\n",
              " 'edge_featurizer': <dgllife.utils.featurizers.BaseBondFeaturizer at 0x7f9b63ac5ef0>,\n",
              " 'frac_test': 0.1,\n",
              " 'frac_train': 0.8,\n",
              " 'frac_val': 0.1,\n",
              " 'graph_feat_size': 200,\n",
              " 'lr': 0.0031622776601683794,\n",
              " 'metric_name': 'roc_auc_score',\n",
              " 'model': 'AttentiveFPPredictor',\n",
              " 'n_tasks': 1,\n",
              " 'node_data_field': 'hv',\n",
              " 'node_feat_size': 39,\n",
              " 'node_featurizer': <dgllife.utils.featurizers.BaseAtomFeaturizer at 0x7f9b63ac5128>,\n",
              " 'num_epochs': 800,\n",
              " 'num_layers': 2,\n",
              " 'num_timesteps': 2,\n",
              " 'patience': 80,\n",
              " 'random_seed': 8,\n",
              " 'smiles_to_graph': <function dgllife.utils.mol_to_graph.smiles_to_bigraph>,\n",
              " 'weight_decay': 1e-05}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73ZBLfQxuHQA",
        "colab_type": "code",
        "outputId": "2c132d5f-c542-4975-fe9b-b97936522fa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(args['num_epochs']):\n",
        "        # Train\n",
        "        run_a_train_epoch(args, epoch, model, train_loader, loss_criterion, optimizer)\n",
        "\n",
        "        # Validation and early stop\n",
        "        val_score = run_an_eval_epoch(args, model, val_loader)\n",
        "        early_stop = stopper.step(val_score, model)\n",
        "        print('epoch {:d}/{:d}, validation {} {:.4f}, best validation {} {:.4f}'.format(\n",
        "            epoch + 1, args['num_epochs'], args['metric_name'],\n",
        "            val_score, args['metric_name'], stopper.best_score))\n",
        "        if early_stop:\n",
        "            break"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero(Tensor input, *, Tensor out)\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(Tensor input, *, bool as_tuple)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "epoch 112/800, batch 72/75, loss 0.3523\n",
            "epoch 112/800, batch 73/75, loss 0.2296\n",
            "epoch 112/800, batch 74/75, loss 0.2499\n",
            "epoch 112/800, batch 75/75, loss 0.1678\n",
            "epoch 112/800, training roc_auc_score 0.8814\n",
            "EarlyStopping counter: 16 out of 80\n",
            "epoch 112/800, validation roc_auc_score 0.7778, best validation roc_auc_score 0.8157\n",
            "epoch 113/800, batch 1/75, loss 0.7658\n",
            "epoch 113/800, batch 2/75, loss 0.7350\n",
            "epoch 113/800, batch 3/75, loss 0.3124\n",
            "epoch 113/800, batch 4/75, loss 0.5193\n",
            "epoch 113/800, batch 5/75, loss 0.2121\n",
            "epoch 113/800, batch 6/75, loss 0.1496\n",
            "epoch 113/800, batch 7/75, loss 0.1643\n",
            "epoch 113/800, batch 8/75, loss 1.9297\n",
            "epoch 113/800, batch 9/75, loss 0.1326\n",
            "epoch 113/800, batch 10/75, loss 1.4693\n",
            "epoch 113/800, batch 11/75, loss 0.2189\n",
            "epoch 113/800, batch 12/75, loss 0.2303\n",
            "epoch 113/800, batch 13/75, loss 0.1888\n",
            "epoch 113/800, batch 14/75, loss 0.1243\n",
            "epoch 113/800, batch 15/75, loss 2.3184\n",
            "epoch 113/800, batch 16/75, loss 0.1402\n",
            "epoch 113/800, batch 17/75, loss 0.2533\n",
            "epoch 113/800, batch 18/75, loss 0.2282\n",
            "epoch 113/800, batch 19/75, loss 0.2272\n",
            "epoch 113/800, batch 20/75, loss 1.5353\n",
            "epoch 113/800, batch 21/75, loss 0.1233\n",
            "epoch 113/800, batch 22/75, loss 0.1552\n",
            "epoch 113/800, batch 23/75, loss 0.1277\n",
            "epoch 113/800, batch 24/75, loss 0.1939\n",
            "epoch 113/800, batch 25/75, loss 0.1503\n",
            "epoch 113/800, batch 26/75, loss 4.7747\n",
            "epoch 113/800, batch 27/75, loss 0.1779\n",
            "epoch 113/800, batch 28/75, loss 0.2654\n",
            "epoch 113/800, batch 29/75, loss 0.1616\n",
            "epoch 113/800, batch 30/75, loss 0.2964\n",
            "epoch 113/800, batch 31/75, loss 0.2267\n",
            "epoch 113/800, batch 32/75, loss 0.2465\n",
            "epoch 113/800, batch 33/75, loss 0.2069\n",
            "epoch 113/800, batch 34/75, loss 0.3380\n",
            "epoch 113/800, batch 35/75, loss 0.1939\n",
            "epoch 113/800, batch 36/75, loss 4.1976\n",
            "epoch 113/800, batch 37/75, loss 0.9518\n",
            "epoch 113/800, batch 38/75, loss 0.2001\n",
            "epoch 113/800, batch 39/75, loss 0.5047\n",
            "epoch 113/800, batch 40/75, loss 0.2672\n",
            "epoch 113/800, batch 41/75, loss 0.4966\n",
            "epoch 113/800, batch 42/75, loss 0.3141\n",
            "epoch 113/800, batch 43/75, loss 0.3536\n",
            "epoch 113/800, batch 44/75, loss 0.4202\n",
            "epoch 113/800, batch 45/75, loss 0.2931\n",
            "epoch 113/800, batch 46/75, loss 0.3630\n",
            "epoch 113/800, batch 47/75, loss 0.3606\n",
            "epoch 113/800, batch 48/75, loss 0.2247\n",
            "epoch 113/800, batch 49/75, loss 0.2963\n",
            "epoch 113/800, batch 50/75, loss 0.2303\n",
            "epoch 113/800, batch 51/75, loss 0.2203\n",
            "epoch 113/800, batch 52/75, loss 0.1835\n",
            "epoch 113/800, batch 53/75, loss 10.8343\n",
            "epoch 113/800, batch 54/75, loss 0.5505\n",
            "epoch 113/800, batch 55/75, loss 0.2673\n",
            "epoch 113/800, batch 56/75, loss 0.3415\n",
            "epoch 113/800, batch 57/75, loss 0.2736\n",
            "epoch 113/800, batch 58/75, loss 0.3844\n",
            "epoch 113/800, batch 59/75, loss 0.4282\n",
            "epoch 113/800, batch 60/75, loss 0.4213\n",
            "epoch 113/800, batch 61/75, loss 0.4126\n",
            "epoch 113/800, batch 62/75, loss 0.5651\n",
            "epoch 113/800, batch 63/75, loss 0.4794\n",
            "epoch 113/800, batch 64/75, loss 0.4186\n",
            "epoch 113/800, batch 65/75, loss 0.4587\n",
            "epoch 113/800, batch 66/75, loss 0.3645\n",
            "epoch 113/800, batch 67/75, loss 0.4216\n",
            "epoch 113/800, batch 68/75, loss 0.3775\n",
            "epoch 113/800, batch 69/75, loss 0.4057\n",
            "epoch 113/800, batch 70/75, loss 0.2114\n",
            "epoch 113/800, batch 71/75, loss 0.2812\n",
            "epoch 113/800, batch 72/75, loss 0.3294\n",
            "epoch 113/800, batch 73/75, loss 0.2528\n",
            "epoch 113/800, batch 74/75, loss 0.2358\n",
            "epoch 113/800, batch 75/75, loss 0.1611\n",
            "epoch 113/800, training roc_auc_score 0.8583\n",
            "EarlyStopping counter: 17 out of 80\n",
            "epoch 113/800, validation roc_auc_score 0.7830, best validation roc_auc_score 0.8157\n",
            "epoch 114/800, batch 1/75, loss 0.6843\n",
            "epoch 114/800, batch 2/75, loss 0.9551\n",
            "epoch 114/800, batch 3/75, loss 0.4242\n",
            "epoch 114/800, batch 4/75, loss 0.4645\n",
            "epoch 114/800, batch 5/75, loss 0.2389\n",
            "epoch 114/800, batch 6/75, loss 0.1381\n",
            "epoch 114/800, batch 7/75, loss 0.1436\n",
            "epoch 114/800, batch 8/75, loss 2.2582\n",
            "epoch 114/800, batch 9/75, loss 0.1214\n",
            "epoch 114/800, batch 10/75, loss 2.0072\n",
            "epoch 114/800, batch 11/75, loss 0.2478\n",
            "epoch 114/800, batch 12/75, loss 0.2167\n",
            "epoch 114/800, batch 13/75, loss 0.2075\n",
            "epoch 114/800, batch 14/75, loss 0.1315\n",
            "epoch 114/800, batch 15/75, loss 2.4022\n",
            "epoch 114/800, batch 16/75, loss 0.1377\n",
            "epoch 114/800, batch 17/75, loss 0.3196\n",
            "epoch 114/800, batch 18/75, loss 0.2673\n",
            "epoch 114/800, batch 19/75, loss 0.2221\n",
            "epoch 114/800, batch 20/75, loss 1.7159\n",
            "epoch 114/800, batch 21/75, loss 0.1611\n",
            "epoch 114/800, batch 22/75, loss 0.2335\n",
            "epoch 114/800, batch 23/75, loss 0.1951\n",
            "epoch 114/800, batch 24/75, loss 0.2568\n",
            "epoch 114/800, batch 25/75, loss 0.1819\n",
            "epoch 114/800, batch 26/75, loss 4.6418\n",
            "epoch 114/800, batch 27/75, loss 0.2643\n",
            "epoch 114/800, batch 28/75, loss 0.5558\n",
            "epoch 114/800, batch 29/75, loss 0.5152\n",
            "epoch 114/800, batch 30/75, loss 0.6486\n",
            "epoch 114/800, batch 31/75, loss 0.4997\n",
            "epoch 114/800, batch 32/75, loss 0.5773\n",
            "epoch 114/800, batch 33/75, loss 0.3826\n",
            "epoch 114/800, batch 34/75, loss 0.4321\n",
            "epoch 114/800, batch 35/75, loss 0.3172\n",
            "epoch 114/800, batch 36/75, loss 4.8257\n",
            "epoch 114/800, batch 37/75, loss 0.7866\n",
            "epoch 114/800, batch 38/75, loss 0.2129\n",
            "epoch 114/800, batch 39/75, loss 0.3260\n",
            "epoch 114/800, batch 40/75, loss 0.2524\n",
            "epoch 114/800, batch 41/75, loss 0.3428\n",
            "epoch 114/800, batch 42/75, loss 0.2366\n",
            "epoch 114/800, batch 43/75, loss 0.2165\n",
            "epoch 114/800, batch 44/75, loss 0.2747\n",
            "epoch 114/800, batch 45/75, loss 0.2345\n",
            "epoch 114/800, batch 46/75, loss 0.2309\n",
            "epoch 114/800, batch 47/75, loss 0.2251\n",
            "epoch 114/800, batch 48/75, loss 0.1650\n",
            "epoch 114/800, batch 49/75, loss 0.2068\n",
            "epoch 114/800, batch 50/75, loss 0.1706\n",
            "epoch 114/800, batch 51/75, loss 0.1534\n",
            "epoch 114/800, batch 52/75, loss 0.0970\n",
            "epoch 114/800, batch 53/75, loss 18.4803\n",
            "epoch 114/800, batch 54/75, loss 0.5743\n",
            "epoch 114/800, batch 55/75, loss 0.2878\n",
            "epoch 114/800, batch 56/75, loss 0.4003\n",
            "epoch 114/800, batch 57/75, loss 0.4082\n",
            "epoch 114/800, batch 58/75, loss 0.6766\n",
            "epoch 114/800, batch 59/75, loss 0.7733\n",
            "epoch 114/800, batch 60/75, loss 0.8493\n",
            "epoch 114/800, batch 61/75, loss 0.8595\n",
            "epoch 114/800, batch 62/75, loss 1.0467\n",
            "epoch 114/800, batch 63/75, loss 0.9783\n",
            "epoch 114/800, batch 64/75, loss 0.7069\n",
            "epoch 114/800, batch 65/75, loss 0.7784\n",
            "epoch 114/800, batch 66/75, loss 0.6204\n",
            "epoch 114/800, batch 67/75, loss 0.7001\n",
            "epoch 114/800, batch 68/75, loss 0.6129\n",
            "epoch 114/800, batch 69/75, loss 0.5821\n",
            "epoch 114/800, batch 70/75, loss 0.3718\n",
            "epoch 114/800, batch 71/75, loss 0.3344\n",
            "epoch 114/800, batch 72/75, loss 0.4895\n",
            "epoch 114/800, batch 73/75, loss 0.3029\n",
            "epoch 114/800, batch 74/75, loss 0.2660\n",
            "epoch 114/800, batch 75/75, loss 0.1945\n",
            "epoch 114/800, training roc_auc_score 0.7780\n",
            "EarlyStopping counter: 18 out of 80\n",
            "epoch 114/800, validation roc_auc_score 0.7613, best validation roc_auc_score 0.8157\n",
            "epoch 115/800, batch 1/75, loss 0.7313\n",
            "epoch 115/800, batch 2/75, loss 0.8075\n",
            "epoch 115/800, batch 3/75, loss 0.3417\n",
            "epoch 115/800, batch 4/75, loss 0.5328\n",
            "epoch 115/800, batch 5/75, loss 0.2021\n",
            "epoch 115/800, batch 6/75, loss 0.1274\n",
            "epoch 115/800, batch 7/75, loss 0.1805\n",
            "epoch 115/800, batch 8/75, loss 2.3166\n",
            "epoch 115/800, batch 9/75, loss 0.0776\n",
            "epoch 115/800, batch 10/75, loss 0.9556\n",
            "epoch 115/800, batch 11/75, loss 1.1926\n",
            "epoch 115/800, batch 12/75, loss 0.2359\n",
            "epoch 115/800, batch 13/75, loss 0.2007\n",
            "epoch 115/800, batch 14/75, loss 0.0926\n",
            "epoch 115/800, batch 15/75, loss 4.4620\n",
            "epoch 115/800, batch 16/75, loss 0.0968\n",
            "epoch 115/800, batch 17/75, loss 0.2532\n",
            "epoch 115/800, batch 18/75, loss 0.2603\n",
            "epoch 115/800, batch 19/75, loss 0.2688\n",
            "epoch 115/800, batch 20/75, loss 3.3286\n",
            "epoch 115/800, batch 21/75, loss 0.1373\n",
            "epoch 115/800, batch 22/75, loss 0.2662\n",
            "epoch 115/800, batch 23/75, loss 0.1581\n",
            "epoch 115/800, batch 24/75, loss 0.2314\n",
            "epoch 115/800, batch 25/75, loss 0.1999\n",
            "epoch 115/800, batch 26/75, loss 3.0787\n",
            "epoch 115/800, batch 27/75, loss 0.3053\n",
            "epoch 115/800, batch 28/75, loss 0.4304\n",
            "epoch 115/800, batch 29/75, loss 0.3210\n",
            "epoch 115/800, batch 30/75, loss 0.5028\n",
            "epoch 115/800, batch 31/75, loss 0.4074\n",
            "epoch 115/800, batch 32/75, loss 0.4787\n",
            "epoch 115/800, batch 33/75, loss 0.3987\n",
            "epoch 115/800, batch 34/75, loss 0.5390\n",
            "epoch 115/800, batch 35/75, loss 0.4271\n",
            "epoch 115/800, batch 36/75, loss 3.1243\n",
            "epoch 115/800, batch 37/75, loss 0.9196\n",
            "epoch 115/800, batch 38/75, loss 0.2768\n",
            "epoch 115/800, batch 39/75, loss 0.4423\n",
            "epoch 115/800, batch 40/75, loss 0.3456\n",
            "epoch 115/800, batch 41/75, loss 0.5055\n",
            "epoch 115/800, batch 42/75, loss 0.3717\n",
            "epoch 115/800, batch 43/75, loss 0.3138\n",
            "epoch 115/800, batch 44/75, loss 0.3421\n",
            "epoch 115/800, batch 45/75, loss 0.3127\n",
            "epoch 115/800, batch 46/75, loss 0.3349\n",
            "epoch 115/800, batch 47/75, loss 0.3038\n",
            "epoch 115/800, batch 48/75, loss 0.2308\n",
            "epoch 115/800, batch 49/75, loss 0.3196\n",
            "epoch 115/800, batch 50/75, loss 0.2223\n",
            "epoch 115/800, batch 51/75, loss 0.1989\n",
            "epoch 115/800, batch 52/75, loss 0.1321\n",
            "epoch 115/800, batch 53/75, loss 11.2034\n",
            "epoch 115/800, batch 54/75, loss 0.4080\n",
            "epoch 115/800, batch 55/75, loss 0.2807\n",
            "epoch 115/800, batch 56/75, loss 0.3167\n",
            "epoch 115/800, batch 57/75, loss 0.2579\n",
            "epoch 115/800, batch 58/75, loss 0.3304\n",
            "epoch 115/800, batch 59/75, loss 0.3803\n",
            "epoch 115/800, batch 60/75, loss 0.4574\n",
            "epoch 115/800, batch 61/75, loss 0.4131\n",
            "epoch 115/800, batch 62/75, loss 0.4881\n",
            "epoch 115/800, batch 63/75, loss 0.4862\n",
            "epoch 115/800, batch 64/75, loss 0.4428\n",
            "epoch 115/800, batch 65/75, loss 0.4584\n",
            "epoch 115/800, batch 66/75, loss 0.3921\n",
            "epoch 115/800, batch 67/75, loss 0.4429\n",
            "epoch 115/800, batch 68/75, loss 0.4087\n",
            "epoch 115/800, batch 69/75, loss 0.4352\n",
            "epoch 115/800, batch 70/75, loss 0.2532\n",
            "epoch 115/800, batch 71/75, loss 0.2972\n",
            "epoch 115/800, batch 72/75, loss 0.3709\n",
            "epoch 115/800, batch 73/75, loss 0.2728\n",
            "epoch 115/800, batch 74/75, loss 0.2536\n",
            "epoch 115/800, batch 75/75, loss 0.1810\n",
            "epoch 115/800, training roc_auc_score 0.8361\n",
            "EarlyStopping counter: 19 out of 80\n",
            "epoch 115/800, validation roc_auc_score 0.7562, best validation roc_auc_score 0.8157\n",
            "epoch 116/800, batch 1/75, loss 0.7432\n",
            "epoch 116/800, batch 2/75, loss 0.9353\n",
            "epoch 116/800, batch 3/75, loss 0.4797\n",
            "epoch 116/800, batch 4/75, loss 0.5963\n",
            "epoch 116/800, batch 5/75, loss 0.2010\n",
            "epoch 116/800, batch 6/75, loss 0.1491\n",
            "epoch 116/800, batch 7/75, loss 0.1731\n",
            "epoch 116/800, batch 8/75, loss 2.6277\n",
            "epoch 116/800, batch 9/75, loss 0.1208\n",
            "epoch 116/800, batch 10/75, loss 1.5910\n",
            "epoch 116/800, batch 11/75, loss 1.1587\n",
            "epoch 116/800, batch 12/75, loss 0.2658\n",
            "epoch 116/800, batch 13/75, loss 0.2259\n",
            "epoch 116/800, batch 14/75, loss 0.1453\n",
            "epoch 116/800, batch 15/75, loss 2.7498\n",
            "epoch 116/800, batch 16/75, loss 0.1379\n",
            "epoch 116/800, batch 17/75, loss 0.2790\n",
            "epoch 116/800, batch 18/75, loss 0.1958\n",
            "epoch 116/800, batch 19/75, loss 0.2166\n",
            "epoch 116/800, batch 20/75, loss 1.8380\n",
            "epoch 116/800, batch 21/75, loss 0.1423\n",
            "epoch 116/800, batch 22/75, loss 0.2151\n",
            "epoch 116/800, batch 23/75, loss 0.1604\n",
            "epoch 116/800, batch 24/75, loss 0.2247\n",
            "epoch 116/800, batch 25/75, loss 0.1726\n",
            "epoch 116/800, batch 26/75, loss 4.6008\n",
            "epoch 116/800, batch 27/75, loss 0.2615\n",
            "epoch 116/800, batch 28/75, loss 0.3374\n",
            "epoch 116/800, batch 29/75, loss 0.2462\n",
            "epoch 116/800, batch 30/75, loss 0.4178\n",
            "epoch 116/800, batch 31/75, loss 0.3455\n",
            "epoch 116/800, batch 32/75, loss 0.3862\n",
            "epoch 116/800, batch 33/75, loss 0.2751\n",
            "epoch 116/800, batch 34/75, loss 0.3903\n",
            "epoch 116/800, batch 35/75, loss 0.2938\n",
            "epoch 116/800, batch 36/75, loss 4.5197\n",
            "epoch 116/800, batch 37/75, loss 0.9226\n",
            "epoch 116/800, batch 38/75, loss 0.3000\n",
            "epoch 116/800, batch 39/75, loss 0.3885\n",
            "epoch 116/800, batch 40/75, loss 0.3149\n",
            "epoch 116/800, batch 41/75, loss 0.5317\n",
            "epoch 116/800, batch 42/75, loss 0.3490\n",
            "epoch 116/800, batch 43/75, loss 0.3782\n",
            "epoch 116/800, batch 44/75, loss 0.4238\n",
            "epoch 116/800, batch 45/75, loss 0.3734\n",
            "epoch 116/800, batch 46/75, loss 0.3982\n",
            "epoch 116/800, batch 47/75, loss 0.3867\n",
            "epoch 116/800, batch 48/75, loss 0.3086\n",
            "epoch 116/800, batch 49/75, loss 0.2952\n",
            "epoch 116/800, batch 50/75, loss 0.2815\n",
            "epoch 116/800, batch 51/75, loss 0.2323\n",
            "epoch 116/800, batch 52/75, loss 0.1794\n",
            "epoch 116/800, batch 53/75, loss 11.5806\n",
            "epoch 116/800, batch 54/75, loss 0.4653\n",
            "epoch 116/800, batch 55/75, loss 0.2688\n",
            "epoch 116/800, batch 56/75, loss 0.3427\n",
            "epoch 116/800, batch 57/75, loss 0.2906\n",
            "epoch 116/800, batch 58/75, loss 0.3865\n",
            "epoch 116/800, batch 59/75, loss 0.4604\n",
            "epoch 116/800, batch 60/75, loss 0.4814\n",
            "epoch 116/800, batch 61/75, loss 0.5073\n",
            "epoch 116/800, batch 62/75, loss 0.6052\n",
            "epoch 116/800, batch 63/75, loss 0.5156\n",
            "epoch 116/800, batch 64/75, loss 0.4827\n",
            "epoch 116/800, batch 65/75, loss 0.5262\n",
            "epoch 116/800, batch 66/75, loss 0.4455\n",
            "epoch 116/800, batch 67/75, loss 0.4906\n",
            "epoch 116/800, batch 68/75, loss 0.4513\n",
            "epoch 116/800, batch 69/75, loss 0.4467\n",
            "epoch 116/800, batch 70/75, loss 0.3074\n",
            "epoch 116/800, batch 71/75, loss 0.3362\n",
            "epoch 116/800, batch 72/75, loss 0.4021\n",
            "epoch 116/800, batch 73/75, loss 0.3041\n",
            "epoch 116/800, batch 74/75, loss 0.3080\n",
            "epoch 116/800, batch 75/75, loss 0.1885\n",
            "epoch 116/800, training roc_auc_score 0.8167\n",
            "EarlyStopping counter: 20 out of 80\n",
            "epoch 116/800, validation roc_auc_score 0.7861, best validation roc_auc_score 0.8157\n",
            "epoch 117/800, batch 1/75, loss 0.6930\n",
            "epoch 117/800, batch 2/75, loss 0.9326\n",
            "epoch 117/800, batch 3/75, loss 0.3948\n",
            "epoch 117/800, batch 4/75, loss 0.5784\n",
            "epoch 117/800, batch 5/75, loss 0.2422\n",
            "epoch 117/800, batch 6/75, loss 0.1780\n",
            "epoch 117/800, batch 7/75, loss 0.1998\n",
            "epoch 117/800, batch 8/75, loss 1.7504\n",
            "epoch 117/800, batch 9/75, loss 0.1302\n",
            "epoch 117/800, batch 10/75, loss 0.5772\n",
            "epoch 117/800, batch 11/75, loss 0.8545\n",
            "epoch 117/800, batch 12/75, loss 0.2373\n",
            "epoch 117/800, batch 13/75, loss 0.2276\n",
            "epoch 117/800, batch 14/75, loss 0.1349\n",
            "epoch 117/800, batch 15/75, loss 3.6153\n",
            "epoch 117/800, batch 16/75, loss 0.1758\n",
            "epoch 117/800, batch 17/75, loss 0.2405\n",
            "epoch 117/800, batch 18/75, loss 0.2941\n",
            "epoch 117/800, batch 19/75, loss 0.2188\n",
            "epoch 117/800, batch 20/75, loss 2.3514\n",
            "epoch 117/800, batch 21/75, loss 0.1454\n",
            "epoch 117/800, batch 22/75, loss 0.1788\n",
            "epoch 117/800, batch 23/75, loss 0.1478\n",
            "epoch 117/800, batch 24/75, loss 0.1947\n",
            "epoch 117/800, batch 25/75, loss 0.1509\n",
            "epoch 117/800, batch 26/75, loss 3.7209\n",
            "epoch 117/800, batch 27/75, loss 0.2295\n",
            "epoch 117/800, batch 28/75, loss 0.3114\n",
            "epoch 117/800, batch 29/75, loss 0.2009\n",
            "epoch 117/800, batch 30/75, loss 0.3769\n",
            "epoch 117/800, batch 31/75, loss 0.2940\n",
            "epoch 117/800, batch 32/75, loss 0.3180\n",
            "epoch 117/800, batch 33/75, loss 0.3159\n",
            "epoch 117/800, batch 34/75, loss 0.3167\n",
            "epoch 117/800, batch 35/75, loss 0.2626\n",
            "epoch 117/800, batch 36/75, loss 4.5135\n",
            "epoch 117/800, batch 37/75, loss 0.6094\n",
            "epoch 117/800, batch 38/75, loss 0.2308\n",
            "epoch 117/800, batch 39/75, loss 0.3435\n",
            "epoch 117/800, batch 40/75, loss 0.2555\n",
            "epoch 117/800, batch 41/75, loss 0.4354\n",
            "epoch 117/800, batch 42/75, loss 0.3476\n",
            "epoch 117/800, batch 43/75, loss 0.2717\n",
            "epoch 117/800, batch 44/75, loss 0.3323\n",
            "epoch 117/800, batch 45/75, loss 0.2452\n",
            "epoch 117/800, batch 46/75, loss 0.3031\n",
            "epoch 117/800, batch 47/75, loss 0.3038\n",
            "epoch 117/800, batch 48/75, loss 0.2050\n",
            "epoch 117/800, batch 49/75, loss 0.2515\n",
            "epoch 117/800, batch 50/75, loss 0.2066\n",
            "epoch 117/800, batch 51/75, loss 0.1931\n",
            "epoch 117/800, batch 52/75, loss 0.1175\n",
            "epoch 117/800, batch 53/75, loss 11.6365\n",
            "epoch 117/800, batch 54/75, loss 0.3979\n",
            "epoch 117/800, batch 55/75, loss 0.2800\n",
            "epoch 117/800, batch 56/75, loss 0.3088\n",
            "epoch 117/800, batch 57/75, loss 0.3053\n",
            "epoch 117/800, batch 58/75, loss 0.3681\n",
            "epoch 117/800, batch 59/75, loss 0.4403\n",
            "epoch 117/800, batch 60/75, loss 0.4723\n",
            "epoch 117/800, batch 61/75, loss 0.4906\n",
            "epoch 117/800, batch 62/75, loss 0.5633\n",
            "epoch 117/800, batch 63/75, loss 0.5043\n",
            "epoch 117/800, batch 64/75, loss 0.4489\n",
            "epoch 117/800, batch 65/75, loss 0.4152\n",
            "epoch 117/800, batch 66/75, loss 0.3198\n",
            "epoch 117/800, batch 67/75, loss 0.3380\n",
            "epoch 117/800, batch 68/75, loss 0.3093\n",
            "epoch 117/800, batch 69/75, loss 0.2588\n",
            "epoch 117/800, batch 70/75, loss 0.1460\n",
            "epoch 117/800, batch 71/75, loss 0.1533\n",
            "epoch 117/800, batch 72/75, loss 0.1530\n",
            "epoch 117/800, batch 73/75, loss 0.1115\n",
            "epoch 117/800, batch 74/75, loss 0.1077\n",
            "epoch 117/800, batch 75/75, loss 0.0665\n",
            "epoch 117/800, training roc_auc_score 0.8523\n",
            "EarlyStopping counter: 21 out of 80\n",
            "epoch 117/800, validation roc_auc_score 0.6868, best validation roc_auc_score 0.8157\n",
            "epoch 118/800, batch 1/75, loss 0.7101\n",
            "epoch 118/800, batch 2/75, loss 0.8974\n",
            "epoch 118/800, batch 3/75, loss 0.5653\n",
            "epoch 118/800, batch 4/75, loss 0.6142\n",
            "epoch 118/800, batch 5/75, loss 0.0840\n",
            "epoch 118/800, batch 6/75, loss 0.0469\n",
            "epoch 118/800, batch 7/75, loss 0.0913\n",
            "epoch 118/800, batch 8/75, loss 3.1574\n",
            "epoch 118/800, batch 9/75, loss 0.0780\n",
            "epoch 118/800, batch 10/75, loss 5.5214\n",
            "epoch 118/800, batch 11/75, loss 0.8367\n",
            "epoch 118/800, batch 12/75, loss 0.2032\n",
            "epoch 118/800, batch 13/75, loss 0.1485\n",
            "epoch 118/800, batch 14/75, loss 0.1041\n",
            "epoch 118/800, batch 15/75, loss 5.3593\n",
            "epoch 118/800, batch 16/75, loss 0.1266\n",
            "epoch 118/800, batch 17/75, loss 0.2838\n",
            "epoch 118/800, batch 18/75, loss 0.2535\n",
            "epoch 118/800, batch 19/75, loss 0.3478\n",
            "epoch 118/800, batch 20/75, loss 1.8487\n",
            "epoch 118/800, batch 21/75, loss 0.2147\n",
            "epoch 118/800, batch 22/75, loss 0.3060\n",
            "epoch 118/800, batch 23/75, loss 0.1905\n",
            "epoch 118/800, batch 24/75, loss 0.4190\n",
            "epoch 118/800, batch 25/75, loss 0.2821\n",
            "epoch 118/800, batch 26/75, loss 3.0528\n",
            "epoch 118/800, batch 27/75, loss 0.4430\n",
            "epoch 118/800, batch 28/75, loss 0.5547\n",
            "epoch 118/800, batch 29/75, loss 0.3939\n",
            "epoch 118/800, batch 30/75, loss 0.5554\n",
            "epoch 118/800, batch 31/75, loss 0.4837\n",
            "epoch 118/800, batch 32/75, loss 0.4061\n",
            "epoch 118/800, batch 33/75, loss 0.3575\n",
            "epoch 118/800, batch 34/75, loss 0.4812\n",
            "epoch 118/800, batch 35/75, loss 0.3519\n",
            "epoch 118/800, batch 36/75, loss 5.8489\n",
            "epoch 118/800, batch 37/75, loss 0.8248\n",
            "epoch 118/800, batch 38/75, loss 0.2737\n",
            "epoch 118/800, batch 39/75, loss 0.5145\n",
            "epoch 118/800, batch 40/75, loss 0.2705\n",
            "epoch 118/800, batch 41/75, loss 0.5921\n",
            "epoch 118/800, batch 42/75, loss 0.4138\n",
            "epoch 118/800, batch 43/75, loss 0.3983\n",
            "epoch 118/800, batch 44/75, loss 0.4439\n",
            "epoch 118/800, batch 45/75, loss 0.3796\n",
            "epoch 118/800, batch 46/75, loss 0.3723\n",
            "epoch 118/800, batch 47/75, loss 0.3677\n",
            "epoch 118/800, batch 48/75, loss 0.2580\n",
            "epoch 118/800, batch 49/75, loss 0.3714\n",
            "epoch 118/800, batch 50/75, loss 0.2309\n",
            "epoch 118/800, batch 51/75, loss 0.2359\n",
            "epoch 118/800, batch 52/75, loss 0.1231\n",
            "epoch 118/800, batch 53/75, loss 13.2689\n",
            "epoch 118/800, batch 54/75, loss 0.4461\n",
            "epoch 118/800, batch 55/75, loss 0.3347\n",
            "epoch 118/800, batch 56/75, loss 0.4460\n",
            "epoch 118/800, batch 57/75, loss 0.4699\n",
            "epoch 118/800, batch 58/75, loss 0.5503\n",
            "epoch 118/800, batch 59/75, loss 0.7237\n",
            "epoch 118/800, batch 60/75, loss 0.8424\n",
            "epoch 118/800, batch 61/75, loss 0.7452\n",
            "epoch 118/800, batch 62/75, loss 1.0155\n",
            "epoch 118/800, batch 63/75, loss 0.8790\n",
            "epoch 118/800, batch 64/75, loss 0.7283\n",
            "epoch 118/800, batch 65/75, loss 0.6766\n",
            "epoch 118/800, batch 66/75, loss 0.6588\n",
            "epoch 118/800, batch 67/75, loss 0.6918\n",
            "epoch 118/800, batch 68/75, loss 0.5377\n",
            "epoch 118/800, batch 69/75, loss 0.6222\n",
            "epoch 118/800, batch 70/75, loss 0.3801\n",
            "epoch 118/800, batch 71/75, loss 0.3372\n",
            "epoch 118/800, batch 72/75, loss 0.4038\n",
            "epoch 118/800, batch 73/75, loss 0.2947\n",
            "epoch 118/800, batch 74/75, loss 0.3044\n",
            "epoch 118/800, batch 75/75, loss 0.1983\n",
            "epoch 118/800, training roc_auc_score 0.7548\n",
            "EarlyStopping counter: 22 out of 80\n",
            "epoch 118/800, validation roc_auc_score 0.7802, best validation roc_auc_score 0.8157\n",
            "epoch 119/800, batch 1/75, loss 0.4869\n",
            "epoch 119/800, batch 2/75, loss 0.7365\n",
            "epoch 119/800, batch 3/75, loss 0.3039\n",
            "epoch 119/800, batch 4/75, loss 0.5545\n",
            "epoch 119/800, batch 5/75, loss 0.1878\n",
            "epoch 119/800, batch 6/75, loss 0.0998\n",
            "epoch 119/800, batch 7/75, loss 0.1397\n",
            "epoch 119/800, batch 8/75, loss 2.7518\n",
            "epoch 119/800, batch 9/75, loss 0.0811\n",
            "epoch 119/800, batch 10/75, loss 0.9301\n",
            "epoch 119/800, batch 11/75, loss 0.7882\n",
            "epoch 119/800, batch 12/75, loss 0.2041\n",
            "epoch 119/800, batch 13/75, loss 0.1737\n",
            "epoch 119/800, batch 14/75, loss 0.0950\n",
            "epoch 119/800, batch 15/75, loss 4.3662\n",
            "epoch 119/800, batch 16/75, loss 0.1306\n",
            "epoch 119/800, batch 17/75, loss 0.2772\n",
            "epoch 119/800, batch 18/75, loss 0.2170\n",
            "epoch 119/800, batch 19/75, loss 0.2722\n",
            "epoch 119/800, batch 20/75, loss 2.8437\n",
            "epoch 119/800, batch 21/75, loss 0.1508\n",
            "epoch 119/800, batch 22/75, loss 0.2254\n",
            "epoch 119/800, batch 23/75, loss 0.1231\n",
            "epoch 119/800, batch 24/75, loss 0.1960\n",
            "epoch 119/800, batch 25/75, loss 0.1610\n",
            "epoch 119/800, batch 26/75, loss 4.1094\n",
            "epoch 119/800, batch 27/75, loss 0.2253\n",
            "epoch 119/800, batch 28/75, loss 0.3398\n",
            "epoch 119/800, batch 29/75, loss 0.2019\n",
            "epoch 119/800, batch 30/75, loss 0.4003\n",
            "epoch 119/800, batch 31/75, loss 0.2782\n",
            "epoch 119/800, batch 32/75, loss 0.2765\n",
            "epoch 119/800, batch 33/75, loss 0.2857\n",
            "epoch 119/800, batch 34/75, loss 0.3202\n",
            "epoch 119/800, batch 35/75, loss 0.3143\n",
            "epoch 119/800, batch 36/75, loss 4.3755\n",
            "epoch 119/800, batch 37/75, loss 0.7432\n",
            "epoch 119/800, batch 38/75, loss 0.2419\n",
            "epoch 119/800, batch 39/75, loss 0.3621\n",
            "epoch 119/800, batch 40/75, loss 0.2971\n",
            "epoch 119/800, batch 41/75, loss 0.4664\n",
            "epoch 119/800, batch 42/75, loss 0.3457\n",
            "epoch 119/800, batch 43/75, loss 0.2924\n",
            "epoch 119/800, batch 44/75, loss 0.3453\n",
            "epoch 119/800, batch 45/75, loss 0.3052\n",
            "epoch 119/800, batch 46/75, loss 0.3116\n",
            "epoch 119/800, batch 47/75, loss 0.3171\n",
            "epoch 119/800, batch 48/75, loss 0.2164\n",
            "epoch 119/800, batch 49/75, loss 0.3101\n",
            "epoch 119/800, batch 50/75, loss 0.2388\n",
            "epoch 119/800, batch 51/75, loss 0.2209\n",
            "epoch 119/800, batch 52/75, loss 0.1583\n",
            "epoch 119/800, batch 53/75, loss 10.6664\n",
            "epoch 119/800, batch 54/75, loss 0.3227\n",
            "epoch 119/800, batch 55/75, loss 0.2712\n",
            "epoch 119/800, batch 56/75, loss 0.3168\n",
            "epoch 119/800, batch 57/75, loss 0.2272\n",
            "epoch 119/800, batch 58/75, loss 0.3120\n",
            "epoch 119/800, batch 59/75, loss 0.3547\n",
            "epoch 119/800, batch 60/75, loss 0.3860\n",
            "epoch 119/800, batch 61/75, loss 0.3742\n",
            "epoch 119/800, batch 62/75, loss 0.5579\n",
            "epoch 119/800, batch 63/75, loss 0.4109\n",
            "epoch 119/800, batch 64/75, loss 0.3830\n",
            "epoch 119/800, batch 65/75, loss 0.4462\n",
            "epoch 119/800, batch 66/75, loss 0.3283\n",
            "epoch 119/800, batch 67/75, loss 0.3873\n",
            "epoch 119/800, batch 68/75, loss 0.3211\n",
            "epoch 119/800, batch 69/75, loss 0.3672\n",
            "epoch 119/800, batch 70/75, loss 0.2301\n",
            "epoch 119/800, batch 71/75, loss 0.2838\n",
            "epoch 119/800, batch 72/75, loss 0.3282\n",
            "epoch 119/800, batch 73/75, loss 0.2840\n",
            "epoch 119/800, batch 74/75, loss 0.2802\n",
            "epoch 119/800, batch 75/75, loss 0.1858\n",
            "epoch 119/800, training roc_auc_score 0.8350\n",
            "EarlyStopping counter: 23 out of 80\n",
            "epoch 119/800, validation roc_auc_score 0.7849, best validation roc_auc_score 0.8157\n",
            "epoch 120/800, batch 1/75, loss 0.6019\n",
            "epoch 120/800, batch 2/75, loss 0.8115\n",
            "epoch 120/800, batch 3/75, loss 0.5993\n",
            "epoch 120/800, batch 4/75, loss 0.5097\n",
            "epoch 120/800, batch 5/75, loss 0.2659\n",
            "epoch 120/800, batch 6/75, loss 0.1904\n",
            "epoch 120/800, batch 7/75, loss 0.1905\n",
            "epoch 120/800, batch 8/75, loss 1.6448\n",
            "epoch 120/800, batch 9/75, loss 0.0896\n",
            "epoch 120/800, batch 10/75, loss 0.5069\n",
            "epoch 120/800, batch 11/75, loss 0.9419\n",
            "epoch 120/800, batch 12/75, loss 0.2815\n",
            "epoch 120/800, batch 13/75, loss 0.2111\n",
            "epoch 120/800, batch 14/75, loss 0.1294\n",
            "epoch 120/800, batch 15/75, loss 3.9369\n",
            "epoch 120/800, batch 16/75, loss 0.1588\n",
            "epoch 120/800, batch 17/75, loss 0.2827\n",
            "epoch 120/800, batch 18/75, loss 0.3205\n",
            "epoch 120/800, batch 19/75, loss 0.2638\n",
            "epoch 120/800, batch 20/75, loss 1.9845\n",
            "epoch 120/800, batch 21/75, loss 0.1572\n",
            "epoch 120/800, batch 22/75, loss 0.2167\n",
            "epoch 120/800, batch 23/75, loss 0.1409\n",
            "epoch 120/800, batch 24/75, loss 0.2059\n",
            "epoch 120/800, batch 25/75, loss 0.1268\n",
            "epoch 120/800, batch 26/75, loss 4.5970\n",
            "epoch 120/800, batch 27/75, loss 0.1703\n",
            "epoch 120/800, batch 28/75, loss 0.2873\n",
            "epoch 120/800, batch 29/75, loss 0.1386\n",
            "epoch 120/800, batch 30/75, loss 0.2969\n",
            "epoch 120/800, batch 31/75, loss 0.2438\n",
            "epoch 120/800, batch 32/75, loss 0.2690\n",
            "epoch 120/800, batch 33/75, loss 0.2456\n",
            "epoch 120/800, batch 34/75, loss 0.3479\n",
            "epoch 120/800, batch 35/75, loss 0.2638\n",
            "epoch 120/800, batch 36/75, loss 4.3518\n",
            "epoch 120/800, batch 37/75, loss 0.7526\n",
            "epoch 120/800, batch 38/75, loss 0.2181\n",
            "epoch 120/800, batch 39/75, loss 0.3415\n",
            "epoch 120/800, batch 40/75, loss 0.2756\n",
            "epoch 120/800, batch 41/75, loss 0.3722\n",
            "epoch 120/800, batch 42/75, loss 0.2927\n",
            "epoch 120/800, batch 43/75, loss 0.2916\n",
            "epoch 120/800, batch 44/75, loss 0.2907\n",
            "epoch 120/800, batch 45/75, loss 0.2662\n",
            "epoch 120/800, batch 46/75, loss 0.3120\n",
            "epoch 120/800, batch 47/75, loss 0.3088\n",
            "epoch 120/800, batch 48/75, loss 0.2415\n",
            "epoch 120/800, batch 49/75, loss 0.2834\n",
            "epoch 120/800, batch 50/75, loss 0.2549\n",
            "epoch 120/800, batch 51/75, loss 0.2023\n",
            "epoch 120/800, batch 52/75, loss 0.1440\n",
            "epoch 120/800, batch 53/75, loss 10.5632\n",
            "epoch 120/800, batch 54/75, loss 0.3222\n",
            "epoch 120/800, batch 55/75, loss 0.2657\n",
            "epoch 120/800, batch 56/75, loss 0.3025\n",
            "epoch 120/800, batch 57/75, loss 0.2262\n",
            "epoch 120/800, batch 58/75, loss 0.3014\n",
            "epoch 120/800, batch 59/75, loss 0.3540\n",
            "epoch 120/800, batch 60/75, loss 0.3768\n",
            "epoch 120/800, batch 61/75, loss 0.4015\n",
            "epoch 120/800, batch 62/75, loss 0.4824\n",
            "epoch 120/800, batch 63/75, loss 0.3787\n",
            "epoch 120/800, batch 64/75, loss 0.3740\n",
            "epoch 120/800, batch 65/75, loss 0.3552\n",
            "epoch 120/800, batch 66/75, loss 0.3020\n",
            "epoch 120/800, batch 67/75, loss 0.4022\n",
            "epoch 120/800, batch 68/75, loss 0.3072\n",
            "epoch 120/800, batch 69/75, loss 0.3601\n",
            "epoch 120/800, batch 70/75, loss 0.2039\n",
            "epoch 120/800, batch 71/75, loss 0.2353\n",
            "epoch 120/800, batch 72/75, loss 0.2758\n",
            "epoch 120/800, batch 73/75, loss 0.2248\n",
            "epoch 120/800, batch 74/75, loss 0.2344\n",
            "epoch 120/800, batch 75/75, loss 0.2023\n",
            "epoch 120/800, training roc_auc_score 0.8562\n",
            "EarlyStopping counter: 24 out of 80\n",
            "epoch 120/800, validation roc_auc_score 0.7801, best validation roc_auc_score 0.8157\n",
            "epoch 121/800, batch 1/75, loss 1.0254\n",
            "epoch 121/800, batch 2/75, loss 1.2078\n",
            "epoch 121/800, batch 3/75, loss 0.3273\n",
            "epoch 121/800, batch 4/75, loss 0.4945\n",
            "epoch 121/800, batch 5/75, loss 0.2823\n",
            "epoch 121/800, batch 6/75, loss 0.1737\n",
            "epoch 121/800, batch 7/75, loss 0.2034\n",
            "epoch 121/800, batch 8/75, loss 1.7671\n",
            "epoch 121/800, batch 9/75, loss 0.1271\n",
            "epoch 121/800, batch 10/75, loss 0.4928\n",
            "epoch 121/800, batch 11/75, loss 0.9608\n",
            "epoch 121/800, batch 12/75, loss 0.3164\n",
            "epoch 121/800, batch 13/75, loss 0.2483\n",
            "epoch 121/800, batch 14/75, loss 0.1739\n",
            "epoch 121/800, batch 15/75, loss 3.1518\n",
            "epoch 121/800, batch 16/75, loss 0.1956\n",
            "epoch 121/800, batch 17/75, loss 0.3176\n",
            "epoch 121/800, batch 18/75, loss 0.3170\n",
            "epoch 121/800, batch 19/75, loss 0.3157\n",
            "epoch 121/800, batch 20/75, loss 2.1333\n",
            "epoch 121/800, batch 21/75, loss 0.1651\n",
            "epoch 121/800, batch 22/75, loss 0.3105\n",
            "epoch 121/800, batch 23/75, loss 0.2079\n",
            "epoch 121/800, batch 24/75, loss 0.2809\n",
            "epoch 121/800, batch 25/75, loss 0.1897\n",
            "epoch 121/800, batch 26/75, loss 4.7501\n",
            "epoch 121/800, batch 27/75, loss 0.2595\n",
            "epoch 121/800, batch 28/75, loss 0.4343\n",
            "epoch 121/800, batch 29/75, loss 0.2531\n",
            "epoch 121/800, batch 30/75, loss 0.4380\n",
            "epoch 121/800, batch 31/75, loss 0.3589\n",
            "epoch 121/800, batch 32/75, loss 0.4171\n",
            "epoch 121/800, batch 33/75, loss 0.4084\n",
            "epoch 121/800, batch 34/75, loss 0.4904\n",
            "epoch 121/800, batch 35/75, loss 0.3627\n",
            "epoch 121/800, batch 36/75, loss 3.3779\n",
            "epoch 121/800, batch 37/75, loss 0.7912\n",
            "epoch 121/800, batch 38/75, loss 0.3266\n",
            "epoch 121/800, batch 39/75, loss 0.5106\n",
            "epoch 121/800, batch 40/75, loss 0.3415\n",
            "epoch 121/800, batch 41/75, loss 0.5084\n",
            "epoch 121/800, batch 42/75, loss 0.3362\n",
            "epoch 121/800, batch 43/75, loss 0.3500\n",
            "epoch 121/800, batch 44/75, loss 0.4146\n",
            "epoch 121/800, batch 45/75, loss 0.3575\n",
            "epoch 121/800, batch 46/75, loss 0.3139\n",
            "epoch 121/800, batch 47/75, loss 0.3498\n",
            "epoch 121/800, batch 48/75, loss 0.2119\n",
            "epoch 121/800, batch 49/75, loss 0.2769\n",
            "epoch 121/800, batch 50/75, loss 0.2233\n",
            "epoch 121/800, batch 51/75, loss 0.2065\n",
            "epoch 121/800, batch 52/75, loss 0.1436\n",
            "epoch 121/800, batch 53/75, loss 11.2542\n",
            "epoch 121/800, batch 54/75, loss 0.3028\n",
            "epoch 121/800, batch 55/75, loss 0.2637\n",
            "epoch 121/800, batch 56/75, loss 0.2688\n",
            "epoch 121/800, batch 57/75, loss 0.2314\n",
            "epoch 121/800, batch 58/75, loss 0.2972\n",
            "epoch 121/800, batch 59/75, loss 0.3730\n",
            "epoch 121/800, batch 60/75, loss 0.3832\n",
            "epoch 121/800, batch 61/75, loss 0.3147\n",
            "epoch 121/800, batch 62/75, loss 0.5133\n",
            "epoch 121/800, batch 63/75, loss 0.4052\n",
            "epoch 121/800, batch 64/75, loss 0.3502\n",
            "epoch 121/800, batch 65/75, loss 0.3646\n",
            "epoch 121/800, batch 66/75, loss 0.3034\n",
            "epoch 121/800, batch 67/75, loss 0.3948\n",
            "epoch 121/800, batch 68/75, loss 0.3008\n",
            "epoch 121/800, batch 69/75, loss 0.3483\n",
            "epoch 121/800, batch 70/75, loss 0.2567\n",
            "epoch 121/800, batch 71/75, loss 0.2765\n",
            "epoch 121/800, batch 72/75, loss 0.3354\n",
            "epoch 121/800, batch 73/75, loss 0.2863\n",
            "epoch 121/800, batch 74/75, loss 0.2472\n",
            "epoch 121/800, batch 75/75, loss 0.1712\n",
            "epoch 121/800, training roc_auc_score 0.8413\n",
            "EarlyStopping counter: 25 out of 80\n",
            "epoch 121/800, validation roc_auc_score 0.7858, best validation roc_auc_score 0.8157\n",
            "epoch 122/800, batch 1/75, loss 1.0550\n",
            "epoch 122/800, batch 2/75, loss 0.9154\n",
            "epoch 122/800, batch 3/75, loss 0.3527\n",
            "epoch 122/800, batch 4/75, loss 0.4339\n",
            "epoch 122/800, batch 5/75, loss 0.2648\n",
            "epoch 122/800, batch 6/75, loss 0.2191\n",
            "epoch 122/800, batch 7/75, loss 0.2021\n",
            "epoch 122/800, batch 8/75, loss 1.3542\n",
            "epoch 122/800, batch 9/75, loss 0.1208\n",
            "epoch 122/800, batch 10/75, loss 0.5023\n",
            "epoch 122/800, batch 11/75, loss 0.8753\n",
            "epoch 122/800, batch 12/75, loss 0.2443\n",
            "epoch 122/800, batch 13/75, loss 0.2446\n",
            "epoch 122/800, batch 14/75, loss 0.1613\n",
            "epoch 122/800, batch 15/75, loss 3.1647\n",
            "epoch 122/800, batch 16/75, loss 0.1713\n",
            "epoch 122/800, batch 17/75, loss 0.2834\n",
            "epoch 122/800, batch 18/75, loss 0.2696\n",
            "epoch 122/800, batch 19/75, loss 0.2946\n",
            "epoch 122/800, batch 20/75, loss 2.7096\n",
            "epoch 122/800, batch 21/75, loss 0.1358\n",
            "epoch 122/800, batch 22/75, loss 0.2006\n",
            "epoch 122/800, batch 23/75, loss 0.1364\n",
            "epoch 122/800, batch 24/75, loss 0.1987\n",
            "epoch 122/800, batch 25/75, loss 0.1687\n",
            "epoch 122/800, batch 26/75, loss 3.8058\n",
            "epoch 122/800, batch 27/75, loss 0.2039\n",
            "epoch 122/800, batch 28/75, loss 0.3255\n",
            "epoch 122/800, batch 29/75, loss 0.1811\n",
            "epoch 122/800, batch 30/75, loss 0.3382\n",
            "epoch 122/800, batch 31/75, loss 0.2529\n",
            "epoch 122/800, batch 32/75, loss 0.2742\n",
            "epoch 122/800, batch 33/75, loss 0.2560\n",
            "epoch 122/800, batch 34/75, loss 0.3525\n",
            "epoch 122/800, batch 35/75, loss 0.2689\n",
            "epoch 122/800, batch 36/75, loss 4.5892\n",
            "epoch 122/800, batch 37/75, loss 0.8631\n",
            "epoch 122/800, batch 38/75, loss 0.2219\n",
            "epoch 122/800, batch 39/75, loss 0.3912\n",
            "epoch 122/800, batch 40/75, loss 0.2522\n",
            "epoch 122/800, batch 41/75, loss 0.4068\n",
            "epoch 122/800, batch 42/75, loss 0.3103\n",
            "epoch 122/800, batch 43/75, loss 0.2986\n",
            "epoch 122/800, batch 44/75, loss 0.3566\n",
            "epoch 122/800, batch 45/75, loss 0.3002\n",
            "epoch 122/800, batch 46/75, loss 0.2679\n",
            "epoch 122/800, batch 47/75, loss 0.2975\n",
            "epoch 122/800, batch 48/75, loss 0.2258\n",
            "epoch 122/800, batch 49/75, loss 0.2773\n",
            "epoch 122/800, batch 50/75, loss 0.2243\n",
            "epoch 122/800, batch 51/75, loss 0.1957\n",
            "epoch 122/800, batch 52/75, loss 0.1238\n",
            "epoch 122/800, batch 53/75, loss 12.5069\n",
            "epoch 122/800, batch 54/75, loss 0.4556\n",
            "epoch 122/800, batch 55/75, loss 0.2933\n",
            "epoch 122/800, batch 56/75, loss 0.3457\n",
            "epoch 122/800, batch 57/75, loss 0.2954\n",
            "epoch 122/800, batch 58/75, loss 0.3594\n",
            "epoch 122/800, batch 59/75, loss 0.4273\n",
            "epoch 122/800, batch 60/75, loss 0.4341\n",
            "epoch 122/800, batch 61/75, loss 0.4394\n",
            "epoch 122/800, batch 62/75, loss 0.5918\n",
            "epoch 122/800, batch 63/75, loss 0.5747\n",
            "epoch 122/800, batch 64/75, loss 0.4451\n",
            "epoch 122/800, batch 65/75, loss 0.5390\n",
            "epoch 122/800, batch 66/75, loss 0.4416\n",
            "epoch 122/800, batch 67/75, loss 0.5095\n",
            "epoch 122/800, batch 68/75, loss 0.3922\n",
            "epoch 122/800, batch 69/75, loss 0.5077\n",
            "epoch 122/800, batch 70/75, loss 0.3034\n",
            "epoch 122/800, batch 71/75, loss 0.3197\n",
            "epoch 122/800, batch 72/75, loss 0.3593\n",
            "epoch 122/800, batch 73/75, loss 0.2765\n",
            "epoch 122/800, batch 74/75, loss 0.2801\n",
            "epoch 122/800, batch 75/75, loss 0.1788\n",
            "epoch 122/800, training roc_auc_score 0.8312\n",
            "EarlyStopping counter: 26 out of 80\n",
            "epoch 122/800, validation roc_auc_score 0.7878, best validation roc_auc_score 0.8157\n",
            "epoch 123/800, batch 1/75, loss 0.7006\n",
            "epoch 123/800, batch 2/75, loss 0.9402\n",
            "epoch 123/800, batch 3/75, loss 0.2763\n",
            "epoch 123/800, batch 4/75, loss 0.5335\n",
            "epoch 123/800, batch 5/75, loss 0.2129\n",
            "epoch 123/800, batch 6/75, loss 0.1681\n",
            "epoch 123/800, batch 7/75, loss 0.1624\n",
            "epoch 123/800, batch 8/75, loss 1.9184\n",
            "epoch 123/800, batch 9/75, loss 0.0777\n",
            "epoch 123/800, batch 10/75, loss 0.3448\n",
            "epoch 123/800, batch 11/75, loss 0.9121\n",
            "epoch 123/800, batch 12/75, loss 0.2234\n",
            "epoch 123/800, batch 13/75, loss 0.2248\n",
            "epoch 123/800, batch 14/75, loss 0.1171\n",
            "epoch 123/800, batch 15/75, loss 3.1726\n",
            "epoch 123/800, batch 16/75, loss 0.1304\n",
            "epoch 123/800, batch 17/75, loss 0.2530\n",
            "epoch 123/800, batch 18/75, loss 0.2115\n",
            "epoch 123/800, batch 19/75, loss 0.2811\n",
            "epoch 123/800, batch 20/75, loss 3.5573\n",
            "epoch 123/800, batch 21/75, loss 0.1535\n",
            "epoch 123/800, batch 22/75, loss 0.2205\n",
            "epoch 123/800, batch 23/75, loss 0.1441\n",
            "epoch 123/800, batch 24/75, loss 0.2173\n",
            "epoch 123/800, batch 25/75, loss 0.1898\n",
            "epoch 123/800, batch 26/75, loss 3.4901\n",
            "epoch 123/800, batch 27/75, loss 0.2954\n",
            "epoch 123/800, batch 28/75, loss 0.3589\n",
            "epoch 123/800, batch 29/75, loss 0.2308\n",
            "epoch 123/800, batch 30/75, loss 0.4374\n",
            "epoch 123/800, batch 31/75, loss 0.3385\n",
            "epoch 123/800, batch 32/75, loss 0.3552\n",
            "epoch 123/800, batch 33/75, loss 0.3363\n",
            "epoch 123/800, batch 34/75, loss 0.3959\n",
            "epoch 123/800, batch 35/75, loss 0.3243\n",
            "epoch 123/800, batch 36/75, loss 3.0259\n",
            "epoch 123/800, batch 37/75, loss 0.7501\n",
            "epoch 123/800, batch 38/75, loss 0.3070\n",
            "epoch 123/800, batch 39/75, loss 0.4586\n",
            "epoch 123/800, batch 40/75, loss 0.2911\n",
            "epoch 123/800, batch 41/75, loss 0.4168\n",
            "epoch 123/800, batch 42/75, loss 0.3440\n",
            "epoch 123/800, batch 43/75, loss 0.3063\n",
            "epoch 123/800, batch 44/75, loss 0.3467\n",
            "epoch 123/800, batch 45/75, loss 0.3197\n",
            "epoch 123/800, batch 46/75, loss 0.3055\n",
            "epoch 123/800, batch 47/75, loss 0.3303\n",
            "epoch 123/800, batch 48/75, loss 0.2158\n",
            "epoch 123/800, batch 49/75, loss 0.2894\n",
            "epoch 123/800, batch 50/75, loss 0.2200\n",
            "epoch 123/800, batch 51/75, loss 0.2300\n",
            "epoch 123/800, batch 52/75, loss 0.1104\n",
            "epoch 123/800, batch 53/75, loss 9.8532\n",
            "epoch 123/800, batch 54/75, loss 0.3228\n",
            "epoch 123/800, batch 55/75, loss 0.2497\n",
            "epoch 123/800, batch 56/75, loss 0.2852\n",
            "epoch 123/800, batch 57/75, loss 0.1946\n",
            "epoch 123/800, batch 58/75, loss 0.2899\n",
            "epoch 123/800, batch 59/75, loss 0.3464\n",
            "epoch 123/800, batch 60/75, loss 0.3499\n",
            "epoch 123/800, batch 61/75, loss 0.3047\n",
            "epoch 123/800, batch 62/75, loss 0.4620\n",
            "epoch 123/800, batch 63/75, loss 0.4318\n",
            "epoch 123/800, batch 64/75, loss 0.3460\n",
            "epoch 123/800, batch 65/75, loss 0.3809\n",
            "epoch 123/800, batch 66/75, loss 0.3076\n",
            "epoch 123/800, batch 67/75, loss 0.3732\n",
            "epoch 123/800, batch 68/75, loss 0.3091\n",
            "epoch 123/800, batch 69/75, loss 0.3675\n",
            "epoch 123/800, batch 70/75, loss 0.2331\n",
            "epoch 123/800, batch 71/75, loss 0.2830\n",
            "epoch 123/800, batch 72/75, loss 0.3416\n",
            "epoch 123/800, batch 73/75, loss 0.2336\n",
            "epoch 123/800, batch 74/75, loss 0.2523\n",
            "epoch 123/800, batch 75/75, loss 0.1527\n",
            "epoch 123/800, training roc_auc_score 0.8627\n",
            "EarlyStopping counter: 27 out of 80\n",
            "epoch 123/800, validation roc_auc_score 0.7950, best validation roc_auc_score 0.8157\n",
            "epoch 124/800, batch 1/75, loss 0.7650\n",
            "epoch 124/800, batch 2/75, loss 0.8575\n",
            "epoch 124/800, batch 3/75, loss 0.2650\n",
            "epoch 124/800, batch 4/75, loss 0.4113\n",
            "epoch 124/800, batch 5/75, loss 0.2017\n",
            "epoch 124/800, batch 6/75, loss 0.1582\n",
            "epoch 124/800, batch 7/75, loss 0.1765\n",
            "epoch 124/800, batch 8/75, loss 1.6574\n",
            "epoch 124/800, batch 9/75, loss 0.1025\n",
            "epoch 124/800, batch 10/75, loss 0.5263\n",
            "epoch 124/800, batch 11/75, loss 0.8781\n",
            "epoch 124/800, batch 12/75, loss 0.2154\n",
            "epoch 124/800, batch 13/75, loss 0.2144\n",
            "epoch 124/800, batch 14/75, loss 0.1504\n",
            "epoch 124/800, batch 15/75, loss 3.1909\n",
            "epoch 124/800, batch 16/75, loss 0.1729\n",
            "epoch 124/800, batch 17/75, loss 0.2481\n",
            "epoch 124/800, batch 18/75, loss 0.2676\n",
            "epoch 124/800, batch 19/75, loss 0.2223\n",
            "epoch 124/800, batch 20/75, loss 1.5733\n",
            "epoch 124/800, batch 21/75, loss 0.1427\n",
            "epoch 124/800, batch 22/75, loss 0.1861\n",
            "epoch 124/800, batch 23/75, loss 0.1221\n",
            "epoch 124/800, batch 24/75, loss 0.1970\n",
            "epoch 124/800, batch 25/75, loss 0.1524\n",
            "epoch 124/800, batch 26/75, loss 3.5697\n",
            "epoch 124/800, batch 27/75, loss 0.2260\n",
            "epoch 124/800, batch 28/75, loss 0.2955\n",
            "epoch 124/800, batch 29/75, loss 0.1607\n",
            "epoch 124/800, batch 30/75, loss 0.3109\n",
            "epoch 124/800, batch 31/75, loss 0.2409\n",
            "epoch 124/800, batch 32/75, loss 0.2584\n",
            "epoch 124/800, batch 33/75, loss 0.2441\n",
            "epoch 124/800, batch 34/75, loss 0.3258\n",
            "epoch 124/800, batch 35/75, loss 0.2303\n",
            "epoch 124/800, batch 36/75, loss 4.3986\n",
            "epoch 124/800, batch 37/75, loss 0.7381\n",
            "epoch 124/800, batch 38/75, loss 0.1969\n",
            "epoch 124/800, batch 39/75, loss 0.4206\n",
            "epoch 124/800, batch 40/75, loss 0.2277\n",
            "epoch 124/800, batch 41/75, loss 0.4369\n",
            "epoch 124/800, batch 42/75, loss 0.2924\n",
            "epoch 124/800, batch 43/75, loss 0.3014\n",
            "epoch 124/800, batch 44/75, loss 0.3264\n",
            "epoch 124/800, batch 45/75, loss 0.2947\n",
            "epoch 124/800, batch 46/75, loss 0.2785\n",
            "epoch 124/800, batch 47/75, loss 0.3455\n",
            "epoch 124/800, batch 48/75, loss 0.2160\n",
            "epoch 124/800, batch 49/75, loss 0.2797\n",
            "epoch 124/800, batch 50/75, loss 0.2380\n",
            "epoch 124/800, batch 51/75, loss 0.2308\n",
            "epoch 124/800, batch 52/75, loss 0.1497\n",
            "epoch 124/800, batch 53/75, loss 11.3752\n",
            "epoch 124/800, batch 54/75, loss 0.4375\n",
            "epoch 124/800, batch 55/75, loss 0.2809\n",
            "epoch 124/800, batch 56/75, loss 0.3091\n",
            "epoch 124/800, batch 57/75, loss 0.2704\n",
            "epoch 124/800, batch 58/75, loss 0.3401\n",
            "epoch 124/800, batch 59/75, loss 0.4412\n",
            "epoch 124/800, batch 60/75, loss 0.4046\n",
            "epoch 124/800, batch 61/75, loss 0.3972\n",
            "epoch 124/800, batch 62/75, loss 0.5270\n",
            "epoch 124/800, batch 63/75, loss 0.4545\n",
            "epoch 124/800, batch 64/75, loss 0.3917\n",
            "epoch 124/800, batch 65/75, loss 0.4459\n",
            "epoch 124/800, batch 66/75, loss 0.3566\n",
            "epoch 124/800, batch 67/75, loss 0.3573\n",
            "epoch 124/800, batch 68/75, loss 0.3620\n",
            "epoch 124/800, batch 69/75, loss 0.3869\n",
            "epoch 124/800, batch 70/75, loss 0.2280\n",
            "epoch 124/800, batch 71/75, loss 0.3054\n",
            "epoch 124/800, batch 72/75, loss 0.3319\n",
            "epoch 124/800, batch 73/75, loss 0.2575\n",
            "epoch 124/800, batch 74/75, loss 0.2706\n",
            "epoch 124/800, batch 75/75, loss 0.1726\n",
            "epoch 124/800, training roc_auc_score 0.8617\n",
            "EarlyStopping counter: 28 out of 80\n",
            "epoch 124/800, validation roc_auc_score 0.7854, best validation roc_auc_score 0.8157\n",
            "epoch 125/800, batch 1/75, loss 0.9072\n",
            "epoch 125/800, batch 2/75, loss 1.0387\n",
            "epoch 125/800, batch 3/75, loss 0.3364\n",
            "epoch 125/800, batch 4/75, loss 0.6061\n",
            "epoch 125/800, batch 5/75, loss 0.2145\n",
            "epoch 125/800, batch 6/75, loss 0.1706\n",
            "epoch 125/800, batch 7/75, loss 0.1836\n",
            "epoch 125/800, batch 8/75, loss 1.5473\n",
            "epoch 125/800, batch 9/75, loss 0.1003\n",
            "epoch 125/800, batch 10/75, loss 0.5747\n",
            "epoch 125/800, batch 11/75, loss 0.8060\n",
            "epoch 125/800, batch 12/75, loss 0.2518\n",
            "epoch 125/800, batch 13/75, loss 0.2175\n",
            "epoch 125/800, batch 14/75, loss 0.1201\n",
            "epoch 125/800, batch 15/75, loss 3.0146\n",
            "epoch 125/800, batch 16/75, loss 0.1478\n",
            "epoch 125/800, batch 17/75, loss 0.2759\n",
            "epoch 125/800, batch 18/75, loss 0.2436\n",
            "epoch 125/800, batch 19/75, loss 0.2474\n",
            "epoch 125/800, batch 20/75, loss 2.8000\n",
            "epoch 125/800, batch 21/75, loss 0.1368\n",
            "epoch 125/800, batch 22/75, loss 0.1972\n",
            "epoch 125/800, batch 23/75, loss 0.1181\n",
            "epoch 125/800, batch 24/75, loss 0.1696\n",
            "epoch 125/800, batch 25/75, loss 0.1540\n",
            "epoch 125/800, batch 26/75, loss 4.1873\n",
            "epoch 125/800, batch 27/75, loss 0.2085\n",
            "epoch 125/800, batch 28/75, loss 0.3807\n",
            "epoch 125/800, batch 29/75, loss 0.1809\n",
            "epoch 125/800, batch 30/75, loss 0.3926\n",
            "epoch 125/800, batch 31/75, loss 0.2852\n",
            "epoch 125/800, batch 32/75, loss 0.2832\n",
            "epoch 125/800, batch 33/75, loss 0.2499\n",
            "epoch 125/800, batch 34/75, loss 0.3225\n",
            "epoch 125/800, batch 35/75, loss 0.2669\n",
            "epoch 125/800, batch 36/75, loss 4.0490\n",
            "epoch 125/800, batch 37/75, loss 0.6055\n",
            "epoch 125/800, batch 38/75, loss 0.2249\n",
            "epoch 125/800, batch 39/75, loss 0.4085\n",
            "epoch 125/800, batch 40/75, loss 0.2474\n",
            "epoch 125/800, batch 41/75, loss 0.4614\n",
            "epoch 125/800, batch 42/75, loss 0.3150\n",
            "epoch 125/800, batch 43/75, loss 0.3115\n",
            "epoch 125/800, batch 44/75, loss 0.3492\n",
            "epoch 125/800, batch 45/75, loss 0.2899\n",
            "epoch 125/800, batch 46/75, loss 0.2635\n",
            "epoch 125/800, batch 47/75, loss 0.3699\n",
            "epoch 125/800, batch 48/75, loss 0.2217\n",
            "epoch 125/800, batch 49/75, loss 0.3114\n",
            "epoch 125/800, batch 50/75, loss 0.2462\n",
            "epoch 125/800, batch 51/75, loss 0.2417\n",
            "epoch 125/800, batch 52/75, loss 0.1474\n",
            "epoch 125/800, batch 53/75, loss 8.8356\n",
            "epoch 125/800, batch 54/75, loss 0.2999\n",
            "epoch 125/800, batch 55/75, loss 0.2767\n",
            "epoch 125/800, batch 56/75, loss 0.3171\n",
            "epoch 125/800, batch 57/75, loss 0.2229\n",
            "epoch 125/800, batch 58/75, loss 0.3152\n",
            "epoch 125/800, batch 59/75, loss 0.3774\n",
            "epoch 125/800, batch 60/75, loss 0.3575\n",
            "epoch 125/800, batch 61/75, loss 0.3427\n",
            "epoch 125/800, batch 62/75, loss 0.5192\n",
            "epoch 125/800, batch 63/75, loss 0.4047\n",
            "epoch 125/800, batch 64/75, loss 0.3090\n",
            "epoch 125/800, batch 65/75, loss 0.3936\n",
            "epoch 125/800, batch 66/75, loss 0.2970\n",
            "epoch 125/800, batch 67/75, loss 0.3613\n",
            "epoch 125/800, batch 68/75, loss 0.3459\n",
            "epoch 125/800, batch 69/75, loss 0.3564\n",
            "epoch 125/800, batch 70/75, loss 0.2308\n",
            "epoch 125/800, batch 71/75, loss 0.2639\n",
            "epoch 125/800, batch 72/75, loss 0.3609\n",
            "epoch 125/800, batch 73/75, loss 0.2663\n",
            "epoch 125/800, batch 74/75, loss 0.2897\n",
            "epoch 125/800, batch 75/75, loss 0.1891\n",
            "epoch 125/800, training roc_auc_score 0.8662\n",
            "EarlyStopping counter: 29 out of 80\n",
            "epoch 125/800, validation roc_auc_score 0.7818, best validation roc_auc_score 0.8157\n",
            "epoch 126/800, batch 1/75, loss 1.0649\n",
            "epoch 126/800, batch 2/75, loss 0.9206\n",
            "epoch 126/800, batch 3/75, loss 0.4208\n",
            "epoch 126/800, batch 4/75, loss 0.5348\n",
            "epoch 126/800, batch 5/75, loss 0.2041\n",
            "epoch 126/800, batch 6/75, loss 0.1683\n",
            "epoch 126/800, batch 7/75, loss 0.1837\n",
            "epoch 126/800, batch 8/75, loss 1.7168\n",
            "epoch 126/800, batch 9/75, loss 0.0964\n",
            "epoch 126/800, batch 10/75, loss 0.4179\n",
            "epoch 126/800, batch 11/75, loss 0.8737\n",
            "epoch 126/800, batch 12/75, loss 0.2499\n",
            "epoch 126/800, batch 13/75, loss 0.2526\n",
            "epoch 126/800, batch 14/75, loss 0.1664\n",
            "epoch 126/800, batch 15/75, loss 2.7942\n",
            "epoch 126/800, batch 16/75, loss 0.1654\n",
            "epoch 126/800, batch 17/75, loss 0.2911\n",
            "epoch 126/800, batch 18/75, loss 0.2924\n",
            "epoch 126/800, batch 19/75, loss 0.2806\n",
            "epoch 126/800, batch 20/75, loss 1.4628\n",
            "epoch 126/800, batch 21/75, loss 0.1537\n",
            "epoch 126/800, batch 22/75, loss 0.2103\n",
            "epoch 126/800, batch 23/75, loss 0.1242\n",
            "epoch 126/800, batch 24/75, loss 0.1803\n",
            "epoch 126/800, batch 25/75, loss 0.1354\n",
            "epoch 126/800, batch 26/75, loss 3.9176\n",
            "epoch 126/800, batch 27/75, loss 0.1942\n",
            "epoch 126/800, batch 28/75, loss 0.3395\n",
            "epoch 126/800, batch 29/75, loss 0.1382\n",
            "epoch 126/800, batch 30/75, loss 0.3361\n",
            "epoch 126/800, batch 31/75, loss 0.2110\n",
            "epoch 126/800, batch 32/75, loss 0.2014\n",
            "epoch 126/800, batch 33/75, loss 0.2002\n",
            "epoch 126/800, batch 34/75, loss 0.2924\n",
            "epoch 126/800, batch 35/75, loss 0.2170\n",
            "epoch 126/800, batch 36/75, loss 4.5810\n",
            "epoch 126/800, batch 37/75, loss 0.9952\n",
            "epoch 126/800, batch 38/75, loss 0.1782\n",
            "epoch 126/800, batch 39/75, loss 0.3668\n",
            "epoch 126/800, batch 40/75, loss 0.2371\n",
            "epoch 126/800, batch 41/75, loss 0.4080\n",
            "epoch 126/800, batch 42/75, loss 0.3257\n",
            "epoch 126/800, batch 43/75, loss 0.2769\n",
            "epoch 126/800, batch 44/75, loss 0.3545\n",
            "epoch 126/800, batch 45/75, loss 0.2543\n",
            "epoch 126/800, batch 46/75, loss 0.2724\n",
            "epoch 126/800, batch 47/75, loss 0.3477\n",
            "epoch 126/800, batch 48/75, loss 0.2174\n",
            "epoch 126/800, batch 49/75, loss 0.2854\n",
            "epoch 126/800, batch 50/75, loss 0.2542\n",
            "epoch 126/800, batch 51/75, loss 0.2382\n",
            "epoch 126/800, batch 52/75, loss 0.1388\n",
            "epoch 126/800, batch 53/75, loss 9.3422\n",
            "epoch 126/800, batch 54/75, loss 0.4011\n",
            "epoch 126/800, batch 55/75, loss 0.2841\n",
            "epoch 126/800, batch 56/75, loss 0.3192\n",
            "epoch 126/800, batch 57/75, loss 0.2127\n",
            "epoch 126/800, batch 58/75, loss 0.2961\n",
            "epoch 126/800, batch 59/75, loss 0.3437\n",
            "epoch 126/800, batch 60/75, loss 0.3342\n",
            "epoch 126/800, batch 61/75, loss 0.2943\n",
            "epoch 126/800, batch 62/75, loss 0.4253\n",
            "epoch 126/800, batch 63/75, loss 0.3510\n",
            "epoch 126/800, batch 64/75, loss 0.3197\n",
            "epoch 126/800, batch 65/75, loss 0.3499\n",
            "epoch 126/800, batch 66/75, loss 0.2705\n",
            "epoch 126/800, batch 67/75, loss 0.2965\n",
            "epoch 126/800, batch 68/75, loss 0.2709\n",
            "epoch 126/800, batch 69/75, loss 0.3200\n",
            "epoch 126/800, batch 70/75, loss 0.2168\n",
            "epoch 126/800, batch 71/75, loss 0.2795\n",
            "epoch 126/800, batch 72/75, loss 0.3074\n",
            "epoch 126/800, batch 73/75, loss 0.2357\n",
            "epoch 126/800, batch 74/75, loss 0.2271\n",
            "epoch 126/800, batch 75/75, loss 0.1811\n",
            "epoch 126/800, training roc_auc_score 0.8732\n",
            "EarlyStopping counter: 30 out of 80\n",
            "epoch 126/800, validation roc_auc_score 0.7706, best validation roc_auc_score 0.8157\n",
            "epoch 127/800, batch 1/75, loss 0.6722\n",
            "epoch 127/800, batch 2/75, loss 1.0497\n",
            "epoch 127/800, batch 3/75, loss 0.2238\n",
            "epoch 127/800, batch 4/75, loss 0.5374\n",
            "epoch 127/800, batch 5/75, loss 0.1832\n",
            "epoch 127/800, batch 6/75, loss 0.1623\n",
            "epoch 127/800, batch 7/75, loss 0.1817\n",
            "epoch 127/800, batch 8/75, loss 1.8256\n",
            "epoch 127/800, batch 9/75, loss 0.0771\n",
            "epoch 127/800, batch 10/75, loss 0.4737\n",
            "epoch 127/800, batch 11/75, loss 0.8079\n",
            "epoch 127/800, batch 12/75, loss 0.2396\n",
            "epoch 127/800, batch 13/75, loss 0.1924\n",
            "epoch 127/800, batch 14/75, loss 0.1194\n",
            "epoch 127/800, batch 15/75, loss 2.8407\n",
            "epoch 127/800, batch 16/75, loss 0.1443\n",
            "epoch 127/800, batch 17/75, loss 0.2363\n",
            "epoch 127/800, batch 18/75, loss 0.2422\n",
            "epoch 127/800, batch 19/75, loss 0.2431\n",
            "epoch 127/800, batch 20/75, loss 1.3050\n",
            "epoch 127/800, batch 21/75, loss 0.1578\n",
            "epoch 127/800, batch 22/75, loss 0.1794\n",
            "epoch 127/800, batch 23/75, loss 0.1092\n",
            "epoch 127/800, batch 24/75, loss 0.1471\n",
            "epoch 127/800, batch 25/75, loss 0.1162\n",
            "epoch 127/800, batch 26/75, loss 4.1011\n",
            "epoch 127/800, batch 27/75, loss 0.2151\n",
            "epoch 127/800, batch 28/75, loss 0.3210\n",
            "epoch 127/800, batch 29/75, loss 0.1357\n",
            "epoch 127/800, batch 30/75, loss 0.3021\n",
            "epoch 127/800, batch 31/75, loss 0.1869\n",
            "epoch 127/800, batch 32/75, loss 0.2238\n",
            "epoch 127/800, batch 33/75, loss 0.1985\n",
            "epoch 127/800, batch 34/75, loss 0.2829\n",
            "epoch 127/800, batch 35/75, loss 0.2065\n",
            "epoch 127/800, batch 36/75, loss 4.4495\n",
            "epoch 127/800, batch 37/75, loss 0.9574\n",
            "epoch 127/800, batch 38/75, loss 0.1874\n",
            "epoch 127/800, batch 39/75, loss 0.3762\n",
            "epoch 127/800, batch 40/75, loss 0.2117\n",
            "epoch 127/800, batch 41/75, loss 0.4126\n",
            "epoch 127/800, batch 42/75, loss 0.2651\n",
            "epoch 127/800, batch 43/75, loss 0.2739\n",
            "epoch 127/800, batch 44/75, loss 0.3382\n",
            "epoch 127/800, batch 45/75, loss 0.2694\n",
            "epoch 127/800, batch 46/75, loss 0.3008\n",
            "epoch 127/800, batch 47/75, loss 0.3444\n",
            "epoch 127/800, batch 48/75, loss 0.2339\n",
            "epoch 127/800, batch 49/75, loss 0.3181\n",
            "epoch 127/800, batch 50/75, loss 0.2543\n",
            "epoch 127/800, batch 51/75, loss 0.2489\n",
            "epoch 127/800, batch 52/75, loss 0.1606\n",
            "epoch 127/800, batch 53/75, loss 8.1534\n",
            "epoch 127/800, batch 54/75, loss 0.2861\n",
            "epoch 127/800, batch 55/75, loss 0.2794\n",
            "epoch 127/800, batch 56/75, loss 0.3166\n",
            "epoch 127/800, batch 57/75, loss 0.2336\n",
            "epoch 127/800, batch 58/75, loss 0.3082\n",
            "epoch 127/800, batch 59/75, loss 0.3741\n",
            "epoch 127/800, batch 60/75, loss 0.3226\n",
            "epoch 127/800, batch 61/75, loss 0.3047\n",
            "epoch 127/800, batch 62/75, loss 0.4778\n",
            "epoch 127/800, batch 63/75, loss 0.4061\n",
            "epoch 127/800, batch 64/75, loss 0.3089\n",
            "epoch 127/800, batch 65/75, loss 0.3369\n",
            "epoch 127/800, batch 66/75, loss 0.2586\n",
            "epoch 127/800, batch 67/75, loss 0.3232\n",
            "epoch 127/800, batch 68/75, loss 0.2771\n",
            "epoch 127/800, batch 69/75, loss 0.3624\n",
            "epoch 127/800, batch 70/75, loss 0.1694\n",
            "epoch 127/800, batch 71/75, loss 0.2490\n",
            "epoch 127/800, batch 72/75, loss 0.3064\n",
            "epoch 127/800, batch 73/75, loss 0.2151\n",
            "epoch 127/800, batch 74/75, loss 0.2173\n",
            "epoch 127/800, batch 75/75, loss 0.1453\n",
            "epoch 127/800, training roc_auc_score 0.8882\n",
            "EarlyStopping counter: 31 out of 80\n",
            "epoch 127/800, validation roc_auc_score 0.7722, best validation roc_auc_score 0.8157\n",
            "epoch 128/800, batch 1/75, loss 0.9663\n",
            "epoch 128/800, batch 2/75, loss 0.7908\n",
            "epoch 128/800, batch 3/75, loss 0.2232\n",
            "epoch 128/800, batch 4/75, loss 0.5770\n",
            "epoch 128/800, batch 5/75, loss 0.1625\n",
            "epoch 128/800, batch 6/75, loss 0.1483\n",
            "epoch 128/800, batch 7/75, loss 0.1658\n",
            "epoch 128/800, batch 8/75, loss 2.0192\n",
            "epoch 128/800, batch 9/75, loss 0.0749\n",
            "epoch 128/800, batch 10/75, loss 0.3972\n",
            "epoch 128/800, batch 11/75, loss 0.7739\n",
            "epoch 128/800, batch 12/75, loss 0.2392\n",
            "epoch 128/800, batch 13/75, loss 0.1666\n",
            "epoch 128/800, batch 14/75, loss 0.1286\n",
            "epoch 128/800, batch 15/75, loss 2.8929\n",
            "epoch 128/800, batch 16/75, loss 0.1157\n",
            "epoch 128/800, batch 17/75, loss 0.2530\n",
            "epoch 128/800, batch 18/75, loss 0.2557\n",
            "epoch 128/800, batch 19/75, loss 0.2991\n",
            "epoch 128/800, batch 20/75, loss 2.1002\n",
            "epoch 128/800, batch 21/75, loss 0.1239\n",
            "epoch 128/800, batch 22/75, loss 0.1574\n",
            "epoch 128/800, batch 23/75, loss 0.1132\n",
            "epoch 128/800, batch 24/75, loss 0.1582\n",
            "epoch 128/800, batch 25/75, loss 0.1089\n",
            "epoch 128/800, batch 26/75, loss 4.5182\n",
            "epoch 128/800, batch 27/75, loss 0.1945\n",
            "epoch 128/800, batch 28/75, loss 0.2975\n",
            "epoch 128/800, batch 29/75, loss 0.1596\n",
            "epoch 128/800, batch 30/75, loss 0.3202\n",
            "epoch 128/800, batch 31/75, loss 0.1882\n",
            "epoch 128/800, batch 32/75, loss 0.2187\n",
            "epoch 128/800, batch 33/75, loss 0.1709\n",
            "epoch 128/800, batch 34/75, loss 0.2798\n",
            "epoch 128/800, batch 35/75, loss 0.2061\n",
            "epoch 128/800, batch 36/75, loss 4.3040\n",
            "epoch 128/800, batch 37/75, loss 0.7015\n",
            "epoch 128/800, batch 38/75, loss 0.1919\n",
            "epoch 128/800, batch 39/75, loss 0.4146\n",
            "epoch 128/800, batch 40/75, loss 0.2177\n",
            "epoch 128/800, batch 41/75, loss 0.4178\n",
            "epoch 128/800, batch 42/75, loss 0.2784\n",
            "epoch 128/800, batch 43/75, loss 0.2962\n",
            "epoch 128/800, batch 44/75, loss 0.3625\n",
            "epoch 128/800, batch 45/75, loss 0.2747\n",
            "epoch 128/800, batch 46/75, loss 0.2695\n",
            "epoch 128/800, batch 47/75, loss 0.3099\n",
            "epoch 128/800, batch 48/75, loss 0.2186\n",
            "epoch 128/800, batch 49/75, loss 0.2395\n",
            "epoch 128/800, batch 50/75, loss 0.2411\n",
            "epoch 128/800, batch 51/75, loss 0.2122\n",
            "epoch 128/800, batch 52/75, loss 0.1371\n",
            "epoch 128/800, batch 53/75, loss 9.4779\n",
            "epoch 128/800, batch 54/75, loss 0.2621\n",
            "epoch 128/800, batch 55/75, loss 0.2510\n",
            "epoch 128/800, batch 56/75, loss 0.2775\n",
            "epoch 128/800, batch 57/75, loss 0.2364\n",
            "epoch 128/800, batch 58/75, loss 0.2866\n",
            "epoch 128/800, batch 59/75, loss 0.3378\n",
            "epoch 128/800, batch 60/75, loss 0.2837\n",
            "epoch 128/800, batch 61/75, loss 0.3010\n",
            "epoch 128/800, batch 62/75, loss 0.3977\n",
            "epoch 128/800, batch 63/75, loss 0.3401\n",
            "epoch 128/800, batch 64/75, loss 0.2729\n",
            "epoch 128/800, batch 65/75, loss 0.3070\n",
            "epoch 128/800, batch 66/75, loss 0.2466\n",
            "epoch 128/800, batch 67/75, loss 0.3150\n",
            "epoch 128/800, batch 68/75, loss 0.2623\n",
            "epoch 128/800, batch 69/75, loss 0.2549\n",
            "epoch 128/800, batch 70/75, loss 0.1757\n",
            "epoch 128/800, batch 71/75, loss 0.2066\n",
            "epoch 128/800, batch 72/75, loss 0.2606\n",
            "epoch 128/800, batch 73/75, loss 0.1913\n",
            "epoch 128/800, batch 74/75, loss 0.1776\n",
            "epoch 128/800, batch 75/75, loss 0.1529\n",
            "epoch 128/800, training roc_auc_score 0.8774\n",
            "EarlyStopping counter: 32 out of 80\n",
            "epoch 128/800, validation roc_auc_score 0.7714, best validation roc_auc_score 0.8157\n",
            "epoch 129/800, batch 1/75, loss 0.9183\n",
            "epoch 129/800, batch 2/75, loss 1.2867\n",
            "epoch 129/800, batch 3/75, loss 0.2978\n",
            "epoch 129/800, batch 4/75, loss 0.4777\n",
            "epoch 129/800, batch 5/75, loss 0.1461\n",
            "epoch 129/800, batch 6/75, loss 0.1341\n",
            "epoch 129/800, batch 7/75, loss 0.1285\n",
            "epoch 129/800, batch 8/75, loss 2.1787\n",
            "epoch 129/800, batch 9/75, loss 0.0945\n",
            "epoch 129/800, batch 10/75, loss 0.3937\n",
            "epoch 129/800, batch 11/75, loss 0.7197\n",
            "epoch 129/800, batch 12/75, loss 0.2171\n",
            "epoch 129/800, batch 13/75, loss 0.2167\n",
            "epoch 129/800, batch 14/75, loss 0.1242\n",
            "epoch 129/800, batch 15/75, loss 2.9581\n",
            "epoch 129/800, batch 16/75, loss 0.1298\n",
            "epoch 129/800, batch 17/75, loss 0.2733\n",
            "epoch 129/800, batch 18/75, loss 0.2700\n",
            "epoch 129/800, batch 19/75, loss 0.2614\n",
            "epoch 129/800, batch 20/75, loss 2.5753\n",
            "epoch 129/800, batch 21/75, loss 0.1657\n",
            "epoch 129/800, batch 22/75, loss 0.2002\n",
            "epoch 129/800, batch 23/75, loss 0.1523\n",
            "epoch 129/800, batch 24/75, loss 0.2358\n",
            "epoch 129/800, batch 25/75, loss 0.1647\n",
            "epoch 129/800, batch 26/75, loss 3.1514\n",
            "epoch 129/800, batch 27/75, loss 0.2359\n",
            "epoch 129/800, batch 28/75, loss 0.4033\n",
            "epoch 129/800, batch 29/75, loss 0.2386\n",
            "epoch 129/800, batch 30/75, loss 0.3954\n",
            "epoch 129/800, batch 31/75, loss 0.2767\n",
            "epoch 129/800, batch 32/75, loss 0.3700\n",
            "epoch 129/800, batch 33/75, loss 0.3078\n",
            "epoch 129/800, batch 34/75, loss 0.4769\n",
            "epoch 129/800, batch 35/75, loss 0.2796\n",
            "epoch 129/800, batch 36/75, loss 3.8077\n",
            "epoch 129/800, batch 37/75, loss 0.7532\n",
            "epoch 129/800, batch 38/75, loss 0.2873\n",
            "epoch 129/800, batch 39/75, loss 0.5313\n",
            "epoch 129/800, batch 40/75, loss 0.3242\n",
            "epoch 129/800, batch 41/75, loss 0.4716\n",
            "epoch 129/800, batch 42/75, loss 0.3850\n",
            "epoch 129/800, batch 43/75, loss 0.3811\n",
            "epoch 129/800, batch 44/75, loss 0.4773\n",
            "epoch 129/800, batch 45/75, loss 0.3204\n",
            "epoch 129/800, batch 46/75, loss 0.3251\n",
            "epoch 129/800, batch 47/75, loss 0.4106\n",
            "epoch 129/800, batch 48/75, loss 0.2549\n",
            "epoch 129/800, batch 49/75, loss 0.3554\n",
            "epoch 129/800, batch 50/75, loss 0.2373\n",
            "epoch 129/800, batch 51/75, loss 0.2090\n",
            "epoch 129/800, batch 52/75, loss 0.1144\n",
            "epoch 129/800, batch 53/75, loss 9.6016\n",
            "epoch 129/800, batch 54/75, loss 0.2679\n",
            "epoch 129/800, batch 55/75, loss 0.2816\n",
            "epoch 129/800, batch 56/75, loss 0.3200\n",
            "epoch 129/800, batch 57/75, loss 0.2027\n",
            "epoch 129/800, batch 58/75, loss 0.3433\n",
            "epoch 129/800, batch 59/75, loss 0.3378\n",
            "epoch 129/800, batch 60/75, loss 0.3113\n",
            "epoch 129/800, batch 61/75, loss 0.3208\n",
            "epoch 129/800, batch 62/75, loss 0.5399\n",
            "epoch 129/800, batch 63/75, loss 0.3867\n",
            "epoch 129/800, batch 64/75, loss 0.3339\n",
            "epoch 129/800, batch 65/75, loss 0.3583\n",
            "epoch 129/800, batch 66/75, loss 0.3045\n",
            "epoch 129/800, batch 67/75, loss 0.3320\n",
            "epoch 129/800, batch 68/75, loss 0.3311\n",
            "epoch 129/800, batch 69/75, loss 0.3506\n",
            "epoch 129/800, batch 70/75, loss 0.2092\n",
            "epoch 129/800, batch 71/75, loss 0.2659\n",
            "epoch 129/800, batch 72/75, loss 0.3254\n",
            "epoch 129/800, batch 73/75, loss 0.2817\n",
            "epoch 129/800, batch 74/75, loss 0.2413\n",
            "epoch 129/800, batch 75/75, loss 0.1538\n",
            "epoch 129/800, training roc_auc_score 0.8664\n",
            "EarlyStopping counter: 33 out of 80\n",
            "epoch 129/800, validation roc_auc_score 0.7915, best validation roc_auc_score 0.8157\n",
            "epoch 130/800, batch 1/75, loss 1.1843\n",
            "epoch 130/800, batch 2/75, loss 1.2679\n",
            "epoch 130/800, batch 3/75, loss 0.2964\n",
            "epoch 130/800, batch 4/75, loss 0.5230\n",
            "epoch 130/800, batch 5/75, loss 0.1784\n",
            "epoch 130/800, batch 6/75, loss 0.1555\n",
            "epoch 130/800, batch 7/75, loss 0.1939\n",
            "epoch 130/800, batch 8/75, loss 1.9995\n",
            "epoch 130/800, batch 9/75, loss 0.1077\n",
            "epoch 130/800, batch 10/75, loss 0.4712\n",
            "epoch 130/800, batch 11/75, loss 0.8122\n",
            "epoch 130/800, batch 12/75, loss 0.2614\n",
            "epoch 130/800, batch 13/75, loss 0.2448\n",
            "epoch 130/800, batch 14/75, loss 0.1481\n",
            "epoch 130/800, batch 15/75, loss 2.1970\n",
            "epoch 130/800, batch 16/75, loss 0.1378\n",
            "epoch 130/800, batch 17/75, loss 0.3115\n",
            "epoch 130/800, batch 18/75, loss 0.2801\n",
            "epoch 130/800, batch 19/75, loss 0.3186\n",
            "epoch 130/800, batch 20/75, loss 1.6228\n",
            "epoch 130/800, batch 21/75, loss 0.1772\n",
            "epoch 130/800, batch 22/75, loss 0.1986\n",
            "epoch 130/800, batch 23/75, loss 0.1346\n",
            "epoch 130/800, batch 24/75, loss 0.2032\n",
            "epoch 130/800, batch 25/75, loss 0.1618\n",
            "epoch 130/800, batch 26/75, loss 3.5539\n",
            "epoch 130/800, batch 27/75, loss 0.1961\n",
            "epoch 130/800, batch 28/75, loss 0.3144\n",
            "epoch 130/800, batch 29/75, loss 0.1649\n",
            "epoch 130/800, batch 30/75, loss 0.3178\n",
            "epoch 130/800, batch 31/75, loss 0.2443\n",
            "epoch 130/800, batch 32/75, loss 0.2349\n",
            "epoch 130/800, batch 33/75, loss 0.2054\n",
            "epoch 130/800, batch 34/75, loss 0.2844\n",
            "epoch 130/800, batch 35/75, loss 0.2190\n",
            "epoch 130/800, batch 36/75, loss 4.4360\n",
            "epoch 130/800, batch 37/75, loss 0.8368\n",
            "epoch 130/800, batch 38/75, loss 0.2055\n",
            "epoch 130/800, batch 39/75, loss 0.3328\n",
            "epoch 130/800, batch 40/75, loss 0.2198\n",
            "epoch 130/800, batch 41/75, loss 0.3672\n",
            "epoch 130/800, batch 42/75, loss 0.3105\n",
            "epoch 130/800, batch 43/75, loss 0.3222\n",
            "epoch 130/800, batch 44/75, loss 0.3225\n",
            "epoch 130/800, batch 45/75, loss 0.2715\n",
            "epoch 130/800, batch 46/75, loss 0.2494\n",
            "epoch 130/800, batch 47/75, loss 0.3327\n",
            "epoch 130/800, batch 48/75, loss 0.2289\n",
            "epoch 130/800, batch 49/75, loss 0.2720\n",
            "epoch 130/800, batch 50/75, loss 0.2313\n",
            "epoch 130/800, batch 51/75, loss 0.2042\n",
            "epoch 130/800, batch 52/75, loss 0.1237\n",
            "epoch 130/800, batch 53/75, loss 9.8490\n",
            "epoch 130/800, batch 54/75, loss 0.2882\n",
            "epoch 130/800, batch 55/75, loss 0.2679\n",
            "epoch 130/800, batch 56/75, loss 0.2944\n",
            "epoch 130/800, batch 57/75, loss 0.2481\n",
            "epoch 130/800, batch 58/75, loss 0.3238\n",
            "epoch 130/800, batch 59/75, loss 0.3860\n",
            "epoch 130/800, batch 60/75, loss 0.3532\n",
            "epoch 130/800, batch 61/75, loss 0.3579\n",
            "epoch 130/800, batch 62/75, loss 0.5054\n",
            "epoch 130/800, batch 63/75, loss 0.4021\n",
            "epoch 130/800, batch 64/75, loss 0.3611\n",
            "epoch 130/800, batch 65/75, loss 0.3752\n",
            "epoch 130/800, batch 66/75, loss 0.2982\n",
            "epoch 130/800, batch 67/75, loss 0.3801\n",
            "epoch 130/800, batch 68/75, loss 0.3045\n",
            "epoch 130/800, batch 69/75, loss 0.3139\n",
            "epoch 130/800, batch 70/75, loss 0.2059\n",
            "epoch 130/800, batch 71/75, loss 0.2589\n",
            "epoch 130/800, batch 72/75, loss 0.3226\n",
            "epoch 130/800, batch 73/75, loss 0.2496\n",
            "epoch 130/800, batch 74/75, loss 0.2334\n",
            "epoch 130/800, batch 75/75, loss 0.1648\n",
            "epoch 130/800, training roc_auc_score 0.8699\n",
            "EarlyStopping counter: 34 out of 80\n",
            "epoch 130/800, validation roc_auc_score 0.7861, best validation roc_auc_score 0.8157\n",
            "epoch 131/800, batch 1/75, loss 0.8747\n",
            "epoch 131/800, batch 2/75, loss 0.8809\n",
            "epoch 131/800, batch 3/75, loss 0.2757\n",
            "epoch 131/800, batch 4/75, loss 0.4883\n",
            "epoch 131/800, batch 5/75, loss 0.1706\n",
            "epoch 131/800, batch 6/75, loss 0.1286\n",
            "epoch 131/800, batch 7/75, loss 0.1899\n",
            "epoch 131/800, batch 8/75, loss 1.6195\n",
            "epoch 131/800, batch 9/75, loss 0.1336\n",
            "epoch 131/800, batch 10/75, loss 0.4142\n",
            "epoch 131/800, batch 11/75, loss 0.7474\n",
            "epoch 131/800, batch 12/75, loss 0.1908\n",
            "epoch 131/800, batch 13/75, loss 0.2407\n",
            "epoch 131/800, batch 14/75, loss 0.1212\n",
            "epoch 131/800, batch 15/75, loss 3.2487\n",
            "epoch 131/800, batch 16/75, loss 0.1027\n",
            "epoch 131/800, batch 17/75, loss 0.2672\n",
            "epoch 131/800, batch 18/75, loss 0.2138\n",
            "epoch 131/800, batch 19/75, loss 0.2384\n",
            "epoch 131/800, batch 20/75, loss 3.2165\n",
            "epoch 131/800, batch 21/75, loss 0.1596\n",
            "epoch 131/800, batch 22/75, loss 0.2008\n",
            "epoch 131/800, batch 23/75, loss 0.1272\n",
            "epoch 131/800, batch 24/75, loss 0.2350\n",
            "epoch 131/800, batch 25/75, loss 0.2048\n",
            "epoch 131/800, batch 26/75, loss 4.2272\n",
            "epoch 131/800, batch 27/75, loss 0.2932\n",
            "epoch 131/800, batch 28/75, loss 0.3943\n",
            "epoch 131/800, batch 29/75, loss 0.2699\n",
            "epoch 131/800, batch 30/75, loss 0.4142\n",
            "epoch 131/800, batch 31/75, loss 0.3262\n",
            "epoch 131/800, batch 32/75, loss 0.3626\n",
            "epoch 131/800, batch 33/75, loss 0.3694\n",
            "epoch 131/800, batch 34/75, loss 0.4597\n",
            "epoch 131/800, batch 35/75, loss 0.3230\n",
            "epoch 131/800, batch 36/75, loss 2.9069\n",
            "epoch 131/800, batch 37/75, loss 0.9362\n",
            "epoch 131/800, batch 38/75, loss 0.2732\n",
            "epoch 131/800, batch 39/75, loss 0.4624\n",
            "epoch 131/800, batch 40/75, loss 0.3101\n",
            "epoch 131/800, batch 41/75, loss 0.3967\n",
            "epoch 131/800, batch 42/75, loss 0.2911\n",
            "epoch 131/800, batch 43/75, loss 0.3207\n",
            "epoch 131/800, batch 44/75, loss 0.3446\n",
            "epoch 131/800, batch 45/75, loss 0.2569\n",
            "epoch 131/800, batch 46/75, loss 0.2858\n",
            "epoch 131/800, batch 47/75, loss 0.3090\n",
            "epoch 131/800, batch 48/75, loss 0.1861\n",
            "epoch 131/800, batch 49/75, loss 0.2647\n",
            "epoch 131/800, batch 50/75, loss 0.2194\n",
            "epoch 131/800, batch 51/75, loss 0.1807\n",
            "epoch 131/800, batch 52/75, loss 0.1114\n",
            "epoch 131/800, batch 53/75, loss 10.9365\n",
            "epoch 131/800, batch 54/75, loss 0.3535\n",
            "epoch 131/800, batch 55/75, loss 0.2551\n",
            "epoch 131/800, batch 56/75, loss 0.2717\n",
            "epoch 131/800, batch 57/75, loss 0.2118\n",
            "epoch 131/800, batch 58/75, loss 0.3198\n",
            "epoch 131/800, batch 59/75, loss 0.3383\n",
            "epoch 131/800, batch 60/75, loss 0.3100\n",
            "epoch 131/800, batch 61/75, loss 0.3311\n",
            "epoch 131/800, batch 62/75, loss 0.4660\n",
            "epoch 131/800, batch 63/75, loss 0.3964\n",
            "epoch 131/800, batch 64/75, loss 0.3648\n",
            "epoch 131/800, batch 65/75, loss 0.3997\n",
            "epoch 131/800, batch 66/75, loss 0.2685\n",
            "epoch 131/800, batch 67/75, loss 0.3691\n",
            "epoch 131/800, batch 68/75, loss 0.3142\n",
            "epoch 131/800, batch 69/75, loss 0.3401\n",
            "epoch 131/800, batch 70/75, loss 0.2007\n",
            "epoch 131/800, batch 71/75, loss 0.2860\n",
            "epoch 131/800, batch 72/75, loss 0.2990\n",
            "epoch 131/800, batch 73/75, loss 0.2433\n",
            "epoch 131/800, batch 74/75, loss 0.2209\n",
            "epoch 131/800, batch 75/75, loss 0.1558\n",
            "epoch 131/800, training roc_auc_score 0.8556\n",
            "EarlyStopping counter: 35 out of 80\n",
            "epoch 131/800, validation roc_auc_score 0.7952, best validation roc_auc_score 0.8157\n",
            "epoch 132/800, batch 1/75, loss 1.2027\n",
            "epoch 132/800, batch 2/75, loss 1.1002\n",
            "epoch 132/800, batch 3/75, loss 0.2683\n",
            "epoch 132/800, batch 4/75, loss 0.5818\n",
            "epoch 132/800, batch 5/75, loss 0.1741\n",
            "epoch 132/800, batch 6/75, loss 0.1834\n",
            "epoch 132/800, batch 7/75, loss 0.1805\n",
            "epoch 132/800, batch 8/75, loss 1.7546\n",
            "epoch 132/800, batch 9/75, loss 0.1066\n",
            "epoch 132/800, batch 10/75, loss 0.5192\n",
            "epoch 132/800, batch 11/75, loss 0.8846\n",
            "epoch 132/800, batch 12/75, loss 0.1923\n",
            "epoch 132/800, batch 13/75, loss 0.2180\n",
            "epoch 132/800, batch 14/75, loss 0.1682\n",
            "epoch 132/800, batch 15/75, loss 2.9116\n",
            "epoch 132/800, batch 16/75, loss 0.1643\n",
            "epoch 132/800, batch 17/75, loss 0.2986\n",
            "epoch 132/800, batch 18/75, loss 0.3322\n",
            "epoch 132/800, batch 19/75, loss 0.2905\n",
            "epoch 132/800, batch 20/75, loss 1.5148\n",
            "epoch 132/800, batch 21/75, loss 0.2207\n",
            "epoch 132/800, batch 22/75, loss 0.2449\n",
            "epoch 132/800, batch 23/75, loss 0.1963\n",
            "epoch 132/800, batch 24/75, loss 0.2756\n",
            "epoch 132/800, batch 25/75, loss 0.2538\n",
            "epoch 132/800, batch 26/75, loss 3.1198\n",
            "epoch 132/800, batch 27/75, loss 0.2939\n",
            "epoch 132/800, batch 28/75, loss 0.3484\n",
            "epoch 132/800, batch 29/75, loss 0.1869\n",
            "epoch 132/800, batch 30/75, loss 0.3687\n",
            "epoch 132/800, batch 31/75, loss 0.2723\n",
            "epoch 132/800, batch 32/75, loss 0.3011\n",
            "epoch 132/800, batch 33/75, loss 0.2916\n",
            "epoch 132/800, batch 34/75, loss 0.3565\n",
            "epoch 132/800, batch 35/75, loss 0.2537\n",
            "epoch 132/800, batch 36/75, loss 3.9235\n",
            "epoch 132/800, batch 37/75, loss 0.8873\n",
            "epoch 132/800, batch 38/75, loss 0.2192\n",
            "epoch 132/800, batch 39/75, loss 0.3811\n",
            "epoch 132/800, batch 40/75, loss 0.2647\n",
            "epoch 132/800, batch 41/75, loss 0.4335\n",
            "epoch 132/800, batch 42/75, loss 0.3452\n",
            "epoch 132/800, batch 43/75, loss 0.2668\n",
            "epoch 132/800, batch 44/75, loss 0.3667\n",
            "epoch 132/800, batch 45/75, loss 0.2643\n",
            "epoch 132/800, batch 46/75, loss 0.2620\n",
            "epoch 132/800, batch 47/75, loss 0.3281\n",
            "epoch 132/800, batch 48/75, loss 0.2407\n",
            "epoch 132/800, batch 49/75, loss 0.2959\n",
            "epoch 132/800, batch 50/75, loss 0.2132\n",
            "epoch 132/800, batch 51/75, loss 0.2069\n",
            "epoch 132/800, batch 52/75, loss 0.1266\n",
            "epoch 132/800, batch 53/75, loss 11.9769\n",
            "epoch 132/800, batch 54/75, loss 0.5042\n",
            "epoch 132/800, batch 55/75, loss 0.2302\n",
            "epoch 132/800, batch 56/75, loss 0.2875\n",
            "epoch 132/800, batch 57/75, loss 0.2085\n",
            "epoch 132/800, batch 58/75, loss 0.3758\n",
            "epoch 132/800, batch 59/75, loss 0.3593\n",
            "epoch 132/800, batch 60/75, loss 0.3226\n",
            "epoch 132/800, batch 61/75, loss 0.3242\n",
            "epoch 132/800, batch 62/75, loss 0.5134\n",
            "epoch 132/800, batch 63/75, loss 0.4166\n",
            "epoch 132/800, batch 64/75, loss 0.3773\n",
            "epoch 132/800, batch 65/75, loss 0.4121\n",
            "epoch 132/800, batch 66/75, loss 0.3110\n",
            "epoch 132/800, batch 67/75, loss 0.3803\n",
            "epoch 132/800, batch 68/75, loss 0.3573\n",
            "epoch 132/800, batch 69/75, loss 0.3945\n",
            "epoch 132/800, batch 70/75, loss 0.1938\n",
            "epoch 132/800, batch 71/75, loss 0.2735\n",
            "epoch 132/800, batch 72/75, loss 0.3201\n",
            "epoch 132/800, batch 73/75, loss 0.2820\n",
            "epoch 132/800, batch 74/75, loss 0.2216\n",
            "epoch 132/800, batch 75/75, loss 0.1582\n",
            "epoch 132/800, training roc_auc_score 0.8597\n",
            "EarlyStopping counter: 36 out of 80\n",
            "epoch 132/800, validation roc_auc_score 0.8017, best validation roc_auc_score 0.8157\n",
            "epoch 133/800, batch 1/75, loss 1.1169\n",
            "epoch 133/800, batch 2/75, loss 1.4610\n",
            "epoch 133/800, batch 3/75, loss 0.3129\n",
            "epoch 133/800, batch 4/75, loss 0.4568\n",
            "epoch 133/800, batch 5/75, loss 0.1588\n",
            "epoch 133/800, batch 6/75, loss 0.1655\n",
            "epoch 133/800, batch 7/75, loss 0.1558\n",
            "epoch 133/800, batch 8/75, loss 1.3121\n",
            "epoch 133/800, batch 9/75, loss 0.1008\n",
            "epoch 133/800, batch 10/75, loss 0.4929\n",
            "epoch 133/800, batch 11/75, loss 0.7585\n",
            "epoch 133/800, batch 12/75, loss 0.2644\n",
            "epoch 133/800, batch 13/75, loss 0.2244\n",
            "epoch 133/800, batch 14/75, loss 0.1109\n",
            "epoch 133/800, batch 15/75, loss 3.1007\n",
            "epoch 133/800, batch 16/75, loss 0.0973\n",
            "epoch 133/800, batch 17/75, loss 0.2356\n",
            "epoch 133/800, batch 18/75, loss 0.2124\n",
            "epoch 133/800, batch 19/75, loss 0.2629\n",
            "epoch 133/800, batch 20/75, loss 2.1162\n",
            "epoch 133/800, batch 21/75, loss 0.1907\n",
            "epoch 133/800, batch 22/75, loss 0.1870\n",
            "epoch 133/800, batch 23/75, loss 0.1453\n",
            "epoch 133/800, batch 24/75, loss 0.1997\n",
            "epoch 133/800, batch 25/75, loss 0.1965\n",
            "epoch 133/800, batch 26/75, loss 3.8403\n",
            "epoch 133/800, batch 27/75, loss 0.2275\n",
            "epoch 133/800, batch 28/75, loss 0.3243\n",
            "epoch 133/800, batch 29/75, loss 0.2132\n",
            "epoch 133/800, batch 30/75, loss 0.4057\n",
            "epoch 133/800, batch 31/75, loss 0.3314\n",
            "epoch 133/800, batch 32/75, loss 0.3369\n",
            "epoch 133/800, batch 33/75, loss 0.3132\n",
            "epoch 133/800, batch 34/75, loss 0.3722\n",
            "epoch 133/800, batch 35/75, loss 0.3135\n",
            "epoch 133/800, batch 36/75, loss 3.0212\n",
            "epoch 133/800, batch 37/75, loss 0.7444\n",
            "epoch 133/800, batch 38/75, loss 0.2655\n",
            "epoch 133/800, batch 39/75, loss 0.4052\n",
            "epoch 133/800, batch 40/75, loss 0.2614\n",
            "epoch 133/800, batch 41/75, loss 0.3884\n",
            "epoch 133/800, batch 42/75, loss 0.3016\n",
            "epoch 133/800, batch 43/75, loss 0.3306\n",
            "epoch 133/800, batch 44/75, loss 0.3012\n",
            "epoch 133/800, batch 45/75, loss 0.2464\n",
            "epoch 133/800, batch 46/75, loss 0.2751\n",
            "epoch 133/800, batch 47/75, loss 0.2986\n",
            "epoch 133/800, batch 48/75, loss 0.2075\n",
            "epoch 133/800, batch 49/75, loss 0.2274\n",
            "epoch 133/800, batch 50/75, loss 0.1654\n",
            "epoch 133/800, batch 51/75, loss 0.1643\n",
            "epoch 133/800, batch 52/75, loss 0.0824\n",
            "epoch 133/800, batch 53/75, loss 13.1976\n",
            "epoch 133/800, batch 54/75, loss 0.3986\n",
            "epoch 133/800, batch 55/75, loss 0.2216\n",
            "epoch 133/800, batch 56/75, loss 0.2435\n",
            "epoch 133/800, batch 57/75, loss 0.1884\n",
            "epoch 133/800, batch 58/75, loss 0.2915\n",
            "epoch 133/800, batch 59/75, loss 0.3510\n",
            "epoch 133/800, batch 60/75, loss 0.3788\n",
            "epoch 133/800, batch 61/75, loss 0.3849\n",
            "epoch 133/800, batch 62/75, loss 0.5453\n",
            "epoch 133/800, batch 63/75, loss 0.4786\n",
            "epoch 133/800, batch 64/75, loss 0.4005\n",
            "epoch 133/800, batch 65/75, loss 0.4838\n",
            "epoch 133/800, batch 66/75, loss 0.3624\n",
            "epoch 133/800, batch 67/75, loss 0.4492\n",
            "epoch 133/800, batch 68/75, loss 0.3896\n",
            "epoch 133/800, batch 69/75, loss 0.4759\n",
            "epoch 133/800, batch 70/75, loss 0.2499\n",
            "epoch 133/800, batch 71/75, loss 0.2883\n",
            "epoch 133/800, batch 72/75, loss 0.3635\n",
            "epoch 133/800, batch 73/75, loss 0.2796\n",
            "epoch 133/800, batch 74/75, loss 0.2974\n",
            "epoch 133/800, batch 75/75, loss 0.2315\n",
            "epoch 133/800, training roc_auc_score 0.8520\n",
            "EarlyStopping counter: 37 out of 80\n",
            "epoch 133/800, validation roc_auc_score 0.8053, best validation roc_auc_score 0.8157\n",
            "epoch 134/800, batch 1/75, loss 1.1291\n",
            "epoch 134/800, batch 2/75, loss 1.0414\n",
            "epoch 134/800, batch 3/75, loss 0.2785\n",
            "epoch 134/800, batch 4/75, loss 0.5742\n",
            "epoch 134/800, batch 5/75, loss 0.1914\n",
            "epoch 134/800, batch 6/75, loss 0.1465\n",
            "epoch 134/800, batch 7/75, loss 0.1592\n",
            "epoch 134/800, batch 8/75, loss 1.6589\n",
            "epoch 134/800, batch 9/75, loss 0.1100\n",
            "epoch 134/800, batch 10/75, loss 0.6030\n",
            "epoch 134/800, batch 11/75, loss 0.5788\n",
            "epoch 134/800, batch 12/75, loss 0.2321\n",
            "epoch 134/800, batch 13/75, loss 0.1942\n",
            "epoch 134/800, batch 14/75, loss 0.1104\n",
            "epoch 134/800, batch 15/75, loss 2.7186\n",
            "epoch 134/800, batch 16/75, loss 0.1194\n",
            "epoch 134/800, batch 17/75, loss 0.2261\n",
            "epoch 134/800, batch 18/75, loss 0.1906\n",
            "epoch 134/800, batch 19/75, loss 0.2430\n",
            "epoch 134/800, batch 20/75, loss 2.8007\n",
            "epoch 134/800, batch 21/75, loss 0.1418\n",
            "epoch 134/800, batch 22/75, loss 0.2060\n",
            "epoch 134/800, batch 23/75, loss 0.1273\n",
            "epoch 134/800, batch 24/75, loss 0.1869\n",
            "epoch 134/800, batch 25/75, loss 0.1725\n",
            "epoch 134/800, batch 26/75, loss 3.2569\n",
            "epoch 134/800, batch 27/75, loss 0.2670\n",
            "epoch 134/800, batch 28/75, loss 0.3271\n",
            "epoch 134/800, batch 29/75, loss 0.2073\n",
            "epoch 134/800, batch 30/75, loss 0.3913\n",
            "epoch 134/800, batch 31/75, loss 0.3020\n",
            "epoch 134/800, batch 32/75, loss 0.3823\n",
            "epoch 134/800, batch 33/75, loss 0.3497\n",
            "epoch 134/800, batch 34/75, loss 0.3877\n",
            "epoch 134/800, batch 35/75, loss 0.3053\n",
            "epoch 134/800, batch 36/75, loss 3.1512\n",
            "epoch 134/800, batch 37/75, loss 0.9398\n",
            "epoch 134/800, batch 38/75, loss 0.2456\n",
            "epoch 134/800, batch 39/75, loss 0.3439\n",
            "epoch 134/800, batch 40/75, loss 0.2557\n",
            "epoch 134/800, batch 41/75, loss 0.3496\n",
            "epoch 134/800, batch 42/75, loss 0.2829\n",
            "epoch 134/800, batch 43/75, loss 0.2666\n",
            "epoch 134/800, batch 44/75, loss 0.2647\n",
            "epoch 134/800, batch 45/75, loss 0.2140\n",
            "epoch 134/800, batch 46/75, loss 0.1891\n",
            "epoch 134/800, batch 47/75, loss 0.1641\n",
            "epoch 134/800, batch 48/75, loss 0.0634\n",
            "epoch 134/800, batch 49/75, loss 0.0735\n",
            "epoch 134/800, batch 50/75, loss 0.0506\n",
            "epoch 134/800, batch 51/75, loss 0.0655\n",
            "epoch 134/800, batch 52/75, loss 0.0255\n",
            "epoch 134/800, batch 53/75, loss 39.0596\n",
            "epoch 134/800, batch 54/75, loss 2.6188\n",
            "epoch 134/800, batch 55/75, loss 0.1438\n",
            "epoch 134/800, batch 56/75, loss 0.2368\n",
            "epoch 134/800, batch 57/75, loss 0.2624\n",
            "epoch 134/800, batch 58/75, loss 0.5240\n",
            "epoch 134/800, batch 59/75, loss 0.7811\n",
            "epoch 134/800, batch 60/75, loss 1.0262\n",
            "epoch 134/800, batch 61/75, loss 1.0055\n",
            "epoch 134/800, batch 62/75, loss 1.5107\n",
            "epoch 134/800, batch 63/75, loss 1.4158\n",
            "epoch 134/800, batch 64/75, loss 1.3313\n",
            "epoch 134/800, batch 65/75, loss 1.3256\n",
            "epoch 134/800, batch 66/75, loss 1.1511\n",
            "epoch 134/800, batch 67/75, loss 1.3688\n",
            "epoch 134/800, batch 68/75, loss 1.1046\n",
            "epoch 134/800, batch 69/75, loss 1.2965\n",
            "epoch 134/800, batch 70/75, loss 0.8699\n",
            "epoch 134/800, batch 71/75, loss 0.8624\n",
            "epoch 134/800, batch 72/75, loss 0.8592\n",
            "epoch 134/800, batch 73/75, loss 0.7898\n",
            "epoch 134/800, batch 74/75, loss 0.6724\n",
            "epoch 134/800, batch 75/75, loss 0.3763\n",
            "epoch 134/800, training roc_auc_score 0.6650\n",
            "EarlyStopping counter: 38 out of 80\n",
            "epoch 134/800, validation roc_auc_score 0.8079, best validation roc_auc_score 0.8157\n",
            "epoch 135/800, batch 1/75, loss 1.0617\n",
            "epoch 135/800, batch 2/75, loss 1.0570\n",
            "epoch 135/800, batch 3/75, loss 0.4330\n",
            "epoch 135/800, batch 4/75, loss 0.7608\n",
            "epoch 135/800, batch 5/75, loss 0.2583\n",
            "epoch 135/800, batch 6/75, loss 0.2401\n",
            "epoch 135/800, batch 7/75, loss 0.3028\n",
            "epoch 135/800, batch 8/75, loss 1.2637\n",
            "epoch 135/800, batch 9/75, loss 0.1738\n",
            "epoch 135/800, batch 10/75, loss 0.7866\n",
            "epoch 135/800, batch 11/75, loss 0.6807\n",
            "epoch 135/800, batch 12/75, loss 0.2320\n",
            "epoch 135/800, batch 13/75, loss 0.2107\n",
            "epoch 135/800, batch 14/75, loss 0.1147\n",
            "epoch 135/800, batch 15/75, loss 3.2146\n",
            "epoch 135/800, batch 16/75, loss 0.1002\n",
            "epoch 135/800, batch 17/75, loss 0.2631\n",
            "epoch 135/800, batch 18/75, loss 0.2127\n",
            "epoch 135/800, batch 19/75, loss 0.2590\n",
            "epoch 135/800, batch 20/75, loss 2.4811\n",
            "epoch 135/800, batch 21/75, loss 0.2318\n",
            "epoch 135/800, batch 22/75, loss 0.1835\n",
            "epoch 135/800, batch 23/75, loss 0.1237\n",
            "epoch 135/800, batch 24/75, loss 0.2456\n",
            "epoch 135/800, batch 25/75, loss 0.1846\n",
            "epoch 135/800, batch 26/75, loss 4.2438\n",
            "epoch 135/800, batch 27/75, loss 0.2650\n",
            "epoch 135/800, batch 28/75, loss 0.3148\n",
            "epoch 135/800, batch 29/75, loss 0.1769\n",
            "epoch 135/800, batch 30/75, loss 0.4107\n",
            "epoch 135/800, batch 31/75, loss 0.2934\n",
            "epoch 135/800, batch 32/75, loss 0.2658\n",
            "epoch 135/800, batch 33/75, loss 0.3244\n",
            "epoch 135/800, batch 34/75, loss 0.3647\n",
            "epoch 135/800, batch 35/75, loss 0.2988\n",
            "epoch 135/800, batch 36/75, loss 3.9464\n",
            "epoch 135/800, batch 37/75, loss 1.0607\n",
            "epoch 135/800, batch 38/75, loss 0.2109\n",
            "epoch 135/800, batch 39/75, loss 0.3849\n",
            "epoch 135/800, batch 40/75, loss 0.2740\n",
            "epoch 135/800, batch 41/75, loss 0.3968\n",
            "epoch 135/800, batch 42/75, loss 0.3439\n",
            "epoch 135/800, batch 43/75, loss 0.2714\n",
            "epoch 135/800, batch 44/75, loss 0.3842\n",
            "epoch 135/800, batch 45/75, loss 0.2585\n",
            "epoch 135/800, batch 46/75, loss 0.3047\n",
            "epoch 135/800, batch 47/75, loss 0.3257\n",
            "epoch 135/800, batch 48/75, loss 0.1875\n",
            "epoch 135/800, batch 49/75, loss 0.2243\n",
            "epoch 135/800, batch 50/75, loss 0.2208\n",
            "epoch 135/800, batch 51/75, loss 0.1770\n",
            "epoch 135/800, batch 52/75, loss 0.1147\n",
            "epoch 135/800, batch 53/75, loss 11.8630\n",
            "epoch 135/800, batch 54/75, loss 0.8629\n",
            "epoch 135/800, batch 55/75, loss 0.2691\n",
            "epoch 135/800, batch 56/75, loss 0.3198\n",
            "epoch 135/800, batch 57/75, loss 0.2615\n",
            "epoch 135/800, batch 58/75, loss 0.3443\n",
            "epoch 135/800, batch 59/75, loss 0.4159\n",
            "epoch 135/800, batch 60/75, loss 0.4317\n",
            "epoch 135/800, batch 61/75, loss 0.4290\n",
            "epoch 135/800, batch 62/75, loss 0.6015\n",
            "epoch 135/800, batch 63/75, loss 0.6272\n",
            "epoch 135/800, batch 64/75, loss 0.4723\n",
            "epoch 135/800, batch 65/75, loss 0.5059\n",
            "epoch 135/800, batch 66/75, loss 0.4440\n",
            "epoch 135/800, batch 67/75, loss 0.5460\n",
            "epoch 135/800, batch 68/75, loss 0.4401\n",
            "epoch 135/800, batch 69/75, loss 0.5084\n",
            "epoch 135/800, batch 70/75, loss 0.3584\n",
            "epoch 135/800, batch 71/75, loss 0.4121\n",
            "epoch 135/800, batch 72/75, loss 0.3904\n",
            "epoch 135/800, batch 73/75, loss 0.3431\n",
            "epoch 135/800, batch 74/75, loss 0.3644\n",
            "epoch 135/800, batch 75/75, loss 0.2339\n",
            "epoch 135/800, training roc_auc_score 0.8425\n",
            "EarlyStopping counter: 39 out of 80\n",
            "epoch 135/800, validation roc_auc_score 0.7910, best validation roc_auc_score 0.8157\n",
            "epoch 136/800, batch 1/75, loss 1.2156\n",
            "epoch 136/800, batch 2/75, loss 1.4258\n",
            "epoch 136/800, batch 3/75, loss 0.2601\n",
            "epoch 136/800, batch 4/75, loss 0.5200\n",
            "epoch 136/800, batch 5/75, loss 0.2978\n",
            "epoch 136/800, batch 6/75, loss 0.1395\n",
            "epoch 136/800, batch 7/75, loss 0.1876\n",
            "epoch 136/800, batch 8/75, loss 1.8297\n",
            "epoch 136/800, batch 9/75, loss 0.1334\n",
            "epoch 136/800, batch 10/75, loss 0.5032\n",
            "epoch 136/800, batch 11/75, loss 0.5836\n",
            "epoch 136/800, batch 12/75, loss 0.1932\n",
            "epoch 136/800, batch 13/75, loss 0.1949\n",
            "epoch 136/800, batch 14/75, loss 0.1619\n",
            "epoch 136/800, batch 15/75, loss 2.5877\n",
            "epoch 136/800, batch 16/75, loss 0.1259\n",
            "epoch 136/800, batch 17/75, loss 0.2488\n",
            "epoch 136/800, batch 18/75, loss 0.2737\n",
            "epoch 136/800, batch 19/75, loss 0.2519\n",
            "epoch 136/800, batch 20/75, loss 4.1608\n",
            "epoch 136/800, batch 21/75, loss 0.1483\n",
            "epoch 136/800, batch 22/75, loss 0.2606\n",
            "epoch 136/800, batch 23/75, loss 0.1685\n",
            "epoch 136/800, batch 24/75, loss 0.2037\n",
            "epoch 136/800, batch 25/75, loss 0.2113\n",
            "epoch 136/800, batch 26/75, loss 3.4708\n",
            "epoch 136/800, batch 27/75, loss 0.2804\n",
            "epoch 136/800, batch 28/75, loss 0.3393\n",
            "epoch 136/800, batch 29/75, loss 0.2251\n",
            "epoch 136/800, batch 30/75, loss 0.3973\n",
            "epoch 136/800, batch 31/75, loss 0.2839\n",
            "epoch 136/800, batch 32/75, loss 0.3215\n",
            "epoch 136/800, batch 33/75, loss 0.3016\n",
            "epoch 136/800, batch 34/75, loss 0.3481\n",
            "epoch 136/800, batch 35/75, loss 0.2731\n",
            "epoch 136/800, batch 36/75, loss 4.3519\n",
            "epoch 136/800, batch 37/75, loss 0.7998\n",
            "epoch 136/800, batch 38/75, loss 0.2382\n",
            "epoch 136/800, batch 39/75, loss 0.3543\n",
            "epoch 136/800, batch 40/75, loss 0.2243\n",
            "epoch 136/800, batch 41/75, loss 0.4140\n",
            "epoch 136/800, batch 42/75, loss 0.2608\n",
            "epoch 136/800, batch 43/75, loss 0.2713\n",
            "epoch 136/800, batch 44/75, loss 0.3218\n",
            "epoch 136/800, batch 45/75, loss 0.2113\n",
            "epoch 136/800, batch 46/75, loss 0.2502\n",
            "epoch 136/800, batch 47/75, loss 0.2560\n",
            "epoch 136/800, batch 48/75, loss 0.1595\n",
            "epoch 136/800, batch 49/75, loss 0.1511\n",
            "epoch 136/800, batch 50/75, loss 0.1698\n",
            "epoch 136/800, batch 51/75, loss 0.1614\n",
            "epoch 136/800, batch 52/75, loss 0.0891\n",
            "epoch 136/800, batch 53/75, loss 13.8664\n",
            "epoch 136/800, batch 54/75, loss 0.6621\n",
            "epoch 136/800, batch 55/75, loss 0.2911\n",
            "epoch 136/800, batch 56/75, loss 0.3483\n",
            "epoch 136/800, batch 57/75, loss 0.3019\n",
            "epoch 136/800, batch 58/75, loss 0.4873\n",
            "epoch 136/800, batch 59/75, loss 0.6011\n",
            "epoch 136/800, batch 60/75, loss 0.5158\n",
            "epoch 136/800, batch 61/75, loss 0.5796\n",
            "epoch 136/800, batch 62/75, loss 0.7231\n",
            "epoch 136/800, batch 63/75, loss 0.6017\n",
            "epoch 136/800, batch 64/75, loss 0.4962\n",
            "epoch 136/800, batch 65/75, loss 0.5996\n",
            "epoch 136/800, batch 66/75, loss 0.4784\n",
            "epoch 136/800, batch 67/75, loss 0.5990\n",
            "epoch 136/800, batch 68/75, loss 0.4419\n",
            "epoch 136/800, batch 69/75, loss 0.5484\n",
            "epoch 136/800, batch 70/75, loss 0.3233\n",
            "epoch 136/800, batch 71/75, loss 0.4217\n",
            "epoch 136/800, batch 72/75, loss 0.4096\n",
            "epoch 136/800, batch 73/75, loss 0.3228\n",
            "epoch 136/800, batch 74/75, loss 0.3082\n",
            "epoch 136/800, batch 75/75, loss 0.1902\n",
            "epoch 136/800, training roc_auc_score 0.8172\n",
            "EarlyStopping counter: 40 out of 80\n",
            "epoch 136/800, validation roc_auc_score 0.7961, best validation roc_auc_score 0.8157\n",
            "epoch 137/800, batch 1/75, loss 1.1757\n",
            "epoch 137/800, batch 2/75, loss 1.1425\n",
            "epoch 137/800, batch 3/75, loss 0.2205\n",
            "epoch 137/800, batch 4/75, loss 0.6372\n",
            "epoch 137/800, batch 5/75, loss 0.1834\n",
            "epoch 137/800, batch 6/75, loss 0.1512\n",
            "epoch 137/800, batch 7/75, loss 0.1742\n",
            "epoch 137/800, batch 8/75, loss 1.4517\n",
            "epoch 137/800, batch 9/75, loss 0.1114\n",
            "epoch 137/800, batch 10/75, loss 0.4031\n",
            "epoch 137/800, batch 11/75, loss 0.3061\n",
            "epoch 137/800, batch 12/75, loss 0.1936\n",
            "epoch 137/800, batch 13/75, loss 0.2301\n",
            "epoch 137/800, batch 14/75, loss 0.1160\n",
            "epoch 137/800, batch 15/75, loss 2.8241\n",
            "epoch 137/800, batch 16/75, loss 0.1263\n",
            "epoch 137/800, batch 17/75, loss 0.2679\n",
            "epoch 137/800, batch 18/75, loss 0.2241\n",
            "epoch 137/800, batch 19/75, loss 0.2390\n",
            "epoch 137/800, batch 20/75, loss 2.5483\n",
            "epoch 137/800, batch 21/75, loss 0.1556\n",
            "epoch 137/800, batch 22/75, loss 0.1753\n",
            "epoch 137/800, batch 23/75, loss 0.1112\n",
            "epoch 137/800, batch 24/75, loss 0.1378\n",
            "epoch 137/800, batch 25/75, loss 0.1338\n",
            "epoch 137/800, batch 26/75, loss 4.4567\n",
            "epoch 137/800, batch 27/75, loss 0.1526\n",
            "epoch 137/800, batch 28/75, loss 0.2850\n",
            "epoch 137/800, batch 29/75, loss 0.1372\n",
            "epoch 137/800, batch 30/75, loss 0.2970\n",
            "epoch 137/800, batch 31/75, loss 0.1703\n",
            "epoch 137/800, batch 32/75, loss 0.2212\n",
            "epoch 137/800, batch 33/75, loss 0.2040\n",
            "epoch 137/800, batch 34/75, loss 0.2864\n",
            "epoch 137/800, batch 35/75, loss 0.2319\n",
            "epoch 137/800, batch 36/75, loss 4.3094\n",
            "epoch 137/800, batch 37/75, loss 0.9819\n",
            "epoch 137/800, batch 38/75, loss 0.2074\n",
            "epoch 137/800, batch 39/75, loss 0.2948\n",
            "epoch 137/800, batch 40/75, loss 0.2122\n",
            "epoch 137/800, batch 41/75, loss 0.3769\n",
            "epoch 137/800, batch 42/75, loss 0.2949\n",
            "epoch 137/800, batch 43/75, loss 0.3057\n",
            "epoch 137/800, batch 44/75, loss 0.3905\n",
            "epoch 137/800, batch 45/75, loss 0.2939\n",
            "epoch 137/800, batch 46/75, loss 0.3258\n",
            "epoch 137/800, batch 47/75, loss 0.3583\n",
            "epoch 137/800, batch 48/75, loss 0.2447\n",
            "epoch 137/800, batch 49/75, loss 0.2651\n",
            "epoch 137/800, batch 50/75, loss 0.2214\n",
            "epoch 137/800, batch 51/75, loss 0.2223\n",
            "epoch 137/800, batch 52/75, loss 0.1462\n",
            "epoch 137/800, batch 53/75, loss 10.0272\n",
            "epoch 137/800, batch 54/75, loss 0.5163\n",
            "epoch 137/800, batch 55/75, loss 0.3406\n",
            "epoch 137/800, batch 56/75, loss 0.3603\n",
            "epoch 137/800, batch 57/75, loss 0.2596\n",
            "epoch 137/800, batch 58/75, loss 0.3745\n",
            "epoch 137/800, batch 59/75, loss 0.4147\n",
            "epoch 137/800, batch 60/75, loss 0.3331\n",
            "epoch 137/800, batch 61/75, loss 0.3375\n",
            "epoch 137/800, batch 62/75, loss 0.4925\n",
            "epoch 137/800, batch 63/75, loss 0.4289\n",
            "epoch 137/800, batch 64/75, loss 0.3693\n",
            "epoch 137/800, batch 65/75, loss 0.4245\n",
            "epoch 137/800, batch 66/75, loss 0.3604\n",
            "epoch 137/800, batch 67/75, loss 0.3871\n",
            "epoch 137/800, batch 68/75, loss 0.2786\n",
            "epoch 137/800, batch 69/75, loss 0.3578\n",
            "epoch 137/800, batch 70/75, loss 0.2162\n",
            "epoch 137/800, batch 71/75, loss 0.2804\n",
            "epoch 137/800, batch 72/75, loss 0.2684\n",
            "epoch 137/800, batch 73/75, loss 0.2098\n",
            "epoch 137/800, batch 74/75, loss 0.2024\n",
            "epoch 137/800, batch 75/75, loss 0.1542\n",
            "epoch 137/800, training roc_auc_score 0.8680\n",
            "EarlyStopping counter: 41 out of 80\n",
            "epoch 137/800, validation roc_auc_score 0.7885, best validation roc_auc_score 0.8157\n",
            "epoch 138/800, batch 1/75, loss 1.2125\n",
            "epoch 138/800, batch 2/75, loss 1.0362\n",
            "epoch 138/800, batch 3/75, loss 0.2632\n",
            "epoch 138/800, batch 4/75, loss 0.5072\n",
            "epoch 138/800, batch 5/75, loss 0.1737\n",
            "epoch 138/800, batch 6/75, loss 0.1435\n",
            "epoch 138/800, batch 7/75, loss 0.1433\n",
            "epoch 138/800, batch 8/75, loss 1.9508\n",
            "epoch 138/800, batch 9/75, loss 0.0959\n",
            "epoch 138/800, batch 10/75, loss 0.3420\n",
            "epoch 138/800, batch 11/75, loss 0.2716\n",
            "epoch 138/800, batch 12/75, loss 0.2215\n",
            "epoch 138/800, batch 13/75, loss 0.2046\n",
            "epoch 138/800, batch 14/75, loss 0.0988\n",
            "epoch 138/800, batch 15/75, loss 3.4980\n",
            "epoch 138/800, batch 16/75, loss 0.1099\n",
            "epoch 138/800, batch 17/75, loss 0.3002\n",
            "epoch 138/800, batch 18/75, loss 0.2388\n",
            "epoch 138/800, batch 19/75, loss 0.2494\n",
            "epoch 138/800, batch 20/75, loss 1.4849\n",
            "epoch 138/800, batch 21/75, loss 0.1526\n",
            "epoch 138/800, batch 22/75, loss 0.2261\n",
            "epoch 138/800, batch 23/75, loss 0.1337\n",
            "epoch 138/800, batch 24/75, loss 0.1876\n",
            "epoch 138/800, batch 25/75, loss 0.1274\n",
            "epoch 138/800, batch 26/75, loss 4.0400\n",
            "epoch 138/800, batch 27/75, loss 0.2491\n",
            "epoch 138/800, batch 28/75, loss 0.3201\n",
            "epoch 138/800, batch 29/75, loss 0.1658\n",
            "epoch 138/800, batch 30/75, loss 0.3535\n",
            "epoch 138/800, batch 31/75, loss 0.2609\n",
            "epoch 138/800, batch 32/75, loss 0.2791\n",
            "epoch 138/800, batch 33/75, loss 0.2768\n",
            "epoch 138/800, batch 34/75, loss 0.3591\n",
            "epoch 138/800, batch 35/75, loss 0.2982\n",
            "epoch 138/800, batch 36/75, loss 2.9672\n",
            "epoch 138/800, batch 37/75, loss 0.8488\n",
            "epoch 138/800, batch 38/75, loss 0.2207\n",
            "epoch 138/800, batch 39/75, loss 0.4275\n",
            "epoch 138/800, batch 40/75, loss 0.2771\n",
            "epoch 138/800, batch 41/75, loss 0.4065\n",
            "epoch 138/800, batch 42/75, loss 0.3574\n",
            "epoch 138/800, batch 43/75, loss 0.2833\n",
            "epoch 138/800, batch 44/75, loss 0.2844\n",
            "epoch 138/800, batch 45/75, loss 0.2216\n",
            "epoch 138/800, batch 46/75, loss 0.2643\n",
            "epoch 138/800, batch 47/75, loss 0.2877\n",
            "epoch 138/800, batch 48/75, loss 0.2179\n",
            "epoch 138/800, batch 49/75, loss 0.2364\n",
            "epoch 138/800, batch 50/75, loss 0.1922\n",
            "epoch 138/800, batch 51/75, loss 0.2138\n",
            "epoch 138/800, batch 52/75, loss 0.0972\n",
            "epoch 138/800, batch 53/75, loss 10.8954\n",
            "epoch 138/800, batch 54/75, loss 0.5483\n",
            "epoch 138/800, batch 55/75, loss 0.2842\n",
            "epoch 138/800, batch 56/75, loss 0.3026\n",
            "epoch 138/800, batch 57/75, loss 0.2130\n",
            "epoch 138/800, batch 58/75, loss 0.2914\n",
            "epoch 138/800, batch 59/75, loss 0.3640\n",
            "epoch 138/800, batch 60/75, loss 0.2998\n",
            "epoch 138/800, batch 61/75, loss 0.2684\n",
            "epoch 138/800, batch 62/75, loss 0.4708\n",
            "epoch 138/800, batch 63/75, loss 0.4009\n",
            "epoch 138/800, batch 64/75, loss 0.3135\n",
            "epoch 138/800, batch 65/75, loss 0.3286\n",
            "epoch 138/800, batch 66/75, loss 0.2893\n",
            "epoch 138/800, batch 67/75, loss 0.3773\n",
            "epoch 138/800, batch 68/75, loss 0.2741\n",
            "epoch 138/800, batch 69/75, loss 0.3571\n",
            "epoch 138/800, batch 70/75, loss 0.2029\n",
            "epoch 138/800, batch 71/75, loss 0.2343\n",
            "epoch 138/800, batch 72/75, loss 0.2615\n",
            "epoch 138/800, batch 73/75, loss 0.2394\n",
            "epoch 138/800, batch 74/75, loss 0.2295\n",
            "epoch 138/800, batch 75/75, loss 0.1949\n",
            "epoch 138/800, training roc_auc_score 0.8731\n",
            "EarlyStopping counter: 42 out of 80\n",
            "epoch 138/800, validation roc_auc_score 0.7987, best validation roc_auc_score 0.8157\n",
            "epoch 139/800, batch 1/75, loss 1.4458\n",
            "epoch 139/800, batch 2/75, loss 0.9100\n",
            "epoch 139/800, batch 3/75, loss 0.2362\n",
            "epoch 139/800, batch 4/75, loss 0.4596\n",
            "epoch 139/800, batch 5/75, loss 0.1925\n",
            "epoch 139/800, batch 6/75, loss 0.1539\n",
            "epoch 139/800, batch 7/75, loss 0.1520\n",
            "epoch 139/800, batch 8/75, loss 1.6019\n",
            "epoch 139/800, batch 9/75, loss 0.1197\n",
            "epoch 139/800, batch 10/75, loss 0.5006\n",
            "epoch 139/800, batch 11/75, loss 0.7570\n",
            "epoch 139/800, batch 12/75, loss 0.1936\n",
            "epoch 139/800, batch 13/75, loss 0.2642\n",
            "epoch 139/800, batch 14/75, loss 0.1129\n",
            "epoch 139/800, batch 15/75, loss 2.7445\n",
            "epoch 139/800, batch 16/75, loss 0.1516\n",
            "epoch 139/800, batch 17/75, loss 0.3028\n",
            "epoch 139/800, batch 18/75, loss 0.2326\n",
            "epoch 139/800, batch 19/75, loss 0.2809\n",
            "epoch 139/800, batch 20/75, loss 2.3114\n",
            "epoch 139/800, batch 21/75, loss 0.1605\n",
            "epoch 139/800, batch 22/75, loss 0.2557\n",
            "epoch 139/800, batch 23/75, loss 0.2019\n",
            "epoch 139/800, batch 24/75, loss 0.2565\n",
            "epoch 139/800, batch 25/75, loss 0.2178\n",
            "epoch 139/800, batch 26/75, loss 3.2660\n",
            "epoch 139/800, batch 27/75, loss 0.2682\n",
            "epoch 139/800, batch 28/75, loss 0.3210\n",
            "epoch 139/800, batch 29/75, loss 0.1960\n",
            "epoch 139/800, batch 30/75, loss 0.3630\n",
            "epoch 139/800, batch 31/75, loss 0.2625\n",
            "epoch 139/800, batch 32/75, loss 0.2772\n",
            "epoch 139/800, batch 33/75, loss 0.2471\n",
            "epoch 139/800, batch 34/75, loss 0.3587\n",
            "epoch 139/800, batch 35/75, loss 0.2332\n",
            "epoch 139/800, batch 36/75, loss 4.8071\n",
            "epoch 139/800, batch 37/75, loss 0.7162\n",
            "epoch 139/800, batch 38/75, loss 0.2404\n",
            "epoch 139/800, batch 39/75, loss 0.3523\n",
            "epoch 139/800, batch 40/75, loss 0.2547\n",
            "epoch 139/800, batch 41/75, loss 0.4114\n",
            "epoch 139/800, batch 42/75, loss 0.3720\n",
            "epoch 139/800, batch 43/75, loss 0.3249\n",
            "epoch 139/800, batch 44/75, loss 0.3224\n",
            "epoch 139/800, batch 45/75, loss 0.2493\n",
            "epoch 139/800, batch 46/75, loss 0.2803\n",
            "epoch 139/800, batch 47/75, loss 0.3547\n",
            "epoch 139/800, batch 48/75, loss 0.2130\n",
            "epoch 139/800, batch 49/75, loss 0.2350\n",
            "epoch 139/800, batch 50/75, loss 0.1888\n",
            "epoch 139/800, batch 51/75, loss 0.2076\n",
            "epoch 139/800, batch 52/75, loss 0.1254\n",
            "epoch 139/800, batch 53/75, loss 11.2820\n",
            "epoch 139/800, batch 54/75, loss 0.4140\n",
            "epoch 139/800, batch 55/75, loss 0.2899\n",
            "epoch 139/800, batch 56/75, loss 0.2944\n",
            "epoch 139/800, batch 57/75, loss 0.2094\n",
            "epoch 139/800, batch 58/75, loss 0.3198\n",
            "epoch 139/800, batch 59/75, loss 0.3609\n",
            "epoch 139/800, batch 60/75, loss 0.3438\n",
            "epoch 139/800, batch 61/75, loss 0.3421\n",
            "epoch 139/800, batch 62/75, loss 0.4760\n",
            "epoch 139/800, batch 63/75, loss 0.3937\n",
            "epoch 139/800, batch 64/75, loss 0.3251\n",
            "epoch 139/800, batch 65/75, loss 0.3541\n",
            "epoch 139/800, batch 66/75, loss 0.3592\n",
            "epoch 139/800, batch 67/75, loss 0.3166\n",
            "epoch 139/800, batch 68/75, loss 0.2882\n",
            "epoch 139/800, batch 69/75, loss 0.4149\n",
            "epoch 139/800, batch 70/75, loss 0.2321\n",
            "epoch 139/800, batch 71/75, loss 0.2606\n",
            "epoch 139/800, batch 72/75, loss 0.2949\n",
            "epoch 139/800, batch 73/75, loss 0.2320\n",
            "epoch 139/800, batch 74/75, loss 0.2201\n",
            "epoch 139/800, batch 75/75, loss 0.1639\n",
            "epoch 139/800, training roc_auc_score 0.8539\n",
            "EarlyStopping counter: 43 out of 80\n",
            "epoch 139/800, validation roc_auc_score 0.7924, best validation roc_auc_score 0.8157\n",
            "epoch 140/800, batch 1/75, loss 1.1696\n",
            "epoch 140/800, batch 2/75, loss 0.9309\n",
            "epoch 140/800, batch 3/75, loss 0.2257\n",
            "epoch 140/800, batch 4/75, loss 0.3858\n",
            "epoch 140/800, batch 5/75, loss 0.1738\n",
            "epoch 140/800, batch 6/75, loss 0.1313\n",
            "epoch 140/800, batch 7/75, loss 0.1373\n",
            "epoch 140/800, batch 8/75, loss 1.8721\n",
            "epoch 140/800, batch 9/75, loss 0.1126\n",
            "epoch 140/800, batch 10/75, loss 0.7414\n",
            "epoch 140/800, batch 11/75, loss 0.4347\n",
            "epoch 140/800, batch 12/75, loss 0.2488\n",
            "epoch 140/800, batch 13/75, loss 0.2213\n",
            "epoch 140/800, batch 14/75, loss 0.1175\n",
            "epoch 140/800, batch 15/75, loss 2.7397\n",
            "epoch 140/800, batch 16/75, loss 0.1248\n",
            "epoch 140/800, batch 17/75, loss 0.2776\n",
            "epoch 140/800, batch 18/75, loss 0.1991\n",
            "epoch 140/800, batch 19/75, loss 0.2582\n",
            "epoch 140/800, batch 20/75, loss 3.4245\n",
            "epoch 140/800, batch 21/75, loss 0.1283\n",
            "epoch 140/800, batch 22/75, loss 0.2163\n",
            "epoch 140/800, batch 23/75, loss 0.1294\n",
            "epoch 140/800, batch 24/75, loss 0.1912\n",
            "epoch 140/800, batch 25/75, loss 0.1772\n",
            "epoch 140/800, batch 26/75, loss 4.3853\n",
            "epoch 140/800, batch 27/75, loss 0.2780\n",
            "epoch 140/800, batch 28/75, loss 0.3314\n",
            "epoch 140/800, batch 29/75, loss 0.2372\n",
            "epoch 140/800, batch 30/75, loss 0.4297\n",
            "epoch 140/800, batch 31/75, loss 0.2906\n",
            "epoch 140/800, batch 32/75, loss 0.3047\n",
            "epoch 140/800, batch 33/75, loss 0.2882\n",
            "epoch 140/800, batch 34/75, loss 0.3783\n",
            "epoch 140/800, batch 35/75, loss 0.3362\n",
            "epoch 140/800, batch 36/75, loss 3.7565\n",
            "epoch 140/800, batch 37/75, loss 0.6733\n",
            "epoch 140/800, batch 38/75, loss 0.2936\n",
            "epoch 140/800, batch 39/75, loss 0.3878\n",
            "epoch 140/800, batch 40/75, loss 0.2959\n",
            "epoch 140/800, batch 41/75, loss 0.4595\n",
            "epoch 140/800, batch 42/75, loss 0.3319\n",
            "epoch 140/800, batch 43/75, loss 0.3239\n",
            "epoch 140/800, batch 44/75, loss 0.3302\n",
            "epoch 140/800, batch 45/75, loss 0.2443\n",
            "epoch 140/800, batch 46/75, loss 0.2611\n",
            "epoch 140/800, batch 47/75, loss 0.2742\n",
            "epoch 140/800, batch 48/75, loss 0.2294\n",
            "epoch 140/800, batch 49/75, loss 0.2048\n",
            "epoch 140/800, batch 50/75, loss 0.2018\n",
            "epoch 140/800, batch 51/75, loss 0.1966\n",
            "epoch 140/800, batch 52/75, loss 0.0913\n",
            "epoch 140/800, batch 53/75, loss 13.6235\n",
            "epoch 140/800, batch 54/75, loss 0.4605\n",
            "epoch 140/800, batch 55/75, loss 0.2373\n",
            "epoch 140/800, batch 56/75, loss 0.3574\n",
            "epoch 140/800, batch 57/75, loss 0.2848\n",
            "epoch 140/800, batch 58/75, loss 0.3923\n",
            "epoch 140/800, batch 59/75, loss 0.4453\n",
            "epoch 140/800, batch 60/75, loss 0.4774\n",
            "epoch 140/800, batch 61/75, loss 0.4637\n",
            "epoch 140/800, batch 62/75, loss 0.6530\n",
            "epoch 140/800, batch 63/75, loss 0.5417\n",
            "epoch 140/800, batch 64/75, loss 0.4393\n",
            "epoch 140/800, batch 65/75, loss 0.4881\n",
            "epoch 140/800, batch 66/75, loss 0.4531\n",
            "epoch 140/800, batch 67/75, loss 0.4813\n",
            "epoch 140/800, batch 68/75, loss 0.3891\n",
            "epoch 140/800, batch 69/75, loss 0.5276\n",
            "epoch 140/800, batch 70/75, loss 0.2918\n",
            "epoch 140/800, batch 71/75, loss 0.3269\n",
            "epoch 140/800, batch 72/75, loss 0.3446\n",
            "epoch 140/800, batch 73/75, loss 0.3109\n",
            "epoch 140/800, batch 74/75, loss 0.3484\n",
            "epoch 140/800, batch 75/75, loss 0.2019\n",
            "epoch 140/800, training roc_auc_score 0.8157\n",
            "EarlyStopping counter: 44 out of 80\n",
            "epoch 140/800, validation roc_auc_score 0.7918, best validation roc_auc_score 0.8157\n",
            "epoch 141/800, batch 1/75, loss 0.8609\n",
            "epoch 141/800, batch 2/75, loss 1.3267\n",
            "epoch 141/800, batch 3/75, loss 0.2772\n",
            "epoch 141/800, batch 4/75, loss 0.4681\n",
            "epoch 141/800, batch 5/75, loss 0.2358\n",
            "epoch 141/800, batch 6/75, loss 0.1704\n",
            "epoch 141/800, batch 7/75, loss 0.1534\n",
            "epoch 141/800, batch 8/75, loss 1.7020\n",
            "epoch 141/800, batch 9/75, loss 0.1378\n",
            "epoch 141/800, batch 10/75, loss 0.3422\n",
            "epoch 141/800, batch 11/75, loss 0.3078\n",
            "epoch 141/800, batch 12/75, loss 0.2415\n",
            "epoch 141/800, batch 13/75, loss 0.2244\n",
            "epoch 141/800, batch 14/75, loss 0.1316\n",
            "epoch 141/800, batch 15/75, loss 2.8419\n",
            "epoch 141/800, batch 16/75, loss 0.1439\n",
            "epoch 141/800, batch 17/75, loss 0.3053\n",
            "epoch 141/800, batch 18/75, loss 0.1759\n",
            "epoch 141/800, batch 19/75, loss 0.2313\n",
            "epoch 141/800, batch 20/75, loss 3.7881\n",
            "epoch 141/800, batch 21/75, loss 0.1242\n",
            "epoch 141/800, batch 22/75, loss 0.2581\n",
            "epoch 141/800, batch 23/75, loss 0.1304\n",
            "epoch 141/800, batch 24/75, loss 0.1936\n",
            "epoch 141/800, batch 25/75, loss 0.1611\n",
            "epoch 141/800, batch 26/75, loss 4.1871\n",
            "epoch 141/800, batch 27/75, loss 0.2104\n",
            "epoch 141/800, batch 28/75, loss 0.2855\n",
            "epoch 141/800, batch 29/75, loss 0.1609\n",
            "epoch 141/800, batch 30/75, loss 0.3641\n",
            "epoch 141/800, batch 31/75, loss 0.2487\n",
            "epoch 141/800, batch 32/75, loss 0.2677\n",
            "epoch 141/800, batch 33/75, loss 0.2728\n",
            "epoch 141/800, batch 34/75, loss 0.3507\n",
            "epoch 141/800, batch 35/75, loss 0.3093\n",
            "epoch 141/800, batch 36/75, loss 4.7731\n",
            "epoch 141/800, batch 37/75, loss 0.7674\n",
            "epoch 141/800, batch 38/75, loss 0.2459\n",
            "epoch 141/800, batch 39/75, loss 0.4455\n",
            "epoch 141/800, batch 40/75, loss 0.2984\n",
            "epoch 141/800, batch 41/75, loss 0.5144\n",
            "epoch 141/800, batch 42/75, loss 0.3544\n",
            "epoch 141/800, batch 43/75, loss 0.3866\n",
            "epoch 141/800, batch 44/75, loss 0.3904\n",
            "epoch 141/800, batch 45/75, loss 0.2888\n",
            "epoch 141/800, batch 46/75, loss 0.3555\n",
            "epoch 141/800, batch 47/75, loss 0.3312\n",
            "epoch 141/800, batch 48/75, loss 0.2570\n",
            "epoch 141/800, batch 49/75, loss 0.2803\n",
            "epoch 141/800, batch 50/75, loss 0.2441\n",
            "epoch 141/800, batch 51/75, loss 0.2212\n",
            "epoch 141/800, batch 52/75, loss 0.1490\n",
            "epoch 141/800, batch 53/75, loss 9.8677\n",
            "epoch 141/800, batch 54/75, loss 0.3505\n",
            "epoch 141/800, batch 55/75, loss 0.2567\n",
            "epoch 141/800, batch 56/75, loss 0.3239\n",
            "epoch 141/800, batch 57/75, loss 0.2128\n",
            "epoch 141/800, batch 58/75, loss 0.3004\n",
            "epoch 141/800, batch 59/75, loss 0.3355\n",
            "epoch 141/800, batch 60/75, loss 0.3401\n",
            "epoch 141/800, batch 61/75, loss 0.2625\n",
            "epoch 141/800, batch 62/75, loss 0.4276\n",
            "epoch 141/800, batch 63/75, loss 0.3671\n",
            "epoch 141/800, batch 64/75, loss 0.2879\n",
            "epoch 141/800, batch 65/75, loss 0.3199\n",
            "epoch 141/800, batch 66/75, loss 0.2617\n",
            "epoch 141/800, batch 67/75, loss 0.3337\n",
            "epoch 141/800, batch 68/75, loss 0.2423\n",
            "epoch 141/800, batch 69/75, loss 0.2548\n",
            "epoch 141/800, batch 70/75, loss 0.2145\n",
            "epoch 141/800, batch 71/75, loss 0.2202\n",
            "epoch 141/800, batch 72/75, loss 0.2928\n",
            "epoch 141/800, batch 73/75, loss 0.2121\n",
            "epoch 141/800, batch 74/75, loss 0.2280\n",
            "epoch 141/800, batch 75/75, loss 0.1558\n",
            "epoch 141/800, training roc_auc_score 0.8516\n",
            "EarlyStopping counter: 45 out of 80\n",
            "epoch 141/800, validation roc_auc_score 0.7859, best validation roc_auc_score 0.8157\n",
            "epoch 142/800, batch 1/75, loss 1.2048\n",
            "epoch 142/800, batch 2/75, loss 0.9614\n",
            "epoch 142/800, batch 3/75, loss 0.1876\n",
            "epoch 142/800, batch 4/75, loss 0.4018\n",
            "epoch 142/800, batch 5/75, loss 0.1803\n",
            "epoch 142/800, batch 6/75, loss 0.1338\n",
            "epoch 142/800, batch 7/75, loss 0.1521\n",
            "epoch 142/800, batch 8/75, loss 1.9582\n",
            "epoch 142/800, batch 9/75, loss 0.1188\n",
            "epoch 142/800, batch 10/75, loss 0.4710\n",
            "epoch 142/800, batch 11/75, loss 0.4022\n",
            "epoch 142/800, batch 12/75, loss 0.2234\n",
            "epoch 142/800, batch 13/75, loss 0.2055\n",
            "epoch 142/800, batch 14/75, loss 0.0996\n",
            "epoch 142/800, batch 15/75, loss 3.2510\n",
            "epoch 142/800, batch 16/75, loss 0.1239\n",
            "epoch 142/800, batch 17/75, loss 0.2346\n",
            "epoch 142/800, batch 18/75, loss 0.2301\n",
            "epoch 142/800, batch 19/75, loss 0.2060\n",
            "epoch 142/800, batch 20/75, loss 3.4091\n",
            "epoch 142/800, batch 21/75, loss 0.1346\n",
            "epoch 142/800, batch 22/75, loss 0.2076\n",
            "epoch 142/800, batch 23/75, loss 0.1657\n",
            "epoch 142/800, batch 24/75, loss 0.2327\n",
            "epoch 142/800, batch 25/75, loss 0.2123\n",
            "epoch 142/800, batch 26/75, loss 3.6343\n",
            "epoch 142/800, batch 27/75, loss 0.3044\n",
            "epoch 142/800, batch 28/75, loss 0.3978\n",
            "epoch 142/800, batch 29/75, loss 0.3143\n",
            "epoch 142/800, batch 30/75, loss 0.5510\n",
            "epoch 142/800, batch 31/75, loss 0.3825\n",
            "epoch 142/800, batch 32/75, loss 0.4323\n",
            "epoch 142/800, batch 33/75, loss 0.3746\n",
            "epoch 142/800, batch 34/75, loss 0.4570\n",
            "epoch 142/800, batch 35/75, loss 0.3600\n",
            "epoch 142/800, batch 36/75, loss 3.9071\n",
            "epoch 142/800, batch 37/75, loss 0.5957\n",
            "epoch 142/800, batch 38/75, loss 0.3152\n",
            "epoch 142/800, batch 39/75, loss 0.4940\n",
            "epoch 142/800, batch 40/75, loss 0.3528\n",
            "epoch 142/800, batch 41/75, loss 0.5134\n",
            "epoch 142/800, batch 42/75, loss 0.3726\n",
            "epoch 142/800, batch 43/75, loss 0.3443\n",
            "epoch 142/800, batch 44/75, loss 0.3494\n",
            "epoch 142/800, batch 45/75, loss 0.2751\n",
            "epoch 142/800, batch 46/75, loss 0.2239\n",
            "epoch 142/800, batch 47/75, loss 0.2702\n",
            "epoch 142/800, batch 48/75, loss 0.2009\n",
            "epoch 142/800, batch 49/75, loss 0.1935\n",
            "epoch 142/800, batch 50/75, loss 0.1736\n",
            "epoch 142/800, batch 51/75, loss 0.1555\n",
            "epoch 142/800, batch 52/75, loss 0.0953\n",
            "epoch 142/800, batch 53/75, loss 12.5181\n",
            "epoch 142/800, batch 54/75, loss 0.5384\n",
            "epoch 142/800, batch 55/75, loss 0.2615\n",
            "epoch 142/800, batch 56/75, loss 0.3004\n",
            "epoch 142/800, batch 57/75, loss 0.2726\n",
            "epoch 142/800, batch 58/75, loss 0.3332\n",
            "epoch 142/800, batch 59/75, loss 0.4672\n",
            "epoch 142/800, batch 60/75, loss 0.4519\n",
            "epoch 142/800, batch 61/75, loss 0.4480\n",
            "epoch 142/800, batch 62/75, loss 0.6310\n",
            "epoch 142/800, batch 63/75, loss 0.5268\n",
            "epoch 142/800, batch 64/75, loss 0.4634\n",
            "epoch 142/800, batch 65/75, loss 0.4939\n",
            "epoch 142/800, batch 66/75, loss 0.4190\n",
            "epoch 142/800, batch 67/75, loss 0.4320\n",
            "epoch 142/800, batch 68/75, loss 0.4276\n",
            "epoch 142/800, batch 69/75, loss 0.4140\n",
            "epoch 142/800, batch 70/75, loss 0.2473\n",
            "epoch 142/800, batch 71/75, loss 0.2356\n",
            "epoch 142/800, batch 72/75, loss 0.2058\n",
            "epoch 142/800, batch 73/75, loss 0.1194\n",
            "epoch 142/800, batch 74/75, loss 0.1068\n",
            "epoch 142/800, batch 75/75, loss 0.0725\n",
            "epoch 142/800, training roc_auc_score 0.8300\n",
            "EarlyStopping counter: 46 out of 80\n",
            "epoch 142/800, validation roc_auc_score 0.7347, best validation roc_auc_score 0.8157\n",
            "epoch 143/800, batch 1/75, loss 1.2129\n",
            "epoch 143/800, batch 2/75, loss 0.7697\n",
            "epoch 143/800, batch 3/75, loss 0.6292\n",
            "epoch 143/800, batch 4/75, loss 0.4402\n",
            "epoch 143/800, batch 5/75, loss 0.0499\n",
            "epoch 143/800, batch 6/75, loss 0.0733\n",
            "epoch 143/800, batch 7/75, loss 0.0694\n",
            "epoch 143/800, batch 8/75, loss 2.3568\n",
            "epoch 143/800, batch 9/75, loss 0.1028\n",
            "epoch 143/800, batch 10/75, loss 1.6860\n",
            "epoch 143/800, batch 11/75, loss 0.3008\n",
            "epoch 143/800, batch 12/75, loss 0.1644\n",
            "epoch 143/800, batch 13/75, loss 0.2559\n",
            "epoch 143/800, batch 14/75, loss 0.1294\n",
            "epoch 143/800, batch 15/75, loss 3.2226\n",
            "epoch 143/800, batch 16/75, loss 0.2634\n",
            "epoch 143/800, batch 17/75, loss 0.4513\n",
            "epoch 143/800, batch 18/75, loss 0.4302\n",
            "epoch 143/800, batch 19/75, loss 0.4741\n",
            "epoch 143/800, batch 20/75, loss 2.4383\n",
            "epoch 143/800, batch 21/75, loss 0.3679\n",
            "epoch 143/800, batch 22/75, loss 0.5998\n",
            "epoch 143/800, batch 23/75, loss 0.4111\n",
            "epoch 143/800, batch 24/75, loss 0.5439\n",
            "epoch 143/800, batch 25/75, loss 0.5165\n",
            "epoch 143/800, batch 26/75, loss 2.1026\n",
            "epoch 143/800, batch 27/75, loss 0.3621\n",
            "epoch 143/800, batch 28/75, loss 0.5492\n",
            "epoch 143/800, batch 29/75, loss 0.2734\n",
            "epoch 143/800, batch 30/75, loss 0.5264\n",
            "epoch 143/800, batch 31/75, loss 0.3071\n",
            "epoch 143/800, batch 32/75, loss 0.2920\n",
            "epoch 143/800, batch 33/75, loss 0.2240\n",
            "epoch 143/800, batch 34/75, loss 0.2999\n",
            "epoch 143/800, batch 35/75, loss 0.2327\n",
            "epoch 143/800, batch 36/75, loss 6.0116\n",
            "epoch 143/800, batch 37/75, loss 0.9095\n",
            "epoch 143/800, batch 38/75, loss 0.1962\n",
            "epoch 143/800, batch 39/75, loss 0.3273\n",
            "epoch 143/800, batch 40/75, loss 0.2006\n",
            "epoch 143/800, batch 41/75, loss 0.4556\n",
            "epoch 143/800, batch 42/75, loss 0.2765\n",
            "epoch 143/800, batch 43/75, loss 0.2536\n",
            "epoch 143/800, batch 44/75, loss 0.2982\n",
            "epoch 143/800, batch 45/75, loss 0.2686\n",
            "epoch 143/800, batch 46/75, loss 0.2092\n",
            "epoch 143/800, batch 47/75, loss 0.2603\n",
            "epoch 143/800, batch 48/75, loss 0.1583\n",
            "epoch 143/800, batch 49/75, loss 0.1837\n",
            "epoch 143/800, batch 50/75, loss 0.1423\n",
            "epoch 143/800, batch 51/75, loss 0.1417\n",
            "epoch 143/800, batch 52/75, loss 0.0736\n",
            "epoch 143/800, batch 53/75, loss 17.0790\n",
            "epoch 143/800, batch 54/75, loss 0.4758\n",
            "epoch 143/800, batch 55/75, loss 0.3219\n",
            "epoch 143/800, batch 56/75, loss 0.4418\n",
            "epoch 143/800, batch 57/75, loss 0.4343\n",
            "epoch 143/800, batch 58/75, loss 0.6767\n",
            "epoch 143/800, batch 59/75, loss 0.8272\n",
            "epoch 143/800, batch 60/75, loss 0.6925\n",
            "epoch 143/800, batch 61/75, loss 0.8654\n",
            "epoch 143/800, batch 62/75, loss 1.0904\n",
            "epoch 143/800, batch 63/75, loss 0.9695\n",
            "epoch 143/800, batch 64/75, loss 0.7769\n",
            "epoch 143/800, batch 65/75, loss 0.8360\n",
            "epoch 143/800, batch 66/75, loss 0.6793\n",
            "epoch 143/800, batch 67/75, loss 0.7555\n",
            "epoch 143/800, batch 68/75, loss 0.5637\n",
            "epoch 143/800, batch 69/75, loss 0.5628\n",
            "epoch 143/800, batch 70/75, loss 0.3802\n",
            "epoch 143/800, batch 71/75, loss 0.3673\n",
            "epoch 143/800, batch 72/75, loss 0.4342\n",
            "epoch 143/800, batch 73/75, loss 0.3479\n",
            "epoch 143/800, batch 74/75, loss 0.3626\n",
            "epoch 143/800, batch 75/75, loss 0.2669\n",
            "epoch 143/800, training roc_auc_score 0.7734\n",
            "EarlyStopping counter: 47 out of 80\n",
            "epoch 143/800, validation roc_auc_score 0.7983, best validation roc_auc_score 0.8157\n",
            "epoch 144/800, batch 1/75, loss 0.9410\n",
            "epoch 144/800, batch 2/75, loss 1.3287\n",
            "epoch 144/800, batch 3/75, loss 0.3514\n",
            "epoch 144/800, batch 4/75, loss 0.4747\n",
            "epoch 144/800, batch 5/75, loss 0.2297\n",
            "epoch 144/800, batch 6/75, loss 0.2235\n",
            "epoch 144/800, batch 7/75, loss 0.1733\n",
            "epoch 144/800, batch 8/75, loss 1.9307\n",
            "epoch 144/800, batch 9/75, loss 0.1580\n",
            "epoch 144/800, batch 10/75, loss 0.4674\n",
            "epoch 144/800, batch 11/75, loss 0.3238\n",
            "epoch 144/800, batch 12/75, loss 0.2955\n",
            "epoch 144/800, batch 13/75, loss 0.2471\n",
            "epoch 144/800, batch 14/75, loss 0.1410\n",
            "epoch 144/800, batch 15/75, loss 3.7222\n",
            "epoch 144/800, batch 16/75, loss 0.1169\n",
            "epoch 144/800, batch 17/75, loss 0.2660\n",
            "epoch 144/800, batch 18/75, loss 0.2244\n",
            "epoch 144/800, batch 19/75, loss 0.2777\n",
            "epoch 144/800, batch 20/75, loss 2.4617\n",
            "epoch 144/800, batch 21/75, loss 0.1974\n",
            "epoch 144/800, batch 22/75, loss 0.1875\n",
            "epoch 144/800, batch 23/75, loss 0.1176\n",
            "epoch 144/800, batch 24/75, loss 0.2101\n",
            "epoch 144/800, batch 25/75, loss 0.2007\n",
            "epoch 144/800, batch 26/75, loss 3.5473\n",
            "epoch 144/800, batch 27/75, loss 0.2154\n",
            "epoch 144/800, batch 28/75, loss 0.2324\n",
            "epoch 144/800, batch 29/75, loss 0.1651\n",
            "epoch 144/800, batch 30/75, loss 0.3593\n",
            "epoch 144/800, batch 31/75, loss 0.2415\n",
            "epoch 144/800, batch 32/75, loss 0.2862\n",
            "epoch 144/800, batch 33/75, loss 0.2707\n",
            "epoch 144/800, batch 34/75, loss 0.4327\n",
            "epoch 144/800, batch 35/75, loss 0.3006\n",
            "epoch 144/800, batch 36/75, loss 3.9565\n",
            "epoch 144/800, batch 37/75, loss 0.7227\n",
            "epoch 144/800, batch 38/75, loss 0.2858\n",
            "epoch 144/800, batch 39/75, loss 0.5060\n",
            "epoch 144/800, batch 40/75, loss 0.2638\n",
            "epoch 144/800, batch 41/75, loss 0.5380\n",
            "epoch 144/800, batch 42/75, loss 0.3350\n",
            "epoch 144/800, batch 43/75, loss 0.2707\n",
            "epoch 144/800, batch 44/75, loss 0.3335\n",
            "epoch 144/800, batch 45/75, loss 0.2624\n",
            "epoch 144/800, batch 46/75, loss 0.2604\n",
            "epoch 144/800, batch 47/75, loss 0.3708\n",
            "epoch 144/800, batch 48/75, loss 0.1916\n",
            "epoch 144/800, batch 49/75, loss 0.2063\n",
            "epoch 144/800, batch 50/75, loss 0.1931\n",
            "epoch 144/800, batch 51/75, loss 0.1659\n",
            "epoch 144/800, batch 52/75, loss 0.1081\n",
            "epoch 144/800, batch 53/75, loss 11.7759\n",
            "epoch 144/800, batch 54/75, loss 0.3449\n",
            "epoch 144/800, batch 55/75, loss 0.2711\n",
            "epoch 144/800, batch 56/75, loss 0.3092\n",
            "epoch 144/800, batch 57/75, loss 0.2025\n",
            "epoch 144/800, batch 58/75, loss 0.3708\n",
            "epoch 144/800, batch 59/75, loss 0.4827\n",
            "epoch 144/800, batch 60/75, loss 0.4397\n",
            "epoch 144/800, batch 61/75, loss 0.5065\n",
            "epoch 144/800, batch 62/75, loss 0.5767\n",
            "epoch 144/800, batch 63/75, loss 0.5967\n",
            "epoch 144/800, batch 64/75, loss 0.4300\n",
            "epoch 144/800, batch 65/75, loss 0.5238\n",
            "epoch 144/800, batch 66/75, loss 0.3913\n",
            "epoch 144/800, batch 67/75, loss 0.4622\n",
            "epoch 144/800, batch 68/75, loss 0.4224\n",
            "epoch 144/800, batch 69/75, loss 0.3913\n",
            "epoch 144/800, batch 70/75, loss 0.3042\n",
            "epoch 144/800, batch 71/75, loss 0.3232\n",
            "epoch 144/800, batch 72/75, loss 0.3964\n",
            "epoch 144/800, batch 73/75, loss 0.3067\n",
            "epoch 144/800, batch 74/75, loss 0.2415\n",
            "epoch 144/800, batch 75/75, loss 0.1969\n",
            "epoch 144/800, training roc_auc_score 0.8501\n",
            "EarlyStopping counter: 48 out of 80\n",
            "epoch 144/800, validation roc_auc_score 0.7941, best validation roc_auc_score 0.8157\n",
            "epoch 145/800, batch 1/75, loss 1.1186\n",
            "epoch 145/800, batch 2/75, loss 1.4184\n",
            "epoch 145/800, batch 3/75, loss 0.2296\n",
            "epoch 145/800, batch 4/75, loss 0.4021\n",
            "epoch 145/800, batch 5/75, loss 0.2543\n",
            "epoch 145/800, batch 6/75, loss 0.1420\n",
            "epoch 145/800, batch 7/75, loss 0.1812\n",
            "epoch 145/800, batch 8/75, loss 2.4175\n",
            "epoch 145/800, batch 9/75, loss 0.1106\n",
            "epoch 145/800, batch 10/75, loss 0.6144\n",
            "epoch 145/800, batch 11/75, loss 0.3375\n",
            "epoch 145/800, batch 12/75, loss 0.1802\n",
            "epoch 145/800, batch 13/75, loss 0.1919\n",
            "epoch 145/800, batch 14/75, loss 0.1272\n",
            "epoch 145/800, batch 15/75, loss 3.5827\n",
            "epoch 145/800, batch 16/75, loss 0.1333\n",
            "epoch 145/800, batch 17/75, loss 0.2351\n",
            "epoch 145/800, batch 18/75, loss 0.2140\n",
            "epoch 145/800, batch 19/75, loss 0.2124\n",
            "epoch 145/800, batch 20/75, loss 2.7666\n",
            "epoch 145/800, batch 21/75, loss 0.1523\n",
            "epoch 145/800, batch 22/75, loss 0.2083\n",
            "epoch 145/800, batch 23/75, loss 0.1329\n",
            "epoch 145/800, batch 24/75, loss 0.2349\n",
            "epoch 145/800, batch 25/75, loss 0.1366\n",
            "epoch 145/800, batch 26/75, loss 4.1133\n",
            "epoch 145/800, batch 27/75, loss 0.1917\n",
            "epoch 145/800, batch 28/75, loss 0.3316\n",
            "epoch 145/800, batch 29/75, loss 0.1808\n",
            "epoch 145/800, batch 30/75, loss 0.3886\n",
            "epoch 145/800, batch 31/75, loss 0.2737\n",
            "epoch 145/800, batch 32/75, loss 0.3159\n",
            "epoch 145/800, batch 33/75, loss 0.2323\n",
            "epoch 145/800, batch 34/75, loss 0.4055\n",
            "epoch 145/800, batch 35/75, loss 0.2545\n",
            "epoch 145/800, batch 36/75, loss 3.8719\n",
            "epoch 145/800, batch 37/75, loss 0.6168\n",
            "epoch 145/800, batch 38/75, loss 0.2362\n",
            "epoch 145/800, batch 39/75, loss 0.4690\n",
            "epoch 145/800, batch 40/75, loss 0.2623\n",
            "epoch 145/800, batch 41/75, loss 0.4330\n",
            "epoch 145/800, batch 42/75, loss 0.3260\n",
            "epoch 145/800, batch 43/75, loss 0.3001\n",
            "epoch 145/800, batch 44/75, loss 0.3990\n",
            "epoch 145/800, batch 45/75, loss 0.2693\n",
            "epoch 145/800, batch 46/75, loss 0.3074\n",
            "epoch 145/800, batch 47/75, loss 0.3363\n",
            "epoch 145/800, batch 48/75, loss 0.1986\n",
            "epoch 145/800, batch 49/75, loss 0.2540\n",
            "epoch 145/800, batch 50/75, loss 0.2075\n",
            "epoch 145/800, batch 51/75, loss 0.2075\n",
            "epoch 145/800, batch 52/75, loss 0.1415\n",
            "epoch 145/800, batch 53/75, loss 11.9790\n",
            "epoch 145/800, batch 54/75, loss 0.3354\n",
            "epoch 145/800, batch 55/75, loss 0.2914\n",
            "epoch 145/800, batch 56/75, loss 0.3620\n",
            "epoch 145/800, batch 57/75, loss 0.2793\n",
            "epoch 145/800, batch 58/75, loss 0.4486\n",
            "epoch 145/800, batch 59/75, loss 0.5201\n",
            "epoch 145/800, batch 60/75, loss 0.4574\n",
            "epoch 145/800, batch 61/75, loss 0.4927\n",
            "epoch 145/800, batch 62/75, loss 0.7251\n",
            "epoch 145/800, batch 63/75, loss 0.5956\n",
            "epoch 145/800, batch 64/75, loss 0.4871\n",
            "epoch 145/800, batch 65/75, loss 0.5179\n",
            "epoch 145/800, batch 66/75, loss 0.4169\n",
            "epoch 145/800, batch 67/75, loss 0.4963\n",
            "epoch 145/800, batch 68/75, loss 0.4004\n",
            "epoch 145/800, batch 69/75, loss 0.4744\n",
            "epoch 145/800, batch 70/75, loss 0.2745\n",
            "epoch 145/800, batch 71/75, loss 0.3707\n",
            "epoch 145/800, batch 72/75, loss 0.3068\n",
            "epoch 145/800, batch 73/75, loss 0.2435\n",
            "epoch 145/800, batch 74/75, loss 0.1829\n",
            "epoch 145/800, batch 75/75, loss 0.1846\n",
            "epoch 145/800, training roc_auc_score 0.8350\n",
            "EarlyStopping counter: 49 out of 80\n",
            "epoch 145/800, validation roc_auc_score 0.7970, best validation roc_auc_score 0.8157\n",
            "epoch 146/800, batch 1/75, loss 0.8088\n",
            "epoch 146/800, batch 2/75, loss 1.0537\n",
            "epoch 146/800, batch 3/75, loss 0.2308\n",
            "epoch 146/800, batch 4/75, loss 0.3667\n",
            "epoch 146/800, batch 5/75, loss 0.1295\n",
            "epoch 146/800, batch 6/75, loss 0.0941\n",
            "epoch 146/800, batch 7/75, loss 0.0822\n",
            "epoch 146/800, batch 8/75, loss 2.0493\n",
            "epoch 146/800, batch 9/75, loss 0.0917\n",
            "epoch 146/800, batch 10/75, loss 2.4767\n",
            "epoch 146/800, batch 11/75, loss 0.2162\n",
            "epoch 146/800, batch 12/75, loss 0.2028\n",
            "epoch 146/800, batch 13/75, loss 0.1728\n",
            "epoch 146/800, batch 14/75, loss 0.1354\n",
            "epoch 146/800, batch 15/75, loss 2.9916\n",
            "epoch 146/800, batch 16/75, loss 0.1436\n",
            "epoch 146/800, batch 17/75, loss 0.3342\n",
            "epoch 146/800, batch 18/75, loss 0.3961\n",
            "epoch 146/800, batch 19/75, loss 0.3934\n",
            "epoch 146/800, batch 20/75, loss 3.2919\n",
            "epoch 146/800, batch 21/75, loss 0.2815\n",
            "epoch 146/800, batch 22/75, loss 0.4172\n",
            "epoch 146/800, batch 23/75, loss 0.2619\n",
            "epoch 146/800, batch 24/75, loss 0.3641\n",
            "epoch 146/800, batch 25/75, loss 0.3327\n",
            "epoch 146/800, batch 26/75, loss 2.8855\n",
            "epoch 146/800, batch 27/75, loss 0.4053\n",
            "epoch 146/800, batch 28/75, loss 0.5506\n",
            "epoch 146/800, batch 29/75, loss 0.2768\n",
            "epoch 146/800, batch 30/75, loss 0.5180\n",
            "epoch 146/800, batch 31/75, loss 0.3880\n",
            "epoch 146/800, batch 32/75, loss 0.3073\n",
            "epoch 146/800, batch 33/75, loss 0.2739\n",
            "epoch 146/800, batch 34/75, loss 0.4182\n",
            "epoch 146/800, batch 35/75, loss 0.2875\n",
            "epoch 146/800, batch 36/75, loss 4.2545\n",
            "epoch 146/800, batch 37/75, loss 0.7005\n",
            "epoch 146/800, batch 38/75, loss 0.1723\n",
            "epoch 146/800, batch 39/75, loss 0.3317\n",
            "epoch 146/800, batch 40/75, loss 0.2132\n",
            "epoch 146/800, batch 41/75, loss 0.3388\n",
            "epoch 146/800, batch 42/75, loss 0.2465\n",
            "epoch 146/800, batch 43/75, loss 0.2448\n",
            "epoch 146/800, batch 44/75, loss 0.2732\n",
            "epoch 146/800, batch 45/75, loss 0.1944\n",
            "epoch 146/800, batch 46/75, loss 0.1942\n",
            "epoch 146/800, batch 47/75, loss 0.2292\n",
            "epoch 146/800, batch 48/75, loss 0.1649\n",
            "epoch 146/800, batch 49/75, loss 0.1756\n",
            "epoch 146/800, batch 50/75, loss 0.1781\n",
            "epoch 146/800, batch 51/75, loss 0.1679\n",
            "epoch 146/800, batch 52/75, loss 0.1105\n",
            "epoch 146/800, batch 53/75, loss 14.3854\n",
            "epoch 146/800, batch 54/75, loss 0.3190\n",
            "epoch 146/800, batch 55/75, loss 0.2721\n",
            "epoch 146/800, batch 56/75, loss 0.3520\n",
            "epoch 146/800, batch 57/75, loss 0.2839\n",
            "epoch 146/800, batch 58/75, loss 0.4505\n",
            "epoch 146/800, batch 59/75, loss 0.5527\n",
            "epoch 146/800, batch 60/75, loss 0.5214\n",
            "epoch 146/800, batch 61/75, loss 0.4979\n",
            "epoch 146/800, batch 62/75, loss 0.7738\n",
            "epoch 146/800, batch 63/75, loss 0.5791\n",
            "epoch 146/800, batch 64/75, loss 0.5092\n",
            "epoch 146/800, batch 65/75, loss 0.5973\n",
            "epoch 146/800, batch 66/75, loss 0.5072\n",
            "epoch 146/800, batch 67/75, loss 0.5934\n",
            "epoch 146/800, batch 68/75, loss 0.5189\n",
            "epoch 146/800, batch 69/75, loss 0.5012\n",
            "epoch 146/800, batch 70/75, loss 0.3216\n",
            "epoch 146/800, batch 71/75, loss 0.4173\n",
            "epoch 146/800, batch 72/75, loss 0.4704\n",
            "epoch 146/800, batch 73/75, loss 0.3789\n",
            "epoch 146/800, batch 74/75, loss 0.3841\n",
            "epoch 146/800, batch 75/75, loss 0.2530\n",
            "epoch 146/800, training roc_auc_score 0.8181\n",
            "EarlyStopping counter: 50 out of 80\n",
            "epoch 146/800, validation roc_auc_score 0.7855, best validation roc_auc_score 0.8157\n",
            "epoch 147/800, batch 1/75, loss 1.0647\n",
            "epoch 147/800, batch 2/75, loss 1.2078\n",
            "epoch 147/800, batch 3/75, loss 0.3345\n",
            "epoch 147/800, batch 4/75, loss 0.5196\n",
            "epoch 147/800, batch 5/75, loss 0.2519\n",
            "epoch 147/800, batch 6/75, loss 0.2247\n",
            "epoch 147/800, batch 7/75, loss 0.2186\n",
            "epoch 147/800, batch 8/75, loss 1.7085\n",
            "epoch 147/800, batch 9/75, loss 0.1366\n",
            "epoch 147/800, batch 10/75, loss 0.4846\n",
            "epoch 147/800, batch 11/75, loss 0.9476\n",
            "epoch 147/800, batch 12/75, loss 0.3586\n",
            "epoch 147/800, batch 13/75, loss 0.3604\n",
            "epoch 147/800, batch 14/75, loss 0.1539\n",
            "epoch 147/800, batch 15/75, loss 2.5043\n",
            "epoch 147/800, batch 16/75, loss 0.1753\n",
            "epoch 147/800, batch 17/75, loss 0.4015\n",
            "epoch 147/800, batch 18/75, loss 0.3733\n",
            "epoch 147/800, batch 19/75, loss 0.3913\n",
            "epoch 147/800, batch 20/75, loss 2.1827\n",
            "epoch 147/800, batch 21/75, loss 0.1551\n",
            "epoch 147/800, batch 22/75, loss 0.2502\n",
            "epoch 147/800, batch 23/75, loss 0.1091\n",
            "epoch 147/800, batch 24/75, loss 0.1817\n",
            "epoch 147/800, batch 25/75, loss 0.1776\n",
            "epoch 147/800, batch 26/75, loss 3.3086\n",
            "epoch 147/800, batch 27/75, loss 0.2010\n",
            "epoch 147/800, batch 28/75, loss 0.3521\n",
            "epoch 147/800, batch 29/75, loss 0.1352\n",
            "epoch 147/800, batch 30/75, loss 0.3489\n",
            "epoch 147/800, batch 31/75, loss 0.3003\n",
            "epoch 147/800, batch 32/75, loss 0.2267\n",
            "epoch 147/800, batch 33/75, loss 0.2130\n",
            "epoch 147/800, batch 34/75, loss 0.3570\n",
            "epoch 147/800, batch 35/75, loss 0.2159\n",
            "epoch 147/800, batch 36/75, loss 3.8858\n",
            "epoch 147/800, batch 37/75, loss 0.8268\n",
            "epoch 147/800, batch 38/75, loss 0.1681\n",
            "epoch 147/800, batch 39/75, loss 0.3641\n",
            "epoch 147/800, batch 40/75, loss 0.1961\n",
            "epoch 147/800, batch 41/75, loss 0.3415\n",
            "epoch 147/800, batch 42/75, loss 0.2793\n",
            "epoch 147/800, batch 43/75, loss 0.4016\n",
            "epoch 147/800, batch 44/75, loss 0.3183\n",
            "epoch 147/800, batch 45/75, loss 0.2771\n",
            "epoch 147/800, batch 46/75, loss 0.2781\n",
            "epoch 147/800, batch 47/75, loss 0.2732\n",
            "epoch 147/800, batch 48/75, loss 0.1970\n",
            "epoch 147/800, batch 49/75, loss 0.2833\n",
            "epoch 147/800, batch 50/75, loss 0.2503\n",
            "epoch 147/800, batch 51/75, loss 0.2442\n",
            "epoch 147/800, batch 52/75, loss 0.1202\n",
            "epoch 147/800, batch 53/75, loss 10.3518\n",
            "epoch 147/800, batch 54/75, loss 0.4648\n",
            "epoch 147/800, batch 55/75, loss 0.2777\n",
            "epoch 147/800, batch 56/75, loss 0.3395\n",
            "epoch 147/800, batch 57/75, loss 0.2513\n",
            "epoch 147/800, batch 58/75, loss 0.3891\n",
            "epoch 147/800, batch 59/75, loss 0.4478\n",
            "epoch 147/800, batch 60/75, loss 0.3994\n",
            "epoch 147/800, batch 61/75, loss 0.4387\n",
            "epoch 147/800, batch 62/75, loss 0.6442\n",
            "epoch 147/800, batch 63/75, loss 0.5132\n",
            "epoch 147/800, batch 64/75, loss 0.4432\n",
            "epoch 147/800, batch 65/75, loss 0.4567\n",
            "epoch 147/800, batch 66/75, loss 0.3681\n",
            "epoch 147/800, batch 67/75, loss 0.5123\n",
            "epoch 147/800, batch 68/75, loss 0.3662\n",
            "epoch 147/800, batch 69/75, loss 0.4188\n",
            "epoch 147/800, batch 70/75, loss 0.2757\n",
            "epoch 147/800, batch 71/75, loss 0.3304\n",
            "epoch 147/800, batch 72/75, loss 0.3705\n",
            "epoch 147/800, batch 73/75, loss 0.3117\n",
            "epoch 147/800, batch 74/75, loss 0.2635\n",
            "epoch 147/800, batch 75/75, loss 0.1793\n",
            "epoch 147/800, training roc_auc_score 0.8631\n",
            "EarlyStopping counter: 51 out of 80\n",
            "epoch 147/800, validation roc_auc_score 0.7789, best validation roc_auc_score 0.8157\n",
            "epoch 148/800, batch 1/75, loss 0.6445\n",
            "epoch 148/800, batch 2/75, loss 1.1016\n",
            "epoch 148/800, batch 3/75, loss 0.3097\n",
            "epoch 148/800, batch 4/75, loss 0.4413\n",
            "epoch 148/800, batch 5/75, loss 0.2277\n",
            "epoch 148/800, batch 6/75, loss 0.1161\n",
            "epoch 148/800, batch 7/75, loss 0.1974\n",
            "epoch 148/800, batch 8/75, loss 1.7841\n",
            "epoch 148/800, batch 9/75, loss 0.1677\n",
            "epoch 148/800, batch 10/75, loss 0.7321\n",
            "epoch 148/800, batch 11/75, loss 0.2280\n",
            "epoch 148/800, batch 12/75, loss 0.2541\n",
            "epoch 148/800, batch 13/75, loss 0.1939\n",
            "epoch 148/800, batch 14/75, loss 0.1250\n",
            "epoch 148/800, batch 15/75, loss 2.0556\n",
            "epoch 148/800, batch 16/75, loss 0.1594\n",
            "epoch 148/800, batch 17/75, loss 0.2571\n",
            "epoch 148/800, batch 18/75, loss 0.2991\n",
            "epoch 148/800, batch 19/75, loss 0.2771\n",
            "epoch 148/800, batch 20/75, loss 2.0455\n",
            "epoch 148/800, batch 21/75, loss 0.1267\n",
            "epoch 148/800, batch 22/75, loss 0.2132\n",
            "epoch 148/800, batch 23/75, loss 0.1268\n",
            "epoch 148/800, batch 24/75, loss 0.1405\n",
            "epoch 148/800, batch 25/75, loss 0.1407\n",
            "epoch 148/800, batch 26/75, loss 4.2446\n",
            "epoch 148/800, batch 27/75, loss 0.2096\n",
            "epoch 148/800, batch 28/75, loss 0.2763\n",
            "epoch 148/800, batch 29/75, loss 0.1425\n",
            "epoch 148/800, batch 30/75, loss 0.3733\n",
            "epoch 148/800, batch 31/75, loss 0.2919\n",
            "epoch 148/800, batch 32/75, loss 0.2538\n",
            "epoch 148/800, batch 33/75, loss 0.2513\n",
            "epoch 148/800, batch 34/75, loss 0.4017\n",
            "epoch 148/800, batch 35/75, loss 0.2626\n",
            "epoch 148/800, batch 36/75, loss 4.5251\n",
            "epoch 148/800, batch 37/75, loss 0.7115\n",
            "epoch 148/800, batch 38/75, loss 0.2518\n",
            "epoch 148/800, batch 39/75, loss 0.5094\n",
            "epoch 148/800, batch 40/75, loss 0.3369\n",
            "epoch 148/800, batch 41/75, loss 0.5348\n",
            "epoch 148/800, batch 42/75, loss 0.4148\n",
            "epoch 148/800, batch 43/75, loss 0.4008\n",
            "epoch 148/800, batch 44/75, loss 0.4140\n",
            "epoch 148/800, batch 45/75, loss 0.2907\n",
            "epoch 148/800, batch 46/75, loss 0.2907\n",
            "epoch 148/800, batch 47/75, loss 0.3688\n",
            "epoch 148/800, batch 48/75, loss 0.2131\n",
            "epoch 148/800, batch 49/75, loss 0.2547\n",
            "epoch 148/800, batch 50/75, loss 0.2103\n",
            "epoch 148/800, batch 51/75, loss 0.1811\n",
            "epoch 148/800, batch 52/75, loss 0.1033\n",
            "epoch 148/800, batch 53/75, loss 10.0165\n",
            "epoch 148/800, batch 54/75, loss 0.3160\n",
            "epoch 148/800, batch 55/75, loss 0.2482\n",
            "epoch 148/800, batch 56/75, loss 0.2966\n",
            "epoch 148/800, batch 57/75, loss 0.1979\n",
            "epoch 148/800, batch 58/75, loss 0.3160\n",
            "epoch 148/800, batch 59/75, loss 0.3688\n",
            "epoch 148/800, batch 60/75, loss 0.3243\n",
            "epoch 148/800, batch 61/75, loss 0.3298\n",
            "epoch 148/800, batch 62/75, loss 0.4836\n",
            "epoch 148/800, batch 63/75, loss 0.4507\n",
            "epoch 148/800, batch 64/75, loss 0.3543\n",
            "epoch 148/800, batch 65/75, loss 0.3156\n",
            "epoch 148/800, batch 66/75, loss 0.2769\n",
            "epoch 148/800, batch 67/75, loss 0.3603\n",
            "epoch 148/800, batch 68/75, loss 0.3024\n",
            "epoch 148/800, batch 69/75, loss 0.3243\n",
            "epoch 148/800, batch 70/75, loss 0.2090\n",
            "epoch 148/800, batch 71/75, loss 0.2680\n",
            "epoch 148/800, batch 72/75, loss 0.3083\n",
            "epoch 148/800, batch 73/75, loss 0.2469\n",
            "epoch 148/800, batch 74/75, loss 0.2573\n",
            "epoch 148/800, batch 75/75, loss 0.1634\n",
            "epoch 148/800, training roc_auc_score 0.8745\n",
            "EarlyStopping counter: 52 out of 80\n",
            "epoch 148/800, validation roc_auc_score 0.7844, best validation roc_auc_score 0.8157\n",
            "epoch 149/800, batch 1/75, loss 1.1009\n",
            "epoch 149/800, batch 2/75, loss 1.0670\n",
            "epoch 149/800, batch 3/75, loss 0.3065\n",
            "epoch 149/800, batch 4/75, loss 0.3555\n",
            "epoch 149/800, batch 5/75, loss 0.1608\n",
            "epoch 149/800, batch 6/75, loss 0.0927\n",
            "epoch 149/800, batch 7/75, loss 0.1335\n",
            "epoch 149/800, batch 8/75, loss 2.1244\n",
            "epoch 149/800, batch 9/75, loss 0.1090\n",
            "epoch 149/800, batch 10/75, loss 1.0494\n",
            "epoch 149/800, batch 11/75, loss 0.2068\n",
            "epoch 149/800, batch 12/75, loss 0.1823\n",
            "epoch 149/800, batch 13/75, loss 0.2097\n",
            "epoch 149/800, batch 14/75, loss 0.1144\n",
            "epoch 149/800, batch 15/75, loss 3.3131\n",
            "epoch 149/800, batch 16/75, loss 0.1453\n",
            "epoch 149/800, batch 17/75, loss 0.2329\n",
            "epoch 149/800, batch 18/75, loss 0.2907\n",
            "epoch 149/800, batch 19/75, loss 0.2561\n",
            "epoch 149/800, batch 20/75, loss 2.6365\n",
            "epoch 149/800, batch 21/75, loss 0.1303\n",
            "epoch 149/800, batch 22/75, loss 0.1874\n",
            "epoch 149/800, batch 23/75, loss 0.1292\n",
            "epoch 149/800, batch 24/75, loss 0.1770\n",
            "epoch 149/800, batch 25/75, loss 0.1588\n",
            "epoch 149/800, batch 26/75, loss 5.1847\n",
            "epoch 149/800, batch 27/75, loss 0.1733\n",
            "epoch 149/800, batch 28/75, loss 0.2539\n",
            "epoch 149/800, batch 29/75, loss 0.2225\n",
            "epoch 149/800, batch 30/75, loss 0.3089\n",
            "epoch 149/800, batch 31/75, loss 0.2500\n",
            "epoch 149/800, batch 32/75, loss 0.1510\n",
            "epoch 149/800, batch 33/75, loss 0.1268\n",
            "epoch 149/800, batch 34/75, loss 0.1736\n",
            "epoch 149/800, batch 35/75, loss 0.1272\n",
            "epoch 149/800, batch 36/75, loss 10.2253\n",
            "epoch 149/800, batch 37/75, loss 0.4759\n",
            "epoch 149/800, batch 38/75, loss 0.1614\n",
            "epoch 149/800, batch 39/75, loss 0.2749\n",
            "epoch 149/800, batch 40/75, loss 0.2502\n",
            "epoch 149/800, batch 41/75, loss 0.3614\n",
            "epoch 149/800, batch 42/75, loss 0.3305\n",
            "epoch 149/800, batch 43/75, loss 0.4610\n",
            "epoch 149/800, batch 44/75, loss 0.4936\n",
            "epoch 149/800, batch 45/75, loss 0.4115\n",
            "epoch 149/800, batch 46/75, loss 0.5110\n",
            "epoch 149/800, batch 47/75, loss 0.4568\n",
            "epoch 149/800, batch 48/75, loss 0.3303\n",
            "epoch 149/800, batch 49/75, loss 0.3511\n",
            "epoch 149/800, batch 50/75, loss 0.3958\n",
            "epoch 149/800, batch 51/75, loss 0.3039\n",
            "epoch 149/800, batch 52/75, loss 0.1953\n",
            "epoch 149/800, batch 53/75, loss 9.6476\n",
            "epoch 149/800, batch 54/75, loss 0.5204\n",
            "epoch 149/800, batch 55/75, loss 0.4041\n",
            "epoch 149/800, batch 56/75, loss 0.5221\n",
            "epoch 149/800, batch 57/75, loss 0.5188\n",
            "epoch 149/800, batch 58/75, loss 0.6275\n",
            "epoch 149/800, batch 59/75, loss 0.6934\n",
            "epoch 149/800, batch 60/75, loss 0.6349\n",
            "epoch 149/800, batch 61/75, loss 0.5973\n",
            "epoch 149/800, batch 62/75, loss 0.7399\n",
            "epoch 149/800, batch 63/75, loss 0.5940\n",
            "epoch 149/800, batch 64/75, loss 0.6050\n",
            "epoch 149/800, batch 65/75, loss 0.5024\n",
            "epoch 149/800, batch 66/75, loss 0.4451\n",
            "epoch 149/800, batch 67/75, loss 0.5200\n",
            "epoch 149/800, batch 68/75, loss 0.3894\n",
            "epoch 149/800, batch 69/75, loss 0.4020\n",
            "epoch 149/800, batch 70/75, loss 0.2744\n",
            "epoch 149/800, batch 71/75, loss 0.2633\n",
            "epoch 149/800, batch 72/75, loss 0.3060\n",
            "epoch 149/800, batch 73/75, loss 0.2061\n",
            "epoch 149/800, batch 74/75, loss 0.1894\n",
            "epoch 149/800, batch 75/75, loss 0.1299\n",
            "epoch 149/800, training roc_auc_score 0.8016\n",
            "EarlyStopping counter: 53 out of 80\n",
            "epoch 149/800, validation roc_auc_score 0.7929, best validation roc_auc_score 0.8157\n",
            "epoch 150/800, batch 1/75, loss 1.0821\n",
            "epoch 150/800, batch 2/75, loss 1.1545\n",
            "epoch 150/800, batch 3/75, loss 0.2920\n",
            "epoch 150/800, batch 4/75, loss 0.6214\n",
            "epoch 150/800, batch 5/75, loss 0.1482\n",
            "epoch 150/800, batch 6/75, loss 0.0837\n",
            "epoch 150/800, batch 7/75, loss 0.1354\n",
            "epoch 150/800, batch 8/75, loss 2.3959\n",
            "epoch 150/800, batch 9/75, loss 0.1032\n",
            "epoch 150/800, batch 10/75, loss 1.2653\n",
            "epoch 150/800, batch 11/75, loss 0.2713\n",
            "epoch 150/800, batch 12/75, loss 0.1679\n",
            "epoch 150/800, batch 13/75, loss 0.1524\n",
            "epoch 150/800, batch 14/75, loss 0.1152\n",
            "epoch 150/800, batch 15/75, loss 3.4291\n",
            "epoch 150/800, batch 16/75, loss 0.1608\n",
            "epoch 150/800, batch 17/75, loss 0.2233\n",
            "epoch 150/800, batch 18/75, loss 0.2180\n",
            "epoch 150/800, batch 19/75, loss 0.2119\n",
            "epoch 150/800, batch 20/75, loss 1.5456\n",
            "epoch 150/800, batch 21/75, loss 0.1403\n",
            "epoch 150/800, batch 22/75, loss 0.2673\n",
            "epoch 150/800, batch 23/75, loss 0.1827\n",
            "epoch 150/800, batch 24/75, loss 0.3536\n",
            "epoch 150/800, batch 25/75, loss 0.2790\n",
            "epoch 150/800, batch 26/75, loss 4.1723\n",
            "epoch 150/800, batch 27/75, loss 0.2600\n",
            "epoch 150/800, batch 28/75, loss 0.4377\n",
            "epoch 150/800, batch 29/75, loss 0.3896\n",
            "epoch 150/800, batch 30/75, loss 0.4686\n",
            "epoch 150/800, batch 31/75, loss 0.4256\n",
            "epoch 150/800, batch 32/75, loss 0.3357\n",
            "epoch 150/800, batch 33/75, loss 0.3460\n",
            "epoch 150/800, batch 34/75, loss 0.5782\n",
            "epoch 150/800, batch 35/75, loss 0.3820\n",
            "epoch 150/800, batch 36/75, loss 4.3492\n",
            "epoch 150/800, batch 37/75, loss 0.7080\n",
            "epoch 150/800, batch 38/75, loss 0.3687\n",
            "epoch 150/800, batch 39/75, loss 0.5083\n",
            "epoch 150/800, batch 40/75, loss 0.3070\n",
            "epoch 150/800, batch 41/75, loss 0.5134\n",
            "epoch 150/800, batch 42/75, loss 0.3117\n",
            "epoch 150/800, batch 43/75, loss 0.3355\n",
            "epoch 150/800, batch 44/75, loss 0.3267\n",
            "epoch 150/800, batch 45/75, loss 0.2582\n",
            "epoch 150/800, batch 46/75, loss 0.2761\n",
            "epoch 150/800, batch 47/75, loss 0.3326\n",
            "epoch 150/800, batch 48/75, loss 0.1726\n",
            "epoch 150/800, batch 49/75, loss 0.2471\n",
            "epoch 150/800, batch 50/75, loss 0.1696\n",
            "epoch 150/800, batch 51/75, loss 0.1502\n",
            "epoch 150/800, batch 52/75, loss 0.1240\n",
            "epoch 150/800, batch 53/75, loss 12.9387\n",
            "epoch 150/800, batch 54/75, loss 0.8080\n",
            "epoch 150/800, batch 55/75, loss 0.2427\n",
            "epoch 150/800, batch 56/75, loss 0.2975\n",
            "epoch 150/800, batch 57/75, loss 0.2514\n",
            "epoch 150/800, batch 58/75, loss 0.4364\n",
            "epoch 150/800, batch 59/75, loss 0.4561\n",
            "epoch 150/800, batch 60/75, loss 0.4440\n",
            "epoch 150/800, batch 61/75, loss 0.4252\n",
            "epoch 150/800, batch 62/75, loss 0.6704\n",
            "epoch 150/800, batch 63/75, loss 0.5981\n",
            "epoch 150/800, batch 64/75, loss 0.5707\n",
            "epoch 150/800, batch 65/75, loss 0.5026\n",
            "epoch 150/800, batch 66/75, loss 0.4150\n",
            "epoch 150/800, batch 67/75, loss 0.5244\n",
            "epoch 150/800, batch 68/75, loss 0.4229\n",
            "epoch 150/800, batch 69/75, loss 0.4927\n",
            "epoch 150/800, batch 70/75, loss 0.2800\n",
            "epoch 150/800, batch 71/75, loss 0.3489\n",
            "epoch 150/800, batch 72/75, loss 0.4007\n",
            "epoch 150/800, batch 73/75, loss 0.2900\n",
            "epoch 150/800, batch 74/75, loss 0.2494\n",
            "epoch 150/800, batch 75/75, loss 0.1666\n",
            "epoch 150/800, training roc_auc_score 0.8262\n",
            "EarlyStopping counter: 54 out of 80\n",
            "epoch 150/800, validation roc_auc_score 0.7995, best validation roc_auc_score 0.8157\n",
            "epoch 151/800, batch 1/75, loss 0.6065\n",
            "epoch 151/800, batch 2/75, loss 1.4529\n",
            "epoch 151/800, batch 3/75, loss 0.3610\n",
            "epoch 151/800, batch 4/75, loss 0.6333\n",
            "epoch 151/800, batch 5/75, loss 0.1597\n",
            "epoch 151/800, batch 6/75, loss 0.1216\n",
            "epoch 151/800, batch 7/75, loss 0.1704\n",
            "epoch 151/800, batch 8/75, loss 2.0967\n",
            "epoch 151/800, batch 9/75, loss 0.1217\n",
            "epoch 151/800, batch 10/75, loss 0.9588\n",
            "epoch 151/800, batch 11/75, loss 0.3773\n",
            "epoch 151/800, batch 12/75, loss 0.2215\n",
            "epoch 151/800, batch 13/75, loss 0.2369\n",
            "epoch 151/800, batch 14/75, loss 0.1538\n",
            "epoch 151/800, batch 15/75, loss 2.7464\n",
            "epoch 151/800, batch 16/75, loss 0.1801\n",
            "epoch 151/800, batch 17/75, loss 0.2483\n",
            "epoch 151/800, batch 18/75, loss 0.2996\n",
            "epoch 151/800, batch 19/75, loss 0.3308\n",
            "epoch 151/800, batch 20/75, loss 1.4959\n",
            "epoch 151/800, batch 21/75, loss 0.1636\n",
            "epoch 151/800, batch 22/75, loss 0.2335\n",
            "epoch 151/800, batch 23/75, loss 0.1698\n",
            "epoch 151/800, batch 24/75, loss 0.2160\n",
            "epoch 151/800, batch 25/75, loss 0.2082\n",
            "epoch 151/800, batch 26/75, loss 3.8145\n",
            "epoch 151/800, batch 27/75, loss 0.2306\n",
            "epoch 151/800, batch 28/75, loss 0.3039\n",
            "epoch 151/800, batch 29/75, loss 0.2011\n",
            "epoch 151/800, batch 30/75, loss 0.3383\n",
            "epoch 151/800, batch 31/75, loss 0.2960\n",
            "epoch 151/800, batch 32/75, loss 0.3240\n",
            "epoch 151/800, batch 33/75, loss 0.2654\n",
            "epoch 151/800, batch 34/75, loss 0.3416\n",
            "epoch 151/800, batch 35/75, loss 0.2681\n",
            "epoch 151/800, batch 36/75, loss 4.0191\n",
            "epoch 151/800, batch 37/75, loss 0.6508\n",
            "epoch 151/800, batch 38/75, loss 0.2438\n",
            "epoch 151/800, batch 39/75, loss 0.4296\n",
            "epoch 151/800, batch 40/75, loss 0.2556\n",
            "epoch 151/800, batch 41/75, loss 0.4494\n",
            "epoch 151/800, batch 42/75, loss 0.3065\n",
            "epoch 151/800, batch 43/75, loss 0.3236\n",
            "epoch 151/800, batch 44/75, loss 0.3287\n",
            "epoch 151/800, batch 45/75, loss 0.2355\n",
            "epoch 151/800, batch 46/75, loss 0.2710\n",
            "epoch 151/800, batch 47/75, loss 0.3231\n",
            "epoch 151/800, batch 48/75, loss 0.1797\n",
            "epoch 151/800, batch 49/75, loss 0.2279\n",
            "epoch 151/800, batch 50/75, loss 0.1668\n",
            "epoch 151/800, batch 51/75, loss 0.1720\n",
            "epoch 151/800, batch 52/75, loss 0.1240\n",
            "epoch 151/800, batch 53/75, loss 12.6962\n",
            "epoch 151/800, batch 54/75, loss 0.4988\n",
            "epoch 151/800, batch 55/75, loss 0.2543\n",
            "epoch 151/800, batch 56/75, loss 0.3711\n",
            "epoch 151/800, batch 57/75, loss 0.2539\n",
            "epoch 151/800, batch 58/75, loss 0.4062\n",
            "epoch 151/800, batch 59/75, loss 0.4793\n",
            "epoch 151/800, batch 60/75, loss 0.5505\n",
            "epoch 151/800, batch 61/75, loss 0.4832\n",
            "epoch 151/800, batch 62/75, loss 0.6698\n",
            "epoch 151/800, batch 63/75, loss 0.5825\n",
            "epoch 151/800, batch 64/75, loss 0.5421\n",
            "epoch 151/800, batch 65/75, loss 0.4817\n",
            "epoch 151/800, batch 66/75, loss 0.4925\n",
            "epoch 151/800, batch 67/75, loss 0.5759\n",
            "epoch 151/800, batch 68/75, loss 0.4292\n",
            "epoch 151/800, batch 69/75, loss 0.5964\n",
            "epoch 151/800, batch 70/75, loss 0.2932\n",
            "epoch 151/800, batch 71/75, loss 0.3812\n",
            "epoch 151/800, batch 72/75, loss 0.4062\n",
            "epoch 151/800, batch 73/75, loss 0.2956\n",
            "epoch 151/800, batch 74/75, loss 0.2178\n",
            "epoch 151/800, batch 75/75, loss 0.1323\n",
            "epoch 151/800, training roc_auc_score 0.8400\n",
            "EarlyStopping counter: 55 out of 80\n",
            "epoch 151/800, validation roc_auc_score 0.7872, best validation roc_auc_score 0.8157\n",
            "epoch 152/800, batch 1/75, loss 1.0853\n",
            "epoch 152/800, batch 2/75, loss 0.9931\n",
            "epoch 152/800, batch 3/75, loss 0.2523\n",
            "epoch 152/800, batch 4/75, loss 0.5237\n",
            "epoch 152/800, batch 5/75, loss 0.1135\n",
            "epoch 152/800, batch 6/75, loss 0.0835\n",
            "epoch 152/800, batch 7/75, loss 0.0943\n",
            "epoch 152/800, batch 8/75, loss 3.3865\n",
            "epoch 152/800, batch 9/75, loss 0.0864\n",
            "epoch 152/800, batch 10/75, loss 2.2632\n",
            "epoch 152/800, batch 11/75, loss 0.4292\n",
            "epoch 152/800, batch 12/75, loss 0.2201\n",
            "epoch 152/800, batch 13/75, loss 0.2547\n",
            "epoch 152/800, batch 14/75, loss 0.2208\n",
            "epoch 152/800, batch 15/75, loss 2.1237\n",
            "epoch 152/800, batch 16/75, loss 0.2493\n",
            "epoch 152/800, batch 17/75, loss 0.4230\n",
            "epoch 152/800, batch 18/75, loss 0.5086\n",
            "epoch 152/800, batch 19/75, loss 0.5270\n",
            "epoch 152/800, batch 20/75, loss 1.0160\n",
            "epoch 152/800, batch 21/75, loss 0.2287\n",
            "epoch 152/800, batch 22/75, loss 0.3081\n",
            "epoch 152/800, batch 23/75, loss 0.2791\n",
            "epoch 152/800, batch 24/75, loss 0.3059\n",
            "epoch 152/800, batch 25/75, loss 0.2481\n",
            "epoch 152/800, batch 26/75, loss 4.0871\n",
            "epoch 152/800, batch 27/75, loss 0.2918\n",
            "epoch 152/800, batch 28/75, loss 0.4963\n",
            "epoch 152/800, batch 29/75, loss 0.2193\n",
            "epoch 152/800, batch 30/75, loss 0.3949\n",
            "epoch 152/800, batch 31/75, loss 0.3434\n",
            "epoch 152/800, batch 32/75, loss 0.3128\n",
            "epoch 152/800, batch 33/75, loss 0.2664\n",
            "epoch 152/800, batch 34/75, loss 0.4174\n",
            "epoch 152/800, batch 35/75, loss 0.2985\n",
            "epoch 152/800, batch 36/75, loss 4.9972\n",
            "epoch 152/800, batch 37/75, loss 0.9735\n",
            "epoch 152/800, batch 38/75, loss 0.1794\n",
            "epoch 152/800, batch 39/75, loss 0.3929\n",
            "epoch 152/800, batch 40/75, loss 0.2090\n",
            "epoch 152/800, batch 41/75, loss 0.3917\n",
            "epoch 152/800, batch 42/75, loss 0.2558\n",
            "epoch 152/800, batch 43/75, loss 0.2623\n",
            "epoch 152/800, batch 44/75, loss 0.3136\n",
            "epoch 152/800, batch 45/75, loss 0.2372\n",
            "epoch 152/800, batch 46/75, loss 0.2331\n",
            "epoch 152/800, batch 47/75, loss 0.2696\n",
            "epoch 152/800, batch 48/75, loss 0.1649\n",
            "epoch 152/800, batch 49/75, loss 0.1723\n",
            "epoch 152/800, batch 50/75, loss 0.1456\n",
            "epoch 152/800, batch 51/75, loss 0.1334\n",
            "epoch 152/800, batch 52/75, loss 0.0837\n",
            "epoch 152/800, batch 53/75, loss 15.2946\n",
            "epoch 152/800, batch 54/75, loss 0.4233\n",
            "epoch 152/800, batch 55/75, loss 0.2919\n",
            "epoch 152/800, batch 56/75, loss 0.3414\n",
            "epoch 152/800, batch 57/75, loss 0.3129\n",
            "epoch 152/800, batch 58/75, loss 0.5745\n",
            "epoch 152/800, batch 59/75, loss 0.6773\n",
            "epoch 152/800, batch 60/75, loss 0.6533\n",
            "epoch 152/800, batch 61/75, loss 0.6723\n",
            "epoch 152/800, batch 62/75, loss 0.8799\n",
            "epoch 152/800, batch 63/75, loss 0.7483\n",
            "epoch 152/800, batch 64/75, loss 0.7085\n",
            "epoch 152/800, batch 65/75, loss 0.6839\n",
            "epoch 152/800, batch 66/75, loss 0.5098\n",
            "epoch 152/800, batch 67/75, loss 0.6460\n",
            "epoch 152/800, batch 68/75, loss 0.5173\n",
            "epoch 152/800, batch 69/75, loss 0.5580\n",
            "epoch 152/800, batch 70/75, loss 0.3150\n",
            "epoch 152/800, batch 71/75, loss 0.3867\n",
            "epoch 152/800, batch 72/75, loss 0.5651\n",
            "epoch 152/800, batch 73/75, loss 0.3416\n",
            "epoch 152/800, batch 74/75, loss 0.2895\n",
            "epoch 152/800, batch 75/75, loss 0.1607\n",
            "epoch 152/800, training roc_auc_score 0.7991\n",
            "EarlyStopping counter: 56 out of 80\n",
            "epoch 152/800, validation roc_auc_score 0.7837, best validation roc_auc_score 0.8157\n",
            "epoch 153/800, batch 1/75, loss 0.8170\n",
            "epoch 153/800, batch 2/75, loss 1.0143\n",
            "epoch 153/800, batch 3/75, loss 0.4041\n",
            "epoch 153/800, batch 4/75, loss 0.4662\n",
            "epoch 153/800, batch 5/75, loss 0.1174\n",
            "epoch 153/800, batch 6/75, loss 0.0827\n",
            "epoch 153/800, batch 7/75, loss 0.1256\n",
            "epoch 153/800, batch 8/75, loss 3.1617\n",
            "epoch 153/800, batch 9/75, loss 0.1286\n",
            "epoch 153/800, batch 10/75, loss 1.5720\n",
            "epoch 153/800, batch 11/75, loss 0.3039\n",
            "epoch 153/800, batch 12/75, loss 0.3079\n",
            "epoch 153/800, batch 13/75, loss 0.2163\n",
            "epoch 153/800, batch 14/75, loss 0.1330\n",
            "epoch 153/800, batch 15/75, loss 2.9551\n",
            "epoch 153/800, batch 16/75, loss 0.1481\n",
            "epoch 153/800, batch 17/75, loss 0.2872\n",
            "epoch 153/800, batch 18/75, loss 0.2515\n",
            "epoch 153/800, batch 19/75, loss 0.2516\n",
            "epoch 153/800, batch 20/75, loss 1.4203\n",
            "epoch 153/800, batch 21/75, loss 0.1265\n",
            "epoch 153/800, batch 22/75, loss 0.1780\n",
            "epoch 153/800, batch 23/75, loss 0.1701\n",
            "epoch 153/800, batch 24/75, loss 0.2451\n",
            "epoch 153/800, batch 25/75, loss 0.2028\n",
            "epoch 153/800, batch 26/75, loss 3.5638\n",
            "epoch 153/800, batch 27/75, loss 0.2330\n",
            "epoch 153/800, batch 28/75, loss 0.3581\n",
            "epoch 153/800, batch 29/75, loss 0.2348\n",
            "epoch 153/800, batch 30/75, loss 0.3543\n",
            "epoch 153/800, batch 31/75, loss 0.3144\n",
            "epoch 153/800, batch 32/75, loss 0.2865\n",
            "epoch 153/800, batch 33/75, loss 0.2408\n",
            "epoch 153/800, batch 34/75, loss 0.3859\n",
            "epoch 153/800, batch 35/75, loss 0.2661\n",
            "epoch 153/800, batch 36/75, loss 3.9436\n",
            "epoch 153/800, batch 37/75, loss 0.8214\n",
            "epoch 153/800, batch 38/75, loss 0.2931\n",
            "epoch 153/800, batch 39/75, loss 0.5494\n",
            "epoch 153/800, batch 40/75, loss 0.2906\n",
            "epoch 153/800, batch 41/75, loss 0.4651\n",
            "epoch 153/800, batch 42/75, loss 0.3445\n",
            "epoch 153/800, batch 43/75, loss 0.3570\n",
            "epoch 153/800, batch 44/75, loss 0.3566\n",
            "epoch 153/800, batch 45/75, loss 0.2538\n",
            "epoch 153/800, batch 46/75, loss 0.2880\n",
            "epoch 153/800, batch 47/75, loss 0.3128\n",
            "epoch 153/800, batch 48/75, loss 0.1939\n",
            "epoch 153/800, batch 49/75, loss 0.2441\n",
            "epoch 153/800, batch 50/75, loss 0.1840\n",
            "epoch 153/800, batch 51/75, loss 0.1618\n",
            "epoch 153/800, batch 52/75, loss 0.1307\n",
            "epoch 153/800, batch 53/75, loss 11.8720\n",
            "epoch 153/800, batch 54/75, loss 0.3921\n",
            "epoch 153/800, batch 55/75, loss 0.2406\n",
            "epoch 153/800, batch 56/75, loss 0.2979\n",
            "epoch 153/800, batch 57/75, loss 0.2356\n",
            "epoch 153/800, batch 58/75, loss 0.3449\n",
            "epoch 153/800, batch 59/75, loss 0.4243\n",
            "epoch 153/800, batch 60/75, loss 0.3626\n",
            "epoch 153/800, batch 61/75, loss 0.3564\n",
            "epoch 153/800, batch 62/75, loss 0.5496\n",
            "epoch 153/800, batch 63/75, loss 0.4625\n",
            "epoch 153/800, batch 64/75, loss 0.3535\n",
            "epoch 153/800, batch 65/75, loss 0.3731\n",
            "epoch 153/800, batch 66/75, loss 0.3202\n",
            "epoch 153/800, batch 67/75, loss 0.3582\n",
            "epoch 153/800, batch 68/75, loss 0.3128\n",
            "epoch 153/800, batch 69/75, loss 0.4438\n",
            "epoch 153/800, batch 70/75, loss 0.2270\n",
            "epoch 153/800, batch 71/75, loss 0.2876\n",
            "epoch 153/800, batch 72/75, loss 0.3137\n",
            "epoch 153/800, batch 73/75, loss 0.2697\n",
            "epoch 153/800, batch 74/75, loss 0.2475\n",
            "epoch 153/800, batch 75/75, loss 0.1660\n",
            "epoch 153/800, training roc_auc_score 0.8502\n",
            "EarlyStopping counter: 57 out of 80\n",
            "epoch 153/800, validation roc_auc_score 0.7903, best validation roc_auc_score 0.8157\n",
            "epoch 154/800, batch 1/75, loss 0.9757\n",
            "epoch 154/800, batch 2/75, loss 0.8338\n",
            "epoch 154/800, batch 3/75, loss 0.2919\n",
            "epoch 154/800, batch 4/75, loss 0.4350\n",
            "epoch 154/800, batch 5/75, loss 0.1590\n",
            "epoch 154/800, batch 6/75, loss 0.1130\n",
            "epoch 154/800, batch 7/75, loss 0.1670\n",
            "epoch 154/800, batch 8/75, loss 2.6634\n",
            "epoch 154/800, batch 9/75, loss 0.1118\n",
            "epoch 154/800, batch 10/75, loss 1.0046\n",
            "epoch 154/800, batch 11/75, loss 0.2866\n",
            "epoch 154/800, batch 12/75, loss 0.2417\n",
            "epoch 154/800, batch 13/75, loss 0.2188\n",
            "epoch 154/800, batch 14/75, loss 0.1637\n",
            "epoch 154/800, batch 15/75, loss 2.6059\n",
            "epoch 154/800, batch 16/75, loss 0.1396\n",
            "epoch 154/800, batch 17/75, loss 0.2331\n",
            "epoch 154/800, batch 18/75, loss 0.3008\n",
            "epoch 154/800, batch 19/75, loss 0.2795\n",
            "epoch 154/800, batch 20/75, loss 1.6845\n",
            "epoch 154/800, batch 21/75, loss 0.1497\n",
            "epoch 154/800, batch 22/75, loss 0.1759\n",
            "epoch 154/800, batch 23/75, loss 0.1842\n",
            "epoch 154/800, batch 24/75, loss 0.2309\n",
            "epoch 154/800, batch 25/75, loss 0.1495\n",
            "epoch 154/800, batch 26/75, loss 4.4268\n",
            "epoch 154/800, batch 27/75, loss 0.2060\n",
            "epoch 154/800, batch 28/75, loss 0.3309\n",
            "epoch 154/800, batch 29/75, loss 0.2198\n",
            "epoch 154/800, batch 30/75, loss 0.3761\n",
            "epoch 154/800, batch 31/75, loss 0.2838\n",
            "epoch 154/800, batch 32/75, loss 0.2745\n",
            "epoch 154/800, batch 33/75, loss 0.2273\n",
            "epoch 154/800, batch 34/75, loss 0.4123\n",
            "epoch 154/800, batch 35/75, loss 0.2685\n",
            "epoch 154/800, batch 36/75, loss 3.8172\n",
            "epoch 154/800, batch 37/75, loss 0.6224\n",
            "epoch 154/800, batch 38/75, loss 0.2646\n",
            "epoch 154/800, batch 39/75, loss 0.4404\n",
            "epoch 154/800, batch 40/75, loss 0.2558\n",
            "epoch 154/800, batch 41/75, loss 0.4589\n",
            "epoch 154/800, batch 42/75, loss 0.3339\n",
            "epoch 154/800, batch 43/75, loss 0.3497\n",
            "epoch 154/800, batch 44/75, loss 0.3517\n",
            "epoch 154/800, batch 45/75, loss 0.2497\n",
            "epoch 154/800, batch 46/75, loss 0.3101\n",
            "epoch 154/800, batch 47/75, loss 0.3253\n",
            "epoch 154/800, batch 48/75, loss 0.2114\n",
            "epoch 154/800, batch 49/75, loss 0.2380\n",
            "epoch 154/800, batch 50/75, loss 0.1892\n",
            "epoch 154/800, batch 51/75, loss 0.2050\n",
            "epoch 154/800, batch 52/75, loss 0.1452\n",
            "epoch 154/800, batch 53/75, loss 9.7238\n",
            "epoch 154/800, batch 54/75, loss 0.3698\n",
            "epoch 154/800, batch 55/75, loss 0.2699\n",
            "epoch 154/800, batch 56/75, loss 0.3147\n",
            "epoch 154/800, batch 57/75, loss 0.2203\n",
            "epoch 154/800, batch 58/75, loss 0.3107\n",
            "epoch 154/800, batch 59/75, loss 0.3922\n",
            "epoch 154/800, batch 60/75, loss 0.3223\n",
            "epoch 154/800, batch 61/75, loss 0.3560\n",
            "epoch 154/800, batch 62/75, loss 0.4873\n",
            "epoch 154/800, batch 63/75, loss 0.4134\n",
            "epoch 154/800, batch 64/75, loss 0.3270\n",
            "epoch 154/800, batch 65/75, loss 0.3476\n",
            "epoch 154/800, batch 66/75, loss 0.2840\n",
            "epoch 154/800, batch 67/75, loss 0.3622\n",
            "epoch 154/800, batch 68/75, loss 0.2910\n",
            "epoch 154/800, batch 69/75, loss 0.3583\n",
            "epoch 154/800, batch 70/75, loss 0.1987\n",
            "epoch 154/800, batch 71/75, loss 0.2776\n",
            "epoch 154/800, batch 72/75, loss 0.3463\n",
            "epoch 154/800, batch 73/75, loss 0.2177\n",
            "epoch 154/800, batch 74/75, loss 0.1998\n",
            "epoch 154/800, batch 75/75, loss 0.1560\n",
            "epoch 154/800, training roc_auc_score 0.8697\n",
            "EarlyStopping counter: 58 out of 80\n",
            "epoch 154/800, validation roc_auc_score 0.7936, best validation roc_auc_score 0.8157\n",
            "epoch 155/800, batch 1/75, loss 0.9395\n",
            "epoch 155/800, batch 2/75, loss 0.6975\n",
            "epoch 155/800, batch 3/75, loss 0.2751\n",
            "epoch 155/800, batch 4/75, loss 0.4204\n",
            "epoch 155/800, batch 5/75, loss 0.1557\n",
            "epoch 155/800, batch 6/75, loss 0.1129\n",
            "epoch 155/800, batch 7/75, loss 0.1728\n",
            "epoch 155/800, batch 8/75, loss 2.0135\n",
            "epoch 155/800, batch 9/75, loss 0.1241\n",
            "epoch 155/800, batch 10/75, loss 0.9835\n",
            "epoch 155/800, batch 11/75, loss 0.2383\n",
            "epoch 155/800, batch 12/75, loss 0.2451\n",
            "epoch 155/800, batch 13/75, loss 0.2140\n",
            "epoch 155/800, batch 14/75, loss 0.1205\n",
            "epoch 155/800, batch 15/75, loss 2.9933\n",
            "epoch 155/800, batch 16/75, loss 0.1246\n",
            "epoch 155/800, batch 17/75, loss 0.2304\n",
            "epoch 155/800, batch 18/75, loss 0.2242\n",
            "epoch 155/800, batch 19/75, loss 0.2485\n",
            "epoch 155/800, batch 20/75, loss 2.0957\n",
            "epoch 155/800, batch 21/75, loss 0.1501\n",
            "epoch 155/800, batch 22/75, loss 0.1828\n",
            "epoch 155/800, batch 23/75, loss 0.1501\n",
            "epoch 155/800, batch 24/75, loss 0.1699\n",
            "epoch 155/800, batch 25/75, loss 0.1659\n",
            "epoch 155/800, batch 26/75, loss 4.6413\n",
            "epoch 155/800, batch 27/75, loss 0.2213\n",
            "epoch 155/800, batch 28/75, loss 0.3084\n",
            "epoch 155/800, batch 29/75, loss 0.1745\n",
            "epoch 155/800, batch 30/75, loss 0.3459\n",
            "epoch 155/800, batch 31/75, loss 0.2976\n",
            "epoch 155/800, batch 32/75, loss 0.3121\n",
            "epoch 155/800, batch 33/75, loss 0.2589\n",
            "epoch 155/800, batch 34/75, loss 0.3685\n",
            "epoch 155/800, batch 35/75, loss 0.2622\n",
            "epoch 155/800, batch 36/75, loss 3.5231\n",
            "epoch 155/800, batch 37/75, loss 0.7538\n",
            "epoch 155/800, batch 38/75, loss 0.2628\n",
            "epoch 155/800, batch 39/75, loss 0.4785\n",
            "epoch 155/800, batch 40/75, loss 0.2910\n",
            "epoch 155/800, batch 41/75, loss 0.4997\n",
            "epoch 155/800, batch 42/75, loss 0.3952\n",
            "epoch 155/800, batch 43/75, loss 0.4002\n",
            "epoch 155/800, batch 44/75, loss 0.3708\n",
            "epoch 155/800, batch 45/75, loss 0.3102\n",
            "epoch 155/800, batch 46/75, loss 0.3528\n",
            "epoch 155/800, batch 47/75, loss 0.4148\n",
            "epoch 155/800, batch 48/75, loss 0.2196\n",
            "epoch 155/800, batch 49/75, loss 0.3129\n",
            "epoch 155/800, batch 50/75, loss 0.2221\n",
            "epoch 155/800, batch 51/75, loss 0.2040\n",
            "epoch 155/800, batch 52/75, loss 0.1373\n",
            "epoch 155/800, batch 53/75, loss 9.2097\n",
            "epoch 155/800, batch 54/75, loss 0.5347\n",
            "epoch 155/800, batch 55/75, loss 0.2584\n",
            "epoch 155/800, batch 56/75, loss 0.3045\n",
            "epoch 155/800, batch 57/75, loss 0.1930\n",
            "epoch 155/800, batch 58/75, loss 0.2950\n",
            "epoch 155/800, batch 59/75, loss 0.3520\n",
            "epoch 155/800, batch 60/75, loss 0.2986\n",
            "epoch 155/800, batch 61/75, loss 0.3354\n",
            "epoch 155/800, batch 62/75, loss 0.4983\n",
            "epoch 155/800, batch 63/75, loss 0.3953\n",
            "epoch 155/800, batch 64/75, loss 0.3290\n",
            "epoch 155/800, batch 65/75, loss 0.3763\n",
            "epoch 155/800, batch 66/75, loss 0.2955\n",
            "epoch 155/800, batch 67/75, loss 0.3439\n",
            "epoch 155/800, batch 68/75, loss 0.3029\n",
            "epoch 155/800, batch 69/75, loss 0.3961\n",
            "epoch 155/800, batch 70/75, loss 0.2095\n",
            "epoch 155/800, batch 71/75, loss 0.2905\n",
            "epoch 155/800, batch 72/75, loss 0.3629\n",
            "epoch 155/800, batch 73/75, loss 0.2558\n",
            "epoch 155/800, batch 74/75, loss 0.2838\n",
            "epoch 155/800, batch 75/75, loss 0.1441\n",
            "epoch 155/800, training roc_auc_score 0.8722\n",
            "EarlyStopping counter: 59 out of 80\n",
            "epoch 155/800, validation roc_auc_score 0.7952, best validation roc_auc_score 0.8157\n",
            "epoch 156/800, batch 1/75, loss 0.9602\n",
            "epoch 156/800, batch 2/75, loss 0.9193\n",
            "epoch 156/800, batch 3/75, loss 0.3748\n",
            "epoch 156/800, batch 4/75, loss 0.4791\n",
            "epoch 156/800, batch 5/75, loss 0.1522\n",
            "epoch 156/800, batch 6/75, loss 0.1155\n",
            "epoch 156/800, batch 7/75, loss 0.1688\n",
            "epoch 156/800, batch 8/75, loss 1.7093\n",
            "epoch 156/800, batch 9/75, loss 0.1366\n",
            "epoch 156/800, batch 10/75, loss 0.8723\n",
            "epoch 156/800, batch 11/75, loss 0.2377\n",
            "epoch 156/800, batch 12/75, loss 0.2128\n",
            "epoch 156/800, batch 13/75, loss 0.2090\n",
            "epoch 156/800, batch 14/75, loss 0.1288\n",
            "epoch 156/800, batch 15/75, loss 2.5559\n",
            "epoch 156/800, batch 16/75, loss 0.1163\n",
            "epoch 156/800, batch 17/75, loss 0.2073\n",
            "epoch 156/800, batch 18/75, loss 0.2378\n",
            "epoch 156/800, batch 19/75, loss 0.2315\n",
            "epoch 156/800, batch 20/75, loss 1.6159\n",
            "epoch 156/800, batch 21/75, loss 0.1542\n",
            "epoch 156/800, batch 22/75, loss 0.1662\n",
            "epoch 156/800, batch 23/75, loss 0.1202\n",
            "epoch 156/800, batch 24/75, loss 0.1806\n",
            "epoch 156/800, batch 25/75, loss 0.1610\n",
            "epoch 156/800, batch 26/75, loss 4.3835\n",
            "epoch 156/800, batch 27/75, loss 0.2147\n",
            "epoch 156/800, batch 28/75, loss 0.3648\n",
            "epoch 156/800, batch 29/75, loss 0.2010\n",
            "epoch 156/800, batch 30/75, loss 0.3689\n",
            "epoch 156/800, batch 31/75, loss 0.3092\n",
            "epoch 156/800, batch 32/75, loss 0.2723\n",
            "epoch 156/800, batch 33/75, loss 0.2019\n",
            "epoch 156/800, batch 34/75, loss 0.3533\n",
            "epoch 156/800, batch 35/75, loss 0.2784\n",
            "epoch 156/800, batch 36/75, loss 4.9505\n",
            "epoch 156/800, batch 37/75, loss 0.7355\n",
            "epoch 156/800, batch 38/75, loss 0.2149\n",
            "epoch 156/800, batch 39/75, loss 0.3890\n",
            "epoch 156/800, batch 40/75, loss 0.2524\n",
            "epoch 156/800, batch 41/75, loss 0.4406\n",
            "epoch 156/800, batch 42/75, loss 0.3533\n",
            "epoch 156/800, batch 43/75, loss 0.3607\n",
            "epoch 156/800, batch 44/75, loss 0.4103\n",
            "epoch 156/800, batch 45/75, loss 0.3103\n",
            "epoch 156/800, batch 46/75, loss 0.2980\n",
            "epoch 156/800, batch 47/75, loss 0.3798\n",
            "epoch 156/800, batch 48/75, loss 0.2382\n",
            "epoch 156/800, batch 49/75, loss 0.2796\n",
            "epoch 156/800, batch 50/75, loss 0.2460\n",
            "epoch 156/800, batch 51/75, loss 0.2290\n",
            "epoch 156/800, batch 52/75, loss 0.1299\n",
            "epoch 156/800, batch 53/75, loss 10.9877\n",
            "epoch 156/800, batch 54/75, loss 0.5483\n",
            "epoch 156/800, batch 55/75, loss 0.3102\n",
            "epoch 156/800, batch 56/75, loss 0.3794\n",
            "epoch 156/800, batch 57/75, loss 0.2871\n",
            "epoch 156/800, batch 58/75, loss 0.4975\n",
            "epoch 156/800, batch 59/75, loss 0.5569\n",
            "epoch 156/800, batch 60/75, loss 0.5302\n",
            "epoch 156/800, batch 61/75, loss 0.5610\n",
            "epoch 156/800, batch 62/75, loss 0.8046\n",
            "epoch 156/800, batch 63/75, loss 0.5819\n",
            "epoch 156/800, batch 64/75, loss 0.5288\n",
            "epoch 156/800, batch 65/75, loss 0.5207\n",
            "epoch 156/800, batch 66/75, loss 0.4181\n",
            "epoch 156/800, batch 67/75, loss 0.5338\n",
            "epoch 156/800, batch 68/75, loss 0.4482\n",
            "epoch 156/800, batch 69/75, loss 0.4985\n",
            "epoch 156/800, batch 70/75, loss 0.2847\n",
            "epoch 156/800, batch 71/75, loss 0.3840\n",
            "epoch 156/800, batch 72/75, loss 0.4052\n",
            "epoch 156/800, batch 73/75, loss 0.3187\n",
            "epoch 156/800, batch 74/75, loss 0.2597\n",
            "epoch 156/800, batch 75/75, loss 0.2004\n",
            "epoch 156/800, training roc_auc_score 0.8408\n",
            "EarlyStopping counter: 60 out of 80\n",
            "epoch 156/800, validation roc_auc_score 0.7965, best validation roc_auc_score 0.8157\n",
            "epoch 157/800, batch 1/75, loss 1.0369\n",
            "epoch 157/800, batch 2/75, loss 0.9169\n",
            "epoch 157/800, batch 3/75, loss 0.3827\n",
            "epoch 157/800, batch 4/75, loss 0.4541\n",
            "epoch 157/800, batch 5/75, loss 0.1802\n",
            "epoch 157/800, batch 6/75, loss 0.1231\n",
            "epoch 157/800, batch 7/75, loss 0.1746\n",
            "epoch 157/800, batch 8/75, loss 2.0588\n",
            "epoch 157/800, batch 9/75, loss 0.1402\n",
            "epoch 157/800, batch 10/75, loss 1.0631\n",
            "epoch 157/800, batch 11/75, loss 0.2663\n",
            "epoch 157/800, batch 12/75, loss 0.2294\n",
            "epoch 157/800, batch 13/75, loss 0.2163\n",
            "epoch 157/800, batch 14/75, loss 0.1310\n",
            "epoch 157/800, batch 15/75, loss 2.6973\n",
            "epoch 157/800, batch 16/75, loss 0.1109\n",
            "epoch 157/800, batch 17/75, loss 0.2197\n",
            "epoch 157/800, batch 18/75, loss 0.2559\n",
            "epoch 157/800, batch 19/75, loss 0.2203\n",
            "epoch 157/800, batch 20/75, loss 1.5620\n",
            "epoch 157/800, batch 21/75, loss 0.1824\n",
            "epoch 157/800, batch 22/75, loss 0.1355\n",
            "epoch 157/800, batch 23/75, loss 0.1174\n",
            "epoch 157/800, batch 24/75, loss 0.1567\n",
            "epoch 157/800, batch 25/75, loss 0.1477\n",
            "epoch 157/800, batch 26/75, loss 4.6096\n",
            "epoch 157/800, batch 27/75, loss 0.1794\n",
            "epoch 157/800, batch 28/75, loss 0.2797\n",
            "epoch 157/800, batch 29/75, loss 0.1472\n",
            "epoch 157/800, batch 30/75, loss 0.2362\n",
            "epoch 157/800, batch 31/75, loss 0.2619\n",
            "epoch 157/800, batch 32/75, loss 0.2442\n",
            "epoch 157/800, batch 33/75, loss 0.2140\n",
            "epoch 157/800, batch 34/75, loss 0.2952\n",
            "epoch 157/800, batch 35/75, loss 0.1884\n",
            "epoch 157/800, batch 36/75, loss 3.7524\n",
            "epoch 157/800, batch 37/75, loss 1.1399\n",
            "epoch 157/800, batch 38/75, loss 0.1938\n",
            "epoch 157/800, batch 39/75, loss 0.3716\n",
            "epoch 157/800, batch 40/75, loss 0.1806\n",
            "epoch 157/800, batch 41/75, loss 0.3550\n",
            "epoch 157/800, batch 42/75, loss 0.2692\n",
            "epoch 157/800, batch 43/75, loss 0.2727\n",
            "epoch 157/800, batch 44/75, loss 0.2825\n",
            "epoch 157/800, batch 45/75, loss 0.2257\n",
            "epoch 157/800, batch 46/75, loss 0.2285\n",
            "epoch 157/800, batch 47/75, loss 0.2758\n",
            "epoch 157/800, batch 48/75, loss 0.1842\n",
            "epoch 157/800, batch 49/75, loss 0.2536\n",
            "epoch 157/800, batch 50/75, loss 0.1691\n",
            "epoch 157/800, batch 51/75, loss 0.1524\n",
            "epoch 157/800, batch 52/75, loss 0.1043\n",
            "epoch 157/800, batch 53/75, loss 13.6230\n",
            "epoch 157/800, batch 54/75, loss 0.3852\n",
            "epoch 157/800, batch 55/75, loss 0.3248\n",
            "epoch 157/800, batch 56/75, loss 0.3818\n",
            "epoch 157/800, batch 57/75, loss 0.3278\n",
            "epoch 157/800, batch 58/75, loss 0.4528\n",
            "epoch 157/800, batch 59/75, loss 0.5144\n",
            "epoch 157/800, batch 60/75, loss 0.4884\n",
            "epoch 157/800, batch 61/75, loss 0.5800\n",
            "epoch 157/800, batch 62/75, loss 0.8067\n",
            "epoch 157/800, batch 63/75, loss 0.6092\n",
            "epoch 157/800, batch 64/75, loss 0.5315\n",
            "epoch 157/800, batch 65/75, loss 0.5491\n",
            "epoch 157/800, batch 66/75, loss 0.4466\n",
            "epoch 157/800, batch 67/75, loss 0.5286\n",
            "epoch 157/800, batch 68/75, loss 0.4640\n",
            "epoch 157/800, batch 69/75, loss 0.5281\n",
            "epoch 157/800, batch 70/75, loss 0.3013\n",
            "epoch 157/800, batch 71/75, loss 0.3320\n",
            "epoch 157/800, batch 72/75, loss 0.3674\n",
            "epoch 157/800, batch 73/75, loss 0.2555\n",
            "epoch 157/800, batch 74/75, loss 0.2567\n",
            "epoch 157/800, batch 75/75, loss 0.1878\n",
            "epoch 157/800, training roc_auc_score 0.8378\n",
            "EarlyStopping counter: 61 out of 80\n",
            "epoch 157/800, validation roc_auc_score 0.7918, best validation roc_auc_score 0.8157\n",
            "epoch 158/800, batch 1/75, loss 0.7493\n",
            "epoch 158/800, batch 2/75, loss 0.7620\n",
            "epoch 158/800, batch 3/75, loss 0.4441\n",
            "epoch 158/800, batch 4/75, loss 0.3760\n",
            "epoch 158/800, batch 5/75, loss 0.1491\n",
            "epoch 158/800, batch 6/75, loss 0.1130\n",
            "epoch 158/800, batch 7/75, loss 0.1865\n",
            "epoch 158/800, batch 8/75, loss 2.0010\n",
            "epoch 158/800, batch 9/75, loss 0.1432\n",
            "epoch 158/800, batch 10/75, loss 1.6380\n",
            "epoch 158/800, batch 11/75, loss 0.2478\n",
            "epoch 158/800, batch 12/75, loss 0.2893\n",
            "epoch 158/800, batch 13/75, loss 0.2537\n",
            "epoch 158/800, batch 14/75, loss 0.1267\n",
            "epoch 158/800, batch 15/75, loss 2.1973\n",
            "epoch 158/800, batch 16/75, loss 0.1441\n",
            "epoch 158/800, batch 17/75, loss 0.2501\n",
            "epoch 158/800, batch 18/75, loss 0.3440\n",
            "epoch 158/800, batch 19/75, loss 0.3209\n",
            "epoch 158/800, batch 20/75, loss 1.6291\n",
            "epoch 158/800, batch 21/75, loss 0.1557\n",
            "epoch 158/800, batch 22/75, loss 0.1672\n",
            "epoch 158/800, batch 23/75, loss 0.1317\n",
            "epoch 158/800, batch 24/75, loss 0.1939\n",
            "epoch 158/800, batch 25/75, loss 0.1926\n",
            "epoch 158/800, batch 26/75, loss 4.0181\n",
            "epoch 158/800, batch 27/75, loss 0.2248\n",
            "epoch 158/800, batch 28/75, loss 0.3507\n",
            "epoch 158/800, batch 29/75, loss 0.1621\n",
            "epoch 158/800, batch 30/75, loss 0.2769\n",
            "epoch 158/800, batch 31/75, loss 0.2875\n",
            "epoch 158/800, batch 32/75, loss 0.2848\n",
            "epoch 158/800, batch 33/75, loss 0.2176\n",
            "epoch 158/800, batch 34/75, loss 0.3735\n",
            "epoch 158/800, batch 35/75, loss 0.2287\n",
            "epoch 158/800, batch 36/75, loss 4.0519\n",
            "epoch 158/800, batch 37/75, loss 0.6887\n",
            "epoch 158/800, batch 38/75, loss 0.2410\n",
            "epoch 158/800, batch 39/75, loss 0.4116\n",
            "epoch 158/800, batch 40/75, loss 0.2052\n",
            "epoch 158/800, batch 41/75, loss 0.4287\n",
            "epoch 158/800, batch 42/75, loss 0.2750\n",
            "epoch 158/800, batch 43/75, loss 0.3529\n",
            "epoch 158/800, batch 44/75, loss 0.3409\n",
            "epoch 158/800, batch 45/75, loss 0.2839\n",
            "epoch 158/800, batch 46/75, loss 0.2546\n",
            "epoch 158/800, batch 47/75, loss 0.3808\n",
            "epoch 158/800, batch 48/75, loss 0.2562\n",
            "epoch 158/800, batch 49/75, loss 0.3123\n",
            "epoch 158/800, batch 50/75, loss 0.2510\n",
            "epoch 158/800, batch 51/75, loss 0.2284\n",
            "epoch 158/800, batch 52/75, loss 0.1476\n",
            "epoch 158/800, batch 53/75, loss 9.7886\n",
            "epoch 158/800, batch 54/75, loss 0.5050\n",
            "epoch 158/800, batch 55/75, loss 0.3021\n",
            "epoch 158/800, batch 56/75, loss 0.4157\n",
            "epoch 158/800, batch 57/75, loss 0.2884\n",
            "epoch 158/800, batch 58/75, loss 0.4235\n",
            "epoch 158/800, batch 59/75, loss 0.5295\n",
            "epoch 158/800, batch 60/75, loss 0.4964\n",
            "epoch 158/800, batch 61/75, loss 0.5291\n",
            "epoch 158/800, batch 62/75, loss 0.7225\n",
            "epoch 158/800, batch 63/75, loss 0.6017\n",
            "epoch 158/800, batch 64/75, loss 0.5009\n",
            "epoch 158/800, batch 65/75, loss 0.4981\n",
            "epoch 158/800, batch 66/75, loss 0.4159\n",
            "epoch 158/800, batch 67/75, loss 0.5139\n",
            "epoch 158/800, batch 68/75, loss 0.4630\n",
            "epoch 158/800, batch 69/75, loss 0.5828\n",
            "epoch 158/800, batch 70/75, loss 0.3267\n",
            "epoch 158/800, batch 71/75, loss 0.4022\n",
            "epoch 158/800, batch 72/75, loss 0.4507\n",
            "epoch 158/800, batch 73/75, loss 0.3434\n",
            "epoch 158/800, batch 74/75, loss 0.3158\n",
            "epoch 158/800, batch 75/75, loss 0.2677\n",
            "epoch 158/800, training roc_auc_score 0.8614\n",
            "EarlyStopping counter: 62 out of 80\n",
            "epoch 158/800, validation roc_auc_score 0.7963, best validation roc_auc_score 0.8157\n",
            "epoch 159/800, batch 1/75, loss 0.8056\n",
            "epoch 159/800, batch 2/75, loss 1.2315\n",
            "epoch 159/800, batch 3/75, loss 0.3969\n",
            "epoch 159/800, batch 4/75, loss 0.4474\n",
            "epoch 159/800, batch 5/75, loss 0.1982\n",
            "epoch 159/800, batch 6/75, loss 0.1692\n",
            "epoch 159/800, batch 7/75, loss 0.2514\n",
            "epoch 159/800, batch 8/75, loss 1.7363\n",
            "epoch 159/800, batch 9/75, loss 0.1601\n",
            "epoch 159/800, batch 10/75, loss 0.7726\n",
            "epoch 159/800, batch 11/75, loss 0.3400\n",
            "epoch 159/800, batch 12/75, loss 0.2263\n",
            "epoch 159/800, batch 13/75, loss 0.2889\n",
            "epoch 159/800, batch 14/75, loss 0.1778\n",
            "epoch 159/800, batch 15/75, loss 2.2800\n",
            "epoch 159/800, batch 16/75, loss 0.1453\n",
            "epoch 159/800, batch 17/75, loss 0.2379\n",
            "epoch 159/800, batch 18/75, loss 0.2551\n",
            "epoch 159/800, batch 19/75, loss 0.2446\n",
            "epoch 159/800, batch 20/75, loss 1.6822\n",
            "epoch 159/800, batch 21/75, loss 0.1444\n",
            "epoch 159/800, batch 22/75, loss 0.1596\n",
            "epoch 159/800, batch 23/75, loss 0.1450\n",
            "epoch 159/800, batch 24/75, loss 0.1800\n",
            "epoch 159/800, batch 25/75, loss 0.1447\n",
            "epoch 159/800, batch 26/75, loss 5.0192\n",
            "epoch 159/800, batch 27/75, loss 0.1645\n",
            "epoch 159/800, batch 28/75, loss 0.2631\n",
            "epoch 159/800, batch 29/75, loss 0.1454\n",
            "epoch 159/800, batch 30/75, loss 0.2611\n",
            "epoch 159/800, batch 31/75, loss 0.2093\n",
            "epoch 159/800, batch 32/75, loss 0.2165\n",
            "epoch 159/800, batch 33/75, loss 0.1634\n",
            "epoch 159/800, batch 34/75, loss 0.2911\n",
            "epoch 159/800, batch 35/75, loss 0.1963\n",
            "epoch 159/800, batch 36/75, loss 5.7445\n",
            "epoch 159/800, batch 37/75, loss 1.1942\n",
            "epoch 159/800, batch 38/75, loss 0.1804\n",
            "epoch 159/800, batch 39/75, loss 0.3399\n",
            "epoch 159/800, batch 40/75, loss 0.1995\n",
            "epoch 159/800, batch 41/75, loss 0.3681\n",
            "epoch 159/800, batch 42/75, loss 0.2685\n",
            "epoch 159/800, batch 43/75, loss 0.2606\n",
            "epoch 159/800, batch 44/75, loss 0.2933\n",
            "epoch 159/800, batch 45/75, loss 0.2292\n",
            "epoch 159/800, batch 46/75, loss 0.2415\n",
            "epoch 159/800, batch 47/75, loss 0.3105\n",
            "epoch 159/800, batch 48/75, loss 0.2166\n",
            "epoch 159/800, batch 49/75, loss 0.2632\n",
            "epoch 159/800, batch 50/75, loss 0.2301\n",
            "epoch 159/800, batch 51/75, loss 0.2152\n",
            "epoch 159/800, batch 52/75, loss 0.1552\n",
            "epoch 159/800, batch 53/75, loss 8.5278\n",
            "epoch 159/800, batch 54/75, loss 0.3757\n",
            "epoch 159/800, batch 55/75, loss 0.2875\n",
            "epoch 159/800, batch 56/75, loss 0.3323\n",
            "epoch 159/800, batch 57/75, loss 0.2033\n",
            "epoch 159/800, batch 58/75, loss 0.2852\n",
            "epoch 159/800, batch 59/75, loss 0.3799\n",
            "epoch 159/800, batch 60/75, loss 0.3220\n",
            "epoch 159/800, batch 61/75, loss 0.3403\n",
            "epoch 159/800, batch 62/75, loss 0.4778\n",
            "epoch 159/800, batch 63/75, loss 0.3988\n",
            "epoch 159/800, batch 64/75, loss 0.3306\n",
            "epoch 159/800, batch 65/75, loss 0.3399\n",
            "epoch 159/800, batch 66/75, loss 0.3082\n",
            "epoch 159/800, batch 67/75, loss 0.3623\n",
            "epoch 159/800, batch 68/75, loss 0.3024\n",
            "epoch 159/800, batch 69/75, loss 0.3782\n",
            "epoch 159/800, batch 70/75, loss 0.2136\n",
            "epoch 159/800, batch 71/75, loss 0.2581\n",
            "epoch 159/800, batch 72/75, loss 0.3113\n",
            "epoch 159/800, batch 73/75, loss 0.2470\n",
            "epoch 159/800, batch 74/75, loss 0.2316\n",
            "epoch 159/800, batch 75/75, loss 0.1693\n",
            "epoch 159/800, training roc_auc_score 0.8710\n",
            "EarlyStopping counter: 63 out of 80\n",
            "epoch 159/800, validation roc_auc_score 0.7929, best validation roc_auc_score 0.8157\n",
            "epoch 160/800, batch 1/75, loss 0.5688\n",
            "epoch 160/800, batch 2/75, loss 0.9483\n",
            "epoch 160/800, batch 3/75, loss 0.3400\n",
            "epoch 160/800, batch 4/75, loss 0.4390\n",
            "epoch 160/800, batch 5/75, loss 0.1498\n",
            "epoch 160/800, batch 6/75, loss 0.1226\n",
            "epoch 160/800, batch 7/75, loss 0.1579\n",
            "epoch 160/800, batch 8/75, loss 2.3162\n",
            "epoch 160/800, batch 9/75, loss 0.1606\n",
            "epoch 160/800, batch 10/75, loss 1.3941\n",
            "epoch 160/800, batch 11/75, loss 0.3043\n",
            "epoch 160/800, batch 12/75, loss 0.2202\n",
            "epoch 160/800, batch 13/75, loss 0.2456\n",
            "epoch 160/800, batch 14/75, loss 0.1552\n",
            "epoch 160/800, batch 15/75, loss 2.4743\n",
            "epoch 160/800, batch 16/75, loss 0.1401\n",
            "epoch 160/800, batch 17/75, loss 0.2461\n",
            "epoch 160/800, batch 18/75, loss 0.2252\n",
            "epoch 160/800, batch 19/75, loss 0.2463\n",
            "epoch 160/800, batch 20/75, loss 1.7891\n",
            "epoch 160/800, batch 21/75, loss 0.1478\n",
            "epoch 160/800, batch 22/75, loss 0.1672\n",
            "epoch 160/800, batch 23/75, loss 0.1576\n",
            "epoch 160/800, batch 24/75, loss 0.2113\n",
            "epoch 160/800, batch 25/75, loss 0.1682\n",
            "epoch 160/800, batch 26/75, loss 3.7805\n",
            "epoch 160/800, batch 27/75, loss 0.2514\n",
            "epoch 160/800, batch 28/75, loss 0.3008\n",
            "epoch 160/800, batch 29/75, loss 0.1814\n",
            "epoch 160/800, batch 30/75, loss 0.3574\n",
            "epoch 160/800, batch 31/75, loss 0.2313\n",
            "epoch 160/800, batch 32/75, loss 0.2481\n",
            "epoch 160/800, batch 33/75, loss 0.1867\n",
            "epoch 160/800, batch 34/75, loss 0.3738\n",
            "epoch 160/800, batch 35/75, loss 0.2347\n",
            "epoch 160/800, batch 36/75, loss 3.8942\n",
            "epoch 160/800, batch 37/75, loss 0.6064\n",
            "epoch 160/800, batch 38/75, loss 0.2143\n",
            "epoch 160/800, batch 39/75, loss 0.4569\n",
            "epoch 160/800, batch 40/75, loss 0.2296\n",
            "epoch 160/800, batch 41/75, loss 0.4897\n",
            "epoch 160/800, batch 42/75, loss 0.2566\n",
            "epoch 160/800, batch 43/75, loss 0.2887\n",
            "epoch 160/800, batch 44/75, loss 0.3464\n",
            "epoch 160/800, batch 45/75, loss 0.2792\n",
            "epoch 160/800, batch 46/75, loss 0.2419\n",
            "epoch 160/800, batch 47/75, loss 0.3398\n",
            "epoch 160/800, batch 48/75, loss 0.1759\n",
            "epoch 160/800, batch 49/75, loss 0.2578\n",
            "epoch 160/800, batch 50/75, loss 0.2537\n",
            "epoch 160/800, batch 51/75, loss 0.2413\n",
            "epoch 160/800, batch 52/75, loss 0.1341\n",
            "epoch 160/800, batch 53/75, loss 10.6974\n",
            "epoch 160/800, batch 54/75, loss 0.4339\n",
            "epoch 160/800, batch 55/75, loss 0.3039\n",
            "epoch 160/800, batch 56/75, loss 0.3425\n",
            "epoch 160/800, batch 57/75, loss 0.2840\n",
            "epoch 160/800, batch 58/75, loss 0.3814\n",
            "epoch 160/800, batch 59/75, loss 0.4958\n",
            "epoch 160/800, batch 60/75, loss 0.4492\n",
            "epoch 160/800, batch 61/75, loss 0.4522\n",
            "epoch 160/800, batch 62/75, loss 0.6214\n",
            "epoch 160/800, batch 63/75, loss 0.4923\n",
            "epoch 160/800, batch 64/75, loss 0.4429\n",
            "epoch 160/800, batch 65/75, loss 0.4581\n",
            "epoch 160/800, batch 66/75, loss 0.3833\n",
            "epoch 160/800, batch 67/75, loss 0.4593\n",
            "epoch 160/800, batch 68/75, loss 0.3768\n",
            "epoch 160/800, batch 69/75, loss 0.4296\n",
            "epoch 160/800, batch 70/75, loss 0.3383\n",
            "epoch 160/800, batch 71/75, loss 0.3393\n",
            "epoch 160/800, batch 72/75, loss 0.3931\n",
            "epoch 160/800, batch 73/75, loss 0.2861\n",
            "epoch 160/800, batch 74/75, loss 0.3174\n",
            "epoch 160/800, batch 75/75, loss 0.1967\n",
            "epoch 160/800, training roc_auc_score 0.8616\n",
            "EarlyStopping counter: 64 out of 80\n",
            "epoch 160/800, validation roc_auc_score 0.7966, best validation roc_auc_score 0.8157\n",
            "epoch 161/800, batch 1/75, loss 0.8227\n",
            "epoch 161/800, batch 2/75, loss 1.0094\n",
            "epoch 161/800, batch 3/75, loss 0.3189\n",
            "epoch 161/800, batch 4/75, loss 0.3990\n",
            "epoch 161/800, batch 5/75, loss 0.1808\n",
            "epoch 161/800, batch 6/75, loss 0.1468\n",
            "epoch 161/800, batch 7/75, loss 0.2782\n",
            "epoch 161/800, batch 8/75, loss 1.8985\n",
            "epoch 161/800, batch 9/75, loss 0.1512\n",
            "epoch 161/800, batch 10/75, loss 1.1091\n",
            "epoch 161/800, batch 11/75, loss 0.2806\n",
            "epoch 161/800, batch 12/75, loss 0.2520\n",
            "epoch 161/800, batch 13/75, loss 0.2660\n",
            "epoch 161/800, batch 14/75, loss 0.1738\n",
            "epoch 161/800, batch 15/75, loss 2.6652\n",
            "epoch 161/800, batch 16/75, loss 0.1397\n",
            "epoch 161/800, batch 17/75, loss 0.2609\n",
            "epoch 161/800, batch 18/75, loss 0.2560\n",
            "epoch 161/800, batch 19/75, loss 0.2421\n",
            "epoch 161/800, batch 20/75, loss 1.8314\n",
            "epoch 161/800, batch 21/75, loss 0.1456\n",
            "epoch 161/800, batch 22/75, loss 0.1625\n",
            "epoch 161/800, batch 23/75, loss 0.1533\n",
            "epoch 161/800, batch 24/75, loss 0.2227\n",
            "epoch 161/800, batch 25/75, loss 0.1536\n",
            "epoch 161/800, batch 26/75, loss 4.2215\n",
            "epoch 161/800, batch 27/75, loss 0.2303\n",
            "epoch 161/800, batch 28/75, loss 0.2731\n",
            "epoch 161/800, batch 29/75, loss 0.1700\n",
            "epoch 161/800, batch 30/75, loss 0.3031\n",
            "epoch 161/800, batch 31/75, loss 0.2340\n",
            "epoch 161/800, batch 32/75, loss 0.2150\n",
            "epoch 161/800, batch 33/75, loss 0.2043\n",
            "epoch 161/800, batch 34/75, loss 0.3256\n",
            "epoch 161/800, batch 35/75, loss 0.2455\n",
            "epoch 161/800, batch 36/75, loss 3.8383\n",
            "epoch 161/800, batch 37/75, loss 0.7463\n",
            "epoch 161/800, batch 38/75, loss 0.2084\n",
            "epoch 161/800, batch 39/75, loss 0.4037\n",
            "epoch 161/800, batch 40/75, loss 0.2158\n",
            "epoch 161/800, batch 41/75, loss 0.4577\n",
            "epoch 161/800, batch 42/75, loss 0.2480\n",
            "epoch 161/800, batch 43/75, loss 0.3190\n",
            "epoch 161/800, batch 44/75, loss 0.3221\n",
            "epoch 161/800, batch 45/75, loss 0.2470\n",
            "epoch 161/800, batch 46/75, loss 0.2431\n",
            "epoch 161/800, batch 47/75, loss 0.3657\n",
            "epoch 161/800, batch 48/75, loss 0.2001\n",
            "epoch 161/800, batch 49/75, loss 0.2631\n",
            "epoch 161/800, batch 50/75, loss 0.2192\n",
            "epoch 161/800, batch 51/75, loss 0.2270\n",
            "epoch 161/800, batch 52/75, loss 0.1719\n",
            "epoch 161/800, batch 53/75, loss 9.2351\n",
            "epoch 161/800, batch 54/75, loss 0.5810\n",
            "epoch 161/800, batch 55/75, loss 0.2848\n",
            "epoch 161/800, batch 56/75, loss 0.3398\n",
            "epoch 161/800, batch 57/75, loss 0.2452\n",
            "epoch 161/800, batch 58/75, loss 0.3406\n",
            "epoch 161/800, batch 59/75, loss 0.3583\n",
            "epoch 161/800, batch 60/75, loss 0.3345\n",
            "epoch 161/800, batch 61/75, loss 0.3638\n",
            "epoch 161/800, batch 62/75, loss 0.5141\n",
            "epoch 161/800, batch 63/75, loss 0.3920\n",
            "epoch 161/800, batch 64/75, loss 0.3680\n",
            "epoch 161/800, batch 65/75, loss 0.3642\n",
            "epoch 161/800, batch 66/75, loss 0.2675\n",
            "epoch 161/800, batch 67/75, loss 0.4202\n",
            "epoch 161/800, batch 68/75, loss 0.3105\n",
            "epoch 161/800, batch 69/75, loss 0.4384\n",
            "epoch 161/800, batch 70/75, loss 0.2729\n",
            "epoch 161/800, batch 71/75, loss 0.2487\n",
            "epoch 161/800, batch 72/75, loss 0.3652\n",
            "epoch 161/800, batch 73/75, loss 0.2702\n",
            "epoch 161/800, batch 74/75, loss 0.2568\n",
            "epoch 161/800, batch 75/75, loss 0.1840\n",
            "epoch 161/800, training roc_auc_score 0.8739\n",
            "EarlyStopping counter: 65 out of 80\n",
            "epoch 161/800, validation roc_auc_score 0.8010, best validation roc_auc_score 0.8157\n",
            "epoch 162/800, batch 1/75, loss 1.1466\n",
            "epoch 162/800, batch 2/75, loss 0.9364\n",
            "epoch 162/800, batch 3/75, loss 0.3045\n",
            "epoch 162/800, batch 4/75, loss 0.4301\n",
            "epoch 162/800, batch 5/75, loss 0.1839\n",
            "epoch 162/800, batch 6/75, loss 0.1443\n",
            "epoch 162/800, batch 7/75, loss 0.2444\n",
            "epoch 162/800, batch 8/75, loss 1.9717\n",
            "epoch 162/800, batch 9/75, loss 0.1613\n",
            "epoch 162/800, batch 10/75, loss 0.8090\n",
            "epoch 162/800, batch 11/75, loss 0.2954\n",
            "epoch 162/800, batch 12/75, loss 0.2422\n",
            "epoch 162/800, batch 13/75, loss 0.2805\n",
            "epoch 162/800, batch 14/75, loss 0.1857\n",
            "epoch 162/800, batch 15/75, loss 2.9081\n",
            "epoch 162/800, batch 16/75, loss 0.1732\n",
            "epoch 162/800, batch 17/75, loss 0.2836\n",
            "epoch 162/800, batch 18/75, loss 0.2873\n",
            "epoch 162/800, batch 19/75, loss 0.2449\n",
            "epoch 162/800, batch 20/75, loss 2.6141\n",
            "epoch 162/800, batch 21/75, loss 0.1907\n",
            "epoch 162/800, batch 22/75, loss 0.1729\n",
            "epoch 162/800, batch 23/75, loss 0.1646\n",
            "epoch 162/800, batch 24/75, loss 0.1758\n",
            "epoch 162/800, batch 25/75, loss 0.1960\n",
            "epoch 162/800, batch 26/75, loss 4.9216\n",
            "epoch 162/800, batch 27/75, loss 0.2582\n",
            "epoch 162/800, batch 28/75, loss 0.2855\n",
            "epoch 162/800, batch 29/75, loss 0.1901\n",
            "epoch 162/800, batch 30/75, loss 0.2980\n",
            "epoch 162/800, batch 31/75, loss 0.2719\n",
            "epoch 162/800, batch 32/75, loss 0.2301\n",
            "epoch 162/800, batch 33/75, loss 0.2480\n",
            "epoch 162/800, batch 34/75, loss 0.3255\n",
            "epoch 162/800, batch 35/75, loss 0.2343\n",
            "epoch 162/800, batch 36/75, loss 3.5780\n",
            "epoch 162/800, batch 37/75, loss 0.6483\n",
            "epoch 162/800, batch 38/75, loss 0.2312\n",
            "epoch 162/800, batch 39/75, loss 0.3468\n",
            "epoch 162/800, batch 40/75, loss 0.2441\n",
            "epoch 162/800, batch 41/75, loss 0.3887\n",
            "epoch 162/800, batch 42/75, loss 0.2803\n",
            "epoch 162/800, batch 43/75, loss 0.3053\n",
            "epoch 162/800, batch 44/75, loss 0.3317\n",
            "epoch 162/800, batch 45/75, loss 0.2446\n",
            "epoch 162/800, batch 46/75, loss 0.2380\n",
            "epoch 162/800, batch 47/75, loss 0.3395\n",
            "epoch 162/800, batch 48/75, loss 0.2177\n",
            "epoch 162/800, batch 49/75, loss 0.2747\n",
            "epoch 162/800, batch 50/75, loss 0.2343\n",
            "epoch 162/800, batch 51/75, loss 0.1969\n",
            "epoch 162/800, batch 52/75, loss 0.1300\n",
            "epoch 162/800, batch 53/75, loss 10.9049\n",
            "epoch 162/800, batch 54/75, loss 0.4050\n",
            "epoch 162/800, batch 55/75, loss 0.2758\n",
            "epoch 162/800, batch 56/75, loss 0.3258\n",
            "epoch 162/800, batch 57/75, loss 0.2166\n",
            "epoch 162/800, batch 58/75, loss 0.3047\n",
            "epoch 162/800, batch 59/75, loss 0.3504\n",
            "epoch 162/800, batch 60/75, loss 0.3383\n",
            "epoch 162/800, batch 61/75, loss 0.3256\n",
            "epoch 162/800, batch 62/75, loss 0.5219\n",
            "epoch 162/800, batch 63/75, loss 0.3846\n",
            "epoch 162/800, batch 64/75, loss 0.3318\n",
            "epoch 162/800, batch 65/75, loss 0.3758\n",
            "epoch 162/800, batch 66/75, loss 0.2888\n",
            "epoch 162/800, batch 67/75, loss 0.3581\n",
            "epoch 162/800, batch 68/75, loss 0.3121\n",
            "epoch 162/800, batch 69/75, loss 0.3559\n",
            "epoch 162/800, batch 70/75, loss 0.1956\n",
            "epoch 162/800, batch 71/75, loss 0.2724\n",
            "epoch 162/800, batch 72/75, loss 0.3402\n",
            "epoch 162/800, batch 73/75, loss 0.2239\n",
            "epoch 162/800, batch 74/75, loss 0.2272\n",
            "epoch 162/800, batch 75/75, loss 0.1518\n",
            "epoch 162/800, training roc_auc_score 0.8559\n",
            "EarlyStopping counter: 66 out of 80\n",
            "epoch 162/800, validation roc_auc_score 0.8027, best validation roc_auc_score 0.8157\n",
            "epoch 163/800, batch 1/75, loss 0.9455\n",
            "epoch 163/800, batch 2/75, loss 0.6577\n",
            "epoch 163/800, batch 3/75, loss 0.3230\n",
            "epoch 163/800, batch 4/75, loss 0.3508\n",
            "epoch 163/800, batch 5/75, loss 0.1653\n",
            "epoch 163/800, batch 6/75, loss 0.1275\n",
            "epoch 163/800, batch 7/75, loss 0.1802\n",
            "epoch 163/800, batch 8/75, loss 1.5896\n",
            "epoch 163/800, batch 9/75, loss 0.1667\n",
            "epoch 163/800, batch 10/75, loss 1.0843\n",
            "epoch 163/800, batch 11/75, loss 0.2909\n",
            "epoch 163/800, batch 12/75, loss 0.2076\n",
            "epoch 163/800, batch 13/75, loss 0.2354\n",
            "epoch 163/800, batch 14/75, loss 0.1544\n",
            "epoch 163/800, batch 15/75, loss 3.5444\n",
            "epoch 163/800, batch 16/75, loss 0.1518\n",
            "epoch 163/800, batch 17/75, loss 0.2834\n",
            "epoch 163/800, batch 18/75, loss 0.2613\n",
            "epoch 163/800, batch 19/75, loss 0.3112\n",
            "epoch 163/800, batch 20/75, loss 2.1467\n",
            "epoch 163/800, batch 21/75, loss 0.2313\n",
            "epoch 163/800, batch 22/75, loss 0.2502\n",
            "epoch 163/800, batch 23/75, loss 0.2041\n",
            "epoch 163/800, batch 24/75, loss 0.2776\n",
            "epoch 163/800, batch 25/75, loss 0.2792\n",
            "epoch 163/800, batch 26/75, loss 3.9295\n",
            "epoch 163/800, batch 27/75, loss 0.3394\n",
            "epoch 163/800, batch 28/75, loss 0.4107\n",
            "epoch 163/800, batch 29/75, loss 0.3452\n",
            "epoch 163/800, batch 30/75, loss 0.5654\n",
            "epoch 163/800, batch 31/75, loss 0.4271\n",
            "epoch 163/800, batch 32/75, loss 0.4128\n",
            "epoch 163/800, batch 33/75, loss 0.4317\n",
            "epoch 163/800, batch 34/75, loss 0.6263\n",
            "epoch 163/800, batch 35/75, loss 0.3591\n",
            "epoch 163/800, batch 36/75, loss 3.1334\n",
            "epoch 163/800, batch 37/75, loss 0.6284\n",
            "epoch 163/800, batch 38/75, loss 0.3199\n",
            "epoch 163/800, batch 39/75, loss 0.5131\n",
            "epoch 163/800, batch 40/75, loss 0.2616\n",
            "epoch 163/800, batch 41/75, loss 0.4525\n",
            "epoch 163/800, batch 42/75, loss 0.3597\n",
            "epoch 163/800, batch 43/75, loss 0.2966\n",
            "epoch 163/800, batch 44/75, loss 0.3122\n",
            "epoch 163/800, batch 45/75, loss 0.1860\n",
            "epoch 163/800, batch 46/75, loss 0.1878\n",
            "epoch 163/800, batch 47/75, loss 0.2236\n",
            "epoch 163/800, batch 48/75, loss 0.0974\n",
            "epoch 163/800, batch 49/75, loss 0.1065\n",
            "epoch 163/800, batch 50/75, loss 0.0680\n",
            "epoch 163/800, batch 51/75, loss 0.0415\n",
            "epoch 163/800, batch 52/75, loss 0.0175\n",
            "epoch 163/800, batch 53/75, loss 44.3334\n",
            "epoch 163/800, batch 54/75, loss 1.0681\n",
            "epoch 163/800, batch 55/75, loss 0.1202\n",
            "epoch 163/800, batch 56/75, loss 0.1690\n",
            "epoch 163/800, batch 57/75, loss 0.1733\n",
            "epoch 163/800, batch 58/75, loss 0.2324\n",
            "epoch 163/800, batch 59/75, loss 0.4475\n",
            "epoch 163/800, batch 60/75, loss 0.5145\n",
            "epoch 163/800, batch 61/75, loss 0.4799\n",
            "epoch 163/800, batch 62/75, loss 0.8471\n",
            "epoch 163/800, batch 63/75, loss 0.7016\n",
            "epoch 163/800, batch 64/75, loss 0.6750\n",
            "epoch 163/800, batch 65/75, loss 0.7193\n",
            "epoch 163/800, batch 66/75, loss 0.5286\n",
            "epoch 163/800, batch 67/75, loss 0.6818\n",
            "epoch 163/800, batch 68/75, loss 0.7188\n",
            "epoch 163/800, batch 69/75, loss 0.7828\n",
            "epoch 163/800, batch 70/75, loss 0.4350\n",
            "epoch 163/800, batch 71/75, loss 0.5120\n",
            "epoch 163/800, batch 72/75, loss 0.6107\n",
            "epoch 163/800, batch 73/75, loss 0.4584\n",
            "epoch 163/800, batch 74/75, loss 0.4771\n",
            "epoch 163/800, batch 75/75, loss 0.2913\n",
            "epoch 163/800, training roc_auc_score 0.6941\n",
            "EarlyStopping counter: 67 out of 80\n",
            "epoch 163/800, validation roc_auc_score 0.8141, best validation roc_auc_score 0.8157\n",
            "epoch 164/800, batch 1/75, loss 1.0357\n",
            "epoch 164/800, batch 2/75, loss 1.0370\n",
            "epoch 164/800, batch 3/75, loss 0.6631\n",
            "epoch 164/800, batch 4/75, loss 0.5474\n",
            "epoch 164/800, batch 5/75, loss 0.2666\n",
            "epoch 164/800, batch 6/75, loss 0.1710\n",
            "epoch 164/800, batch 7/75, loss 0.3242\n",
            "epoch 164/800, batch 8/75, loss 2.0390\n",
            "epoch 164/800, batch 9/75, loss 0.2044\n",
            "epoch 164/800, batch 10/75, loss 1.2493\n",
            "epoch 164/800, batch 11/75, loss 0.4017\n",
            "epoch 164/800, batch 12/75, loss 0.3337\n",
            "epoch 164/800, batch 13/75, loss 0.3858\n",
            "epoch 164/800, batch 14/75, loss 0.1437\n",
            "epoch 164/800, batch 15/75, loss 3.1100\n",
            "epoch 164/800, batch 16/75, loss 0.1801\n",
            "epoch 164/800, batch 17/75, loss 0.2977\n",
            "epoch 164/800, batch 18/75, loss 0.3574\n",
            "epoch 164/800, batch 19/75, loss 0.2984\n",
            "epoch 164/800, batch 20/75, loss 1.0324\n",
            "epoch 164/800, batch 21/75, loss 0.1607\n",
            "epoch 164/800, batch 22/75, loss 0.2390\n",
            "epoch 164/800, batch 23/75, loss 0.1084\n",
            "epoch 164/800, batch 24/75, loss 0.1242\n",
            "epoch 164/800, batch 25/75, loss 0.1375\n",
            "epoch 164/800, batch 26/75, loss 4.2790\n",
            "epoch 164/800, batch 27/75, loss 0.1904\n",
            "epoch 164/800, batch 28/75, loss 0.2992\n",
            "epoch 164/800, batch 29/75, loss 0.1933\n",
            "epoch 164/800, batch 30/75, loss 0.3222\n",
            "epoch 164/800, batch 31/75, loss 0.2753\n",
            "epoch 164/800, batch 32/75, loss 0.2750\n",
            "epoch 164/800, batch 33/75, loss 0.2802\n",
            "epoch 164/800, batch 34/75, loss 0.4878\n",
            "epoch 164/800, batch 35/75, loss 0.3027\n",
            "epoch 164/800, batch 36/75, loss 4.8415\n",
            "epoch 164/800, batch 37/75, loss 0.8219\n",
            "epoch 164/800, batch 38/75, loss 0.2278\n",
            "epoch 164/800, batch 39/75, loss 0.3889\n",
            "epoch 164/800, batch 40/75, loss 0.2571\n",
            "epoch 164/800, batch 41/75, loss 0.4656\n",
            "epoch 164/800, batch 42/75, loss 0.3602\n",
            "epoch 164/800, batch 43/75, loss 0.3924\n",
            "epoch 164/800, batch 44/75, loss 0.3499\n",
            "epoch 164/800, batch 45/75, loss 0.3049\n",
            "epoch 164/800, batch 46/75, loss 0.2794\n",
            "epoch 164/800, batch 47/75, loss 0.3498\n",
            "epoch 164/800, batch 48/75, loss 0.2464\n",
            "epoch 164/800, batch 49/75, loss 0.3209\n",
            "epoch 164/800, batch 50/75, loss 0.2590\n",
            "epoch 164/800, batch 51/75, loss 0.1732\n",
            "epoch 164/800, batch 52/75, loss 0.1230\n",
            "epoch 164/800, batch 53/75, loss 13.3805\n",
            "epoch 164/800, batch 54/75, loss 0.5433\n",
            "epoch 164/800, batch 55/75, loss 0.3156\n",
            "epoch 164/800, batch 56/75, loss 0.4212\n",
            "epoch 164/800, batch 57/75, loss 0.3004\n",
            "epoch 164/800, batch 58/75, loss 0.4366\n",
            "epoch 164/800, batch 59/75, loss 0.5153\n",
            "epoch 164/800, batch 60/75, loss 0.4864\n",
            "epoch 164/800, batch 61/75, loss 0.4791\n",
            "epoch 164/800, batch 62/75, loss 0.7588\n",
            "epoch 164/800, batch 63/75, loss 0.5388\n",
            "epoch 164/800, batch 64/75, loss 0.4648\n",
            "epoch 164/800, batch 65/75, loss 0.5187\n",
            "epoch 164/800, batch 66/75, loss 0.4236\n",
            "epoch 164/800, batch 67/75, loss 0.4486\n",
            "epoch 164/800, batch 68/75, loss 0.4264\n",
            "epoch 164/800, batch 69/75, loss 0.4624\n",
            "epoch 164/800, batch 70/75, loss 0.2791\n",
            "epoch 164/800, batch 71/75, loss 0.3376\n",
            "epoch 164/800, batch 72/75, loss 0.4514\n",
            "epoch 164/800, batch 73/75, loss 0.2712\n",
            "epoch 164/800, batch 74/75, loss 0.3532\n",
            "epoch 164/800, batch 75/75, loss 0.1890\n",
            "epoch 164/800, training roc_auc_score 0.8276\n",
            "EarlyStopping counter: 68 out of 80\n",
            "epoch 164/800, validation roc_auc_score 0.8070, best validation roc_auc_score 0.8157\n",
            "epoch 165/800, batch 1/75, loss 0.9657\n",
            "epoch 165/800, batch 2/75, loss 0.9999\n",
            "epoch 165/800, batch 3/75, loss 0.3000\n",
            "epoch 165/800, batch 4/75, loss 0.4540\n",
            "epoch 165/800, batch 5/75, loss 0.1845\n",
            "epoch 165/800, batch 6/75, loss 0.1645\n",
            "epoch 165/800, batch 7/75, loss 0.2038\n",
            "epoch 165/800, batch 8/75, loss 2.0396\n",
            "epoch 165/800, batch 9/75, loss 0.1402\n",
            "epoch 165/800, batch 10/75, loss 0.5831\n",
            "epoch 165/800, batch 11/75, loss 0.5426\n",
            "epoch 165/800, batch 12/75, loss 0.2668\n",
            "epoch 165/800, batch 13/75, loss 0.2732\n",
            "epoch 165/800, batch 14/75, loss 0.1354\n",
            "epoch 165/800, batch 15/75, loss 1.9649\n",
            "epoch 165/800, batch 16/75, loss 0.0971\n",
            "epoch 165/800, batch 17/75, loss 0.2478\n",
            "epoch 165/800, batch 18/75, loss 0.2771\n",
            "epoch 165/800, batch 19/75, loss 0.2565\n",
            "epoch 165/800, batch 20/75, loss 1.5953\n",
            "epoch 165/800, batch 21/75, loss 0.0960\n",
            "epoch 165/800, batch 22/75, loss 0.1972\n",
            "epoch 165/800, batch 23/75, loss 0.1034\n",
            "epoch 165/800, batch 24/75, loss 0.1107\n",
            "epoch 165/800, batch 25/75, loss 0.1083\n",
            "epoch 165/800, batch 26/75, loss 4.3826\n",
            "epoch 165/800, batch 27/75, loss 0.1289\n",
            "epoch 165/800, batch 28/75, loss 0.2582\n",
            "epoch 165/800, batch 29/75, loss 0.1419\n",
            "epoch 165/800, batch 30/75, loss 0.2517\n",
            "epoch 165/800, batch 31/75, loss 0.2314\n",
            "epoch 165/800, batch 32/75, loss 0.2174\n",
            "epoch 165/800, batch 33/75, loss 0.1707\n",
            "epoch 165/800, batch 34/75, loss 0.4055\n",
            "epoch 165/800, batch 35/75, loss 0.2027\n",
            "epoch 165/800, batch 36/75, loss 5.1934\n",
            "epoch 165/800, batch 37/75, loss 0.5973\n",
            "epoch 165/800, batch 38/75, loss 0.1779\n",
            "epoch 165/800, batch 39/75, loss 0.3650\n",
            "epoch 165/800, batch 40/75, loss 0.1854\n",
            "epoch 165/800, batch 41/75, loss 0.3242\n",
            "epoch 165/800, batch 42/75, loss 0.2744\n",
            "epoch 165/800, batch 43/75, loss 0.2807\n",
            "epoch 165/800, batch 44/75, loss 0.3272\n",
            "epoch 165/800, batch 45/75, loss 0.2716\n",
            "epoch 165/800, batch 46/75, loss 0.2701\n",
            "epoch 165/800, batch 47/75, loss 0.3391\n",
            "epoch 165/800, batch 48/75, loss 0.2172\n",
            "epoch 165/800, batch 49/75, loss 0.2727\n",
            "epoch 165/800, batch 50/75, loss 0.2861\n",
            "epoch 165/800, batch 51/75, loss 0.2334\n",
            "epoch 165/800, batch 52/75, loss 0.1353\n",
            "epoch 165/800, batch 53/75, loss 10.7500\n",
            "epoch 165/800, batch 54/75, loss 0.3079\n",
            "epoch 165/800, batch 55/75, loss 0.3062\n",
            "epoch 165/800, batch 56/75, loss 0.3326\n",
            "epoch 165/800, batch 57/75, loss 0.3004\n",
            "epoch 165/800, batch 58/75, loss 0.3563\n",
            "epoch 165/800, batch 59/75, loss 0.5224\n",
            "epoch 165/800, batch 60/75, loss 0.4677\n",
            "epoch 165/800, batch 61/75, loss 0.4776\n",
            "epoch 165/800, batch 62/75, loss 0.6678\n",
            "epoch 165/800, batch 63/75, loss 0.4656\n",
            "epoch 165/800, batch 64/75, loss 0.4453\n",
            "epoch 165/800, batch 65/75, loss 0.4879\n",
            "epoch 165/800, batch 66/75, loss 0.3246\n",
            "epoch 165/800, batch 67/75, loss 0.4110\n",
            "epoch 165/800, batch 68/75, loss 0.3452\n",
            "epoch 165/800, batch 69/75, loss 0.4005\n",
            "epoch 165/800, batch 70/75, loss 0.2471\n",
            "epoch 165/800, batch 71/75, loss 0.2593\n",
            "epoch 165/800, batch 72/75, loss 0.3364\n",
            "epoch 165/800, batch 73/75, loss 0.2264\n",
            "epoch 165/800, batch 74/75, loss 0.2534\n",
            "epoch 165/800, batch 75/75, loss 0.1421\n",
            "epoch 165/800, training roc_auc_score 0.8678\n",
            "EarlyStopping counter: 69 out of 80\n",
            "epoch 165/800, validation roc_auc_score 0.7984, best validation roc_auc_score 0.8157\n",
            "epoch 166/800, batch 1/75, loss 0.9703\n",
            "epoch 166/800, batch 2/75, loss 1.0045\n",
            "epoch 166/800, batch 3/75, loss 0.2318\n",
            "epoch 166/800, batch 4/75, loss 0.5984\n",
            "epoch 166/800, batch 5/75, loss 0.1580\n",
            "epoch 166/800, batch 6/75, loss 0.1179\n",
            "epoch 166/800, batch 7/75, loss 0.1821\n",
            "epoch 166/800, batch 8/75, loss 2.0218\n",
            "epoch 166/800, batch 9/75, loss 0.1138\n",
            "epoch 166/800, batch 10/75, loss 1.4503\n",
            "epoch 166/800, batch 11/75, loss 0.5665\n",
            "epoch 166/800, batch 12/75, loss 0.3040\n",
            "epoch 166/800, batch 13/75, loss 0.3354\n",
            "epoch 166/800, batch 14/75, loss 0.1329\n",
            "epoch 166/800, batch 15/75, loss 1.7735\n",
            "epoch 166/800, batch 16/75, loss 0.1756\n",
            "epoch 166/800, batch 17/75, loss 0.2778\n",
            "epoch 166/800, batch 18/75, loss 0.4679\n",
            "epoch 166/800, batch 19/75, loss 0.3826\n",
            "epoch 166/800, batch 20/75, loss 1.1908\n",
            "epoch 166/800, batch 21/75, loss 0.1865\n",
            "epoch 166/800, batch 22/75, loss 0.2534\n",
            "epoch 166/800, batch 23/75, loss 0.1252\n",
            "epoch 166/800, batch 24/75, loss 0.1722\n",
            "epoch 166/800, batch 25/75, loss 0.1318\n",
            "epoch 166/800, batch 26/75, loss 3.9945\n",
            "epoch 166/800, batch 27/75, loss 0.2196\n",
            "epoch 166/800, batch 28/75, loss 0.3851\n",
            "epoch 166/800, batch 29/75, loss 0.2536\n",
            "epoch 166/800, batch 30/75, loss 0.4334\n",
            "epoch 166/800, batch 31/75, loss 0.4113\n",
            "epoch 166/800, batch 32/75, loss 0.4101\n",
            "epoch 166/800, batch 33/75, loss 0.3109\n",
            "epoch 166/800, batch 34/75, loss 0.5478\n",
            "epoch 166/800, batch 35/75, loss 0.2813\n",
            "epoch 166/800, batch 36/75, loss 3.6694\n",
            "epoch 166/800, batch 37/75, loss 0.4889\n",
            "epoch 166/800, batch 38/75, loss 0.1747\n",
            "epoch 166/800, batch 39/75, loss 0.4025\n",
            "epoch 166/800, batch 40/75, loss 0.1889\n",
            "epoch 166/800, batch 41/75, loss 0.4198\n",
            "epoch 166/800, batch 42/75, loss 0.2200\n",
            "epoch 166/800, batch 43/75, loss 0.2388\n",
            "epoch 166/800, batch 44/75, loss 0.2346\n",
            "epoch 166/800, batch 45/75, loss 0.1792\n",
            "epoch 166/800, batch 46/75, loss 0.1782\n",
            "epoch 166/800, batch 47/75, loss 0.2266\n",
            "epoch 166/800, batch 48/75, loss 0.1230\n",
            "epoch 166/800, batch 49/75, loss 0.2591\n",
            "epoch 166/800, batch 50/75, loss 0.1831\n",
            "epoch 166/800, batch 51/75, loss 0.1113\n",
            "epoch 166/800, batch 52/75, loss 0.0754\n",
            "epoch 166/800, batch 53/75, loss 15.8880\n",
            "epoch 166/800, batch 54/75, loss 0.4917\n",
            "epoch 166/800, batch 55/75, loss 0.2625\n",
            "epoch 166/800, batch 56/75, loss 0.2586\n",
            "epoch 166/800, batch 57/75, loss 0.2329\n",
            "epoch 166/800, batch 58/75, loss 0.3602\n",
            "epoch 166/800, batch 59/75, loss 0.4589\n",
            "epoch 166/800, batch 60/75, loss 0.4369\n",
            "epoch 166/800, batch 61/75, loss 0.5156\n",
            "epoch 166/800, batch 62/75, loss 0.6697\n",
            "epoch 166/800, batch 63/75, loss 0.5018\n",
            "epoch 166/800, batch 64/75, loss 0.4704\n",
            "epoch 166/800, batch 65/75, loss 0.4900\n",
            "epoch 166/800, batch 66/75, loss 0.3986\n",
            "epoch 166/800, batch 67/75, loss 0.4736\n",
            "epoch 166/800, batch 68/75, loss 0.3380\n",
            "epoch 166/800, batch 69/75, loss 0.3820\n",
            "epoch 166/800, batch 70/75, loss 0.2240\n",
            "epoch 166/800, batch 71/75, loss 0.2334\n",
            "epoch 166/800, batch 72/75, loss 0.3296\n",
            "epoch 166/800, batch 73/75, loss 0.2054\n",
            "epoch 166/800, batch 74/75, loss 0.2292\n",
            "epoch 166/800, batch 75/75, loss 0.1373\n",
            "epoch 166/800, training roc_auc_score 0.8434\n",
            "EarlyStopping counter: 70 out of 80\n",
            "epoch 166/800, validation roc_auc_score 0.7930, best validation roc_auc_score 0.8157\n",
            "epoch 167/800, batch 1/75, loss 0.8157\n",
            "epoch 167/800, batch 2/75, loss 1.0699\n",
            "epoch 167/800, batch 3/75, loss 0.2131\n",
            "epoch 167/800, batch 4/75, loss 0.3753\n",
            "epoch 167/800, batch 5/75, loss 0.1390\n",
            "epoch 167/800, batch 6/75, loss 0.0915\n",
            "epoch 167/800, batch 7/75, loss 0.1232\n",
            "epoch 167/800, batch 8/75, loss 2.3379\n",
            "epoch 167/800, batch 9/75, loss 0.0949\n",
            "epoch 167/800, batch 10/75, loss 1.3299\n",
            "epoch 167/800, batch 11/75, loss 0.4137\n",
            "epoch 167/800, batch 12/75, loss 0.2512\n",
            "epoch 167/800, batch 13/75, loss 0.2193\n",
            "epoch 167/800, batch 14/75, loss 0.1133\n",
            "epoch 167/800, batch 15/75, loss 2.2855\n",
            "epoch 167/800, batch 16/75, loss 0.1507\n",
            "epoch 167/800, batch 17/75, loss 0.2608\n",
            "epoch 167/800, batch 18/75, loss 0.3571\n",
            "epoch 167/800, batch 19/75, loss 0.2643\n",
            "epoch 167/800, batch 20/75, loss 1.6087\n",
            "epoch 167/800, batch 21/75, loss 0.1221\n",
            "epoch 167/800, batch 22/75, loss 0.2339\n",
            "epoch 167/800, batch 23/75, loss 0.1476\n",
            "epoch 167/800, batch 24/75, loss 0.1702\n",
            "epoch 167/800, batch 25/75, loss 0.1597\n",
            "epoch 167/800, batch 26/75, loss 4.3177\n",
            "epoch 167/800, batch 27/75, loss 0.2032\n",
            "epoch 167/800, batch 28/75, loss 0.3898\n",
            "epoch 167/800, batch 29/75, loss 0.2015\n",
            "epoch 167/800, batch 30/75, loss 0.4109\n",
            "epoch 167/800, batch 31/75, loss 0.3282\n",
            "epoch 167/800, batch 32/75, loss 0.3344\n",
            "epoch 167/800, batch 33/75, loss 0.2748\n",
            "epoch 167/800, batch 34/75, loss 0.5124\n",
            "epoch 167/800, batch 35/75, loss 0.3282\n",
            "epoch 167/800, batch 36/75, loss 3.4997\n",
            "epoch 167/800, batch 37/75, loss 0.9264\n",
            "epoch 167/800, batch 38/75, loss 0.2663\n",
            "epoch 167/800, batch 39/75, loss 0.5172\n",
            "epoch 167/800, batch 40/75, loss 0.2684\n",
            "epoch 167/800, batch 41/75, loss 0.4960\n",
            "epoch 167/800, batch 42/75, loss 0.3092\n",
            "epoch 167/800, batch 43/75, loss 0.3438\n",
            "epoch 167/800, batch 44/75, loss 0.3963\n",
            "epoch 167/800, batch 45/75, loss 0.2920\n",
            "epoch 167/800, batch 46/75, loss 0.2669\n",
            "epoch 167/800, batch 47/75, loss 0.3617\n",
            "epoch 167/800, batch 48/75, loss 0.1996\n",
            "epoch 167/800, batch 49/75, loss 0.2804\n",
            "epoch 167/800, batch 50/75, loss 0.2228\n",
            "epoch 167/800, batch 51/75, loss 0.1933\n",
            "epoch 167/800, batch 52/75, loss 0.1202\n",
            "epoch 167/800, batch 53/75, loss 11.7518\n",
            "epoch 167/800, batch 54/75, loss 0.3838\n",
            "epoch 167/800, batch 55/75, loss 0.2886\n",
            "epoch 167/800, batch 56/75, loss 0.3311\n",
            "epoch 167/800, batch 57/75, loss 0.3087\n",
            "epoch 167/800, batch 58/75, loss 0.4043\n",
            "epoch 167/800, batch 59/75, loss 0.4899\n",
            "epoch 167/800, batch 60/75, loss 0.4684\n",
            "epoch 167/800, batch 61/75, loss 0.5727\n",
            "epoch 167/800, batch 62/75, loss 0.6888\n",
            "epoch 167/800, batch 63/75, loss 0.5015\n",
            "epoch 167/800, batch 64/75, loss 0.4437\n",
            "epoch 167/800, batch 65/75, loss 0.4656\n",
            "epoch 167/800, batch 66/75, loss 0.3765\n",
            "epoch 167/800, batch 67/75, loss 0.3620\n",
            "epoch 167/800, batch 68/75, loss 0.3552\n",
            "epoch 167/800, batch 69/75, loss 0.3452\n",
            "epoch 167/800, batch 70/75, loss 0.2567\n",
            "epoch 167/800, batch 71/75, loss 0.2612\n",
            "epoch 167/800, batch 72/75, loss 0.3303\n",
            "epoch 167/800, batch 73/75, loss 0.2293\n",
            "epoch 167/800, batch 74/75, loss 0.2287\n",
            "epoch 167/800, batch 75/75, loss 0.1565\n",
            "epoch 167/800, training roc_auc_score 0.8480\n",
            "EarlyStopping counter: 71 out of 80\n",
            "epoch 167/800, validation roc_auc_score 0.7907, best validation roc_auc_score 0.8157\n",
            "epoch 168/800, batch 1/75, loss 0.9958\n",
            "epoch 168/800, batch 2/75, loss 0.9440\n",
            "epoch 168/800, batch 3/75, loss 0.2332\n",
            "epoch 168/800, batch 4/75, loss 0.4312\n",
            "epoch 168/800, batch 5/75, loss 0.1617\n",
            "epoch 168/800, batch 6/75, loss 0.1266\n",
            "epoch 168/800, batch 7/75, loss 0.1724\n",
            "epoch 168/800, batch 8/75, loss 2.3022\n",
            "epoch 168/800, batch 9/75, loss 0.1203\n",
            "epoch 168/800, batch 10/75, loss 1.1391\n",
            "epoch 168/800, batch 11/75, loss 0.3240\n",
            "epoch 168/800, batch 12/75, loss 0.2733\n",
            "epoch 168/800, batch 13/75, loss 0.2264\n",
            "epoch 168/800, batch 14/75, loss 0.1133\n",
            "epoch 168/800, batch 15/75, loss 2.0743\n",
            "epoch 168/800, batch 16/75, loss 0.1276\n",
            "epoch 168/800, batch 17/75, loss 0.2425\n",
            "epoch 168/800, batch 18/75, loss 0.3118\n",
            "epoch 168/800, batch 19/75, loss 0.2218\n",
            "epoch 168/800, batch 20/75, loss 1.5556\n",
            "epoch 168/800, batch 21/75, loss 0.1083\n",
            "epoch 168/800, batch 22/75, loss 0.1517\n",
            "epoch 168/800, batch 23/75, loss 0.1039\n",
            "epoch 168/800, batch 24/75, loss 0.1258\n",
            "epoch 168/800, batch 25/75, loss 0.0912\n",
            "epoch 168/800, batch 26/75, loss 4.7474\n",
            "epoch 168/800, batch 27/75, loss 0.1424\n",
            "epoch 168/800, batch 28/75, loss 0.3013\n",
            "epoch 168/800, batch 29/75, loss 0.1256\n",
            "epoch 168/800, batch 30/75, loss 0.2826\n",
            "epoch 168/800, batch 31/75, loss 0.2064\n",
            "epoch 168/800, batch 32/75, loss 0.2230\n",
            "epoch 168/800, batch 33/75, loss 0.2037\n",
            "epoch 168/800, batch 34/75, loss 0.3789\n",
            "epoch 168/800, batch 35/75, loss 0.2884\n",
            "epoch 168/800, batch 36/75, loss 5.4399\n",
            "epoch 168/800, batch 37/75, loss 0.6951\n",
            "epoch 168/800, batch 38/75, loss 0.2116\n",
            "epoch 168/800, batch 39/75, loss 0.3852\n",
            "epoch 168/800, batch 40/75, loss 0.2725\n",
            "epoch 168/800, batch 41/75, loss 0.4020\n",
            "epoch 168/800, batch 42/75, loss 0.2901\n",
            "epoch 168/800, batch 43/75, loss 0.3242\n",
            "epoch 168/800, batch 44/75, loss 0.3833\n",
            "epoch 168/800, batch 45/75, loss 0.3061\n",
            "epoch 168/800, batch 46/75, loss 0.3230\n",
            "epoch 168/800, batch 47/75, loss 0.3957\n",
            "epoch 168/800, batch 48/75, loss 0.2389\n",
            "epoch 168/800, batch 49/75, loss 0.2843\n",
            "epoch 168/800, batch 50/75, loss 0.2706\n",
            "epoch 168/800, batch 51/75, loss 0.2312\n",
            "epoch 168/800, batch 52/75, loss 0.1642\n",
            "epoch 168/800, batch 53/75, loss 10.2938\n",
            "epoch 168/800, batch 54/75, loss 0.3474\n",
            "epoch 168/800, batch 55/75, loss 0.3340\n",
            "epoch 168/800, batch 56/75, loss 0.3790\n",
            "epoch 168/800, batch 57/75, loss 0.2885\n",
            "epoch 168/800, batch 58/75, loss 0.4063\n",
            "epoch 168/800, batch 59/75, loss 0.4593\n",
            "epoch 168/800, batch 60/75, loss 0.4665\n",
            "epoch 168/800, batch 61/75, loss 0.5034\n",
            "epoch 168/800, batch 62/75, loss 0.5901\n",
            "epoch 168/800, batch 63/75, loss 0.4831\n",
            "epoch 168/800, batch 64/75, loss 0.4214\n",
            "epoch 168/800, batch 65/75, loss 0.4250\n",
            "epoch 168/800, batch 66/75, loss 0.3562\n",
            "epoch 168/800, batch 67/75, loss 0.3756\n",
            "epoch 168/800, batch 68/75, loss 0.3228\n",
            "epoch 168/800, batch 69/75, loss 0.3369\n",
            "epoch 168/800, batch 70/75, loss 0.2462\n",
            "epoch 168/800, batch 71/75, loss 0.2725\n",
            "epoch 168/800, batch 72/75, loss 0.3529\n",
            "epoch 168/800, batch 73/75, loss 0.2038\n",
            "epoch 168/800, batch 74/75, loss 0.2095\n",
            "epoch 168/800, batch 75/75, loss 0.1548\n",
            "epoch 168/800, training roc_auc_score 0.8551\n",
            "EarlyStopping counter: 72 out of 80\n",
            "epoch 168/800, validation roc_auc_score 0.7818, best validation roc_auc_score 0.8157\n",
            "epoch 169/800, batch 1/75, loss 0.9970\n",
            "epoch 169/800, batch 2/75, loss 0.8554\n",
            "epoch 169/800, batch 3/75, loss 0.2047\n",
            "epoch 169/800, batch 4/75, loss 0.4043\n",
            "epoch 169/800, batch 5/75, loss 0.1508\n",
            "epoch 169/800, batch 6/75, loss 0.0750\n",
            "epoch 169/800, batch 7/75, loss 0.1579\n",
            "epoch 169/800, batch 8/75, loss 2.5434\n",
            "epoch 169/800, batch 9/75, loss 0.1290\n",
            "epoch 169/800, batch 10/75, loss 0.7455\n",
            "epoch 169/800, batch 11/75, loss 0.6002\n",
            "epoch 169/800, batch 12/75, loss 0.2404\n",
            "epoch 169/800, batch 13/75, loss 0.1713\n",
            "epoch 169/800, batch 14/75, loss 0.1307\n",
            "epoch 169/800, batch 15/75, loss 2.2621\n",
            "epoch 169/800, batch 16/75, loss 0.1036\n",
            "epoch 169/800, batch 17/75, loss 0.2601\n",
            "epoch 169/800, batch 18/75, loss 0.4606\n",
            "epoch 169/800, batch 19/75, loss 0.2310\n",
            "epoch 169/800, batch 20/75, loss 1.3465\n",
            "epoch 169/800, batch 21/75, loss 0.1134\n",
            "epoch 169/800, batch 22/75, loss 0.2331\n",
            "epoch 169/800, batch 23/75, loss 0.1144\n",
            "epoch 169/800, batch 24/75, loss 0.1557\n",
            "epoch 169/800, batch 25/75, loss 0.1592\n",
            "epoch 169/800, batch 26/75, loss 3.9881\n",
            "epoch 169/800, batch 27/75, loss 0.2274\n",
            "epoch 169/800, batch 28/75, loss 0.4508\n",
            "epoch 169/800, batch 29/75, loss 0.2198\n",
            "epoch 169/800, batch 30/75, loss 0.4184\n",
            "epoch 169/800, batch 31/75, loss 0.3780\n",
            "epoch 169/800, batch 32/75, loss 0.3909\n",
            "epoch 169/800, batch 33/75, loss 0.3240\n",
            "epoch 169/800, batch 34/75, loss 0.4864\n",
            "epoch 169/800, batch 35/75, loss 0.3105\n",
            "epoch 169/800, batch 36/75, loss 3.9647\n",
            "epoch 169/800, batch 37/75, loss 0.7371\n",
            "epoch 169/800, batch 38/75, loss 0.2239\n",
            "epoch 169/800, batch 39/75, loss 0.4617\n",
            "epoch 169/800, batch 40/75, loss 0.2475\n",
            "epoch 169/800, batch 41/75, loss 0.4568\n",
            "epoch 169/800, batch 42/75, loss 0.3049\n",
            "epoch 169/800, batch 43/75, loss 0.2956\n",
            "epoch 169/800, batch 44/75, loss 0.3393\n",
            "epoch 169/800, batch 45/75, loss 0.2900\n",
            "epoch 169/800, batch 46/75, loss 0.2732\n",
            "epoch 169/800, batch 47/75, loss 0.3201\n",
            "epoch 169/800, batch 48/75, loss 0.1745\n",
            "epoch 169/800, batch 49/75, loss 0.2293\n",
            "epoch 169/800, batch 50/75, loss 0.1892\n",
            "epoch 169/800, batch 51/75, loss 0.1386\n",
            "epoch 169/800, batch 52/75, loss 0.1043\n",
            "epoch 169/800, batch 53/75, loss 15.5913\n",
            "epoch 169/800, batch 54/75, loss 0.2890\n",
            "epoch 169/800, batch 55/75, loss 0.2961\n",
            "epoch 169/800, batch 56/75, loss 0.3513\n",
            "epoch 169/800, batch 57/75, loss 0.2944\n",
            "epoch 169/800, batch 58/75, loss 0.4861\n",
            "epoch 169/800, batch 59/75, loss 0.6038\n",
            "epoch 169/800, batch 60/75, loss 0.6398\n",
            "epoch 169/800, batch 61/75, loss 0.6706\n",
            "epoch 169/800, batch 62/75, loss 0.8483\n",
            "epoch 169/800, batch 63/75, loss 0.7799\n",
            "epoch 169/800, batch 64/75, loss 0.7006\n",
            "epoch 169/800, batch 65/75, loss 0.7562\n",
            "epoch 169/800, batch 66/75, loss 0.5854\n",
            "epoch 169/800, batch 67/75, loss 0.5488\n",
            "epoch 169/800, batch 68/75, loss 0.5178\n",
            "epoch 169/800, batch 69/75, loss 0.5117\n",
            "epoch 169/800, batch 70/75, loss 0.3440\n",
            "epoch 169/800, batch 71/75, loss 0.3662\n",
            "epoch 169/800, batch 72/75, loss 0.4049\n",
            "epoch 169/800, batch 73/75, loss 0.2946\n",
            "epoch 169/800, batch 74/75, loss 0.2302\n",
            "epoch 169/800, batch 75/75, loss 0.1773\n",
            "epoch 169/800, training roc_auc_score 0.8249\n",
            "EarlyStopping counter: 73 out of 80\n",
            "epoch 169/800, validation roc_auc_score 0.8035, best validation roc_auc_score 0.8157\n",
            "epoch 170/800, batch 1/75, loss 0.6982\n",
            "epoch 170/800, batch 2/75, loss 1.0157\n",
            "epoch 170/800, batch 3/75, loss 0.2797\n",
            "epoch 170/800, batch 4/75, loss 0.5235\n",
            "epoch 170/800, batch 5/75, loss 0.1622\n",
            "epoch 170/800, batch 6/75, loss 0.1174\n",
            "epoch 170/800, batch 7/75, loss 0.1596\n",
            "epoch 170/800, batch 8/75, loss 2.0755\n",
            "epoch 170/800, batch 9/75, loss 0.1115\n",
            "epoch 170/800, batch 10/75, loss 0.7062\n",
            "epoch 170/800, batch 11/75, loss 0.4404\n",
            "epoch 170/800, batch 12/75, loss 0.2408\n",
            "epoch 170/800, batch 13/75, loss 0.1835\n",
            "epoch 170/800, batch 14/75, loss 0.1086\n",
            "epoch 170/800, batch 15/75, loss 1.9736\n",
            "epoch 170/800, batch 16/75, loss 0.1253\n",
            "epoch 170/800, batch 17/75, loss 0.2313\n",
            "epoch 170/800, batch 18/75, loss 0.4128\n",
            "epoch 170/800, batch 19/75, loss 0.1941\n",
            "epoch 170/800, batch 20/75, loss 2.8624\n",
            "epoch 170/800, batch 21/75, loss 0.0977\n",
            "epoch 170/800, batch 22/75, loss 0.1603\n",
            "epoch 170/800, batch 23/75, loss 0.0778\n",
            "epoch 170/800, batch 24/75, loss 0.1061\n",
            "epoch 170/800, batch 25/75, loss 0.1339\n",
            "epoch 170/800, batch 26/75, loss 4.9682\n",
            "epoch 170/800, batch 27/75, loss 0.1582\n",
            "epoch 170/800, batch 28/75, loss 0.2980\n",
            "epoch 170/800, batch 29/75, loss 0.1161\n",
            "epoch 170/800, batch 30/75, loss 0.2923\n",
            "epoch 170/800, batch 31/75, loss 0.2961\n",
            "epoch 170/800, batch 32/75, loss 0.2604\n",
            "epoch 170/800, batch 33/75, loss 0.2001\n",
            "epoch 170/800, batch 34/75, loss 0.3588\n",
            "epoch 170/800, batch 35/75, loss 0.2694\n",
            "epoch 170/800, batch 36/75, loss 4.8700\n",
            "epoch 170/800, batch 37/75, loss 1.0694\n",
            "epoch 170/800, batch 38/75, loss 0.1937\n",
            "epoch 170/800, batch 39/75, loss 0.4234\n",
            "epoch 170/800, batch 40/75, loss 0.2520\n",
            "epoch 170/800, batch 41/75, loss 0.4385\n",
            "epoch 170/800, batch 42/75, loss 0.2956\n",
            "epoch 170/800, batch 43/75, loss 0.3534\n",
            "epoch 170/800, batch 44/75, loss 0.4054\n",
            "epoch 170/800, batch 45/75, loss 0.3291\n",
            "epoch 170/800, batch 46/75, loss 0.3195\n",
            "epoch 170/800, batch 47/75, loss 0.3931\n",
            "epoch 170/800, batch 48/75, loss 0.2482\n",
            "epoch 170/800, batch 49/75, loss 0.3440\n",
            "epoch 170/800, batch 50/75, loss 0.2840\n",
            "epoch 170/800, batch 51/75, loss 0.2408\n",
            "epoch 170/800, batch 52/75, loss 0.1521\n",
            "epoch 170/800, batch 53/75, loss 10.5518\n",
            "epoch 170/800, batch 54/75, loss 0.3612\n",
            "epoch 170/800, batch 55/75, loss 0.3109\n",
            "epoch 170/800, batch 56/75, loss 0.3893\n",
            "epoch 170/800, batch 57/75, loss 0.2987\n",
            "epoch 170/800, batch 58/75, loss 0.3949\n",
            "epoch 170/800, batch 59/75, loss 0.5379\n",
            "epoch 170/800, batch 60/75, loss 0.4572\n",
            "epoch 170/800, batch 61/75, loss 0.4703\n",
            "epoch 170/800, batch 62/75, loss 0.6356\n",
            "epoch 170/800, batch 63/75, loss 0.5001\n",
            "epoch 170/800, batch 64/75, loss 0.5070\n",
            "epoch 170/800, batch 65/75, loss 0.4785\n",
            "epoch 170/800, batch 66/75, loss 0.3647\n",
            "epoch 170/800, batch 67/75, loss 0.4087\n",
            "epoch 170/800, batch 68/75, loss 0.3763\n",
            "epoch 170/800, batch 69/75, loss 0.3608\n",
            "epoch 170/800, batch 70/75, loss 0.2596\n",
            "epoch 170/800, batch 71/75, loss 0.2847\n",
            "epoch 170/800, batch 72/75, loss 0.3247\n",
            "epoch 170/800, batch 73/75, loss 0.2058\n",
            "epoch 170/800, batch 74/75, loss 0.2013\n",
            "epoch 170/800, batch 75/75, loss 0.1424\n",
            "epoch 170/800, training roc_auc_score 0.8505\n",
            "EarlyStopping counter: 74 out of 80\n",
            "epoch 170/800, validation roc_auc_score 0.7939, best validation roc_auc_score 0.8157\n",
            "epoch 171/800, batch 1/75, loss 1.4772\n",
            "epoch 171/800, batch 2/75, loss 0.8164\n",
            "epoch 171/800, batch 3/75, loss 0.2629\n",
            "epoch 171/800, batch 4/75, loss 0.3668\n",
            "epoch 171/800, batch 5/75, loss 0.1191\n",
            "epoch 171/800, batch 6/75, loss 0.0633\n",
            "epoch 171/800, batch 7/75, loss 0.1534\n",
            "epoch 171/800, batch 8/75, loss 2.9215\n",
            "epoch 171/800, batch 9/75, loss 0.1601\n",
            "epoch 171/800, batch 10/75, loss 1.2390\n",
            "epoch 171/800, batch 11/75, loss 0.3956\n",
            "epoch 171/800, batch 12/75, loss 0.2429\n",
            "epoch 171/800, batch 13/75, loss 0.1718\n",
            "epoch 171/800, batch 14/75, loss 0.1086\n",
            "epoch 171/800, batch 15/75, loss 2.0067\n",
            "epoch 171/800, batch 16/75, loss 0.1084\n",
            "epoch 171/800, batch 17/75, loss 0.2051\n",
            "epoch 171/800, batch 18/75, loss 0.4068\n",
            "epoch 171/800, batch 19/75, loss 0.1583\n",
            "epoch 171/800, batch 20/75, loss 2.0216\n",
            "epoch 171/800, batch 21/75, loss 0.0709\n",
            "epoch 171/800, batch 22/75, loss 0.1513\n",
            "epoch 171/800, batch 23/75, loss 0.0641\n",
            "epoch 171/800, batch 24/75, loss 0.0826\n",
            "epoch 171/800, batch 25/75, loss 0.0946\n",
            "epoch 171/800, batch 26/75, loss 5.1569\n",
            "epoch 171/800, batch 27/75, loss 0.1354\n",
            "epoch 171/800, batch 28/75, loss 0.2872\n",
            "epoch 171/800, batch 29/75, loss 0.1827\n",
            "epoch 171/800, batch 30/75, loss 0.2641\n",
            "epoch 171/800, batch 31/75, loss 0.2191\n",
            "epoch 171/800, batch 32/75, loss 0.2737\n",
            "epoch 171/800, batch 33/75, loss 0.2235\n",
            "epoch 171/800, batch 34/75, loss 0.4058\n",
            "epoch 171/800, batch 35/75, loss 0.2652\n",
            "epoch 171/800, batch 36/75, loss 4.1477\n",
            "epoch 171/800, batch 37/75, loss 0.6787\n",
            "epoch 171/800, batch 38/75, loss 0.1850\n",
            "epoch 171/800, batch 39/75, loss 0.4579\n",
            "epoch 171/800, batch 40/75, loss 0.2453\n",
            "epoch 171/800, batch 41/75, loss 0.4318\n",
            "epoch 171/800, batch 42/75, loss 0.3008\n",
            "epoch 171/800, batch 43/75, loss 0.3054\n",
            "epoch 171/800, batch 44/75, loss 0.3526\n",
            "epoch 171/800, batch 45/75, loss 0.2926\n",
            "epoch 171/800, batch 46/75, loss 0.2713\n",
            "epoch 171/800, batch 47/75, loss 0.2711\n",
            "epoch 171/800, batch 48/75, loss 0.1518\n",
            "epoch 171/800, batch 49/75, loss 0.2092\n",
            "epoch 171/800, batch 50/75, loss 0.1746\n",
            "epoch 171/800, batch 51/75, loss 0.1106\n",
            "epoch 171/800, batch 52/75, loss 0.0617\n",
            "epoch 171/800, batch 53/75, loss 22.8740\n",
            "epoch 171/800, batch 54/75, loss 0.5831\n",
            "epoch 171/800, batch 55/75, loss 0.2626\n",
            "epoch 171/800, batch 56/75, loss 0.2861\n",
            "epoch 171/800, batch 57/75, loss 0.3749\n",
            "epoch 171/800, batch 58/75, loss 0.5751\n",
            "epoch 171/800, batch 59/75, loss 0.7317\n",
            "epoch 171/800, batch 60/75, loss 0.6811\n",
            "epoch 171/800, batch 61/75, loss 0.8509\n",
            "epoch 171/800, batch 62/75, loss 1.1144\n",
            "epoch 171/800, batch 63/75, loss 0.9488\n",
            "epoch 171/800, batch 64/75, loss 0.8701\n",
            "epoch 171/800, batch 65/75, loss 0.9457\n",
            "epoch 171/800, batch 66/75, loss 0.7528\n",
            "epoch 171/800, batch 67/75, loss 0.8043\n",
            "epoch 171/800, batch 68/75, loss 0.7349\n",
            "epoch 171/800, batch 69/75, loss 0.6310\n",
            "epoch 171/800, batch 70/75, loss 0.4457\n",
            "epoch 171/800, batch 71/75, loss 0.5704\n",
            "epoch 171/800, batch 72/75, loss 0.6169\n",
            "epoch 171/800, batch 73/75, loss 0.4073\n",
            "epoch 171/800, batch 74/75, loss 0.4000\n",
            "epoch 171/800, batch 75/75, loss 0.2638\n",
            "epoch 171/800, training roc_auc_score 0.7545\n",
            "EarlyStopping counter: 75 out of 80\n",
            "epoch 171/800, validation roc_auc_score 0.7822, best validation roc_auc_score 0.8157\n",
            "epoch 172/800, batch 1/75, loss 0.9500\n",
            "epoch 172/800, batch 2/75, loss 0.9211\n",
            "epoch 172/800, batch 3/75, loss 0.3998\n",
            "epoch 172/800, batch 4/75, loss 0.5477\n",
            "epoch 172/800, batch 5/75, loss 0.2241\n",
            "epoch 172/800, batch 6/75, loss 0.1432\n",
            "epoch 172/800, batch 7/75, loss 0.2533\n",
            "epoch 172/800, batch 8/75, loss 2.8534\n",
            "epoch 172/800, batch 9/75, loss 0.1715\n",
            "epoch 172/800, batch 10/75, loss 0.7551\n",
            "epoch 172/800, batch 11/75, loss 0.4456\n",
            "epoch 172/800, batch 12/75, loss 0.2877\n",
            "epoch 172/800, batch 13/75, loss 0.2278\n",
            "epoch 172/800, batch 14/75, loss 0.1081\n",
            "epoch 172/800, batch 15/75, loss 2.3174\n",
            "epoch 172/800, batch 16/75, loss 0.1196\n",
            "epoch 172/800, batch 17/75, loss 0.2378\n",
            "epoch 172/800, batch 18/75, loss 0.3742\n",
            "epoch 172/800, batch 19/75, loss 0.1979\n",
            "epoch 172/800, batch 20/75, loss 1.6649\n",
            "epoch 172/800, batch 21/75, loss 0.0864\n",
            "epoch 172/800, batch 22/75, loss 0.1641\n",
            "epoch 172/800, batch 23/75, loss 0.0673\n",
            "epoch 172/800, batch 24/75, loss 0.1033\n",
            "epoch 172/800, batch 25/75, loss 0.0916\n",
            "epoch 172/800, batch 26/75, loss 5.6461\n",
            "epoch 172/800, batch 27/75, loss 0.1011\n",
            "epoch 172/800, batch 28/75, loss 0.2671\n",
            "epoch 172/800, batch 29/75, loss 0.1312\n",
            "epoch 172/800, batch 30/75, loss 0.2911\n",
            "epoch 172/800, batch 31/75, loss 0.1737\n",
            "epoch 172/800, batch 32/75, loss 0.1739\n",
            "epoch 172/800, batch 33/75, loss 0.1883\n",
            "epoch 172/800, batch 34/75, loss 0.2903\n",
            "epoch 172/800, batch 35/75, loss 0.1903\n",
            "epoch 172/800, batch 36/75, loss 4.7979\n",
            "epoch 172/800, batch 37/75, loss 1.0905\n",
            "epoch 172/800, batch 38/75, loss 0.1826\n",
            "epoch 172/800, batch 39/75, loss 0.3829\n",
            "epoch 172/800, batch 40/75, loss 0.2163\n",
            "epoch 172/800, batch 41/75, loss 0.4468\n",
            "epoch 172/800, batch 42/75, loss 0.2656\n",
            "epoch 172/800, batch 43/75, loss 0.3258\n",
            "epoch 172/800, batch 44/75, loss 0.3254\n",
            "epoch 172/800, batch 45/75, loss 0.2671\n",
            "epoch 172/800, batch 46/75, loss 0.3651\n",
            "epoch 172/800, batch 47/75, loss 0.3589\n",
            "epoch 172/800, batch 48/75, loss 0.2791\n",
            "epoch 172/800, batch 49/75, loss 0.3117\n",
            "epoch 172/800, batch 50/75, loss 0.3151\n",
            "epoch 172/800, batch 51/75, loss 0.2430\n",
            "epoch 172/800, batch 52/75, loss 0.1527\n",
            "epoch 172/800, batch 53/75, loss 12.5256\n",
            "epoch 172/800, batch 54/75, loss 0.3019\n",
            "epoch 172/800, batch 55/75, loss 0.3558\n",
            "epoch 172/800, batch 56/75, loss 0.4520\n",
            "epoch 172/800, batch 57/75, loss 0.3443\n",
            "epoch 172/800, batch 58/75, loss 0.4914\n",
            "epoch 172/800, batch 59/75, loss 0.6045\n",
            "epoch 172/800, batch 60/75, loss 0.4937\n",
            "epoch 172/800, batch 61/75, loss 0.5462\n",
            "epoch 172/800, batch 62/75, loss 0.6908\n",
            "epoch 172/800, batch 63/75, loss 0.5331\n",
            "epoch 172/800, batch 64/75, loss 0.4690\n",
            "epoch 172/800, batch 65/75, loss 0.5606\n",
            "epoch 172/800, batch 66/75, loss 0.3800\n",
            "epoch 172/800, batch 67/75, loss 0.4455\n",
            "epoch 172/800, batch 68/75, loss 0.3756\n",
            "epoch 172/800, batch 69/75, loss 0.4168\n",
            "epoch 172/800, batch 70/75, loss 0.2869\n",
            "epoch 172/800, batch 71/75, loss 0.2746\n",
            "epoch 172/800, batch 72/75, loss 0.4113\n",
            "epoch 172/800, batch 73/75, loss 0.2688\n",
            "epoch 172/800, batch 74/75, loss 0.2309\n",
            "epoch 172/800, batch 75/75, loss 0.1938\n",
            "epoch 172/800, training roc_auc_score 0.8312\n",
            "EarlyStopping counter: 76 out of 80\n",
            "epoch 172/800, validation roc_auc_score 0.8026, best validation roc_auc_score 0.8157\n",
            "epoch 173/800, batch 1/75, loss 0.9556\n",
            "epoch 173/800, batch 2/75, loss 0.8862\n",
            "epoch 173/800, batch 3/75, loss 0.3162\n",
            "epoch 173/800, batch 4/75, loss 0.4702\n",
            "epoch 173/800, batch 5/75, loss 0.2056\n",
            "epoch 173/800, batch 6/75, loss 0.1350\n",
            "epoch 173/800, batch 7/75, loss 0.1539\n",
            "epoch 173/800, batch 8/75, loss 2.2854\n",
            "epoch 173/800, batch 9/75, loss 0.1568\n",
            "epoch 173/800, batch 10/75, loss 0.9165\n",
            "epoch 173/800, batch 11/75, loss 0.3438\n",
            "epoch 173/800, batch 12/75, loss 0.3417\n",
            "epoch 173/800, batch 13/75, loss 0.2109\n",
            "epoch 173/800, batch 14/75, loss 0.1146\n",
            "epoch 173/800, batch 15/75, loss 2.5601\n",
            "epoch 173/800, batch 16/75, loss 0.1114\n",
            "epoch 173/800, batch 17/75, loss 0.2515\n",
            "epoch 173/800, batch 18/75, loss 0.3337\n",
            "epoch 173/800, batch 19/75, loss 0.2168\n",
            "epoch 173/800, batch 20/75, loss 2.1964\n",
            "epoch 173/800, batch 21/75, loss 0.0786\n",
            "epoch 173/800, batch 22/75, loss 0.2280\n",
            "epoch 173/800, batch 23/75, loss 0.0827\n",
            "epoch 173/800, batch 24/75, loss 0.1095\n",
            "epoch 173/800, batch 25/75, loss 0.0982\n",
            "epoch 173/800, batch 26/75, loss 5.3079\n",
            "epoch 173/800, batch 27/75, loss 0.1452\n",
            "epoch 173/800, batch 28/75, loss 0.3279\n",
            "epoch 173/800, batch 29/75, loss 0.1268\n",
            "epoch 173/800, batch 30/75, loss 0.3140\n",
            "epoch 173/800, batch 31/75, loss 0.2235\n",
            "epoch 173/800, batch 32/75, loss 0.2793\n",
            "epoch 173/800, batch 33/75, loss 0.1926\n",
            "epoch 173/800, batch 34/75, loss 0.3860\n",
            "epoch 173/800, batch 35/75, loss 0.3299\n",
            "epoch 173/800, batch 36/75, loss 4.5144\n",
            "epoch 173/800, batch 37/75, loss 0.9526\n",
            "epoch 173/800, batch 38/75, loss 0.2254\n",
            "epoch 173/800, batch 39/75, loss 0.3990\n",
            "epoch 173/800, batch 40/75, loss 0.2479\n",
            "epoch 173/800, batch 41/75, loss 0.4409\n",
            "epoch 173/800, batch 42/75, loss 0.2741\n",
            "epoch 173/800, batch 43/75, loss 0.3259\n",
            "epoch 173/800, batch 44/75, loss 0.3648\n",
            "epoch 173/800, batch 45/75, loss 0.2944\n",
            "epoch 173/800, batch 46/75, loss 0.3033\n",
            "epoch 173/800, batch 47/75, loss 0.3379\n",
            "epoch 173/800, batch 48/75, loss 0.2207\n",
            "epoch 173/800, batch 49/75, loss 0.3109\n",
            "epoch 173/800, batch 50/75, loss 0.2676\n",
            "epoch 173/800, batch 51/75, loss 0.2715\n",
            "epoch 173/800, batch 52/75, loss 0.1427\n",
            "epoch 173/800, batch 53/75, loss 11.3881\n",
            "epoch 173/800, batch 54/75, loss 0.3554\n",
            "epoch 173/800, batch 55/75, loss 0.3529\n",
            "epoch 173/800, batch 56/75, loss 0.3748\n",
            "epoch 173/800, batch 57/75, loss 0.3534\n",
            "epoch 173/800, batch 58/75, loss 0.4361\n",
            "epoch 173/800, batch 59/75, loss 0.4982\n",
            "epoch 173/800, batch 60/75, loss 0.4660\n",
            "epoch 173/800, batch 61/75, loss 0.5183\n",
            "epoch 173/800, batch 62/75, loss 0.6160\n",
            "epoch 173/800, batch 63/75, loss 0.5729\n",
            "epoch 173/800, batch 64/75, loss 0.4897\n",
            "epoch 173/800, batch 65/75, loss 0.5150\n",
            "epoch 173/800, batch 66/75, loss 0.3899\n",
            "epoch 173/800, batch 67/75, loss 0.4031\n",
            "epoch 173/800, batch 68/75, loss 0.3567\n",
            "epoch 173/800, batch 69/75, loss 0.3958\n",
            "epoch 173/800, batch 70/75, loss 0.2806\n",
            "epoch 173/800, batch 71/75, loss 0.2788\n",
            "epoch 173/800, batch 72/75, loss 0.3516\n",
            "epoch 173/800, batch 73/75, loss 0.2415\n",
            "epoch 173/800, batch 74/75, loss 0.2707\n",
            "epoch 173/800, batch 75/75, loss 0.1323\n",
            "epoch 173/800, training roc_auc_score 0.8376\n",
            "EarlyStopping counter: 77 out of 80\n",
            "epoch 173/800, validation roc_auc_score 0.7889, best validation roc_auc_score 0.8157\n",
            "epoch 174/800, batch 1/75, loss 0.8977\n",
            "epoch 174/800, batch 2/75, loss 0.8127\n",
            "epoch 174/800, batch 3/75, loss 0.3242\n",
            "epoch 174/800, batch 4/75, loss 0.4541\n",
            "epoch 174/800, batch 5/75, loss 0.1556\n",
            "epoch 174/800, batch 6/75, loss 0.1009\n",
            "epoch 174/800, batch 7/75, loss 0.2080\n",
            "epoch 174/800, batch 8/75, loss 2.3430\n",
            "epoch 174/800, batch 9/75, loss 0.1376\n",
            "epoch 174/800, batch 10/75, loss 0.7867\n",
            "epoch 174/800, batch 11/75, loss 0.7528\n",
            "epoch 174/800, batch 12/75, loss 0.3055\n",
            "epoch 174/800, batch 13/75, loss 0.1952\n",
            "epoch 174/800, batch 14/75, loss 0.1383\n",
            "epoch 174/800, batch 15/75, loss 2.7402\n",
            "epoch 174/800, batch 16/75, loss 0.1422\n",
            "epoch 174/800, batch 17/75, loss 0.2522\n",
            "epoch 174/800, batch 18/75, loss 0.3749\n",
            "epoch 174/800, batch 19/75, loss 0.2253\n",
            "epoch 174/800, batch 20/75, loss 1.3353\n",
            "epoch 174/800, batch 21/75, loss 0.1105\n",
            "epoch 174/800, batch 22/75, loss 0.1593\n",
            "epoch 174/800, batch 23/75, loss 0.0999\n",
            "epoch 174/800, batch 24/75, loss 0.1463\n",
            "epoch 174/800, batch 25/75, loss 0.1165\n",
            "epoch 174/800, batch 26/75, loss 4.9414\n",
            "epoch 174/800, batch 27/75, loss 0.1678\n",
            "epoch 174/800, batch 28/75, loss 0.2684\n",
            "epoch 174/800, batch 29/75, loss 0.1350\n",
            "epoch 174/800, batch 30/75, loss 0.2651\n",
            "epoch 174/800, batch 31/75, loss 0.2324\n",
            "epoch 174/800, batch 32/75, loss 0.2065\n",
            "epoch 174/800, batch 33/75, loss 0.2035\n",
            "epoch 174/800, batch 34/75, loss 0.2665\n",
            "epoch 174/800, batch 35/75, loss 0.2525\n",
            "epoch 174/800, batch 36/75, loss 4.8093\n",
            "epoch 174/800, batch 37/75, loss 0.9075\n",
            "epoch 174/800, batch 38/75, loss 0.1966\n",
            "epoch 174/800, batch 39/75, loss 0.3484\n",
            "epoch 174/800, batch 40/75, loss 0.2051\n",
            "epoch 174/800, batch 41/75, loss 0.3288\n",
            "epoch 174/800, batch 42/75, loss 0.2729\n",
            "epoch 174/800, batch 43/75, loss 0.2684\n",
            "epoch 174/800, batch 44/75, loss 0.2918\n",
            "epoch 174/800, batch 45/75, loss 0.2236\n",
            "epoch 174/800, batch 46/75, loss 0.2418\n",
            "epoch 174/800, batch 47/75, loss 0.2840\n",
            "epoch 174/800, batch 48/75, loss 0.2262\n",
            "epoch 174/800, batch 49/75, loss 0.3121\n",
            "epoch 174/800, batch 50/75, loss 0.2708\n",
            "epoch 174/800, batch 51/75, loss 0.2059\n",
            "epoch 174/800, batch 52/75, loss 0.1353\n",
            "epoch 174/800, batch 53/75, loss 11.1601\n",
            "epoch 174/800, batch 54/75, loss 0.4796\n",
            "epoch 174/800, batch 55/75, loss 0.3329\n",
            "epoch 174/800, batch 56/75, loss 0.3740\n",
            "epoch 174/800, batch 57/75, loss 0.3479\n",
            "epoch 174/800, batch 58/75, loss 0.4913\n",
            "epoch 174/800, batch 59/75, loss 0.5237\n",
            "epoch 174/800, batch 60/75, loss 0.5033\n",
            "epoch 174/800, batch 61/75, loss 0.5368\n",
            "epoch 174/800, batch 62/75, loss 0.6991\n",
            "epoch 174/800, batch 63/75, loss 0.5645\n",
            "epoch 174/800, batch 64/75, loss 0.5138\n",
            "epoch 174/800, batch 65/75, loss 0.5730\n",
            "epoch 174/800, batch 66/75, loss 0.4488\n",
            "epoch 174/800, batch 67/75, loss 0.4901\n",
            "epoch 174/800, batch 68/75, loss 0.3567\n",
            "epoch 174/800, batch 69/75, loss 0.4573\n",
            "epoch 174/800, batch 70/75, loss 0.3044\n",
            "epoch 174/800, batch 71/75, loss 0.2997\n",
            "epoch 174/800, batch 72/75, loss 0.3398\n",
            "epoch 174/800, batch 73/75, loss 0.2353\n",
            "epoch 174/800, batch 74/75, loss 0.2314\n",
            "epoch 174/800, batch 75/75, loss 0.1686\n",
            "epoch 174/800, training roc_auc_score 0.8378\n",
            "EarlyStopping counter: 78 out of 80\n",
            "epoch 174/800, validation roc_auc_score 0.7606, best validation roc_auc_score 0.8157\n",
            "epoch 175/800, batch 1/75, loss 0.8691\n",
            "epoch 175/800, batch 2/75, loss 0.9310\n",
            "epoch 175/800, batch 3/75, loss 0.3859\n",
            "epoch 175/800, batch 4/75, loss 0.5539\n",
            "epoch 175/800, batch 5/75, loss 0.1667\n",
            "epoch 175/800, batch 6/75, loss 0.0950\n",
            "epoch 175/800, batch 7/75, loss 0.1866\n",
            "epoch 175/800, batch 8/75, loss 2.2056\n",
            "epoch 175/800, batch 9/75, loss 0.1267\n",
            "epoch 175/800, batch 10/75, loss 1.1086\n",
            "epoch 175/800, batch 11/75, loss 0.9940\n",
            "epoch 175/800, batch 12/75, loss 0.2689\n",
            "epoch 175/800, batch 13/75, loss 0.2019\n",
            "epoch 175/800, batch 14/75, loss 0.1484\n",
            "epoch 175/800, batch 15/75, loss 2.6373\n",
            "epoch 175/800, batch 16/75, loss 0.1342\n",
            "epoch 175/800, batch 17/75, loss 0.3178\n",
            "epoch 175/800, batch 18/75, loss 0.4139\n",
            "epoch 175/800, batch 19/75, loss 0.2618\n",
            "epoch 175/800, batch 20/75, loss 1.7220\n",
            "epoch 175/800, batch 21/75, loss 0.1337\n",
            "epoch 175/800, batch 22/75, loss 0.2177\n",
            "epoch 175/800, batch 23/75, loss 0.1284\n",
            "epoch 175/800, batch 24/75, loss 0.2028\n",
            "epoch 175/800, batch 25/75, loss 0.1769\n",
            "epoch 175/800, batch 26/75, loss 3.6924\n",
            "epoch 175/800, batch 27/75, loss 0.1838\n",
            "epoch 175/800, batch 28/75, loss 0.3384\n",
            "epoch 175/800, batch 29/75, loss 0.1638\n",
            "epoch 175/800, batch 30/75, loss 0.3549\n",
            "epoch 175/800, batch 31/75, loss 0.2472\n",
            "epoch 175/800, batch 32/75, loss 0.2378\n",
            "epoch 175/800, batch 33/75, loss 0.2310\n",
            "epoch 175/800, batch 34/75, loss 0.3433\n",
            "epoch 175/800, batch 35/75, loss 0.2790\n",
            "epoch 175/800, batch 36/75, loss 4.1856\n",
            "epoch 175/800, batch 37/75, loss 0.6712\n",
            "epoch 175/800, batch 38/75, loss 0.2102\n",
            "epoch 175/800, batch 39/75, loss 0.3662\n",
            "epoch 175/800, batch 40/75, loss 0.2187\n",
            "epoch 175/800, batch 41/75, loss 0.3909\n",
            "epoch 175/800, batch 42/75, loss 0.2646\n",
            "epoch 175/800, batch 43/75, loss 0.2737\n",
            "epoch 175/800, batch 44/75, loss 0.2878\n",
            "epoch 175/800, batch 45/75, loss 0.2224\n",
            "epoch 175/800, batch 46/75, loss 0.2730\n",
            "epoch 175/800, batch 47/75, loss 0.3143\n",
            "epoch 175/800, batch 48/75, loss 0.2067\n",
            "epoch 175/800, batch 49/75, loss 0.3000\n",
            "epoch 175/800, batch 50/75, loss 0.2274\n",
            "epoch 175/800, batch 51/75, loss 0.2154\n",
            "epoch 175/800, batch 52/75, loss 0.1501\n",
            "epoch 175/800, batch 53/75, loss 10.2839\n",
            "epoch 175/800, batch 54/75, loss 0.2833\n",
            "epoch 175/800, batch 55/75, loss 0.2830\n",
            "epoch 175/800, batch 56/75, loss 0.3727\n",
            "epoch 175/800, batch 57/75, loss 0.3059\n",
            "epoch 175/800, batch 58/75, loss 0.4102\n",
            "epoch 175/800, batch 59/75, loss 0.4662\n",
            "epoch 175/800, batch 60/75, loss 0.3908\n",
            "epoch 175/800, batch 61/75, loss 0.4118\n",
            "epoch 175/800, batch 62/75, loss 0.5724\n",
            "epoch 175/800, batch 63/75, loss 0.4805\n",
            "epoch 175/800, batch 64/75, loss 0.4328\n",
            "epoch 175/800, batch 65/75, loss 0.4927\n",
            "epoch 175/800, batch 66/75, loss 0.3837\n",
            "epoch 175/800, batch 67/75, loss 0.3777\n",
            "epoch 175/800, batch 68/75, loss 0.3488\n",
            "epoch 175/800, batch 69/75, loss 0.3756\n",
            "epoch 175/800, batch 70/75, loss 0.2681\n",
            "epoch 175/800, batch 71/75, loss 0.2780\n",
            "epoch 175/800, batch 72/75, loss 0.3818\n",
            "epoch 175/800, batch 73/75, loss 0.2232\n",
            "epoch 175/800, batch 74/75, loss 0.2567\n",
            "epoch 175/800, batch 75/75, loss 0.1374\n",
            "epoch 175/800, training roc_auc_score 0.8578\n",
            "EarlyStopping counter: 79 out of 80\n",
            "epoch 175/800, validation roc_auc_score 0.7922, best validation roc_auc_score 0.8157\n",
            "epoch 176/800, batch 1/75, loss 0.7333\n",
            "epoch 176/800, batch 2/75, loss 0.9688\n",
            "epoch 176/800, batch 3/75, loss 0.3733\n",
            "epoch 176/800, batch 4/75, loss 0.4804\n",
            "epoch 176/800, batch 5/75, loss 0.1863\n",
            "epoch 176/800, batch 6/75, loss 0.1143\n",
            "epoch 176/800, batch 7/75, loss 0.1895\n",
            "epoch 176/800, batch 8/75, loss 2.0677\n",
            "epoch 176/800, batch 9/75, loss 0.1221\n",
            "epoch 176/800, batch 10/75, loss 0.4553\n",
            "epoch 176/800, batch 11/75, loss 0.3901\n",
            "epoch 176/800, batch 12/75, loss 0.2702\n",
            "epoch 176/800, batch 13/75, loss 0.1594\n",
            "epoch 176/800, batch 14/75, loss 0.1210\n",
            "epoch 176/800, batch 15/75, loss 1.7895\n",
            "epoch 176/800, batch 16/75, loss 0.1029\n",
            "epoch 176/800, batch 17/75, loss 0.2089\n",
            "epoch 176/800, batch 18/75, loss 0.3188\n",
            "epoch 176/800, batch 19/75, loss 0.1931\n",
            "epoch 176/800, batch 20/75, loss 1.4758\n",
            "epoch 176/800, batch 21/75, loss 0.0657\n",
            "epoch 176/800, batch 22/75, loss 0.1695\n",
            "epoch 176/800, batch 23/75, loss 0.0647\n",
            "epoch 176/800, batch 24/75, loss 0.1242\n",
            "epoch 176/800, batch 25/75, loss 0.0816\n",
            "epoch 176/800, batch 26/75, loss 5.0328\n",
            "epoch 176/800, batch 27/75, loss 0.1450\n",
            "epoch 176/800, batch 28/75, loss 0.2769\n",
            "epoch 176/800, batch 29/75, loss 0.1084\n",
            "epoch 176/800, batch 30/75, loss 0.2350\n",
            "epoch 176/800, batch 31/75, loss 0.2441\n",
            "epoch 176/800, batch 32/75, loss 0.2292\n",
            "epoch 176/800, batch 33/75, loss 0.1860\n",
            "epoch 176/800, batch 34/75, loss 0.2953\n",
            "epoch 176/800, batch 35/75, loss 0.2459\n",
            "epoch 176/800, batch 36/75, loss 4.7022\n",
            "epoch 176/800, batch 37/75, loss 0.6838\n",
            "epoch 176/800, batch 38/75, loss 0.1475\n",
            "epoch 176/800, batch 39/75, loss 0.3559\n",
            "epoch 176/800, batch 40/75, loss 0.1942\n",
            "epoch 176/800, batch 41/75, loss 0.3658\n",
            "epoch 176/800, batch 42/75, loss 0.2378\n",
            "epoch 176/800, batch 43/75, loss 0.3178\n",
            "epoch 176/800, batch 44/75, loss 0.2913\n",
            "epoch 176/800, batch 45/75, loss 0.2332\n",
            "epoch 176/800, batch 46/75, loss 0.2563\n",
            "epoch 176/800, batch 47/75, loss 0.2689\n",
            "epoch 176/800, batch 48/75, loss 0.2393\n",
            "epoch 176/800, batch 49/75, loss 0.3490\n",
            "epoch 176/800, batch 50/75, loss 0.2476\n",
            "epoch 176/800, batch 51/75, loss 0.2369\n",
            "epoch 176/800, batch 52/75, loss 0.1512\n",
            "epoch 176/800, batch 53/75, loss 8.4124\n",
            "epoch 176/800, batch 54/75, loss 0.4456\n",
            "epoch 176/800, batch 55/75, loss 0.2977\n",
            "epoch 176/800, batch 56/75, loss 0.3394\n",
            "epoch 176/800, batch 57/75, loss 0.2875\n",
            "epoch 176/800, batch 58/75, loss 0.3342\n",
            "epoch 176/800, batch 59/75, loss 0.3425\n",
            "epoch 176/800, batch 60/75, loss 0.3411\n",
            "epoch 176/800, batch 61/75, loss 0.3572\n",
            "epoch 176/800, batch 62/75, loss 0.5213\n",
            "epoch 176/800, batch 63/75, loss 0.3567\n",
            "epoch 176/800, batch 64/75, loss 0.3624\n",
            "epoch 176/800, batch 65/75, loss 0.3424\n",
            "epoch 176/800, batch 66/75, loss 0.3106\n",
            "epoch 176/800, batch 67/75, loss 0.2756\n",
            "epoch 176/800, batch 68/75, loss 0.2765\n",
            "epoch 176/800, batch 69/75, loss 0.3081\n",
            "epoch 176/800, batch 70/75, loss 0.2328\n",
            "epoch 176/800, batch 71/75, loss 0.2120\n",
            "epoch 176/800, batch 72/75, loss 0.2826\n",
            "epoch 176/800, batch 73/75, loss 0.1891\n",
            "epoch 176/800, batch 74/75, loss 0.2176\n",
            "epoch 176/800, batch 75/75, loss 0.1338\n",
            "epoch 176/800, training roc_auc_score 0.8874\n",
            "EarlyStopping counter: 80 out of 80\n",
            "epoch 176/800, validation roc_auc_score 0.7821, best validation roc_auc_score 0.8157\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YaWpHl2Gnr-",
        "colab_type": "code",
        "outputId": "d7ff6e06-0c59-4975-84e6-c6016627ab6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "stopper.load_checkpoint(model)\n",
        "test_score = run_an_eval_epoch(args, model, test_loader)\n",
        "print('test {} {:.4f}'.format(args['metric_name'], test_score))\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test roc_auc_score 0.6887\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkgrHJs6u4ip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}