{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scaffold_GCN_ncov.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1F3JMNZig9NCd7I9ChQixSo2w-3GR_o58",
      "authorship_tag": "ABX9TyOSKv4Fda09pqFD4ZmIZzw0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seyonechithrananda/ncov-ligand-protein/blob/master/Scaffold_GCN_ncov.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnhZ7HBLDf73",
        "colab_type": "code",
        "outputId": "d5b98eba-95bf-4ee4-c81b-1192f7d72f4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!wget -c https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!conda install -q -y -c conda-forge rdkit\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-06 05:45:21--  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8203, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - _libgcc_mutex==0.1=main\n",
            "    - asn1crypto==1.3.0=py37_0\n",
            "    - ca-certificates==2020.1.1=0\n",
            "    - certifi==2019.11.28=py37_0\n",
            "    - cffi==1.14.0=py37h2e261b9_0\n",
            "    - chardet==3.0.4=py37_1003\n",
            "    - conda-package-handling==1.6.0=py37h7b6447c_0\n",
            "    - conda==4.8.2=py37_0\n",
            "    - cryptography==2.8=py37h1ba5d50_0\n",
            "    - idna==2.8=py37_0\n",
            "    - ld_impl_linux-64==2.33.1=h53a641e_7\n",
            "    - libedit==3.1.20181209=hc058e9b_0\n",
            "    - libffi==3.2.1=hd88cf55_4\n",
            "    - libgcc-ng==9.1.0=hdf63c60_0\n",
            "    - libstdcxx-ng==9.1.0=hdf63c60_0\n",
            "    - ncurses==6.2=he6710b0_0\n",
            "    - openssl==1.1.1d=h7b6447c_4\n",
            "    - pip==20.0.2=py37_1\n",
            "    - pycosat==0.6.3=py37h7b6447c_0\n",
            "    - pycparser==2.19=py37_0\n",
            "    - pyopenssl==19.1.0=py37_0\n",
            "    - pysocks==1.7.1=py37_0\n",
            "    - python==3.7.6=h0371630_2\n",
            "    - readline==7.0=h7b6447c_5\n",
            "    - requests==2.22.0=py37_1\n",
            "    - ruamel_yaml==0.15.87=py37h7b6447c_0\n",
            "    - setuptools==45.2.0=py37_0\n",
            "    - six==1.14.0=py37_0\n",
            "    - sqlite==3.31.1=h7b6447c_0\n",
            "    - tk==8.6.8=hbc83047_0\n",
            "    - tqdm==4.42.1=py_0\n",
            "    - urllib3==1.25.8=py37_0\n",
            "    - wheel==0.34.2=py37_0\n",
            "    - xz==5.2.4=h14c3975_4\n",
            "    - yaml==0.1.7=had09818_2\n",
            "    - zlib==1.2.11=h7b6447c_3\n",
            "\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  ca-certificates    conda-forge::ca-certificates-2020.4.5~ --> pkgs/main::ca-certificates-2020.1.1-0\n",
            "  certifi            conda-forge::certifi-2020.4.5.1-py37h~ --> pkgs/main::certifi-2019.11.28-py37_0\n",
            "  conda              conda-forge::conda-4.8.3-py37hc8dfbb8~ --> pkgs/main::conda-4.8.2-py37_0\n",
            "  openssl            conda-forge::openssl-1.1.1g-h516909a_0 --> pkgs/main::openssl-1.1.1d-h7b6447c_4\n",
            "\n",
            "\n",
            "Preparing transaction: \\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - rdkit\n",
            "\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates     pkgs/main::ca-certificates-2020.1.1-0 --> conda-forge::ca-certificates-2020.4.5.1-hecc5488_0\n",
            "  certifi              pkgs/main::certifi-2019.11.28-py37_0 --> conda-forge::certifi-2020.4.5.1-py37hc8dfbb8_0\n",
            "  conda                       pkgs/main::conda-4.8.2-py37_0 --> conda-forge::conda-4.8.3-py37hc8dfbb8_1\n",
            "  openssl              pkgs/main::openssl-1.1.1d-h7b6447c_4 --> conda-forge::openssl-1.1.1g-h516909a_0\n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxPjaPjsGNGb",
        "colab_type": "code",
        "outputId": "67d4bef6-7db9-4a29-f4a5-1c360227a188",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        }
      },
      "source": [
        "!conda install -c dglteam dgl-cuda10.1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - dgl-cuda10.1\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    blas-1.0                   |         openblas          46 KB\n",
            "    certifi-2020.4.5.1         |           py37_0         155 KB\n",
            "    decorator-4.4.2            |             py_0          14 KB\n",
            "    dgl-cuda10.1-0.4.3post2    |           py37_0        11.2 MB  dglteam\n",
            "    networkx-2.4               |             py_0         1.2 MB\n",
            "    scipy-1.4.1                |   py37habc2bb6_0        14.6 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        27.1 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  blas               pkgs/main/linux-64::blas-1.0-openblas\n",
            "  decorator          pkgs/main/noarch::decorator-4.4.2-py_0\n",
            "  dgl-cuda10.1       dglteam/linux-64::dgl-cuda10.1-0.4.3post2-py37_0\n",
            "  networkx           pkgs/main/noarch::networkx-2.4-py_0\n",
            "  scipy              pkgs/main/linux-64::scipy-1.4.1-py37habc2bb6_0\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  certifi            conda-forge::certifi-2020.4.5.1-py37h~ --> pkgs/main::certifi-2020.4.5.1-py37_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "networkx-2.4         | 1.2 MB    | : 100% 1.0/1 [00:00<00:00,  3.21it/s]\n",
            "blas-1.0             | 46 KB     | : 100% 1.0/1 [00:00<00:00, 22.62it/s]\n",
            "scipy-1.4.1          | 14.6 MB   | : 100% 1.0/1 [00:01<00:00, 27.56s/it]               \n",
            "certifi-2020.4.5.1   | 155 KB    | : 100% 1.0/1 [00:00<00:00, 17.19it/s]\n",
            "decorator-4.4.2      | 14 KB     | : 100% 1.0/1 [00:00<00:00, 27.53it/s]\n",
            "dgl-cuda10.1-0.4.3po | 11.2 MB   | : 100% 1.0/1 [00:06<00:00,  6.62s/it]                \n",
            "Preparing transaction: | \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ba7Nw5eGUTr",
        "colab_type": "code",
        "outputId": "2ec01c37-a73a-4f6f-a5e8-0f7233dbda82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        }
      },
      "source": [
        "!conda install -c dglteam dgllife\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - dgllife\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    dgllife-0.2.1              |           py37_0         132 KB  dglteam\n",
            "    joblib-0.14.1              |             py_0         201 KB\n",
            "    scikit-learn-0.22.1        |   py37h22eb022_0         5.3 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         5.6 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  dgllife            dglteam/linux-64::dgllife-0.2.1-py37_0\n",
            "  joblib             pkgs/main/noarch::joblib-0.14.1-py_0\n",
            "  scikit-learn       pkgs/main/linux-64::scikit-learn-0.22.1-py37h22eb022_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "joblib-0.14.1        | 201 KB    | : 100% 1.0/1 [00:00<00:00,  8.99it/s]\n",
            "dgllife-0.2.1        | 132 KB    | : 100% 1.0/1 [00:00<00:00,  2.94s/it]                \n",
            "scikit-learn-0.22.1  | 5.3 MB    | : 100% 1.0/1 [00:00<00:00, 67.75s/it]               \n",
            "Preparing transaction: | \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqRn8KexGdiL",
        "colab_type": "code",
        "outputId": "ebbf5250-60e3-481e-8052-deb26fb96255",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        }
      },
      "source": [
        "!conda install pandas"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - pandas\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    conda-4.8.3                |           py37_0         2.8 MB\n",
            "    openssl-1.1.1g             |       h7b6447c_0         2.5 MB\n",
            "    pandas-1.0.3               |   py37h0573a6f_0         8.6 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        13.9 MB\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  ca-certificates    conda-forge::ca-certificates-2020.4.5~ --> pkgs/main::ca-certificates-2020.1.1-0\n",
            "  conda              conda-forge::conda-4.8.3-py37hc8dfbb8~ --> pkgs/main::conda-4.8.3-py37_0\n",
            "  openssl            conda-forge::openssl-1.1.1g-h516909a_0 --> pkgs/main::openssl-1.1.1g-h7b6447c_0\n",
            "  pandas             conda-forge::pandas-1.0.3-py37h0da468~ --> pkgs/main::pandas-1.0.3-py37h0573a6f_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "conda-4.8.3          | 2.8 MB    | : 100% 1.0/1 [00:00<00:00,  5.20it/s]               \n",
            "pandas-1.0.3         | 8.6 MB    | : 100% 1.0/1 [00:00<00:00,  2.27it/s]\n",
            "openssl-1.1.1g       | 2.5 MB    | : 100% 1.0/1 [00:00<00:00,  8.41it/s]\n",
            "Preparing transaction: \\ \b\bdone\n",
            "Verifying transaction: / \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\bdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVKD7kwtHDUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "import sys \n",
        "import pandas as pd\n",
        "\n",
        "# train --> balanced dataset\n",
        "dataset_train_file = \"/content/drive/My Drive/Project De Novo/AID1706_binarized_sars_full_eval_actives_12k_samples.csv\"\n",
        "dataset_eval_file = \"/content/drive/My Drive/Project De Novo/mpro_xchem.csv\"\n",
        "dataset_train = pd.read_csv(dataset_train_file)\n",
        "dataset_eval = pd.read_csv(dataset_eval_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy7_zDz9LCq2",
        "colab_type": "code",
        "outputId": "eb9bf661-4a09-4be5-d1ff-cc21e45c6613",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "dataset_train.head"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                                   smiles  activity\n",
              "0      C1CC(C1)C(=O)NC2=CC=C(C=C2)N(C(C3=CC(=CC=C3)F)...         1\n",
              "1      CC(C)(C)C1=CC=C(C=C1)N(C(C2=CN=NC=C2)C(=O)NC(C...         1\n",
              "2      CC(C)(C)NC(=O)C(C1=CSC=C1)N(C2=CC=C(C=C2)N)C(=...         1\n",
              "3      CC(C)C(=O)NC1=CC=C(C=C1)N(C(C2=CSC=C2)C(=O)NC(...         1\n",
              "4      CC(C)C(=O)NC1=CC=C(C=C1)N(CC2=CSC=C2)C(=O)CN3C...         1\n",
              "...                                                  ...       ...\n",
              "11994                               C1=CC2=C(C=C1N)NN=C2         0\n",
              "11995  CC(=O)[C@H]1CC[C@@H]2[C@@]1(CC(=O)[C@H]3[C@H]2...         0\n",
              "11996                       C1CN(CCN1CC(CO)O)C2=CC=CC=C2         0\n",
              "11997  CCOC(=O)N1CCC(=C2C3=C(CCC4=C2N=CC=C4)C=C(C=C3)...         0\n",
              "11998                    C1=CC2=C(C=C1OC(F)(F)F)SC(=N2)N         0\n",
              "\n",
              "[11999 rows x 2 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBnPl4isM1FQ",
        "colab_type": "code",
        "outputId": "f6342566-5f1b-44cc-da12-5d84361812fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "dataset_eval.head"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                 smiles  activity\n",
              "0      OC=1C=CC=CC1CNC2=NC=3C=CC=CC3N2         1\n",
              "1        CC(=O)NCCC1=CNC=2C=CC(F)=CC12         1\n",
              "2    O=C([C@@H]1[C@H](C2=CSC=C2)CCC1)N         1\n",
              "3       CN1CCCC=2C=CC(=CC12)S(=O)(=O)N         1\n",
              "4     CC(=O)NC=1C=CC(OC=2N=CC=CN2)=CC1         1\n",
              "..                                 ...       ...\n",
              "875   CC(C)C=1C=CC(NC(=O)N2CCOCC2)=CC1         0\n",
              "876        CN(CC(=O)O)C(=O)C=1C=CC=CN1         0\n",
              "877  CN1CCN(CC1)C(=O)C=2C=CC(F)=C(F)C2         0\n",
              "878      FC=1C=CC=C(F)C1C(=O)N2CCCCCC2         0\n",
              "879             FC=1C=CC=NC1NCC2CCOCC2         0\n",
              "\n",
              "[880 rows x 2 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVrUoqtTHGy8",
        "colab_type": "code",
        "outputId": "703868e9-40aa-4e78-8520-8c7745583257",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "from dgllife.data import MoleculeCSVDataset\n",
        "from dgllife.data.csv_dataset import *\n",
        "from dgllife.utils.featurizers import *\n",
        "from dgllife.utils.mol_to_graph import *\n",
        "\n",
        "# featurize bigraph/molecular graph set for train (SARS-COV-1) set\n",
        "train_set = MoleculeCSVDataset(dataset_train, smiles_to_graph=smiles_to_bigraph, node_featurizer=CanonicalAtomFeaturizer(),\n",
        "                               edge_featurizer=CanonicalBondFeaturizer(), smiles_column='smiles', cache_file_path='/content/drive/My Drive/Project De Novo/graph_featurizer/train.bin', task_names=['activity'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Using backend: pytorch\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "Loading previously saved dgl graphs...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofwpNYUXJASi",
        "colab_type": "code",
        "outputId": "0c647002-e5e3-4779-824c-c763e08cb1e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# featurize bigraph/molecular graph set for test (SARS-COV-2) set\n",
        "test_set = MoleculeCSVDataset(dataset_eval, smiles_to_graph=smiles_to_bigraph, node_featurizer=CanonicalAtomFeaturizer(),\n",
        "                               edge_featurizer=CanonicalBondFeaturizer(), smiles_column='smiles', cache_file_path='/content/drive/My Drive/Project De Novo/graph_featurizer/test.bin', task_names=['activity'])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading previously saved dgl graphs...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1TtgCu8U26K",
        "colab_type": "code",
        "outputId": "54ecc3e3-dfc4-4758-d02a-edd9902049c8",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f29476c3-af99-4ec0-8687-156010863e9d\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-f29476c3-af99-4ec0-8687-156010863e9d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving utils.py to utils.py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'utils.py': b'import dgl\\nimport numpy as np\\nimport random\\nimport torch\\n\\nfrom dgllife.utils.featurizers import one_hot_encoding\\nfrom dgllife.utils.splitters import RandomSplitter\\n\\ndef set_random_seed(seed=0):\\n    \"\"\"Set random seed.\\n    Parameters\\n    ----------\\n    seed : int\\n        Random seed to use\\n    \"\"\"\\n    random.seed(seed)\\n    np.random.seed(seed)\\n    torch.manual_seed(seed)\\n    if torch.cuda.is_available():\\n        torch.cuda.manual_seed(seed)\\n\\n\\ndef load_dataset_for_classification(args):\\n    \"\"\"Load dataset for classification tasks.\\n    Parameters\\n    ----------\\n    args : dict\\n        Configurations.\\n    Returns\\n    -------\\n    dataset\\n        The whole dataset.\\n    train_set\\n        Subset for training.\\n    val_set\\n        Subset for validation.\\n    test_set\\n        Subset for test.\\n    \"\"\"\\n    assert args[\\'dataset\\'] in [\\'Tox21\\']\\n    if args[\\'dataset\\'] == \\'Tox21\\':\\n        from dgllife.data import Tox21\\n        dataset = Tox21(smiles_to_graph=args[\\'smiles_to_graph\\'],\\n                        node_featurizer=args.get(\\'node_featurizer\\', None),\\n                        edge_featurizer=args.get(\\'edge_featurizer\\', None))\\n        train_set, val_set, test_set = RandomSplitter.train_val_test_split(\\n            dataset, frac_train=args[\\'frac_train\\'], frac_val=args[\\'frac_val\\'],\\n            frac_test=args[\\'frac_test\\'], random_state=args[\\'random_seed\\'])\\n\\n    return dataset, train_set, val_set, test_set\\n\\n\\ndef load_dataset_for_regression(args):\\n    \"\"\"Load dataset for regression tasks.\\n    Parameters\\n    ----------\\n    args : dict\\n        Configurations.\\n    Returns\\n    -------\\n    train_set\\n        Subset for training.\\n    val_set\\n        Subset for validation.\\n    test_set\\n        Subset for test.\\n    \"\"\"\\n    assert args[\\'dataset\\'] in [\\'Alchemy\\', \\'Aromaticity\\']\\n\\n    if args[\\'dataset\\'] == \\'Alchemy\\':\\n        from dgllife.data import TencentAlchemyDataset\\n        train_set = TencentAlchemyDataset(mode=\\'dev\\')\\n        val_set = TencentAlchemyDataset(mode=\\'valid\\')\\n        test_set = None\\n\\n    if args[\\'dataset\\'] == \\'Aromaticity\\':\\n        from dgllife.data import PubChemBioAssayAromaticity\\n        dataset = PubChemBioAssayAromaticity(smiles_to_graph=args[\\'smiles_to_graph\\'],\\n                                             node_featurizer=args.get(\\'node_featurizer\\', None),\\n                                             edge_featurizer=args.get(\\'edge_featurizer\\', None))\\n        train_set, val_set, test_set = RandomSplitter.train_val_test_split(\\n            dataset, frac_train=args[\\'frac_train\\'], frac_val=args[\\'frac_val\\'],\\n            frac_test=args[\\'frac_test\\'], random_state=args[\\'random_seed\\'])\\n\\n    return train_set, val_set, test_set\\n\\n\\ndef collate_molgraphs(data):\\n    \"\"\"Batching a list of datapoints for dataloader.\\n    Parameters\\n    ----------\\n    data : list of 3-tuples or 4-tuples.\\n        Each tuple is for a single datapoint, consisting of\\n        a SMILES, a DGLGraph, all-task labels and optionally\\n        a binary mask indicating the existence of labels.\\n    Returns\\n    -------\\n    smiles : list\\n        List of smiles\\n    bg : DGLGraph\\n        The batched DGLGraph.\\n    labels : Tensor of dtype float32 and shape (B, T)\\n        Batched datapoint labels. B is len(data) and\\n        T is the number of total tasks.\\n    masks : Tensor of dtype float32 and shape (B, T)\\n        Batched datapoint binary mask, indicating the\\n        existence of labels. If binary masks are not\\n        provided, return a tensor with ones.\\n    \"\"\"\\n    assert len(data[0]) in [3, 4], \\\\\\n        \\'Expect the tuple to be of length 3 or 4, got {:d}\\'.format(len(data[0]))\\n    if len(data[0]) == 3:\\n        smiles, graphs, labels = map(list, zip(*data))\\n        masks = None\\n    else:\\n        smiles, graphs, labels, masks = map(list, zip(*data))\\n\\n    bg = dgl.batch(graphs)\\n    bg.set_n_initializer(dgl.init.zero_initializer)\\n    bg.set_e_initializer(dgl.init.zero_initializer)\\n    labels = torch.stack(labels, dim=0)\\n\\n    if masks is None:\\n        masks = torch.ones(labels.shape)\\n    else:\\n        masks = torch.stack(masks, dim=0)\\n    return smiles, bg, labels, masks\\n\\n\\ndef load_model(args):\\n    if args[\\'model\\'] == \\'GCN\\':\\n        from dgllife.model import GCNPredictor\\n        model = GCNPredictor(in_feats=args[\\'node_featurizer\\'].feat_size(),\\n                             hidden_feats=args[\\'gcn_hidden_feats\\'],\\n                             classifier_hidden_feats=args[\\'classifier_hidden_feats\\'],\\n                             n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'GAT\\':\\n        from dgllife.model import GATPredictor\\n        model = GATPredictor(in_feats=args[\\'node_featurizer\\'].feat_size(),\\n                             hidden_feats=args[\\'gat_hidden_feats\\'],\\n                             num_heads=args[\\'num_heads\\'],\\n                             classifier_hidden_feats=args[\\'classifier_hidden_feats\\'],\\n                             n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'Weave\\':\\n        from dgllife.model import WeavePredictor\\n        model = WeavePredictor(node_in_feats=args[\\'node_featurizer\\'].feat_size(),\\n                               edge_in_feats=args[\\'edge_featurizer\\'].feat_size(),\\n                               num_gnn_layers=args[\\'num_gnn_layers\\'],\\n                               gnn_hidden_feats=args[\\'gnn_hidden_feats\\'],\\n                               graph_feats=args[\\'graph_feats\\'],\\n                               n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'AttentiveFP\\':\\n        from dgllife.model import AttentiveFPPredictor\\n        model = AttentiveFPPredictor(node_feat_size=args[\\'node_featurizer\\'].feat_size(),\\n                                     edge_feat_size=args[\\'edge_featurizer\\'].feat_size(),\\n                                     num_layers=args[\\'num_layers\\'],\\n                                     num_timesteps=args[\\'num_timesteps\\'],\\n                                     graph_feat_size=args[\\'graph_feat_size\\'],\\n                                     n_tasks=args[\\'n_tasks\\'],\\n                                     dropout=args[\\'dropout\\'])\\n\\n    if args[\\'model\\'] == \\'SchNet\\':\\n        from dgllife.model import SchNetPredictor\\n        model = SchNetPredictor(node_feats=args[\\'node_feats\\'],\\n                                hidden_feats=args[\\'hidden_feats\\'],\\n                                classifier_hidden_feats=args[\\'classifier_hidden_feats\\'],\\n                                n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'MGCN\\':\\n        from dgllife.model import MGCNPredictor\\n        model = MGCNPredictor(feats=args[\\'feats\\'],\\n                              n_layers=args[\\'n_layers\\'],\\n                              classifier_hidden_feats=args[\\'classifier_hidden_feats\\'],\\n                              n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'MPNN\\':\\n        from dgllife.model import MPNNPredictor\\n        model = MPNNPredictor(node_in_feats=args[\\'node_in_feats\\'],\\n                              edge_in_feats=args[\\'edge_in_feats\\'],\\n                              node_out_feats=args[\\'node_out_feats\\'],\\n                              edge_hidden_feats=args[\\'edge_hidden_feats\\'],\\n                              n_tasks=args[\\'n_tasks\\'])\\n\\n    return model\\n\\n\\ndef chirality(atom):\\n    try:\\n        return one_hot_encoding(atom.GetProp(\\'_CIPCode\\'), [\\'R\\', \\'S\\']) + \\\\\\n               [atom.HasProp(\\'_ChiralityPossible\\')]\\n    except:\\n        return [False, False] + [atom.HasProp(\\'_ChiralityPossible\\')]\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKMgchzflhiW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = {\n",
        "    'random_seed': 2,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-3,\n",
        "    'num_epochs': 100,\n",
        "    'node_data_field': 'h',\n",
        "    'frac_train': 0.8,\n",
        "    'frac_val': 0.1,\n",
        "    'frac_test': 0.1,\n",
        "    'in_feats': 74,\n",
        "    'gcn_hidden_feats': [64, 64],\n",
        "    'classifier_hidden_feats': 64,\n",
        "    'patience': 10,\n",
        "    'smiles_to_graph': smiles_to_bigraph,\n",
        "    'node_featurizer': CanonicalAtomFeaturizer(),\n",
        "    'metric_name': 'roc_auc_score'\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuZkFAz-PDvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import utils\n",
        "\n",
        "from dgllife.model import load_pretrained\n",
        "from dgllife.utils import EarlyStopping, Meter\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "from utils import set_random_seed, load_dataset_for_classification, collate_molgraphs, load_model\n",
        "\n",
        "args['device'] = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "set_random_seed(args['random_seed'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJf2RePJHUcx",
        "colab_type": "code",
        "outputId": "68273e2b-f478-4305-8586-36af84a537ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        }
      },
      "source": [
        "from dgllife.utils.splitters import ScaffoldSplitter\n",
        "\n",
        "train_scaffold_set, val_set, test_scaffold_set = ScaffoldSplitter.train_val_test_split(train_set, frac_train=0.8, frac_val=0.2,frac_test=0.0)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start initializing RDKit molecule instances...\n",
            "Creating RDKit molecule instance 1000/11999\n",
            "Creating RDKit molecule instance 2000/11999\n",
            "Creating RDKit molecule instance 3000/11999\n",
            "Creating RDKit molecule instance 4000/11999\n",
            "Creating RDKit molecule instance 5000/11999\n",
            "Creating RDKit molecule instance 6000/11999\n",
            "Creating RDKit molecule instance 7000/11999\n",
            "Creating RDKit molecule instance 8000/11999\n",
            "Creating RDKit molecule instance 9000/11999\n",
            "Creating RDKit molecule instance 10000/11999\n",
            "Creating RDKit molecule instance 11000/11999\n",
            "Start computing Bemis-Murcko scaffolds.\n",
            "Computing Bemis-Murcko for compound 1000/11999\n",
            "Computing Bemis-Murcko for compound 2000/11999\n",
            "Computing Bemis-Murcko for compound 3000/11999\n",
            "Computing Bemis-Murcko for compound 4000/11999\n",
            "Computing Bemis-Murcko for compound 5000/11999\n",
            "Computing Bemis-Murcko for compound 6000/11999\n",
            "Computing Bemis-Murcko for compound 7000/11999\n",
            "Computing Bemis-Murcko for compound 8000/11999\n",
            "Computing Bemis-Murcko for compound 9000/11999\n",
            "Computing Bemis-Murcko for compound 10000/11999\n",
            "Computing Bemis-Murcko for compound 11000/11999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCmuQegvJMcq",
        "colab_type": "code",
        "outputId": "4c886366-4e74-49d8-c7f0-d147c3e565e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "print (len(train_set))\n",
        "print(len(train_scaffold_set))\n",
        "print (len(val_set))\n",
        "print (len(test_set))\n",
        "print(train_set[1])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11999\n",
            "9599\n",
            "2400\n",
            "880\n",
            "('CC(C)(C)C1=CC=C(C=C1)N(C(C2=CN=NC=C2)C(=O)NC(C)(C)C)C(=O)C3=CC=CO3', DGLGraph(num_nodes=32, num_edges=68,\n",
            "         ndata_schemes={'h': Scheme(shape=(74,), dtype=torch.float32)}\n",
            "         edata_schemes={'e': Scheme(shape=(12,), dtype=torch.float32)}), tensor([1.]), tensor([1.]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YjAjoUyLEjT",
        "colab_type": "code",
        "outputId": "3039a16f-51d9-4236-971e-a9cd64963c6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(type(train_set))\n",
        "print(type(train_scaffold_set))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'dgllife.data.csv_dataset.MoleculeCSVDataset'>\n",
            "<class 'dgl.data.utils.Subset'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYfHiYl0T8V4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_scaffold_set,  batch_size=args['batch_size'],\n",
        "                              collate_fn=collate_molgraphs)\n",
        "val_loader = DataLoader(val_set,  batch_size=args['batch_size'],\n",
        "                              collate_fn=collate_molgraphs)\n",
        "\n",
        "test_loader = DataLoader(test_set,  batch_size=args['batch_size'],\n",
        "                          collate_fn=collate_molgraphs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzUMvJO8Qx85",
        "colab_type": "code",
        "outputId": "2c9840e0-4d7e-477b-f372-48e1866b9150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(len(test_loader))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGAUIdpRUAUu",
        "colab_type": "code",
        "outputId": "5e8cb77f-73a6-40c9-fb74-550f8c370a3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "source": [
        "args['n_tasks'] = 1\n",
        "\n",
        "from dgllife.model import GCNPredictor\n",
        "\n",
        "model = GCNPredictor(in_feats=args['node_featurizer'].feat_size('h'),\n",
        "                      hidden_feats=args['gcn_hidden_feats'],\n",
        "                      classifier_hidden_feats=args['classifier_hidden_feats'],\n",
        "                      n_tasks=args['n_tasks'])\n",
        "\n",
        "'''\n",
        "model = GATPredictor(in_feats=args['node_featurizer'].feat_size('h'),\n",
        "                             hidden_feats=args['gat_hidden_feats'],\n",
        "                             num_heads=args['num_heads'],\n",
        "                             classifier_hidden_feats=args['classifier_hidden_feats'],\n",
        "                             n_tasks=args['n_tasks'])\n",
        "'''\n",
        "\n",
        "import dgl.backend as F\n",
        "\n",
        "train_num_pos = F.sum(train_set.labels, dim=0)\n",
        "train_num_indices = F.sum(train_set.mask, dim=0)\n",
        "train_task_pos_weights = (train_num_indices - train_num_pos) / train_num_pos\n",
        "\n",
        "loss_criterion = BCEWithLogitsLoss(pos_weight=train_task_pos_weights.to(args['device']),\n",
        "                                    reduction='none')\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=args['lr'])\n",
        "stopper = EarlyStopping(patience=args['patience'], mode='higher', filename='/content/drive/My Drive/Project De Novo/GCN/train.pth')\n",
        "model.to(args['device'])\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GCNPredictor(\n",
              "  (gnn): GCN(\n",
              "    (gnn_layers): ModuleList(\n",
              "      (0): GCNLayer(\n",
              "        (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x7f8b01b72e18>)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
              "        (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): GCNLayer(\n",
              "        (graph_conv): GraphConv(in=64, out=64, normalization=none, activation=<function relu at 0x7f8b01b72e18>)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (res_connection): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (readout): WeightedSumAndMax(\n",
              "    (weight_and_sum): WeightAndSum(\n",
              "      (atom_weighting): Sequential(\n",
              "        (0): Linear(in_features=64, out_features=1, bias=True)\n",
              "        (1): Sigmoid()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (predict): MLPPredictor(\n",
              "    (predict): Sequential(\n",
              "      (0): Dropout(p=0.0, inplace=False)\n",
              "      (1): Linear(in_features=128, out_features=64, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (4): Linear(in_features=64, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7Fdo_NxLPWz",
        "colab_type": "code",
        "outputId": "3f1983c8-2e40-483f-9c9a-6b71f407bca2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(train_task_pos_weights)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([25.9036])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C941OCzRuXH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def predict(args, model, bg):\n",
        "    node_feats = bg.ndata.pop(args['node_data_field']).to(args['device'])\n",
        "    if args.get('edge_featurizer', None) is not None:\n",
        "        edge_feats = bg.edata.pop(args['edge_data_field']).to(args['device'])\n",
        "        return model(bg, node_feats, edge_feats)\n",
        "    else:\n",
        "        return model(bg, node_feats)\n",
        "\n",
        "def run_a_train_epoch(args, epoch, model, data_loader, loss_criterion, optimizer):\n",
        "    model.train()\n",
        "    train_meter = Meter()\n",
        "    for batch_id, batch_data in enumerate(data_loader):\n",
        "        smiles, bg, labels, masks = batch_data\n",
        "        labels, masks = labels.to(args['device']), masks.to(args['device'])\n",
        "        logits = predict(args, model, bg)\n",
        "        # Mask non-existing labels\n",
        "        loss = (loss_criterion(logits, labels) * (masks != 0).float()).mean()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print('epoch {:d}/{:d}, batch {:d}/{:d}, loss {:.4f}'.format(\n",
        "            epoch + 1, args['num_epochs'], batch_id + 1, len(data_loader), loss.item()))\n",
        "        train_meter.update(logits, labels, masks)\n",
        "    train_score = np.mean(train_meter.compute_metric(args['metric_name']))\n",
        "    print('epoch {:d}/{:d}, training {} {:.4f}'.format(\n",
        "        epoch + 1, args['num_epochs'], args['metric_name'], train_score))\n",
        "\n",
        "def run_an_eval_epoch(args, model, data_loader):\n",
        "    model.eval()\n",
        "    eval_meter = Meter()\n",
        "    with torch.no_grad():\n",
        "        for batch_id, batch_data in enumerate(data_loader):\n",
        "            smiles, bg, labels, masks = batch_data\n",
        "            labels = labels.to(args['device'])\n",
        "            logits = predict(args, model, bg)\n",
        "            eval_meter.update(logits, labels, masks)\n",
        "    return np.mean(eval_meter.compute_metric(args['metric_name']))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73ZBLfQxuHQA",
        "colab_type": "code",
        "outputId": "46527390-26f6-4ea6-b08e-5ea77f17fcc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(args['num_epochs']):\n",
        "        # Train\n",
        "        run_a_train_epoch(args, epoch, model, train_loader, loss_criterion, optimizer)\n",
        "\n",
        "        # Validation and early stop\n",
        "        val_score = run_an_eval_epoch(args, model, val_loader)\n",
        "        early_stop = stopper.step(val_score, model)\n",
        "        print('epoch {:d}/{:d}, validation {} {:.4f}, best validation {} {:.4f}'.format(\n",
        "            epoch + 1, args['num_epochs'], args['metric_name'],\n",
        "            val_score, args['metric_name'], stopper.best_score))\n",
        "        if early_stop:\n",
        "            break"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1/100, batch 1/75, loss 1.3934\n",
            "epoch 1/100, batch 2/75, loss 1.1992\n",
            "epoch 1/100, batch 3/75, loss 0.8496\n",
            "epoch 1/100, batch 4/75, loss 0.7904\n",
            "epoch 1/100, batch 5/75, loss 0.6865\n",
            "epoch 1/100, batch 6/75, loss 0.6768\n",
            "epoch 1/100, batch 7/75, loss 0.6787\n",
            "epoch 1/100, batch 8/75, loss 1.4130\n",
            "epoch 1/100, batch 9/75, loss 0.6644\n",
            "epoch 1/100, batch 10/75, loss 2.5801\n",
            "epoch 1/100, batch 11/75, loss 1.0198\n",
            "epoch 1/100, batch 12/75, loss 0.6735\n",
            "epoch 1/100, batch 13/75, loss 0.7357\n",
            "epoch 1/100, batch 14/75, loss 0.6747\n",
            "epoch 1/100, batch 15/75, loss 2.6378\n",
            "epoch 1/100, batch 16/75, loss 0.6935\n",
            "epoch 1/100, batch 17/75, loss 0.6830\n",
            "epoch 1/100, batch 18/75, loss 0.7093\n",
            "epoch 1/100, batch 19/75, loss 0.7209\n",
            "epoch 1/100, batch 20/75, loss 2.4683\n",
            "epoch 1/100, batch 21/75, loss 0.7085\n",
            "epoch 1/100, batch 22/75, loss 0.6700\n",
            "epoch 1/100, batch 23/75, loss 0.6737\n",
            "epoch 1/100, batch 24/75, loss 0.6891\n",
            "epoch 1/100, batch 25/75, loss 0.6929\n",
            "epoch 1/100, batch 26/75, loss 3.4862\n",
            "epoch 1/100, batch 27/75, loss 0.6564\n",
            "epoch 1/100, batch 28/75, loss 0.6802\n",
            "epoch 1/100, batch 29/75, loss 0.6718\n",
            "epoch 1/100, batch 30/75, loss 0.6686\n",
            "epoch 1/100, batch 31/75, loss 0.6789\n",
            "epoch 1/100, batch 32/75, loss 0.6700\n",
            "epoch 1/100, batch 33/75, loss 0.6684\n",
            "epoch 1/100, batch 34/75, loss 0.6772\n",
            "epoch 1/100, batch 35/75, loss 0.6732\n",
            "epoch 1/100, batch 36/75, loss 3.8011\n",
            "epoch 1/100, batch 37/75, loss 0.8946\n",
            "epoch 1/100, batch 38/75, loss 0.6571\n",
            "epoch 1/100, batch 39/75, loss 0.6667\n",
            "epoch 1/100, batch 40/75, loss 0.6676\n",
            "epoch 1/100, batch 41/75, loss 0.6681\n",
            "epoch 1/100, batch 42/75, loss 0.6566\n",
            "epoch 1/100, batch 43/75, loss 0.6661\n",
            "epoch 1/100, batch 44/75, loss 0.6587\n",
            "epoch 1/100, batch 45/75, loss 0.6583\n",
            "epoch 1/100, batch 46/75, loss 0.6661\n",
            "epoch 1/100, batch 47/75, loss 0.6640\n",
            "epoch 1/100, batch 48/75, loss 0.6553\n",
            "epoch 1/100, batch 49/75, loss 0.6557\n",
            "epoch 1/100, batch 50/75, loss 0.6479\n",
            "epoch 1/100, batch 51/75, loss 0.6454\n",
            "epoch 1/100, batch 52/75, loss 0.6382\n",
            "epoch 1/100, batch 53/75, loss 10.7531\n",
            "epoch 1/100, batch 54/75, loss 1.0282\n",
            "epoch 1/100, batch 55/75, loss 0.6463\n",
            "epoch 1/100, batch 56/75, loss 0.6510\n",
            "epoch 1/100, batch 57/75, loss 0.6630\n",
            "epoch 1/100, batch 58/75, loss 0.6677\n",
            "epoch 1/100, batch 59/75, loss 0.6682\n",
            "epoch 1/100, batch 60/75, loss 0.6693\n",
            "epoch 1/100, batch 61/75, loss 0.6720\n",
            "epoch 1/100, batch 62/75, loss 0.6697\n",
            "epoch 1/100, batch 63/75, loss 0.6758\n",
            "epoch 1/100, batch 64/75, loss 0.6825\n",
            "epoch 1/100, batch 65/75, loss 0.6709\n",
            "epoch 1/100, batch 66/75, loss 0.6694\n",
            "epoch 1/100, batch 67/75, loss 0.6698\n",
            "epoch 1/100, batch 68/75, loss 0.6638\n",
            "epoch 1/100, batch 69/75, loss 0.6599\n",
            "epoch 1/100, batch 70/75, loss 0.6518\n",
            "epoch 1/100, batch 71/75, loss 0.6474\n",
            "epoch 1/100, batch 72/75, loss 0.6503\n",
            "epoch 1/100, batch 73/75, loss 0.6455\n",
            "epoch 1/100, batch 74/75, loss 0.6367\n",
            "epoch 1/100, batch 75/75, loss 0.6368\n",
            "epoch 1/100, training roc_auc_score 0.5952\n",
            "epoch 1/100, validation roc_auc_score 0.7942, best validation roc_auc_score 0.7942\n",
            "epoch 2/100, batch 1/75, loss 0.9757\n",
            "epoch 2/100, batch 2/75, loss 0.8991\n",
            "epoch 2/100, batch 3/75, loss 0.6986\n",
            "epoch 2/100, batch 4/75, loss 0.6964\n",
            "epoch 2/100, batch 5/75, loss 0.6350\n",
            "epoch 2/100, batch 6/75, loss 0.6176\n",
            "epoch 2/100, batch 7/75, loss 0.6306\n",
            "epoch 2/100, batch 8/75, loss 1.1612\n",
            "epoch 2/100, batch 9/75, loss 0.6132\n",
            "epoch 2/100, batch 10/75, loss 1.3006\n",
            "epoch 2/100, batch 11/75, loss 0.7748\n",
            "epoch 2/100, batch 12/75, loss 0.6189\n",
            "epoch 2/100, batch 13/75, loss 0.6753\n",
            "epoch 2/100, batch 14/75, loss 0.6172\n",
            "epoch 2/100, batch 15/75, loss 2.4112\n",
            "epoch 2/100, batch 16/75, loss 0.6168\n",
            "epoch 2/100, batch 17/75, loss 0.6230\n",
            "epoch 2/100, batch 18/75, loss 0.6176\n",
            "epoch 2/100, batch 19/75, loss 0.6314\n",
            "epoch 2/100, batch 20/75, loss 1.8633\n",
            "epoch 2/100, batch 21/75, loss 0.6319\n",
            "epoch 2/100, batch 22/75, loss 0.6114\n",
            "epoch 2/100, batch 23/75, loss 0.6190\n",
            "epoch 2/100, batch 24/75, loss 0.6347\n",
            "epoch 2/100, batch 25/75, loss 0.6362\n",
            "epoch 2/100, batch 26/75, loss 2.9921\n",
            "epoch 2/100, batch 27/75, loss 0.6189\n",
            "epoch 2/100, batch 28/75, loss 0.6416\n",
            "epoch 2/100, batch 29/75, loss 0.6246\n",
            "epoch 2/100, batch 30/75, loss 0.6252\n",
            "epoch 2/100, batch 31/75, loss 0.6318\n",
            "epoch 2/100, batch 32/75, loss 0.6197\n",
            "epoch 2/100, batch 33/75, loss 0.6341\n",
            "epoch 2/100, batch 34/75, loss 0.6446\n",
            "epoch 2/100, batch 35/75, loss 0.6329\n",
            "epoch 2/100, batch 36/75, loss 3.0144\n",
            "epoch 2/100, batch 37/75, loss 0.8090\n",
            "epoch 2/100, batch 38/75, loss 0.6203\n",
            "epoch 2/100, batch 39/75, loss 0.6358\n",
            "epoch 2/100, batch 40/75, loss 0.6316\n",
            "epoch 2/100, batch 41/75, loss 0.6254\n",
            "epoch 2/100, batch 42/75, loss 0.6231\n",
            "epoch 2/100, batch 43/75, loss 0.6365\n",
            "epoch 2/100, batch 44/75, loss 0.6275\n",
            "epoch 2/100, batch 45/75, loss 0.6269\n",
            "epoch 2/100, batch 46/75, loss 0.6262\n",
            "epoch 2/100, batch 47/75, loss 0.6421\n",
            "epoch 2/100, batch 48/75, loss 0.6210\n",
            "epoch 2/100, batch 49/75, loss 0.6208\n",
            "epoch 2/100, batch 50/75, loss 0.6179\n",
            "epoch 2/100, batch 51/75, loss 0.6121\n",
            "epoch 2/100, batch 52/75, loss 0.6045\n",
            "epoch 2/100, batch 53/75, loss 10.3033\n",
            "epoch 2/100, batch 54/75, loss 0.8567\n",
            "epoch 2/100, batch 55/75, loss 0.6096\n",
            "epoch 2/100, batch 56/75, loss 0.6197\n",
            "epoch 2/100, batch 57/75, loss 0.6217\n",
            "epoch 2/100, batch 58/75, loss 0.6341\n",
            "epoch 2/100, batch 59/75, loss 0.6299\n",
            "epoch 2/100, batch 60/75, loss 0.6286\n",
            "epoch 2/100, batch 61/75, loss 0.6331\n",
            "epoch 2/100, batch 62/75, loss 0.6325\n",
            "epoch 2/100, batch 63/75, loss 0.6378\n",
            "epoch 2/100, batch 64/75, loss 0.6431\n",
            "epoch 2/100, batch 65/75, loss 0.6341\n",
            "epoch 2/100, batch 66/75, loss 0.6275\n",
            "epoch 2/100, batch 67/75, loss 0.6249\n",
            "epoch 2/100, batch 68/75, loss 0.6226\n",
            "epoch 2/100, batch 69/75, loss 0.6219\n",
            "epoch 2/100, batch 70/75, loss 0.6091\n",
            "epoch 2/100, batch 71/75, loss 0.6073\n",
            "epoch 2/100, batch 72/75, loss 0.6095\n",
            "epoch 2/100, batch 73/75, loss 0.6072\n",
            "epoch 2/100, batch 74/75, loss 0.5994\n",
            "epoch 2/100, batch 75/75, loss 0.5999\n",
            "epoch 2/100, training roc_auc_score 0.7592\n",
            "epoch 2/100, validation roc_auc_score 0.7951, best validation roc_auc_score 0.7951\n",
            "epoch 3/100, batch 1/75, loss 0.8677\n",
            "epoch 3/100, batch 2/75, loss 0.8461\n",
            "epoch 3/100, batch 3/75, loss 0.6629\n",
            "epoch 3/100, batch 4/75, loss 0.6530\n",
            "epoch 3/100, batch 5/75, loss 0.5967\n",
            "epoch 3/100, batch 6/75, loss 0.5829\n",
            "epoch 3/100, batch 7/75, loss 0.5887\n",
            "epoch 3/100, batch 8/75, loss 1.0444\n",
            "epoch 3/100, batch 9/75, loss 0.5711\n",
            "epoch 3/100, batch 10/75, loss 1.0698\n",
            "epoch 3/100, batch 11/75, loss 0.7017\n",
            "epoch 3/100, batch 12/75, loss 0.5776\n",
            "epoch 3/100, batch 13/75, loss 0.6057\n",
            "epoch 3/100, batch 14/75, loss 0.5794\n",
            "epoch 3/100, batch 15/75, loss 2.0805\n",
            "epoch 3/100, batch 16/75, loss 0.5769\n",
            "epoch 3/100, batch 17/75, loss 0.5863\n",
            "epoch 3/100, batch 18/75, loss 0.5807\n",
            "epoch 3/100, batch 19/75, loss 0.5906\n",
            "epoch 3/100, batch 20/75, loss 1.4949\n",
            "epoch 3/100, batch 21/75, loss 0.5890\n",
            "epoch 3/100, batch 22/75, loss 0.5726\n",
            "epoch 3/100, batch 23/75, loss 0.5735\n",
            "epoch 3/100, batch 24/75, loss 0.5947\n",
            "epoch 3/100, batch 25/75, loss 0.5860\n",
            "epoch 3/100, batch 26/75, loss 2.7590\n",
            "epoch 3/100, batch 27/75, loss 0.5805\n",
            "epoch 3/100, batch 28/75, loss 0.6099\n",
            "epoch 3/100, batch 29/75, loss 0.5789\n",
            "epoch 3/100, batch 30/75, loss 0.5818\n",
            "epoch 3/100, batch 31/75, loss 0.5876\n",
            "epoch 3/100, batch 32/75, loss 0.5789\n",
            "epoch 3/100, batch 33/75, loss 0.5874\n",
            "epoch 3/100, batch 34/75, loss 0.5965\n",
            "epoch 3/100, batch 35/75, loss 0.5859\n",
            "epoch 3/100, batch 36/75, loss 2.8243\n",
            "epoch 3/100, batch 37/75, loss 0.7766\n",
            "epoch 3/100, batch 38/75, loss 0.5757\n",
            "epoch 3/100, batch 39/75, loss 0.5866\n",
            "epoch 3/100, batch 40/75, loss 0.5912\n",
            "epoch 3/100, batch 41/75, loss 0.5814\n",
            "epoch 3/100, batch 42/75, loss 0.5768\n",
            "epoch 3/100, batch 43/75, loss 0.5866\n",
            "epoch 3/100, batch 44/75, loss 0.5799\n",
            "epoch 3/100, batch 45/75, loss 0.5882\n",
            "epoch 3/100, batch 46/75, loss 0.5899\n",
            "epoch 3/100, batch 47/75, loss 0.6000\n",
            "epoch 3/100, batch 48/75, loss 0.5772\n",
            "epoch 3/100, batch 49/75, loss 0.5767\n",
            "epoch 3/100, batch 50/75, loss 0.5839\n",
            "epoch 3/100, batch 51/75, loss 0.5670\n",
            "epoch 3/100, batch 52/75, loss 0.5620\n",
            "epoch 3/100, batch 53/75, loss 10.2813\n",
            "epoch 3/100, batch 54/75, loss 0.7918\n",
            "epoch 3/100, batch 55/75, loss 0.5676\n",
            "epoch 3/100, batch 56/75, loss 0.5797\n",
            "epoch 3/100, batch 57/75, loss 0.5858\n",
            "epoch 3/100, batch 58/75, loss 0.5987\n",
            "epoch 3/100, batch 59/75, loss 0.5926\n",
            "epoch 3/100, batch 60/75, loss 0.5959\n",
            "epoch 3/100, batch 61/75, loss 0.5968\n",
            "epoch 3/100, batch 62/75, loss 0.6031\n",
            "epoch 3/100, batch 63/75, loss 0.6036\n",
            "epoch 3/100, batch 64/75, loss 0.6088\n",
            "epoch 3/100, batch 65/75, loss 0.5978\n",
            "epoch 3/100, batch 66/75, loss 0.5940\n",
            "epoch 3/100, batch 67/75, loss 0.5907\n",
            "epoch 3/100, batch 68/75, loss 0.5929\n",
            "epoch 3/100, batch 69/75, loss 0.5858\n",
            "epoch 3/100, batch 70/75, loss 0.5671\n",
            "epoch 3/100, batch 71/75, loss 0.5673\n",
            "epoch 3/100, batch 72/75, loss 0.5764\n",
            "epoch 3/100, batch 73/75, loss 0.5662\n",
            "epoch 3/100, batch 74/75, loss 0.5589\n",
            "epoch 3/100, batch 75/75, loss 0.5521\n",
            "epoch 3/100, training roc_auc_score 0.8088\n",
            "EarlyStopping counter: 1 out of 10\n",
            "epoch 3/100, validation roc_auc_score 0.7850, best validation roc_auc_score 0.7951\n",
            "epoch 4/100, batch 1/75, loss 0.7521\n",
            "epoch 4/100, batch 2/75, loss 0.7939\n",
            "epoch 4/100, batch 3/75, loss 0.6055\n",
            "epoch 4/100, batch 4/75, loss 0.6425\n",
            "epoch 4/100, batch 5/75, loss 0.5479\n",
            "epoch 4/100, batch 6/75, loss 0.5421\n",
            "epoch 4/100, batch 7/75, loss 0.5495\n",
            "epoch 4/100, batch 8/75, loss 1.1128\n",
            "epoch 4/100, batch 9/75, loss 0.5293\n",
            "epoch 4/100, batch 10/75, loss 0.9704\n",
            "epoch 4/100, batch 11/75, loss 0.7183\n",
            "epoch 4/100, batch 12/75, loss 0.5314\n",
            "epoch 4/100, batch 13/75, loss 0.5588\n",
            "epoch 4/100, batch 14/75, loss 0.5335\n",
            "epoch 4/100, batch 15/75, loss 1.7495\n",
            "epoch 4/100, batch 16/75, loss 0.5348\n",
            "epoch 4/100, batch 17/75, loss 0.5500\n",
            "epoch 4/100, batch 18/75, loss 0.5504\n",
            "epoch 4/100, batch 19/75, loss 0.5506\n",
            "epoch 4/100, batch 20/75, loss 1.2994\n",
            "epoch 4/100, batch 21/75, loss 0.5566\n",
            "epoch 4/100, batch 22/75, loss 0.5341\n",
            "epoch 4/100, batch 23/75, loss 0.5327\n",
            "epoch 4/100, batch 24/75, loss 0.5594\n",
            "epoch 4/100, batch 25/75, loss 0.5562\n",
            "epoch 4/100, batch 26/75, loss 2.5436\n",
            "epoch 4/100, batch 27/75, loss 0.5465\n",
            "epoch 4/100, batch 28/75, loss 0.5857\n",
            "epoch 4/100, batch 29/75, loss 0.5380\n",
            "epoch 4/100, batch 30/75, loss 0.5493\n",
            "epoch 4/100, batch 31/75, loss 0.5485\n",
            "epoch 4/100, batch 32/75, loss 0.5337\n",
            "epoch 4/100, batch 33/75, loss 0.5527\n",
            "epoch 4/100, batch 34/75, loss 0.5576\n",
            "epoch 4/100, batch 35/75, loss 0.5410\n",
            "epoch 4/100, batch 36/75, loss 2.7744\n",
            "epoch 4/100, batch 37/75, loss 0.6755\n",
            "epoch 4/100, batch 38/75, loss 0.5367\n",
            "epoch 4/100, batch 39/75, loss 0.5485\n",
            "epoch 4/100, batch 40/75, loss 0.5538\n",
            "epoch 4/100, batch 41/75, loss 0.5392\n",
            "epoch 4/100, batch 42/75, loss 0.5376\n",
            "epoch 4/100, batch 43/75, loss 0.5479\n",
            "epoch 4/100, batch 44/75, loss 0.5345\n",
            "epoch 4/100, batch 45/75, loss 0.5468\n",
            "epoch 4/100, batch 46/75, loss 0.5379\n",
            "epoch 4/100, batch 47/75, loss 0.5579\n",
            "epoch 4/100, batch 48/75, loss 0.5325\n",
            "epoch 4/100, batch 49/75, loss 0.5291\n",
            "epoch 4/100, batch 50/75, loss 0.5404\n",
            "epoch 4/100, batch 51/75, loss 0.5229\n",
            "epoch 4/100, batch 52/75, loss 0.5116\n",
            "epoch 4/100, batch 53/75, loss 10.2268\n",
            "epoch 4/100, batch 54/75, loss 0.6619\n",
            "epoch 4/100, batch 55/75, loss 0.5266\n",
            "epoch 4/100, batch 56/75, loss 0.5344\n",
            "epoch 4/100, batch 57/75, loss 0.5410\n",
            "epoch 4/100, batch 58/75, loss 0.5674\n",
            "epoch 4/100, batch 59/75, loss 0.5500\n",
            "epoch 4/100, batch 60/75, loss 0.5565\n",
            "epoch 4/100, batch 61/75, loss 0.5563\n",
            "epoch 4/100, batch 62/75, loss 0.5686\n",
            "epoch 4/100, batch 63/75, loss 0.5612\n",
            "epoch 4/100, batch 64/75, loss 0.5728\n",
            "epoch 4/100, batch 65/75, loss 0.5545\n",
            "epoch 4/100, batch 66/75, loss 0.5566\n",
            "epoch 4/100, batch 67/75, loss 0.5529\n",
            "epoch 4/100, batch 68/75, loss 0.5468\n",
            "epoch 4/100, batch 69/75, loss 0.5468\n",
            "epoch 4/100, batch 70/75, loss 0.5253\n",
            "epoch 4/100, batch 71/75, loss 0.5159\n",
            "epoch 4/100, batch 72/75, loss 0.5315\n",
            "epoch 4/100, batch 73/75, loss 0.5216\n",
            "epoch 4/100, batch 74/75, loss 0.5084\n",
            "epoch 4/100, batch 75/75, loss 0.5091\n",
            "epoch 4/100, training roc_auc_score 0.8412\n",
            "EarlyStopping counter: 2 out of 10\n",
            "epoch 4/100, validation roc_auc_score 0.7682, best validation roc_auc_score 0.7951\n",
            "epoch 5/100, batch 1/75, loss 0.6993\n",
            "epoch 5/100, batch 2/75, loss 0.7887\n",
            "epoch 5/100, batch 3/75, loss 0.5599\n",
            "epoch 5/100, batch 4/75, loss 0.5940\n",
            "epoch 5/100, batch 5/75, loss 0.5177\n",
            "epoch 5/100, batch 6/75, loss 0.4969\n",
            "epoch 5/100, batch 7/75, loss 0.5170\n",
            "epoch 5/100, batch 8/75, loss 0.9933\n",
            "epoch 5/100, batch 9/75, loss 0.4798\n",
            "epoch 5/100, batch 10/75, loss 0.7689\n",
            "epoch 5/100, batch 11/75, loss 0.6196\n",
            "epoch 5/100, batch 12/75, loss 0.4842\n",
            "epoch 5/100, batch 13/75, loss 0.5026\n",
            "epoch 5/100, batch 14/75, loss 0.4903\n",
            "epoch 5/100, batch 15/75, loss 1.6849\n",
            "epoch 5/100, batch 16/75, loss 0.4831\n",
            "epoch 5/100, batch 17/75, loss 0.5052\n",
            "epoch 5/100, batch 18/75, loss 0.5021\n",
            "epoch 5/100, batch 19/75, loss 0.5156\n",
            "epoch 5/100, batch 20/75, loss 1.2490\n",
            "epoch 5/100, batch 21/75, loss 0.5147\n",
            "epoch 5/100, batch 22/75, loss 0.4857\n",
            "epoch 5/100, batch 23/75, loss 0.4778\n",
            "epoch 5/100, batch 24/75, loss 0.5167\n",
            "epoch 5/100, batch 25/75, loss 0.5048\n",
            "epoch 5/100, batch 26/75, loss 2.3645\n",
            "epoch 5/100, batch 27/75, loss 0.4963\n",
            "epoch 5/100, batch 28/75, loss 0.5456\n",
            "epoch 5/100, batch 29/75, loss 0.4929\n",
            "epoch 5/100, batch 30/75, loss 0.4903\n",
            "epoch 5/100, batch 31/75, loss 0.5006\n",
            "epoch 5/100, batch 32/75, loss 0.4902\n",
            "epoch 5/100, batch 33/75, loss 0.4991\n",
            "epoch 5/100, batch 34/75, loss 0.5035\n",
            "epoch 5/100, batch 35/75, loss 0.4916\n",
            "epoch 5/100, batch 36/75, loss 2.7144\n",
            "epoch 5/100, batch 37/75, loss 0.6694\n",
            "epoch 5/100, batch 38/75, loss 0.4868\n",
            "epoch 5/100, batch 39/75, loss 0.4996\n",
            "epoch 5/100, batch 40/75, loss 0.4983\n",
            "epoch 5/100, batch 41/75, loss 0.4965\n",
            "epoch 5/100, batch 42/75, loss 0.4911\n",
            "epoch 5/100, batch 43/75, loss 0.4950\n",
            "epoch 5/100, batch 44/75, loss 0.4918\n",
            "epoch 5/100, batch 45/75, loss 0.4940\n",
            "epoch 5/100, batch 46/75, loss 0.4820\n",
            "epoch 5/100, batch 47/75, loss 0.5125\n",
            "epoch 5/100, batch 48/75, loss 0.4736\n",
            "epoch 5/100, batch 49/75, loss 0.4748\n",
            "epoch 5/100, batch 50/75, loss 0.4915\n",
            "epoch 5/100, batch 51/75, loss 0.4755\n",
            "epoch 5/100, batch 52/75, loss 0.4624\n",
            "epoch 5/100, batch 53/75, loss 10.3881\n",
            "epoch 5/100, batch 54/75, loss 0.5642\n",
            "epoch 5/100, batch 55/75, loss 0.4773\n",
            "epoch 5/100, batch 56/75, loss 0.4971\n",
            "epoch 5/100, batch 57/75, loss 0.4934\n",
            "epoch 5/100, batch 58/75, loss 0.5230\n",
            "epoch 5/100, batch 59/75, loss 0.5157\n",
            "epoch 5/100, batch 60/75, loss 0.5219\n",
            "epoch 5/100, batch 61/75, loss 0.5290\n",
            "epoch 5/100, batch 62/75, loss 0.5345\n",
            "epoch 5/100, batch 63/75, loss 0.5281\n",
            "epoch 5/100, batch 64/75, loss 0.5285\n",
            "epoch 5/100, batch 65/75, loss 0.5185\n",
            "epoch 5/100, batch 66/75, loss 0.5120\n",
            "epoch 5/100, batch 67/75, loss 0.4994\n",
            "epoch 5/100, batch 68/75, loss 0.4990\n",
            "epoch 5/100, batch 69/75, loss 0.4922\n",
            "epoch 5/100, batch 70/75, loss 0.4720\n",
            "epoch 5/100, batch 71/75, loss 0.4680\n",
            "epoch 5/100, batch 72/75, loss 0.4798\n",
            "epoch 5/100, batch 73/75, loss 0.4697\n",
            "epoch 5/100, batch 74/75, loss 0.4636\n",
            "epoch 5/100, batch 75/75, loss 0.4539\n",
            "epoch 5/100, training roc_auc_score 0.8540\n",
            "EarlyStopping counter: 3 out of 10\n",
            "epoch 5/100, validation roc_auc_score 0.7233, best validation roc_auc_score 0.7951\n",
            "epoch 6/100, batch 1/75, loss 0.6324\n",
            "epoch 6/100, batch 2/75, loss 0.6439\n",
            "epoch 6/100, batch 3/75, loss 0.5193\n",
            "epoch 6/100, batch 4/75, loss 0.5743\n",
            "epoch 6/100, batch 5/75, loss 0.4547\n",
            "epoch 6/100, batch 6/75, loss 0.4455\n",
            "epoch 6/100, batch 7/75, loss 0.4725\n",
            "epoch 6/100, batch 8/75, loss 0.9680\n",
            "epoch 6/100, batch 9/75, loss 0.4339\n",
            "epoch 6/100, batch 10/75, loss 0.7062\n",
            "epoch 6/100, batch 11/75, loss 0.5915\n",
            "epoch 6/100, batch 12/75, loss 0.4267\n",
            "epoch 6/100, batch 13/75, loss 0.4568\n",
            "epoch 6/100, batch 14/75, loss 0.4438\n",
            "epoch 6/100, batch 15/75, loss 1.5593\n",
            "epoch 6/100, batch 16/75, loss 0.4415\n",
            "epoch 6/100, batch 17/75, loss 0.4548\n",
            "epoch 6/100, batch 18/75, loss 0.4687\n",
            "epoch 6/100, batch 19/75, loss 0.4663\n",
            "epoch 6/100, batch 20/75, loss 1.0014\n",
            "epoch 6/100, batch 21/75, loss 0.4660\n",
            "epoch 6/100, batch 22/75, loss 0.4570\n",
            "epoch 6/100, batch 23/75, loss 0.4348\n",
            "epoch 6/100, batch 24/75, loss 0.4788\n",
            "epoch 6/100, batch 25/75, loss 0.4597\n",
            "epoch 6/100, batch 26/75, loss 2.0586\n",
            "epoch 6/100, batch 27/75, loss 0.4687\n",
            "epoch 6/100, batch 28/75, loss 0.5196\n",
            "epoch 6/100, batch 29/75, loss 0.4512\n",
            "epoch 6/100, batch 30/75, loss 0.4477\n",
            "epoch 6/100, batch 31/75, loss 0.4658\n",
            "epoch 6/100, batch 32/75, loss 0.4421\n",
            "epoch 6/100, batch 33/75, loss 0.4657\n",
            "epoch 6/100, batch 34/75, loss 0.4677\n",
            "epoch 6/100, batch 35/75, loss 0.4466\n",
            "epoch 6/100, batch 36/75, loss 2.4794\n",
            "epoch 6/100, batch 37/75, loss 0.5753\n",
            "epoch 6/100, batch 38/75, loss 0.4370\n",
            "epoch 6/100, batch 39/75, loss 0.4537\n",
            "epoch 6/100, batch 40/75, loss 0.4574\n",
            "epoch 6/100, batch 41/75, loss 0.4368\n",
            "epoch 6/100, batch 42/75, loss 0.4328\n",
            "epoch 6/100, batch 43/75, loss 0.4375\n",
            "epoch 6/100, batch 44/75, loss 0.4384\n",
            "epoch 6/100, batch 45/75, loss 0.4442\n",
            "epoch 6/100, batch 46/75, loss 0.4501\n",
            "epoch 6/100, batch 47/75, loss 0.4530\n",
            "epoch 6/100, batch 48/75, loss 0.4258\n",
            "epoch 6/100, batch 49/75, loss 0.4153\n",
            "epoch 6/100, batch 50/75, loss 0.4433\n",
            "epoch 6/100, batch 51/75, loss 0.4270\n",
            "epoch 6/100, batch 52/75, loss 0.4123\n",
            "epoch 6/100, batch 53/75, loss 10.7036\n",
            "epoch 6/100, batch 54/75, loss 0.5185\n",
            "epoch 6/100, batch 55/75, loss 0.4226\n",
            "epoch 6/100, batch 56/75, loss 0.4389\n",
            "epoch 6/100, batch 57/75, loss 0.4391\n",
            "epoch 6/100, batch 58/75, loss 0.4795\n",
            "epoch 6/100, batch 59/75, loss 0.4689\n",
            "epoch 6/100, batch 60/75, loss 0.4803\n",
            "epoch 6/100, batch 61/75, loss 0.4843\n",
            "epoch 6/100, batch 62/75, loss 0.5052\n",
            "epoch 6/100, batch 63/75, loss 0.5169\n",
            "epoch 6/100, batch 64/75, loss 0.4943\n",
            "epoch 6/100, batch 65/75, loss 0.4850\n",
            "epoch 6/100, batch 66/75, loss 0.4837\n",
            "epoch 6/100, batch 67/75, loss 0.4553\n",
            "epoch 6/100, batch 68/75, loss 0.4680\n",
            "epoch 6/100, batch 69/75, loss 0.4593\n",
            "epoch 6/100, batch 70/75, loss 0.4282\n",
            "epoch 6/100, batch 71/75, loss 0.4293\n",
            "epoch 6/100, batch 72/75, loss 0.4518\n",
            "epoch 6/100, batch 73/75, loss 0.4355\n",
            "epoch 6/100, batch 74/75, loss 0.4301\n",
            "epoch 6/100, batch 75/75, loss 0.4142\n",
            "epoch 6/100, training roc_auc_score 0.8804\n",
            "EarlyStopping counter: 4 out of 10\n",
            "epoch 6/100, validation roc_auc_score 0.7859, best validation roc_auc_score 0.7951\n",
            "epoch 7/100, batch 1/75, loss 0.5464\n",
            "epoch 7/100, batch 2/75, loss 0.5812\n",
            "epoch 7/100, batch 3/75, loss 0.4775\n",
            "epoch 7/100, batch 4/75, loss 0.4470\n",
            "epoch 7/100, batch 5/75, loss 0.4011\n",
            "epoch 7/100, batch 6/75, loss 0.4153\n",
            "epoch 7/100, batch 7/75, loss 0.3992\n",
            "epoch 7/100, batch 8/75, loss 0.9212\n",
            "epoch 7/100, batch 9/75, loss 0.3744\n",
            "epoch 7/100, batch 10/75, loss 0.6363\n",
            "epoch 7/100, batch 11/75, loss 0.5024\n",
            "epoch 7/100, batch 12/75, loss 0.3835\n",
            "epoch 7/100, batch 13/75, loss 0.4282\n",
            "epoch 7/100, batch 14/75, loss 0.3963\n",
            "epoch 7/100, batch 15/75, loss 1.6428\n",
            "epoch 7/100, batch 16/75, loss 0.3846\n",
            "epoch 7/100, batch 17/75, loss 0.3962\n",
            "epoch 7/100, batch 18/75, loss 0.4117\n",
            "epoch 7/100, batch 19/75, loss 0.4255\n",
            "epoch 7/100, batch 20/75, loss 0.9166\n",
            "epoch 7/100, batch 21/75, loss 0.4030\n",
            "epoch 7/100, batch 22/75, loss 0.3932\n",
            "epoch 7/100, batch 23/75, loss 0.3943\n",
            "epoch 7/100, batch 24/75, loss 0.4200\n",
            "epoch 7/100, batch 25/75, loss 0.4058\n",
            "epoch 7/100, batch 26/75, loss 1.9419\n",
            "epoch 7/100, batch 27/75, loss 0.4250\n",
            "epoch 7/100, batch 28/75, loss 0.4727\n",
            "epoch 7/100, batch 29/75, loss 0.4035\n",
            "epoch 7/100, batch 30/75, loss 0.4028\n",
            "epoch 7/100, batch 31/75, loss 0.4263\n",
            "epoch 7/100, batch 32/75, loss 0.3967\n",
            "epoch 7/100, batch 33/75, loss 0.4114\n",
            "epoch 7/100, batch 34/75, loss 0.4228\n",
            "epoch 7/100, batch 35/75, loss 0.4161\n",
            "epoch 7/100, batch 36/75, loss 2.5270\n",
            "epoch 7/100, batch 37/75, loss 0.5941\n",
            "epoch 7/100, batch 38/75, loss 0.3931\n",
            "epoch 7/100, batch 39/75, loss 0.4070\n",
            "epoch 7/100, batch 40/75, loss 0.3959\n",
            "epoch 7/100, batch 41/75, loss 0.4103\n",
            "epoch 7/100, batch 42/75, loss 0.3821\n",
            "epoch 7/100, batch 43/75, loss 0.3882\n",
            "epoch 7/100, batch 44/75, loss 0.3930\n",
            "epoch 7/100, batch 45/75, loss 0.3954\n",
            "epoch 7/100, batch 46/75, loss 0.3665\n",
            "epoch 7/100, batch 47/75, loss 0.4022\n",
            "epoch 7/100, batch 48/75, loss 0.3715\n",
            "epoch 7/100, batch 49/75, loss 0.3731\n",
            "epoch 7/100, batch 50/75, loss 0.3913\n",
            "epoch 7/100, batch 51/75, loss 0.3727\n",
            "epoch 7/100, batch 52/75, loss 0.3515\n",
            "epoch 7/100, batch 53/75, loss 10.3835\n",
            "epoch 7/100, batch 54/75, loss 0.4006\n",
            "epoch 7/100, batch 55/75, loss 0.3844\n",
            "epoch 7/100, batch 56/75, loss 0.3920\n",
            "epoch 7/100, batch 57/75, loss 0.3889\n",
            "epoch 7/100, batch 58/75, loss 0.4223\n",
            "epoch 7/100, batch 59/75, loss 0.4117\n",
            "epoch 7/100, batch 60/75, loss 0.4283\n",
            "epoch 7/100, batch 61/75, loss 0.4064\n",
            "epoch 7/100, batch 62/75, loss 0.4298\n",
            "epoch 7/100, batch 63/75, loss 0.4323\n",
            "epoch 7/100, batch 64/75, loss 0.4343\n",
            "epoch 7/100, batch 65/75, loss 0.4233\n",
            "epoch 7/100, batch 66/75, loss 0.4016\n",
            "epoch 7/100, batch 67/75, loss 0.3974\n",
            "epoch 7/100, batch 68/75, loss 0.4064\n",
            "epoch 7/100, batch 69/75, loss 0.3964\n",
            "epoch 7/100, batch 70/75, loss 0.3793\n",
            "epoch 7/100, batch 71/75, loss 0.3630\n",
            "epoch 7/100, batch 72/75, loss 0.3732\n",
            "epoch 7/100, batch 73/75, loss 0.3759\n",
            "epoch 7/100, batch 74/75, loss 0.3705\n",
            "epoch 7/100, batch 75/75, loss 0.3533\n",
            "epoch 7/100, training roc_auc_score 0.9007\n",
            "EarlyStopping counter: 5 out of 10\n",
            "epoch 7/100, validation roc_auc_score 0.7571, best validation roc_auc_score 0.7951\n",
            "epoch 8/100, batch 1/75, loss 0.5364\n",
            "epoch 8/100, batch 2/75, loss 0.5109\n",
            "epoch 8/100, batch 3/75, loss 0.4156\n",
            "epoch 8/100, batch 4/75, loss 0.4763\n",
            "epoch 8/100, batch 5/75, loss 0.3612\n",
            "epoch 8/100, batch 6/75, loss 0.3518\n",
            "epoch 8/100, batch 7/75, loss 0.3757\n",
            "epoch 8/100, batch 8/75, loss 0.8735\n",
            "epoch 8/100, batch 9/75, loss 0.3308\n",
            "epoch 8/100, batch 10/75, loss 0.4938\n",
            "epoch 8/100, batch 11/75, loss 0.4667\n",
            "epoch 8/100, batch 12/75, loss 0.3372\n",
            "epoch 8/100, batch 13/75, loss 0.3621\n",
            "epoch 8/100, batch 14/75, loss 0.3381\n",
            "epoch 8/100, batch 15/75, loss 1.3448\n",
            "epoch 8/100, batch 16/75, loss 0.3345\n",
            "epoch 8/100, batch 17/75, loss 0.3586\n",
            "epoch 8/100, batch 18/75, loss 0.3612\n",
            "epoch 8/100, batch 19/75, loss 0.3751\n",
            "epoch 8/100, batch 20/75, loss 0.9177\n",
            "epoch 8/100, batch 21/75, loss 0.3883\n",
            "epoch 8/100, batch 22/75, loss 0.3443\n",
            "epoch 8/100, batch 23/75, loss 0.3309\n",
            "epoch 8/100, batch 24/75, loss 0.3695\n",
            "epoch 8/100, batch 25/75, loss 0.3481\n",
            "epoch 8/100, batch 26/75, loss 2.0224\n",
            "epoch 8/100, batch 27/75, loss 0.3741\n",
            "epoch 8/100, batch 28/75, loss 0.4378\n",
            "epoch 8/100, batch 29/75, loss 0.3577\n",
            "epoch 8/100, batch 30/75, loss 0.3459\n",
            "epoch 8/100, batch 31/75, loss 0.3762\n",
            "epoch 8/100, batch 32/75, loss 0.3446\n",
            "epoch 8/100, batch 33/75, loss 0.3666\n",
            "epoch 8/100, batch 34/75, loss 0.3587\n",
            "epoch 8/100, batch 35/75, loss 0.3432\n",
            "epoch 8/100, batch 36/75, loss 2.3306\n",
            "epoch 8/100, batch 37/75, loss 0.4954\n",
            "epoch 8/100, batch 38/75, loss 0.3403\n",
            "epoch 8/100, batch 39/75, loss 0.3651\n",
            "epoch 8/100, batch 40/75, loss 0.3663\n",
            "epoch 8/100, batch 41/75, loss 0.3731\n",
            "epoch 8/100, batch 42/75, loss 0.3499\n",
            "epoch 8/100, batch 43/75, loss 0.3553\n",
            "epoch 8/100, batch 44/75, loss 0.3614\n",
            "epoch 8/100, batch 45/75, loss 0.3595\n",
            "epoch 8/100, batch 46/75, loss 0.3540\n",
            "epoch 8/100, batch 47/75, loss 0.3779\n",
            "epoch 8/100, batch 48/75, loss 0.3428\n",
            "epoch 8/100, batch 49/75, loss 0.3332\n",
            "epoch 8/100, batch 50/75, loss 0.3497\n",
            "epoch 8/100, batch 51/75, loss 0.3306\n",
            "epoch 8/100, batch 52/75, loss 0.3157\n",
            "epoch 8/100, batch 53/75, loss 11.1751\n",
            "epoch 8/100, batch 54/75, loss 0.3737\n",
            "epoch 8/100, batch 55/75, loss 0.3423\n",
            "epoch 8/100, batch 56/75, loss 0.3770\n",
            "epoch 8/100, batch 57/75, loss 0.3794\n",
            "epoch 8/100, batch 58/75, loss 0.4255\n",
            "epoch 8/100, batch 59/75, loss 0.4247\n",
            "epoch 8/100, batch 60/75, loss 0.4082\n",
            "epoch 8/100, batch 61/75, loss 0.3978\n",
            "epoch 8/100, batch 62/75, loss 0.4349\n",
            "epoch 8/100, batch 63/75, loss 0.4170\n",
            "epoch 8/100, batch 64/75, loss 0.4351\n",
            "epoch 8/100, batch 65/75, loss 0.4118\n",
            "epoch 8/100, batch 66/75, loss 0.4043\n",
            "epoch 8/100, batch 67/75, loss 0.3653\n",
            "epoch 8/100, batch 68/75, loss 0.3762\n",
            "epoch 8/100, batch 69/75, loss 0.3777\n",
            "epoch 8/100, batch 70/75, loss 0.3483\n",
            "epoch 8/100, batch 71/75, loss 0.3440\n",
            "epoch 8/100, batch 72/75, loss 0.3655\n",
            "epoch 8/100, batch 73/75, loss 0.3388\n",
            "epoch 8/100, batch 74/75, loss 0.3347\n",
            "epoch 8/100, batch 75/75, loss 0.3265\n",
            "epoch 8/100, training roc_auc_score 0.9037\n",
            "EarlyStopping counter: 6 out of 10\n",
            "epoch 8/100, validation roc_auc_score 0.7815, best validation roc_auc_score 0.7951\n",
            "epoch 9/100, batch 1/75, loss 0.4514\n",
            "epoch 9/100, batch 2/75, loss 0.6063\n",
            "epoch 9/100, batch 3/75, loss 0.3999\n",
            "epoch 9/100, batch 4/75, loss 0.4362\n",
            "epoch 9/100, batch 5/75, loss 0.3449\n",
            "epoch 9/100, batch 6/75, loss 0.3041\n",
            "epoch 9/100, batch 7/75, loss 0.3139\n",
            "epoch 9/100, batch 8/75, loss 0.9255\n",
            "epoch 9/100, batch 9/75, loss 0.2809\n",
            "epoch 9/100, batch 10/75, loss 0.6645\n",
            "epoch 9/100, batch 11/75, loss 0.3687\n",
            "epoch 9/100, batch 12/75, loss 0.3032\n",
            "epoch 9/100, batch 13/75, loss 0.3348\n",
            "epoch 9/100, batch 14/75, loss 0.3189\n",
            "epoch 9/100, batch 15/75, loss 1.5002\n",
            "epoch 9/100, batch 16/75, loss 0.3055\n",
            "epoch 9/100, batch 17/75, loss 0.3447\n",
            "epoch 9/100, batch 18/75, loss 0.3446\n",
            "epoch 9/100, batch 19/75, loss 0.3385\n",
            "epoch 9/100, batch 20/75, loss 0.8981\n",
            "epoch 9/100, batch 21/75, loss 0.3460\n",
            "epoch 9/100, batch 22/75, loss 0.2990\n",
            "epoch 9/100, batch 23/75, loss 0.3073\n",
            "epoch 9/100, batch 24/75, loss 0.3525\n",
            "epoch 9/100, batch 25/75, loss 0.3229\n",
            "epoch 9/100, batch 26/75, loss 2.6735\n",
            "epoch 9/100, batch 27/75, loss 0.3447\n",
            "epoch 9/100, batch 28/75, loss 0.4318\n",
            "epoch 9/100, batch 29/75, loss 0.3255\n",
            "epoch 9/100, batch 30/75, loss 0.3479\n",
            "epoch 9/100, batch 31/75, loss 0.3514\n",
            "epoch 9/100, batch 32/75, loss 0.3351\n",
            "epoch 9/100, batch 33/75, loss 0.3540\n",
            "epoch 9/100, batch 34/75, loss 0.3780\n",
            "epoch 9/100, batch 35/75, loss 0.3563\n",
            "epoch 9/100, batch 36/75, loss 2.2713\n",
            "epoch 9/100, batch 37/75, loss 0.5455\n",
            "epoch 9/100, batch 38/75, loss 0.3512\n",
            "epoch 9/100, batch 39/75, loss 0.3708\n",
            "epoch 9/100, batch 40/75, loss 0.3348\n",
            "epoch 9/100, batch 41/75, loss 0.3775\n",
            "epoch 9/100, batch 42/75, loss 0.3092\n",
            "epoch 9/100, batch 43/75, loss 0.3487\n",
            "epoch 9/100, batch 44/75, loss 0.3432\n",
            "epoch 9/100, batch 45/75, loss 0.3449\n",
            "epoch 9/100, batch 46/75, loss 0.2932\n",
            "epoch 9/100, batch 47/75, loss 0.3279\n",
            "epoch 9/100, batch 48/75, loss 0.3073\n",
            "epoch 9/100, batch 49/75, loss 0.3066\n",
            "epoch 9/100, batch 50/75, loss 0.3250\n",
            "epoch 9/100, batch 51/75, loss 0.3151\n",
            "epoch 9/100, batch 52/75, loss 0.2911\n",
            "epoch 9/100, batch 53/75, loss 12.5185\n",
            "epoch 9/100, batch 54/75, loss 0.4139\n",
            "epoch 9/100, batch 55/75, loss 0.3156\n",
            "epoch 9/100, batch 56/75, loss 0.3317\n",
            "epoch 9/100, batch 57/75, loss 0.3427\n",
            "epoch 9/100, batch 58/75, loss 0.3918\n",
            "epoch 9/100, batch 59/75, loss 0.3908\n",
            "epoch 9/100, batch 60/75, loss 0.4029\n",
            "epoch 9/100, batch 61/75, loss 0.4028\n",
            "epoch 9/100, batch 62/75, loss 0.4186\n",
            "epoch 9/100, batch 63/75, loss 0.4238\n",
            "epoch 9/100, batch 64/75, loss 0.4300\n",
            "epoch 9/100, batch 65/75, loss 0.4012\n",
            "epoch 9/100, batch 66/75, loss 0.4159\n",
            "epoch 9/100, batch 67/75, loss 0.3906\n",
            "epoch 9/100, batch 68/75, loss 0.3828\n",
            "epoch 9/100, batch 69/75, loss 0.3773\n",
            "epoch 9/100, batch 70/75, loss 0.3172\n",
            "epoch 9/100, batch 71/75, loss 0.3347\n",
            "epoch 9/100, batch 72/75, loss 0.3655\n",
            "epoch 9/100, batch 73/75, loss 0.3416\n",
            "epoch 9/100, batch 74/75, loss 0.3506\n",
            "epoch 9/100, batch 75/75, loss 0.3150\n",
            "epoch 9/100, training roc_auc_score 0.8865\n",
            "EarlyStopping counter: 7 out of 10\n",
            "epoch 9/100, validation roc_auc_score 0.7176, best validation roc_auc_score 0.7951\n",
            "epoch 10/100, batch 1/75, loss 0.4042\n",
            "epoch 10/100, batch 2/75, loss 0.5194\n",
            "epoch 10/100, batch 3/75, loss 0.4084\n",
            "epoch 10/100, batch 4/75, loss 0.4200\n",
            "epoch 10/100, batch 5/75, loss 0.3253\n",
            "epoch 10/100, batch 6/75, loss 0.3243\n",
            "epoch 10/100, batch 7/75, loss 0.3146\n",
            "epoch 10/100, batch 8/75, loss 0.7130\n",
            "epoch 10/100, batch 9/75, loss 0.2824\n",
            "epoch 10/100, batch 10/75, loss 0.5479\n",
            "epoch 10/100, batch 11/75, loss 0.3686\n",
            "epoch 10/100, batch 12/75, loss 0.2993\n",
            "epoch 10/100, batch 13/75, loss 0.3009\n",
            "epoch 10/100, batch 14/75, loss 0.2761\n",
            "epoch 10/100, batch 15/75, loss 1.1174\n",
            "epoch 10/100, batch 16/75, loss 0.2838\n",
            "epoch 10/100, batch 17/75, loss 0.3114\n",
            "epoch 10/100, batch 18/75, loss 0.3262\n",
            "epoch 10/100, batch 19/75, loss 0.3259\n",
            "epoch 10/100, batch 20/75, loss 0.7243\n",
            "epoch 10/100, batch 21/75, loss 0.3082\n",
            "epoch 10/100, batch 22/75, loss 0.2934\n",
            "epoch 10/100, batch 23/75, loss 0.2722\n",
            "epoch 10/100, batch 24/75, loss 0.3161\n",
            "epoch 10/100, batch 25/75, loss 0.3086\n",
            "epoch 10/100, batch 26/75, loss 1.9865\n",
            "epoch 10/100, batch 27/75, loss 0.3143\n",
            "epoch 10/100, batch 28/75, loss 0.3881\n",
            "epoch 10/100, batch 29/75, loss 0.2876\n",
            "epoch 10/100, batch 30/75, loss 0.2924\n",
            "epoch 10/100, batch 31/75, loss 0.3139\n",
            "epoch 10/100, batch 32/75, loss 0.2809\n",
            "epoch 10/100, batch 33/75, loss 0.3007\n",
            "epoch 10/100, batch 34/75, loss 0.3312\n",
            "epoch 10/100, batch 35/75, loss 0.3016\n",
            "epoch 10/100, batch 36/75, loss 2.1358\n",
            "epoch 10/100, batch 37/75, loss 0.3765\n",
            "epoch 10/100, batch 38/75, loss 0.3091\n",
            "epoch 10/100, batch 39/75, loss 0.3308\n",
            "epoch 10/100, batch 40/75, loss 0.2882\n",
            "epoch 10/100, batch 41/75, loss 0.3245\n",
            "epoch 10/100, batch 42/75, loss 0.2804\n",
            "epoch 10/100, batch 43/75, loss 0.3224\n",
            "epoch 10/100, batch 44/75, loss 0.3051\n",
            "epoch 10/100, batch 45/75, loss 0.3135\n",
            "epoch 10/100, batch 46/75, loss 0.2873\n",
            "epoch 10/100, batch 47/75, loss 0.2994\n",
            "epoch 10/100, batch 48/75, loss 0.2807\n",
            "epoch 10/100, batch 49/75, loss 0.2771\n",
            "epoch 10/100, batch 50/75, loss 0.3182\n",
            "epoch 10/100, batch 51/75, loss 0.2875\n",
            "epoch 10/100, batch 52/75, loss 0.2673\n",
            "epoch 10/100, batch 53/75, loss 9.9758\n",
            "epoch 10/100, batch 54/75, loss 0.3015\n",
            "epoch 10/100, batch 55/75, loss 0.2914\n",
            "epoch 10/100, batch 56/75, loss 0.2943\n",
            "epoch 10/100, batch 57/75, loss 0.2992\n",
            "epoch 10/100, batch 58/75, loss 0.3266\n",
            "epoch 10/100, batch 59/75, loss 0.3228\n",
            "epoch 10/100, batch 60/75, loss 0.3389\n",
            "epoch 10/100, batch 61/75, loss 0.3288\n",
            "epoch 10/100, batch 62/75, loss 0.3523\n",
            "epoch 10/100, batch 63/75, loss 0.3431\n",
            "epoch 10/100, batch 64/75, loss 0.3538\n",
            "epoch 10/100, batch 65/75, loss 0.3282\n",
            "epoch 10/100, batch 66/75, loss 0.3243\n",
            "epoch 10/100, batch 67/75, loss 0.3177\n",
            "epoch 10/100, batch 68/75, loss 0.2970\n",
            "epoch 10/100, batch 69/75, loss 0.3196\n",
            "epoch 10/100, batch 70/75, loss 0.2631\n",
            "epoch 10/100, batch 71/75, loss 0.2763\n",
            "epoch 10/100, batch 72/75, loss 0.2967\n",
            "epoch 10/100, batch 73/75, loss 0.2847\n",
            "epoch 10/100, batch 74/75, loss 0.2823\n",
            "epoch 10/100, batch 75/75, loss 0.2578\n",
            "epoch 10/100, training roc_auc_score 0.9328\n",
            "EarlyStopping counter: 8 out of 10\n",
            "epoch 10/100, validation roc_auc_score 0.7546, best validation roc_auc_score 0.7951\n",
            "epoch 11/100, batch 1/75, loss 0.4021\n",
            "epoch 11/100, batch 2/75, loss 0.3787\n",
            "epoch 11/100, batch 3/75, loss 0.3553\n",
            "epoch 11/100, batch 4/75, loss 0.3439\n",
            "epoch 11/100, batch 5/75, loss 0.2983\n",
            "epoch 11/100, batch 6/75, loss 0.2501\n",
            "epoch 11/100, batch 7/75, loss 0.2695\n",
            "epoch 11/100, batch 8/75, loss 0.6013\n",
            "epoch 11/100, batch 9/75, loss 0.2338\n",
            "epoch 11/100, batch 10/75, loss 0.4400\n",
            "epoch 11/100, batch 11/75, loss 0.3161\n",
            "epoch 11/100, batch 12/75, loss 0.2377\n",
            "epoch 11/100, batch 13/75, loss 0.2689\n",
            "epoch 11/100, batch 14/75, loss 0.2535\n",
            "epoch 11/100, batch 15/75, loss 1.2087\n",
            "epoch 11/100, batch 16/75, loss 0.2480\n",
            "epoch 11/100, batch 17/75, loss 0.2738\n",
            "epoch 11/100, batch 18/75, loss 0.2883\n",
            "epoch 11/100, batch 19/75, loss 0.3091\n",
            "epoch 11/100, batch 20/75, loss 0.5684\n",
            "epoch 11/100, batch 21/75, loss 0.2535\n",
            "epoch 11/100, batch 22/75, loss 0.2471\n",
            "epoch 11/100, batch 23/75, loss 0.2330\n",
            "epoch 11/100, batch 24/75, loss 0.2777\n",
            "epoch 11/100, batch 25/75, loss 0.2469\n",
            "epoch 11/100, batch 26/75, loss 1.4606\n",
            "epoch 11/100, batch 27/75, loss 0.2846\n",
            "epoch 11/100, batch 28/75, loss 0.3543\n",
            "epoch 11/100, batch 29/75, loss 0.2654\n",
            "epoch 11/100, batch 30/75, loss 0.2396\n",
            "epoch 11/100, batch 31/75, loss 0.2549\n",
            "epoch 11/100, batch 32/75, loss 0.2420\n",
            "epoch 11/100, batch 33/75, loss 0.2707\n",
            "epoch 11/100, batch 34/75, loss 0.2666\n",
            "epoch 11/100, batch 35/75, loss 0.2637\n",
            "epoch 11/100, batch 36/75, loss 2.2378\n",
            "epoch 11/100, batch 37/75, loss 0.4225\n",
            "epoch 11/100, batch 38/75, loss 0.2461\n",
            "epoch 11/100, batch 39/75, loss 0.2730\n",
            "epoch 11/100, batch 40/75, loss 0.2534\n",
            "epoch 11/100, batch 41/75, loss 0.2842\n",
            "epoch 11/100, batch 42/75, loss 0.2617\n",
            "epoch 11/100, batch 43/75, loss 0.3023\n",
            "epoch 11/100, batch 44/75, loss 0.2755\n",
            "epoch 11/100, batch 45/75, loss 0.2810\n",
            "epoch 11/100, batch 46/75, loss 0.2599\n",
            "epoch 11/100, batch 47/75, loss 0.3173\n",
            "epoch 11/100, batch 48/75, loss 0.2383\n",
            "epoch 11/100, batch 49/75, loss 0.2600\n",
            "epoch 11/100, batch 50/75, loss 0.2744\n",
            "epoch 11/100, batch 51/75, loss 0.2607\n",
            "epoch 11/100, batch 52/75, loss 0.2519\n",
            "epoch 11/100, batch 53/75, loss 9.8151\n",
            "epoch 11/100, batch 54/75, loss 0.3072\n",
            "epoch 11/100, batch 55/75, loss 0.2704\n",
            "epoch 11/100, batch 56/75, loss 0.2798\n",
            "epoch 11/100, batch 57/75, loss 0.2870\n",
            "epoch 11/100, batch 58/75, loss 0.3295\n",
            "epoch 11/100, batch 59/75, loss 0.3221\n",
            "epoch 11/100, batch 60/75, loss 0.3253\n",
            "epoch 11/100, batch 61/75, loss 0.3370\n",
            "epoch 11/100, batch 62/75, loss 0.3648\n",
            "epoch 11/100, batch 63/75, loss 0.3279\n",
            "epoch 11/100, batch 64/75, loss 0.3677\n",
            "epoch 11/100, batch 65/75, loss 0.3601\n",
            "epoch 11/100, batch 66/75, loss 0.3353\n",
            "epoch 11/100, batch 67/75, loss 0.3174\n",
            "epoch 11/100, batch 68/75, loss 0.3219\n",
            "epoch 11/100, batch 69/75, loss 0.3207\n",
            "epoch 11/100, batch 70/75, loss 0.2622\n",
            "epoch 11/100, batch 71/75, loss 0.2674\n",
            "epoch 11/100, batch 72/75, loss 0.2852\n",
            "epoch 11/100, batch 73/75, loss 0.2760\n",
            "epoch 11/100, batch 74/75, loss 0.2895\n",
            "epoch 11/100, batch 75/75, loss 0.2416\n",
            "epoch 11/100, training roc_auc_score 0.9440\n",
            "EarlyStopping counter: 9 out of 10\n",
            "epoch 11/100, validation roc_auc_score 0.6949, best validation roc_auc_score 0.7951\n",
            "epoch 12/100, batch 1/75, loss 0.3381\n",
            "epoch 12/100, batch 2/75, loss 0.2892\n",
            "epoch 12/100, batch 3/75, loss 0.2934\n",
            "epoch 12/100, batch 4/75, loss 0.2962\n",
            "epoch 12/100, batch 5/75, loss 0.2780\n",
            "epoch 12/100, batch 6/75, loss 0.2263\n",
            "epoch 12/100, batch 7/75, loss 0.2469\n",
            "epoch 12/100, batch 8/75, loss 0.5549\n",
            "epoch 12/100, batch 9/75, loss 0.2315\n",
            "epoch 12/100, batch 10/75, loss 0.3622\n",
            "epoch 12/100, batch 11/75, loss 0.3925\n",
            "epoch 12/100, batch 12/75, loss 0.2242\n",
            "epoch 12/100, batch 13/75, loss 0.2513\n",
            "epoch 12/100, batch 14/75, loss 0.2382\n",
            "epoch 12/100, batch 15/75, loss 0.9771\n",
            "epoch 12/100, batch 16/75, loss 0.2355\n",
            "epoch 12/100, batch 17/75, loss 0.2726\n",
            "epoch 12/100, batch 18/75, loss 0.2768\n",
            "epoch 12/100, batch 19/75, loss 0.2705\n",
            "epoch 12/100, batch 20/75, loss 0.5693\n",
            "epoch 12/100, batch 21/75, loss 0.2363\n",
            "epoch 12/100, batch 22/75, loss 0.2174\n",
            "epoch 12/100, batch 23/75, loss 0.2068\n",
            "epoch 12/100, batch 24/75, loss 0.2540\n",
            "epoch 12/100, batch 25/75, loss 0.2404\n",
            "epoch 12/100, batch 26/75, loss 1.5351\n",
            "epoch 12/100, batch 27/75, loss 0.2758\n",
            "epoch 12/100, batch 28/75, loss 0.3450\n",
            "epoch 12/100, batch 29/75, loss 0.2356\n",
            "epoch 12/100, batch 30/75, loss 0.2317\n",
            "epoch 12/100, batch 31/75, loss 0.2437\n",
            "epoch 12/100, batch 32/75, loss 0.2189\n",
            "epoch 12/100, batch 33/75, loss 0.2459\n",
            "epoch 12/100, batch 34/75, loss 0.2493\n",
            "epoch 12/100, batch 35/75, loss 0.2629\n",
            "epoch 12/100, batch 36/75, loss 1.4567\n",
            "epoch 12/100, batch 37/75, loss 0.4154\n",
            "epoch 12/100, batch 38/75, loss 0.2156\n",
            "epoch 12/100, batch 39/75, loss 0.2379\n",
            "epoch 12/100, batch 40/75, loss 0.2319\n",
            "epoch 12/100, batch 41/75, loss 0.2679\n",
            "epoch 12/100, batch 42/75, loss 0.2209\n",
            "epoch 12/100, batch 43/75, loss 0.2340\n",
            "epoch 12/100, batch 44/75, loss 0.2519\n",
            "epoch 12/100, batch 45/75, loss 0.2315\n",
            "epoch 12/100, batch 46/75, loss 0.1877\n",
            "epoch 12/100, batch 47/75, loss 0.2274\n",
            "epoch 12/100, batch 48/75, loss 0.2032\n",
            "epoch 12/100, batch 49/75, loss 0.1995\n",
            "epoch 12/100, batch 50/75, loss 0.2282\n",
            "epoch 12/100, batch 51/75, loss 0.2325\n",
            "epoch 12/100, batch 52/75, loss 0.2063\n",
            "epoch 12/100, batch 53/75, loss 9.6289\n",
            "epoch 12/100, batch 54/75, loss 0.2438\n",
            "epoch 12/100, batch 55/75, loss 0.2449\n",
            "epoch 12/100, batch 56/75, loss 0.2469\n",
            "epoch 12/100, batch 57/75, loss 0.2532\n",
            "epoch 12/100, batch 58/75, loss 0.3341\n",
            "epoch 12/100, batch 59/75, loss 0.3009\n",
            "epoch 12/100, batch 60/75, loss 0.3195\n",
            "epoch 12/100, batch 61/75, loss 0.3406\n",
            "epoch 12/100, batch 62/75, loss 0.3870\n",
            "epoch 12/100, batch 63/75, loss 0.3573\n",
            "epoch 12/100, batch 64/75, loss 0.3862\n",
            "epoch 12/100, batch 65/75, loss 0.3634\n",
            "epoch 12/100, batch 66/75, loss 0.3345\n",
            "epoch 12/100, batch 67/75, loss 0.3128\n",
            "epoch 12/100, batch 68/75, loss 0.3167\n",
            "epoch 12/100, batch 69/75, loss 0.3255\n",
            "epoch 12/100, batch 70/75, loss 0.2817\n",
            "epoch 12/100, batch 71/75, loss 0.2480\n",
            "epoch 12/100, batch 72/75, loss 0.2894\n",
            "epoch 12/100, batch 73/75, loss 0.2647\n",
            "epoch 12/100, batch 74/75, loss 0.2584\n",
            "epoch 12/100, batch 75/75, loss 0.2526\n",
            "epoch 12/100, training roc_auc_score 0.9512\n",
            "EarlyStopping counter: 10 out of 10\n",
            "epoch 12/100, validation roc_auc_score 0.7739, best validation roc_auc_score 0.7951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YaWpHl2Gnr-",
        "colab_type": "code",
        "outputId": "f28df7d7-48ad-4456-b790-0ba8288eab9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "stopper.load_checkpoint(model)\n",
        "test_score = run_an_eval_epoch(args, model, test_loader)\n",
        "print('test {} {:.4f}'.format(args['metric_name'], test_score))\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test roc_auc_score 0.5591\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLkl062SLtaF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}