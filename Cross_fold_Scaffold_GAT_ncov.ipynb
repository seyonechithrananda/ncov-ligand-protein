{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cross_fold_Scaffold_GAT_ncov.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1YtIgyEBjDODvdXZ8FZqcgu8tqpVFE_Qq",
      "authorship_tag": "ABX9TyM0hN7P3shpwN6eCmUhhriv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seyonechithrananda/ncov-ligand-protein/blob/master/Cross_fold_Scaffold_GAT_ncov.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnhZ7HBLDf73",
        "colab_type": "code",
        "outputId": "04ad0544-975c-479c-99ff-e5c8ac9a343d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!wget -c https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!conda install -q -y -c conda-forge rdkit\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-07 02:51:02--  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.130.3, 104.16.131.3, 2606:4700::6810:8303, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.130.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 85055499 (81M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-latest-Linux-x86_64.sh’\n",
            "\n",
            "\r          Miniconda   0%[                    ]       0  --.-KB/s               \r         Miniconda3  45%[========>           ]  36.87M   184MB/s               \r        Miniconda3-  93%[=================>  ]  75.52M   189MB/s               \rMiniconda3-latest-L 100%[===================>]  81.12M   190MB/s    in 0.4s    \n",
            "\n",
            "2020-05-07 02:51:02 (190 MB/s) - ‘Miniconda3-latest-Linux-x86_64.sh’ saved [85055499/85055499]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - _libgcc_mutex==0.1=main\n",
            "    - asn1crypto==1.3.0=py37_0\n",
            "    - ca-certificates==2020.1.1=0\n",
            "    - certifi==2019.11.28=py37_0\n",
            "    - cffi==1.14.0=py37h2e261b9_0\n",
            "    - chardet==3.0.4=py37_1003\n",
            "    - conda-package-handling==1.6.0=py37h7b6447c_0\n",
            "    - conda==4.8.2=py37_0\n",
            "    - cryptography==2.8=py37h1ba5d50_0\n",
            "    - idna==2.8=py37_0\n",
            "    - ld_impl_linux-64==2.33.1=h53a641e_7\n",
            "    - libedit==3.1.20181209=hc058e9b_0\n",
            "    - libffi==3.2.1=hd88cf55_4\n",
            "    - libgcc-ng==9.1.0=hdf63c60_0\n",
            "    - libstdcxx-ng==9.1.0=hdf63c60_0\n",
            "    - ncurses==6.2=he6710b0_0\n",
            "    - openssl==1.1.1d=h7b6447c_4\n",
            "    - pip==20.0.2=py37_1\n",
            "    - pycosat==0.6.3=py37h7b6447c_0\n",
            "    - pycparser==2.19=py37_0\n",
            "    - pyopenssl==19.1.0=py37_0\n",
            "    - pysocks==1.7.1=py37_0\n",
            "    - python==3.7.6=h0371630_2\n",
            "    - readline==7.0=h7b6447c_5\n",
            "    - requests==2.22.0=py37_1\n",
            "    - ruamel_yaml==0.15.87=py37h7b6447c_0\n",
            "    - setuptools==45.2.0=py37_0\n",
            "    - six==1.14.0=py37_0\n",
            "    - sqlite==3.31.1=h7b6447c_0\n",
            "    - tk==8.6.8=hbc83047_0\n",
            "    - tqdm==4.42.1=py_0\n",
            "    - urllib3==1.25.8=py37_0\n",
            "    - wheel==0.34.2=py37_0\n",
            "    - xz==5.2.4=h14c3975_4\n",
            "    - yaml==0.1.7=had09818_2\n",
            "    - zlib==1.2.11=h7b6447c_3\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
            "  asn1crypto         pkgs/main/linux-64::asn1crypto-1.3.0-py37_0\n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2020.1.1-0\n",
            "  certifi            pkgs/main/linux-64::certifi-2019.11.28-py37_0\n",
            "  cffi               pkgs/main/linux-64::cffi-1.14.0-py37h2e261b9_0\n",
            "  chardet            pkgs/main/linux-64::chardet-3.0.4-py37_1003\n",
            "  conda              pkgs/main/linux-64::conda-4.8.2-py37_0\n",
            "  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1.6.0-py37h7b6447c_0\n",
            "  cryptography       pkgs/main/linux-64::cryptography-2.8-py37h1ba5d50_0\n",
            "  idna               pkgs/main/linux-64::idna-2.8-py37_0\n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.33.1-h53a641e_7\n",
            "  libedit            pkgs/main/linux-64::libedit-3.1.20181209-hc058e9b_0\n",
            "  libffi             pkgs/main/linux-64::libffi-3.2.1-hd88cf55_4\n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-9.1.0-hdf63c60_0\n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-9.1.0-hdf63c60_0\n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.2-he6710b0_0\n",
            "  openssl            pkgs/main/linux-64::openssl-1.1.1d-h7b6447c_4\n",
            "  pip                pkgs/main/linux-64::pip-20.0.2-py37_1\n",
            "  pycosat            pkgs/main/linux-64::pycosat-0.6.3-py37h7b6447c_0\n",
            "  pycparser          pkgs/main/linux-64::pycparser-2.19-py37_0\n",
            "  pyopenssl          pkgs/main/linux-64::pyopenssl-19.1.0-py37_0\n",
            "  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py37_0\n",
            "  python             pkgs/main/linux-64::python-3.7.6-h0371630_2\n",
            "  readline           pkgs/main/linux-64::readline-7.0-h7b6447c_5\n",
            "  requests           pkgs/main/linux-64::requests-2.22.0-py37_1\n",
            "  ruamel_yaml        pkgs/main/linux-64::ruamel_yaml-0.15.87-py37h7b6447c_0\n",
            "  setuptools         pkgs/main/linux-64::setuptools-45.2.0-py37_0\n",
            "  six                pkgs/main/linux-64::six-1.14.0-py37_0\n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.31.1-h7b6447c_0\n",
            "  tk                 pkgs/main/linux-64::tk-8.6.8-hbc83047_0\n",
            "  tqdm               pkgs/main/noarch::tqdm-4.42.1-py_0\n",
            "  urllib3            pkgs/main/linux-64::urllib3-1.25.8-py37_0\n",
            "  wheel              pkgs/main/linux-64::wheel-0.34.2-py37_0\n",
            "  xz                 pkgs/main/linux-64::xz-5.2.4-h14c3975_4\n",
            "  yaml               pkgs/main/linux-64::yaml-0.1.7-had09818_2\n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.11-h7b6447c_3\n",
            "\n",
            "\n",
            "Preparing transaction: / \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - rdkit\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    boost-1.72.0               |   py37h9de70de_0         316 KB  conda-forge\n",
            "    boost-cpp-1.72.0           |       h8e57a91_0        21.8 MB  conda-forge\n",
            "    bzip2-1.0.8                |       h516909a_2         396 KB  conda-forge\n",
            "    ca-certificates-2020.4.5.1 |       hecc5488_0         146 KB  conda-forge\n",
            "    cairo-1.16.0               |    hcf35c78_1003         1.5 MB  conda-forge\n",
            "    certifi-2020.4.5.1         |   py37hc8dfbb8_0         151 KB  conda-forge\n",
            "    conda-4.8.3                |   py37hc8dfbb8_1         3.0 MB  conda-forge\n",
            "    fontconfig-2.13.1          |    h86ecdb6_1001         340 KB  conda-forge\n",
            "    freetype-2.10.1            |       he06d7ca_0         877 KB  conda-forge\n",
            "    gettext-0.19.8.1           |    hc5be6a0_1002         3.6 MB  conda-forge\n",
            "    glib-2.64.2                |       h6f030ca_0         3.4 MB  conda-forge\n",
            "    icu-64.2                   |       he1b5a44_1        12.6 MB  conda-forge\n",
            "    jpeg-9c                    |    h14c3975_1001         251 KB  conda-forge\n",
            "    libblas-3.8.0              |      14_openblas          10 KB  conda-forge\n",
            "    libcblas-3.8.0             |      14_openblas          10 KB  conda-forge\n",
            "    libgfortran-ng-7.3.0       |       hdf63c60_5         1.7 MB  conda-forge\n",
            "    libiconv-1.15              |    h516909a_1006         2.0 MB  conda-forge\n",
            "    liblapack-3.8.0            |      14_openblas          10 KB  conda-forge\n",
            "    libopenblas-0.3.7          |       h5ec1e0e_6         7.6 MB  conda-forge\n",
            "    libpng-1.6.37              |       hed695b0_1         308 KB  conda-forge\n",
            "    libtiff-4.1.0              |       hc7e4089_6         668 KB  conda-forge\n",
            "    libuuid-2.32.1             |    h14c3975_1000          26 KB  conda-forge\n",
            "    libwebp-base-1.1.0         |       h516909a_3         845 KB  conda-forge\n",
            "    libxcb-1.13                |    h14c3975_1002         396 KB  conda-forge\n",
            "    libxml2-2.9.10             |       hee79883_0         1.3 MB  conda-forge\n",
            "    lz4-c-1.8.3                |    he1b5a44_1001         187 KB  conda-forge\n",
            "    numpy-1.18.4               |   py37h8960a57_0         5.2 MB  conda-forge\n",
            "    olefile-0.46               |             py_0          31 KB  conda-forge\n",
            "    openssl-1.1.1g             |       h516909a_0         2.1 MB  conda-forge\n",
            "    pandas-1.0.3               |   py37h0da4684_1        11.1 MB  conda-forge\n",
            "    pcre-8.44                  |       he1b5a44_0         261 KB  conda-forge\n",
            "    pillow-7.1.2               |   py37hb39fc2d_0         603 KB\n",
            "    pixman-0.38.0              |    h516909a_1003         594 KB  conda-forge\n",
            "    pthread-stubs-0.4          |    h14c3975_1001           5 KB  conda-forge\n",
            "    pycairo-1.19.1             |   py37h01af8b0_3          77 KB  conda-forge\n",
            "    python-dateutil-2.8.1      |             py_0         220 KB  conda-forge\n",
            "    python_abi-3.7             |          1_cp37m           4 KB  conda-forge\n",
            "    pytz-2020.1                |     pyh9f0ad1d_0         227 KB  conda-forge\n",
            "    rdkit-2020.03.1            |   py37hdd87690_3        24.6 MB  conda-forge\n",
            "    xorg-kbproto-1.0.7         |    h14c3975_1002          26 KB  conda-forge\n",
            "    xorg-libice-1.0.10         |       h516909a_0          57 KB  conda-forge\n",
            "    xorg-libsm-1.2.3           |    h84519dc_1000          25 KB  conda-forge\n",
            "    xorg-libx11-1.6.9          |       h516909a_0         918 KB  conda-forge\n",
            "    xorg-libxau-1.0.9          |       h14c3975_0          13 KB  conda-forge\n",
            "    xorg-libxdmcp-1.1.3        |       h516909a_0          18 KB  conda-forge\n",
            "    xorg-libxext-1.3.4         |       h516909a_0          51 KB  conda-forge\n",
            "    xorg-libxrender-0.9.10     |    h516909a_1002          31 KB  conda-forge\n",
            "    xorg-renderproto-0.11.1    |    h14c3975_1002           8 KB  conda-forge\n",
            "    xorg-xextproto-7.3.0       |    h14c3975_1002          27 KB  conda-forge\n",
            "    xorg-xproto-7.0.31         |    h14c3975_1007          72 KB  conda-forge\n",
            "    zstd-1.4.4                 |       h3b9ef0a_2         982 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       110.7 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  boost              conda-forge/linux-64::boost-1.72.0-py37h9de70de_0\n",
            "  boost-cpp          conda-forge/linux-64::boost-cpp-1.72.0-h8e57a91_0\n",
            "  bzip2              conda-forge/linux-64::bzip2-1.0.8-h516909a_2\n",
            "  cairo              conda-forge/linux-64::cairo-1.16.0-hcf35c78_1003\n",
            "  fontconfig         conda-forge/linux-64::fontconfig-2.13.1-h86ecdb6_1001\n",
            "  freetype           conda-forge/linux-64::freetype-2.10.1-he06d7ca_0\n",
            "  gettext            conda-forge/linux-64::gettext-0.19.8.1-hc5be6a0_1002\n",
            "  glib               conda-forge/linux-64::glib-2.64.2-h6f030ca_0\n",
            "  icu                conda-forge/linux-64::icu-64.2-he1b5a44_1\n",
            "  jpeg               conda-forge/linux-64::jpeg-9c-h14c3975_1001\n",
            "  libblas            conda-forge/linux-64::libblas-3.8.0-14_openblas\n",
            "  libcblas           conda-forge/linux-64::libcblas-3.8.0-14_openblas\n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-7.3.0-hdf63c60_5\n",
            "  libiconv           conda-forge/linux-64::libiconv-1.15-h516909a_1006\n",
            "  liblapack          conda-forge/linux-64::liblapack-3.8.0-14_openblas\n",
            "  libopenblas        conda-forge/linux-64::libopenblas-0.3.7-h5ec1e0e_6\n",
            "  libpng             conda-forge/linux-64::libpng-1.6.37-hed695b0_1\n",
            "  libtiff            conda-forge/linux-64::libtiff-4.1.0-hc7e4089_6\n",
            "  libuuid            conda-forge/linux-64::libuuid-2.32.1-h14c3975_1000\n",
            "  libwebp-base       conda-forge/linux-64::libwebp-base-1.1.0-h516909a_3\n",
            "  libxcb             conda-forge/linux-64::libxcb-1.13-h14c3975_1002\n",
            "  libxml2            conda-forge/linux-64::libxml2-2.9.10-hee79883_0\n",
            "  lz4-c              conda-forge/linux-64::lz4-c-1.8.3-he1b5a44_1001\n",
            "  numpy              conda-forge/linux-64::numpy-1.18.4-py37h8960a57_0\n",
            "  olefile            conda-forge/noarch::olefile-0.46-py_0\n",
            "  pandas             conda-forge/linux-64::pandas-1.0.3-py37h0da4684_1\n",
            "  pcre               conda-forge/linux-64::pcre-8.44-he1b5a44_0\n",
            "  pillow             pkgs/main/linux-64::pillow-7.1.2-py37hb39fc2d_0\n",
            "  pixman             conda-forge/linux-64::pixman-0.38.0-h516909a_1003\n",
            "  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-h14c3975_1001\n",
            "  pycairo            conda-forge/linux-64::pycairo-1.19.1-py37h01af8b0_3\n",
            "  python-dateutil    conda-forge/noarch::python-dateutil-2.8.1-py_0\n",
            "  python_abi         conda-forge/linux-64::python_abi-3.7-1_cp37m\n",
            "  pytz               conda-forge/noarch::pytz-2020.1-pyh9f0ad1d_0\n",
            "  rdkit              conda-forge/linux-64::rdkit-2020.03.1-py37hdd87690_3\n",
            "  xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-h14c3975_1002\n",
            "  xorg-libice        conda-forge/linux-64::xorg-libice-1.0.10-h516909a_0\n",
            "  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.3-h84519dc_1000\n",
            "  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.6.9-h516909a_0\n",
            "  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.9-h14c3975_0\n",
            "  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.3-h516909a_0\n",
            "  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.4-h516909a_0\n",
            "  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.10-h516909a_1002\n",
            "  xorg-renderproto   conda-forge/linux-64::xorg-renderproto-0.11.1-h14c3975_1002\n",
            "  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h14c3975_1002\n",
            "  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h14c3975_1007\n",
            "  zstd               conda-forge/linux-64::zstd-1.4.4-h3b9ef0a_2\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates     pkgs/main::ca-certificates-2020.1.1-0 --> conda-forge::ca-certificates-2020.4.5.1-hecc5488_0\n",
            "  certifi              pkgs/main::certifi-2019.11.28-py37_0 --> conda-forge::certifi-2020.4.5.1-py37hc8dfbb8_0\n",
            "  conda                       pkgs/main::conda-4.8.2-py37_0 --> conda-forge::conda-4.8.3-py37hc8dfbb8_1\n",
            "  openssl              pkgs/main::openssl-1.1.1d-h7b6447c_4 --> conda-forge::openssl-1.1.1g-h516909a_0\n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxPjaPjsGNGb",
        "colab_type": "code",
        "outputId": "6f6683b1-1b62-4e5a-b5b2-59b28df39274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        }
      },
      "source": [
        "!conda install -c dglteam dgl-cuda10.1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - dgl-cuda10.1\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    blas-1.0                   |         openblas          46 KB\n",
            "    certifi-2020.4.5.1         |           py37_0         155 KB\n",
            "    decorator-4.4.2            |             py_0          14 KB\n",
            "    dgl-cuda10.1-0.4.3post2    |           py37_0        11.2 MB  dglteam\n",
            "    networkx-2.4               |             py_0         1.2 MB\n",
            "    scipy-1.4.1                |   py37habc2bb6_0        14.6 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        27.1 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  blas               pkgs/main/linux-64::blas-1.0-openblas\n",
            "  decorator          pkgs/main/noarch::decorator-4.4.2-py_0\n",
            "  dgl-cuda10.1       dglteam/linux-64::dgl-cuda10.1-0.4.3post2-py37_0\n",
            "  networkx           pkgs/main/noarch::networkx-2.4-py_0\n",
            "  scipy              pkgs/main/linux-64::scipy-1.4.1-py37habc2bb6_0\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  certifi            conda-forge::certifi-2020.4.5.1-py37h~ --> pkgs/main::certifi-2020.4.5.1-py37_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "networkx-2.4         | 1.2 MB    | : 100% 1.0/1 [00:00<00:00,  3.33it/s]               \n",
            "dgl-cuda10.1-0.4.3po | 11.2 MB   | : 100% 1.0/1 [00:07<00:00, 113.24s/it]               \n",
            "scipy-1.4.1          | 14.6 MB   | : 100% 1.0/1 [00:00<00:00,  2.31it/s]               \n",
            "decorator-4.4.2      | 14 KB     | : 100% 1.0/1 [00:00<00:00, 24.04it/s]\n",
            "blas-1.0             | 46 KB     | : 100% 1.0/1 [00:00<00:00, 20.44it/s]\n",
            "certifi-2020.4.5.1   | 155 KB    | : 100% 1.0/1 [00:00<00:00, 20.95it/s]\n",
            "Preparing transaction: | \b\b/ \b\bdone\n",
            "Verifying transaction: \\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ba7Nw5eGUTr",
        "colab_type": "code",
        "outputId": "4297eb3a-7109-4db8-a4e3-2a6a9ea26751",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        }
      },
      "source": [
        "!conda install -c dglteam dgllife\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - dgllife\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    dgllife-0.2.1              |           py37_0         132 KB  dglteam\n",
            "    joblib-0.14.1              |             py_0         201 KB\n",
            "    scikit-learn-0.22.1        |   py37h22eb022_0         5.3 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         5.6 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  dgllife            dglteam/linux-64::dgllife-0.2.1-py37_0\n",
            "  joblib             pkgs/main/noarch::joblib-0.14.1-py_0\n",
            "  scikit-learn       pkgs/main/linux-64::scikit-learn-0.22.1-py37h22eb022_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "joblib-0.14.1        | 201 KB    | : 100% 1.0/1 [00:00<00:00,  9.10it/s]\n",
            "scikit-learn-0.22.1  | 5.3 MB    | : 100% 1.0/1 [00:00<00:00,  3.18it/s]\n",
            "dgllife-0.2.1        | 132 KB    | : 100% 1.0/1 [00:01<00:00,  1.69s/it]               \n",
            "Preparing transaction: \\ \b\bdone\n",
            "Verifying transaction: / \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqRn8KexGdiL",
        "colab_type": "code",
        "outputId": "8756699c-722e-4abb-cffd-4d13324a09bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        }
      },
      "source": [
        "!conda install pandas"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - pandas\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    conda-4.8.3                |           py37_0         2.8 MB\n",
            "    openssl-1.1.1g             |       h7b6447c_0         2.5 MB\n",
            "    pandas-1.0.3               |   py37h0573a6f_0         8.6 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        13.9 MB\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  ca-certificates    conda-forge::ca-certificates-2020.4.5~ --> pkgs/main::ca-certificates-2020.1.1-0\n",
            "  conda              conda-forge::conda-4.8.3-py37hc8dfbb8~ --> pkgs/main::conda-4.8.3-py37_0\n",
            "  openssl            conda-forge::openssl-1.1.1g-h516909a_0 --> pkgs/main::openssl-1.1.1g-h7b6447c_0\n",
            "  pandas             conda-forge::pandas-1.0.3-py37h0da468~ --> pkgs/main::pandas-1.0.3-py37h0573a6f_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "pandas-1.0.3         | 8.6 MB    | : 100% 1.0/1 [00:00<00:00,  1.18it/s]                \n",
            "conda-4.8.3          | 2.8 MB    | : 100% 1.0/1 [00:00<00:00,  6.42it/s]\n",
            "openssl-1.1.1g       | 2.5 MB    | : 100% 1.0/1 [00:00<00:00,  8.38it/s]\n",
            "Preparing transaction: / \b\b- \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\bdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVKD7kwtHDUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "import sys \n",
        "import pandas as pd\n",
        "\n",
        "# train --> balanced dataset\n",
        "dataset_train_file = \"/content/drive/My Drive/Project De Novo/AID1706_binarized_sars_full_eval_actives_12k_samples.csv\"\n",
        "dataset_eval_file = \"/content/drive/My Drive/Project De Novo/mpro_xchem.csv\"\n",
        "dataset_train = pd.read_csv(dataset_train_file)\n",
        "dataset_eval = pd.read_csv(dataset_eval_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy7_zDz9LCq2",
        "colab_type": "code",
        "outputId": "301484fe-e6d7-4042-a638-2db66a9cb59e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "dataset_train.head"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                                   smiles  activity\n",
              "0      C1CC(C1)C(=O)NC2=CC=C(C=C2)N(C(C3=CC(=CC=C3)F)...         1\n",
              "1      CC(C)(C)C1=CC=C(C=C1)N(C(C2=CN=NC=C2)C(=O)NC(C...         1\n",
              "2      CC(C)(C)NC(=O)C(C1=CSC=C1)N(C2=CC=C(C=C2)N)C(=...         1\n",
              "3      CC(C)C(=O)NC1=CC=C(C=C1)N(C(C2=CSC=C2)C(=O)NC(...         1\n",
              "4      CC(C)C(=O)NC1=CC=C(C=C1)N(CC2=CSC=C2)C(=O)CN3C...         1\n",
              "...                                                  ...       ...\n",
              "11994                               C1=CC2=C(C=C1N)NN=C2         0\n",
              "11995  CC(=O)[C@H]1CC[C@@H]2[C@@]1(CC(=O)[C@H]3[C@H]2...         0\n",
              "11996                       C1CN(CCN1CC(CO)O)C2=CC=CC=C2         0\n",
              "11997  CCOC(=O)N1CCC(=C2C3=C(CCC4=C2N=CC=C4)C=C(C=C3)...         0\n",
              "11998                    C1=CC2=C(C=C1OC(F)(F)F)SC(=N2)N         0\n",
              "\n",
              "[11999 rows x 2 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBnPl4isM1FQ",
        "colab_type": "code",
        "outputId": "66c81c2d-840a-4bd8-fbfe-51b1229aef4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "dataset_eval.head"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                 smiles  activity\n",
              "0      OC=1C=CC=CC1CNC2=NC=3C=CC=CC3N2         1\n",
              "1        CC(=O)NCCC1=CNC=2C=CC(F)=CC12         1\n",
              "2    O=C([C@@H]1[C@H](C2=CSC=C2)CCC1)N         1\n",
              "3       CN1CCCC=2C=CC(=CC12)S(=O)(=O)N         1\n",
              "4     CC(=O)NC=1C=CC(OC=2N=CC=CN2)=CC1         1\n",
              "..                                 ...       ...\n",
              "875   CC(C)C=1C=CC(NC(=O)N2CCOCC2)=CC1         0\n",
              "876        CN(CC(=O)O)C(=O)C=1C=CC=CN1         0\n",
              "877  CN1CCN(CC1)C(=O)C=2C=CC(F)=C(F)C2         0\n",
              "878      FC=1C=CC=C(F)C1C(=O)N2CCCCCC2         0\n",
              "879             FC=1C=CC=NC1NCC2CCOCC2         0\n",
              "\n",
              "[880 rows x 2 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVrUoqtTHGy8",
        "colab_type": "code",
        "outputId": "8ff1ac2d-f186-4470-baa1-9cb742294321",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "from dgllife.data import MoleculeCSVDataset\n",
        "from dgllife.data.csv_dataset import *\n",
        "from dgllife.utils.featurizers import *\n",
        "from dgllife.utils.mol_to_graph import *\n",
        "\n",
        "# featurize bigraph/molecular graph set for train (SARS-COV-1) set\n",
        "train_set = MoleculeCSVDataset(dataset_train, smiles_to_graph=smiles_to_bigraph, node_featurizer=CanonicalAtomFeaturizer(),\n",
        "                               edge_featurizer=CanonicalBondFeaturizer(), smiles_column='smiles', cache_file_path='/content/drive/My Drive/Project De Novo/graph_featurizer/train.bin', task_names=['activity'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Using backend: pytorch\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "Loading previously saved dgl graphs...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofwpNYUXJASi",
        "colab_type": "code",
        "outputId": "74725c4c-cc0c-40dd-f8e0-3d0c1847ea07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# featurize bigraph/molecular graph set for test (SARS-COV-2) set\n",
        "test_set = MoleculeCSVDataset(dataset_eval, smiles_to_graph=smiles_to_bigraph, node_featurizer=CanonicalAtomFeaturizer(),\n",
        "                               edge_featurizer=CanonicalBondFeaturizer(), smiles_column='smiles', cache_file_path='/content/drive/My Drive/Project De Novo/graph_featurizer/test.bin', task_names=['activity'])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading previously saved dgl graphs...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1TtgCu8U26K",
        "colab_type": "code",
        "outputId": "67b4ab9a-d0ef-47d0-aff9-0a63fe975235",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ad2112c2-e3c2-4a30-8737-b319f9b0b7d2\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-ad2112c2-e3c2-4a30-8737-b319f9b0b7d2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving utils.py to utils.py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'utils.py': b'import dgl\\nimport numpy as np\\nimport random\\nimport torch\\n\\nfrom dgllife.utils.featurizers import one_hot_encoding\\nfrom dgllife.utils.splitters import RandomSplitter\\n\\ndef set_random_seed(seed=0):\\n    \"\"\"Set random seed.\\n    Parameters\\n    ----------\\n    seed : int\\n        Random seed to use\\n    \"\"\"\\n    random.seed(seed)\\n    np.random.seed(seed)\\n    torch.manual_seed(seed)\\n    if torch.cuda.is_available():\\n        torch.cuda.manual_seed(seed)\\n\\n\\ndef load_dataset_for_classification(args):\\n    \"\"\"Load dataset for classification tasks.\\n    Parameters\\n    ----------\\n    args : dict\\n        Configurations.\\n    Returns\\n    -------\\n    dataset\\n        The whole dataset.\\n    train_set\\n        Subset for training.\\n    val_set\\n        Subset for validation.\\n    test_set\\n        Subset for test.\\n    \"\"\"\\n    assert args[\\'dataset\\'] in [\\'Tox21\\']\\n    if args[\\'dataset\\'] == \\'Tox21\\':\\n        from dgllife.data import Tox21\\n        dataset = Tox21(smiles_to_graph=args[\\'smiles_to_graph\\'],\\n                        node_featurizer=args.get(\\'node_featurizer\\', None),\\n                        edge_featurizer=args.get(\\'edge_featurizer\\', None))\\n        train_set, val_set, test_set = RandomSplitter.train_val_test_split(\\n            dataset, frac_train=args[\\'frac_train\\'], frac_val=args[\\'frac_val\\'],\\n            frac_test=args[\\'frac_test\\'], random_state=args[\\'random_seed\\'])\\n\\n    return dataset, train_set, val_set, test_set\\n\\n\\ndef load_dataset_for_regression(args):\\n    \"\"\"Load dataset for regression tasks.\\n    Parameters\\n    ----------\\n    args : dict\\n        Configurations.\\n    Returns\\n    -------\\n    train_set\\n        Subset for training.\\n    val_set\\n        Subset for validation.\\n    test_set\\n        Subset for test.\\n    \"\"\"\\n    assert args[\\'dataset\\'] in [\\'Alchemy\\', \\'Aromaticity\\']\\n\\n    if args[\\'dataset\\'] == \\'Alchemy\\':\\n        from dgllife.data import TencentAlchemyDataset\\n        train_set = TencentAlchemyDataset(mode=\\'dev\\')\\n        val_set = TencentAlchemyDataset(mode=\\'valid\\')\\n        test_set = None\\n\\n    if args[\\'dataset\\'] == \\'Aromaticity\\':\\n        from dgllife.data import PubChemBioAssayAromaticity\\n        dataset = PubChemBioAssayAromaticity(smiles_to_graph=args[\\'smiles_to_graph\\'],\\n                                             node_featurizer=args.get(\\'node_featurizer\\', None),\\n                                             edge_featurizer=args.get(\\'edge_featurizer\\', None))\\n        train_set, val_set, test_set = RandomSplitter.train_val_test_split(\\n            dataset, frac_train=args[\\'frac_train\\'], frac_val=args[\\'frac_val\\'],\\n            frac_test=args[\\'frac_test\\'], random_state=args[\\'random_seed\\'])\\n\\n    return train_set, val_set, test_set\\n\\n\\ndef collate_molgraphs(data):\\n    \"\"\"Batching a list of datapoints for dataloader.\\n    Parameters\\n    ----------\\n    data : list of 3-tuples or 4-tuples.\\n        Each tuple is for a single datapoint, consisting of\\n        a SMILES, a DGLGraph, all-task labels and optionally\\n        a binary mask indicating the existence of labels.\\n    Returns\\n    -------\\n    smiles : list\\n        List of smiles\\n    bg : DGLGraph\\n        The batched DGLGraph.\\n    labels : Tensor of dtype float32 and shape (B, T)\\n        Batched datapoint labels. B is len(data) and\\n        T is the number of total tasks.\\n    masks : Tensor of dtype float32 and shape (B, T)\\n        Batched datapoint binary mask, indicating the\\n        existence of labels. If binary masks are not\\n        provided, return a tensor with ones.\\n    \"\"\"\\n    assert len(data[0]) in [3, 4], \\\\\\n        \\'Expect the tuple to be of length 3 or 4, got {:d}\\'.format(len(data[0]))\\n    if len(data[0]) == 3:\\n        smiles, graphs, labels = map(list, zip(*data))\\n        masks = None\\n    else:\\n        smiles, graphs, labels, masks = map(list, zip(*data))\\n\\n    bg = dgl.batch(graphs)\\n    bg.set_n_initializer(dgl.init.zero_initializer)\\n    bg.set_e_initializer(dgl.init.zero_initializer)\\n    labels = torch.stack(labels, dim=0)\\n\\n    if masks is None:\\n        masks = torch.ones(labels.shape)\\n    else:\\n        masks = torch.stack(masks, dim=0)\\n    return smiles, bg, labels, masks\\n\\n\\ndef load_model(args):\\n    if args[\\'model\\'] == \\'GCN\\':\\n        from dgllife.model import GCNPredictor\\n        model = GCNPredictor(in_feats=args[\\'node_featurizer\\'].feat_size(),\\n                             hidden_feats=args[\\'gcn_hidden_feats\\'],\\n                             classifier_hidden_feats=args[\\'classifier_hidden_feats\\'],\\n                             n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'GAT\\':\\n        from dgllife.model import GATPredictor\\n        model = GATPredictor(in_feats=args[\\'node_featurizer\\'].feat_size(),\\n                             hidden_feats=args[\\'gat_hidden_feats\\'],\\n                             num_heads=args[\\'num_heads\\'],\\n                             classifier_hidden_feats=args[\\'classifier_hidden_feats\\'],\\n                             n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'Weave\\':\\n        from dgllife.model import WeavePredictor\\n        model = WeavePredictor(node_in_feats=args[\\'node_featurizer\\'].feat_size(),\\n                               edge_in_feats=args[\\'edge_featurizer\\'].feat_size(),\\n                               num_gnn_layers=args[\\'num_gnn_layers\\'],\\n                               gnn_hidden_feats=args[\\'gnn_hidden_feats\\'],\\n                               graph_feats=args[\\'graph_feats\\'],\\n                               n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'AttentiveFP\\':\\n        from dgllife.model import AttentiveFPPredictor\\n        model = AttentiveFPPredictor(node_feat_size=args[\\'node_featurizer\\'].feat_size(),\\n                                     edge_feat_size=args[\\'edge_featurizer\\'].feat_size(),\\n                                     num_layers=args[\\'num_layers\\'],\\n                                     num_timesteps=args[\\'num_timesteps\\'],\\n                                     graph_feat_size=args[\\'graph_feat_size\\'],\\n                                     n_tasks=args[\\'n_tasks\\'],\\n                                     dropout=args[\\'dropout\\'])\\n\\n    if args[\\'model\\'] == \\'SchNet\\':\\n        from dgllife.model import SchNetPredictor\\n        model = SchNetPredictor(node_feats=args[\\'node_feats\\'],\\n                                hidden_feats=args[\\'hidden_feats\\'],\\n                                classifier_hidden_feats=args[\\'classifier_hidden_feats\\'],\\n                                n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'MGCN\\':\\n        from dgllife.model import MGCNPredictor\\n        model = MGCNPredictor(feats=args[\\'feats\\'],\\n                              n_layers=args[\\'n_layers\\'],\\n                              classifier_hidden_feats=args[\\'classifier_hidden_feats\\'],\\n                              n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'MPNN\\':\\n        from dgllife.model import MPNNPredictor\\n        model = MPNNPredictor(node_in_feats=args[\\'node_in_feats\\'],\\n                              edge_in_feats=args[\\'edge_in_feats\\'],\\n                              node_out_feats=args[\\'node_out_feats\\'],\\n                              edge_hidden_feats=args[\\'edge_hidden_feats\\'],\\n                              n_tasks=args[\\'n_tasks\\'])\\n\\n    return model\\n\\n\\ndef chirality(atom):\\n    try:\\n        return one_hot_encoding(atom.GetProp(\\'_CIPCode\\'), [\\'R\\', \\'S\\']) + \\\\\\n               [atom.HasProp(\\'_ChiralityPossible\\')]\\n    except:\\n        return [False, False] + [atom.HasProp(\\'_ChiralityPossible\\')]\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKMgchzflhiW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = {\n",
        "    'random_seed': 2,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-3,\n",
        "    'num_epochs': 250,\n",
        "    'node_data_field': 'h',\n",
        "    'frac_train': 0.8,\n",
        "    'frac_val': 0.1,\n",
        "    'frac_test': 0.1,\n",
        "    'in_feats': 74,\n",
        "    'gat_hidden_feats': [32, 32],\n",
        "    'classifier_hidden_feats': 64,\n",
        "    'num_heads': [4, 4],\n",
        "    'patience': 50,\n",
        "    'smiles_to_graph': smiles_to_bigraph,\n",
        "    'node_featurizer': CanonicalAtomFeaturizer(),\n",
        "    'metric_name': 'roc_auc_score',\n",
        "    'model': 'GAT'\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuZkFAz-PDvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import utils\n",
        "\n",
        "from dgllife.model import load_pretrained\n",
        "from dgllife.utils import EarlyStopping, Meter\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "from utils import set_random_seed, load_dataset_for_classification, collate_molgraphs, load_model\n",
        "\n",
        "from dgllife.model import GATPredictor\n",
        "\n",
        "args['device'] = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "set_random_seed(args['random_seed'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJf2RePJHUcx",
        "colab_type": "code",
        "outputId": "f770b6f3-7ba7-4c07-e542-9f270a5ba3ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        }
      },
      "source": [
        "from dgllife.utils.splitters import ScaffoldSplitter\n",
        "\n",
        "train_scaffold_set, val_set, test_scaffold_set = ScaffoldSplitter.train_val_test_split(train_set, frac_train=0.8, frac_val=0.2,frac_test=0.0)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start initializing RDKit molecule instances...\n",
            "Creating RDKit molecule instance 1000/11999\n",
            "Creating RDKit molecule instance 2000/11999\n",
            "Creating RDKit molecule instance 3000/11999\n",
            "Creating RDKit molecule instance 4000/11999\n",
            "Creating RDKit molecule instance 5000/11999\n",
            "Creating RDKit molecule instance 6000/11999\n",
            "Creating RDKit molecule instance 7000/11999\n",
            "Creating RDKit molecule instance 8000/11999\n",
            "Creating RDKit molecule instance 9000/11999\n",
            "Creating RDKit molecule instance 10000/11999\n",
            "Creating RDKit molecule instance 11000/11999\n",
            "Start computing Bemis-Murcko scaffolds.\n",
            "Computing Bemis-Murcko for compound 1000/11999\n",
            "Computing Bemis-Murcko for compound 2000/11999\n",
            "Computing Bemis-Murcko for compound 3000/11999\n",
            "Computing Bemis-Murcko for compound 4000/11999\n",
            "Computing Bemis-Murcko for compound 5000/11999\n",
            "Computing Bemis-Murcko for compound 6000/11999\n",
            "Computing Bemis-Murcko for compound 7000/11999\n",
            "Computing Bemis-Murcko for compound 8000/11999\n",
            "Computing Bemis-Murcko for compound 9000/11999\n",
            "Computing Bemis-Murcko for compound 10000/11999\n",
            "Computing Bemis-Murcko for compound 11000/11999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMv1d_1wNisL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "outputId": "52e70153-11c2-43c2-f877-00993baffacf"
      },
      "source": [
        "test_fold_set = ScaffoldSplitter.k_fold_split(test_set, k=5, log_every_n=100)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start initializing RDKit molecule instances...\n",
            "Start computing Bemis-Murcko scaffolds.\n",
            "Computing Bemis-Murcko for compound 100/880\n",
            "Computing Bemis-Murcko for compound 200/880\n",
            "Computing Bemis-Murcko for compound 300/880\n",
            "Computing Bemis-Murcko for compound 400/880\n",
            "Computing Bemis-Murcko for compound 500/880\n",
            "Computing Bemis-Murcko for compound 600/880\n",
            "Computing Bemis-Murcko for compound 700/880\n",
            "Computing Bemis-Murcko for compound 800/880\n",
            "Processing fold 1/5\n",
            "Processing fold 2/5\n",
            "Processing fold 3/5\n",
            "Processing fold 4/5\n",
            "Processing fold 5/5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tdnn3NKXQGVZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b55d6ac7-c0db-43e6-a201-3e3239db1b9c"
      },
      "source": [
        "print(type(test_fold_set[4][1]))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'dgl.data.utils.Subset'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCmuQegvJMcq",
        "colab_type": "code",
        "outputId": "3fc36f19-df02-4ca1-92f1-34f892c8e5bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "print (len(train_set))\n",
        "print(len(train_scaffold_set))\n",
        "print (len(val_set))\n",
        "print (len(test_set))\n",
        "print(train_set[1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11999\n",
            "9599\n",
            "2400\n",
            "880\n",
            "('CC(C)(C)C1=CC=C(C=C1)N(C(C2=CN=NC=C2)C(=O)NC(C)(C)C)C(=O)C3=CC=CO3', DGLGraph(num_nodes=32, num_edges=68,\n",
            "         ndata_schemes={'h': Scheme(shape=(74,), dtype=torch.float32)}\n",
            "         edata_schemes={'e': Scheme(shape=(12,), dtype=torch.float32)}), tensor([1.]), tensor([1.]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YjAjoUyLEjT",
        "colab_type": "code",
        "outputId": "2f2aa0ca-286a-4642-a311-42982230e918",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(type(train_set))\n",
        "print(type(train_scaffold_set))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'dgllife.data.csv_dataset.MoleculeCSVDataset'>\n",
            "<class 'dgl.data.utils.Subset'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYfHiYl0T8V4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_scaffold_set,  batch_size=args['batch_size'],\n",
        "                              collate_fn=collate_molgraphs)\n",
        "val_loader = DataLoader(val_set,  batch_size=args['batch_size'],\n",
        "                              collate_fn=collate_molgraphs)\n",
        "\n",
        "test_loader = DataLoader(test_set,  batch_size=args['batch_size'],\n",
        "                          collate_fn=collate_molgraphs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNFE8Aw7OMUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_fold_loader_1 = DataLoader(test_fold_set[0][0], batch_size=args['batch_size'],\n",
        "                          collate_fn=collate_molgraphs)\n",
        "\n",
        "test_fold_loader_2 = DataLoader(test_fold_set[0][1], batch_size=args['batch_size'],\n",
        "                          collate_fn=collate_molgraphs)\n",
        "test_fold_loader_3 = DataLoader(test_fold_set[1][0], batch_size=args['batch_size'],\n",
        "                          collate_fn=collate_molgraphs)\n",
        "test_fold_loader_4 = DataLoader(test_fold_set[1][1], batch_size=args['batch_size'],\n",
        "                          collate_fn=collate_molgraphs)\n",
        "test_fold_loader_5 = DataLoader(test_fold_set[2][0], batch_size=args['batch_size'],\n",
        "                          collate_fn=collate_molgraphs)\n",
        "test_fold_loader_6 = DataLoader(test_fold_set[2][1], batch_size=args['batch_size'],\n",
        "                          collate_fn=collate_molgraphs)\n",
        "test_fold_loader_7 = DataLoader(test_fold_set[3][0], batch_size=args['batch_size'],\n",
        "                          collate_fn=collate_molgraphs)\n",
        "test_fold_loader_8 = DataLoader(test_fold_set[3][1], batch_size=args['batch_size'],\n",
        "                          collate_fn=collate_molgraphs)\n",
        "test_fold_loader_9 = DataLoader(test_fold_set[4][0], batch_size=args['batch_size'],\n",
        "                          collate_fn=collate_molgraphs)\n",
        "test_fold_loader_10 = DataLoader(test_fold_set[4][1], batch_size=args['batch_size'],\n",
        "                          collate_fn=collate_molgraphs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzUMvJO8Qx85",
        "colab_type": "code",
        "outputId": "7d676ee1-8884-47af-85f2-36fafba7fec1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "print(len(test_loader))\n",
        "print(len(test_fold_loader_1))\n",
        "print(len(test_fold_loader_2))\n",
        "print(len(test_fold_loader_3))\n",
        "print(len(test_fold_loader_4))\n",
        "print(len(test_fold_loader_5))\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGAUIdpRUAUu",
        "colab_type": "code",
        "outputId": "504bea07-d20c-4732-a5dc-2968b1f90986",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 725
        }
      },
      "source": [
        "args['n_tasks'] = 1\n",
        "model = GATPredictor(in_feats=args['node_featurizer'].feat_size('h'),\n",
        "                             hidden_feats=args['gat_hidden_feats'],\n",
        "                             num_heads=args['num_heads'],\n",
        "                             classifier_hidden_feats=args['classifier_hidden_feats'],\n",
        "                             n_tasks=args['n_tasks'])\n",
        "\n",
        "import dgl.backend as F\n",
        "\n",
        "train_num_pos = F.sum(train_set.labels, dim=0)\n",
        "train_num_indices = F.sum(train_set.mask, dim=0)\n",
        "train_task_pos_weights = (train_num_indices - train_num_pos) / train_num_pos\n",
        "\n",
        "loss_criterion = BCEWithLogitsLoss(pos_weight=train_task_pos_weights.to(args['device']),\n",
        "                                    reduction='none')\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=args['lr'])\n",
        "stopper = EarlyStopping(patience=args['patience'], mode='higher', filename='/content/drive/My Drive/Project De Novo/GAT/train_scaffold.pth')\n",
        "model.to(args['device'])\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GATPredictor(\n",
              "  (gnn): GAT(\n",
              "    (gnn_layers): ModuleList(\n",
              "      (0): GATLayer(\n",
              "        (gat_conv): GATConv(\n",
              "          (fc): Linear(in_features=74, out_features=128, bias=False)\n",
              "          (feat_drop): Dropout(p=0.0, inplace=False)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
              "          (res_fc): Linear(in_features=74, out_features=128, bias=False)\n",
              "        )\n",
              "      )\n",
              "      (1): GATLayer(\n",
              "        (gat_conv): GATConv(\n",
              "          (fc): Linear(in_features=128, out_features=128, bias=False)\n",
              "          (feat_drop): Dropout(p=0.0, inplace=False)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
              "          (res_fc): Linear(in_features=128, out_features=128, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (readout): WeightedSumAndMax(\n",
              "    (weight_and_sum): WeightAndSum(\n",
              "      (atom_weighting): Sequential(\n",
              "        (0): Linear(in_features=32, out_features=1, bias=True)\n",
              "        (1): Sigmoid()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (predict): MLPPredictor(\n",
              "    (predict): Sequential(\n",
              "      (0): Dropout(p=0.0, inplace=False)\n",
              "      (1): Linear(in_features=64, out_features=64, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (4): Linear(in_features=64, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7Fdo_NxLPWz",
        "colab_type": "code",
        "outputId": "ef24ad09-9bb1-4dba-ba00-555e74b29ad7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(train_task_pos_weights)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([25.9036])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C941OCzRuXH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def predict(args, model, bg):\n",
        "    node_feats = bg.ndata.pop(args['node_data_field']).to(args['device'])\n",
        "    if args.get('edge_featurizer', None) is not None:\n",
        "        edge_feats = bg.edata.pop(args['edge_data_field']).to(args['device'])\n",
        "        return model(bg, node_feats, edge_feats)\n",
        "    else:\n",
        "        return model(bg, node_feats)\n",
        "\n",
        "def run_a_train_epoch(args, epoch, model, data_loader, loss_criterion, optimizer):\n",
        "    model.train()\n",
        "    train_meter = Meter()\n",
        "    for batch_id, batch_data in enumerate(data_loader):\n",
        "        smiles, bg, labels, masks = batch_data\n",
        "        labels, masks = labels.to(args['device']), masks.to(args['device'])\n",
        "        logits = predict(args, model, bg)\n",
        "        # Mask non-existing labels\n",
        "        loss = (loss_criterion(logits, labels) * (masks != 0).float()).mean()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print('epoch {:d}/{:d}, batch {:d}/{:d}, loss {:.4f}'.format(\n",
        "            epoch + 1, args['num_epochs'], batch_id + 1, len(data_loader), loss.item()))\n",
        "        train_meter.update(logits, labels, masks)\n",
        "    train_score = np.mean(train_meter.compute_metric(args['metric_name']))\n",
        "    print('epoch {:d}/{:d}, training {} {:.4f}'.format(\n",
        "        epoch + 1, args['num_epochs'], args['metric_name'], train_score))\n",
        "\n",
        "def run_an_eval_epoch(args, model, data_loader):\n",
        "    model.eval()\n",
        "    eval_meter = Meter()\n",
        "    with torch.no_grad():\n",
        "        for batch_id, batch_data in enumerate(data_loader):\n",
        "            smiles, bg, labels, masks = batch_data\n",
        "            labels = labels.to(args['device'])\n",
        "            logits = predict(args, model, bg)\n",
        "            eval_meter.update(logits, labels, masks)\n",
        "    return np.mean(eval_meter.compute_metric(args['metric_name']))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73ZBLfQxuHQA",
        "colab_type": "code",
        "outputId": "8b54d44c-d22c-416b-e337-6257c4d29976",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(args['num_epochs']):\n",
        "        # Train\n",
        "        run_a_train_epoch(args, epoch, model, train_loader, loss_criterion, optimizer)\n",
        "\n",
        "        # Validation and early stop\n",
        "        val_score = run_an_eval_epoch(args, model, val_loader)\n",
        "        early_stop = stopper.step(val_score, model)\n",
        "        print('epoch {:d}/{:d}, validation {} {:.4f}, best validation {} {:.4f}'.format(\n",
        "            epoch + 1, args['num_epochs'], args['metric_name'],\n",
        "            val_score, args['metric_name'], stopper.best_score))\n",
        "        if early_stop:\n",
        "            break"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero(Tensor input, *, Tensor out)\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(Tensor input, *, bool as_tuple)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1/250, batch 1/75, loss 1.3207\n",
            "epoch 1/250, batch 2/75, loss 1.0233\n",
            "epoch 1/250, batch 3/75, loss 0.8619\n",
            "epoch 1/250, batch 4/75, loss 0.8339\n",
            "epoch 1/250, batch 5/75, loss 0.7956\n",
            "epoch 1/250, batch 6/75, loss 0.7633\n",
            "epoch 1/250, batch 7/75, loss 0.7624\n",
            "epoch 1/250, batch 8/75, loss 1.5332\n",
            "epoch 1/250, batch 9/75, loss 0.7375\n",
            "epoch 1/250, batch 10/75, loss 2.6329\n",
            "epoch 1/250, batch 11/75, loss 1.0011\n",
            "epoch 1/250, batch 12/75, loss 0.7305\n",
            "epoch 1/250, batch 13/75, loss 0.8190\n",
            "epoch 1/250, batch 14/75, loss 0.7276\n",
            "epoch 1/250, batch 15/75, loss 3.1011\n",
            "epoch 1/250, batch 16/75, loss 0.7229\n",
            "epoch 1/250, batch 17/75, loss 0.7199\n",
            "epoch 1/250, batch 18/75, loss 0.7217\n",
            "epoch 1/250, batch 19/75, loss 0.7218\n",
            "epoch 1/250, batch 20/75, loss 2.4895\n",
            "epoch 1/250, batch 21/75, loss 0.7259\n",
            "epoch 1/250, batch 22/75, loss 0.7239\n",
            "epoch 1/250, batch 23/75, loss 0.7188\n",
            "epoch 1/250, batch 24/75, loss 0.7168\n",
            "epoch 1/250, batch 25/75, loss 0.7195\n",
            "epoch 1/250, batch 26/75, loss 3.8940\n",
            "epoch 1/250, batch 27/75, loss 0.7138\n",
            "epoch 1/250, batch 28/75, loss 0.7166\n",
            "epoch 1/250, batch 29/75, loss 0.7175\n",
            "epoch 1/250, batch 30/75, loss 0.7225\n",
            "epoch 1/250, batch 31/75, loss 0.7335\n",
            "epoch 1/250, batch 32/75, loss 0.7272\n",
            "epoch 1/250, batch 33/75, loss 0.7238\n",
            "epoch 1/250, batch 34/75, loss 0.7190\n",
            "epoch 1/250, batch 35/75, loss 0.7260\n",
            "epoch 1/250, batch 36/75, loss 3.3618\n",
            "epoch 1/250, batch 37/75, loss 0.8685\n",
            "epoch 1/250, batch 38/75, loss 0.7184\n",
            "epoch 1/250, batch 39/75, loss 0.7165\n",
            "epoch 1/250, batch 40/75, loss 0.7186\n",
            "epoch 1/250, batch 41/75, loss 0.7207\n",
            "epoch 1/250, batch 42/75, loss 0.7196\n",
            "epoch 1/250, batch 43/75, loss 0.7075\n",
            "epoch 1/250, batch 44/75, loss 0.7163\n",
            "epoch 1/250, batch 45/75, loss 0.7200\n",
            "epoch 1/250, batch 46/75, loss 0.7092\n",
            "epoch 1/250, batch 47/75, loss 0.7059\n",
            "epoch 1/250, batch 48/75, loss 0.7076\n",
            "epoch 1/250, batch 49/75, loss 0.7059\n",
            "epoch 1/250, batch 50/75, loss 0.6896\n",
            "epoch 1/250, batch 51/75, loss 0.6877\n",
            "epoch 1/250, batch 52/75, loss 0.6839\n",
            "epoch 1/250, batch 53/75, loss 9.7804\n",
            "epoch 1/250, batch 54/75, loss 1.1574\n",
            "epoch 1/250, batch 55/75, loss 0.6929\n",
            "epoch 1/250, batch 56/75, loss 0.6910\n",
            "epoch 1/250, batch 57/75, loss 0.7009\n",
            "epoch 1/250, batch 58/75, loss 0.7063\n",
            "epoch 1/250, batch 59/75, loss 0.7087\n",
            "epoch 1/250, batch 60/75, loss 0.7108\n",
            "epoch 1/250, batch 61/75, loss 0.7099\n",
            "epoch 1/250, batch 62/75, loss 0.7066\n",
            "epoch 1/250, batch 63/75, loss 0.7030\n",
            "epoch 1/250, batch 64/75, loss 0.7030\n",
            "epoch 1/250, batch 65/75, loss 0.7051\n",
            "epoch 1/250, batch 66/75, loss 0.6956\n",
            "epoch 1/250, batch 67/75, loss 0.6970\n",
            "epoch 1/250, batch 68/75, loss 0.6955\n",
            "epoch 1/250, batch 69/75, loss 0.6948\n",
            "epoch 1/250, batch 70/75, loss 0.6927\n",
            "epoch 1/250, batch 71/75, loss 0.6832\n",
            "epoch 1/250, batch 72/75, loss 0.6790\n",
            "epoch 1/250, batch 73/75, loss 0.6748\n",
            "epoch 1/250, batch 74/75, loss 0.6733\n",
            "epoch 1/250, batch 75/75, loss 0.6666\n",
            "epoch 1/250, training roc_auc_score 0.5623\n",
            "epoch 1/250, validation roc_auc_score 0.7917, best validation roc_auc_score 0.7917\n",
            "epoch 2/250, batch 1/75, loss 1.2295\n",
            "epoch 2/250, batch 2/75, loss 0.9556\n",
            "epoch 2/250, batch 3/75, loss 0.7857\n",
            "epoch 2/250, batch 4/75, loss 0.8316\n",
            "epoch 2/250, batch 5/75, loss 0.6553\n",
            "epoch 2/250, batch 6/75, loss 0.6569\n",
            "epoch 2/250, batch 7/75, loss 0.6587\n",
            "epoch 2/250, batch 8/75, loss 1.3065\n",
            "epoch 2/250, batch 9/75, loss 0.6566\n",
            "epoch 2/250, batch 10/75, loss 1.9325\n",
            "epoch 2/250, batch 11/75, loss 0.8969\n",
            "epoch 2/250, batch 12/75, loss 0.6534\n",
            "epoch 2/250, batch 13/75, loss 0.7749\n",
            "epoch 2/250, batch 14/75, loss 0.6532\n",
            "epoch 2/250, batch 15/75, loss 3.1786\n",
            "epoch 2/250, batch 16/75, loss 0.6525\n",
            "epoch 2/250, batch 17/75, loss 0.6540\n",
            "epoch 2/250, batch 18/75, loss 0.6581\n",
            "epoch 2/250, batch 19/75, loss 0.6604\n",
            "epoch 2/250, batch 20/75, loss 2.4124\n",
            "epoch 2/250, batch 21/75, loss 0.6621\n",
            "epoch 2/250, batch 22/75, loss 0.6581\n",
            "epoch 2/250, batch 23/75, loss 0.6585\n",
            "epoch 2/250, batch 24/75, loss 0.6554\n",
            "epoch 2/250, batch 25/75, loss 0.6625\n",
            "epoch 2/250, batch 26/75, loss 3.5051\n",
            "epoch 2/250, batch 27/75, loss 0.6544\n",
            "epoch 2/250, batch 28/75, loss 0.6606\n",
            "epoch 2/250, batch 29/75, loss 0.6597\n",
            "epoch 2/250, batch 30/75, loss 0.6633\n",
            "epoch 2/250, batch 31/75, loss 0.6719\n",
            "epoch 2/250, batch 32/75, loss 0.6584\n",
            "epoch 2/250, batch 33/75, loss 0.6597\n",
            "epoch 2/250, batch 34/75, loss 0.6622\n",
            "epoch 2/250, batch 35/75, loss 0.6636\n",
            "epoch 2/250, batch 36/75, loss 3.4017\n",
            "epoch 2/250, batch 37/75, loss 0.8307\n",
            "epoch 2/250, batch 38/75, loss 0.6551\n",
            "epoch 2/250, batch 39/75, loss 0.6603\n",
            "epoch 2/250, batch 40/75, loss 0.6553\n",
            "epoch 2/250, batch 41/75, loss 0.6545\n",
            "epoch 2/250, batch 42/75, loss 0.6598\n",
            "epoch 2/250, batch 43/75, loss 0.6486\n",
            "epoch 2/250, batch 44/75, loss 0.6568\n",
            "epoch 2/250, batch 45/75, loss 0.6545\n",
            "epoch 2/250, batch 46/75, loss 0.6498\n",
            "epoch 2/250, batch 47/75, loss 0.6455\n",
            "epoch 2/250, batch 48/75, loss 0.6450\n",
            "epoch 2/250, batch 49/75, loss 0.6418\n",
            "epoch 2/250, batch 50/75, loss 0.6356\n",
            "epoch 2/250, batch 51/75, loss 0.6314\n",
            "epoch 2/250, batch 52/75, loss 0.6289\n",
            "epoch 2/250, batch 53/75, loss 10.3393\n",
            "epoch 2/250, batch 54/75, loss 1.1090\n",
            "epoch 2/250, batch 55/75, loss 0.6434\n",
            "epoch 2/250, batch 56/75, loss 0.6394\n",
            "epoch 2/250, batch 57/75, loss 0.6465\n",
            "epoch 2/250, batch 58/75, loss 0.6506\n",
            "epoch 2/250, batch 59/75, loss 0.6549\n",
            "epoch 2/250, batch 60/75, loss 0.6534\n",
            "epoch 2/250, batch 61/75, loss 0.6514\n",
            "epoch 2/250, batch 62/75, loss 0.6523\n",
            "epoch 2/250, batch 63/75, loss 0.6495\n",
            "epoch 2/250, batch 64/75, loss 0.6548\n",
            "epoch 2/250, batch 65/75, loss 0.6476\n",
            "epoch 2/250, batch 66/75, loss 0.6422\n",
            "epoch 2/250, batch 67/75, loss 0.6460\n",
            "epoch 2/250, batch 68/75, loss 0.6416\n",
            "epoch 2/250, batch 69/75, loss 0.6447\n",
            "epoch 2/250, batch 70/75, loss 0.6349\n",
            "epoch 2/250, batch 71/75, loss 0.6281\n",
            "epoch 2/250, batch 72/75, loss 0.6262\n",
            "epoch 2/250, batch 73/75, loss 0.6238\n",
            "epoch 2/250, batch 74/75, loss 0.6234\n",
            "epoch 2/250, batch 75/75, loss 0.6188\n",
            "epoch 2/250, training roc_auc_score 0.6515\n",
            "EarlyStopping counter: 1 out of 50\n",
            "epoch 2/250, validation roc_auc_score 0.7073, best validation roc_auc_score 0.7917\n",
            "epoch 3/250, batch 1/75, loss 1.2657\n",
            "epoch 3/250, batch 2/75, loss 0.8810\n",
            "epoch 3/250, batch 3/75, loss 0.7463\n",
            "epoch 3/250, batch 4/75, loss 0.7839\n",
            "epoch 3/250, batch 5/75, loss 0.6059\n",
            "epoch 3/250, batch 6/75, loss 0.6044\n",
            "epoch 3/250, batch 7/75, loss 0.6056\n",
            "epoch 3/250, batch 8/75, loss 1.2452\n",
            "epoch 3/250, batch 9/75, loss 0.6033\n",
            "epoch 3/250, batch 10/75, loss 1.5280\n",
            "epoch 3/250, batch 11/75, loss 0.8973\n",
            "epoch 3/250, batch 12/75, loss 0.5978\n",
            "epoch 3/250, batch 13/75, loss 0.7451\n",
            "epoch 3/250, batch 14/75, loss 0.6019\n",
            "epoch 3/250, batch 15/75, loss 3.3049\n",
            "epoch 3/250, batch 16/75, loss 0.5978\n",
            "epoch 3/250, batch 17/75, loss 0.6000\n",
            "epoch 3/250, batch 18/75, loss 0.5994\n",
            "epoch 3/250, batch 19/75, loss 0.6034\n",
            "epoch 3/250, batch 20/75, loss 2.3909\n",
            "epoch 3/250, batch 21/75, loss 0.6035\n",
            "epoch 3/250, batch 22/75, loss 0.5975\n",
            "epoch 3/250, batch 23/75, loss 0.6032\n",
            "epoch 3/250, batch 24/75, loss 0.5988\n",
            "epoch 3/250, batch 25/75, loss 0.6013\n",
            "epoch 3/250, batch 26/75, loss 3.7330\n",
            "epoch 3/250, batch 27/75, loss 0.5980\n",
            "epoch 3/250, batch 28/75, loss 0.6016\n",
            "epoch 3/250, batch 29/75, loss 0.6002\n",
            "epoch 3/250, batch 30/75, loss 0.6030\n",
            "epoch 3/250, batch 31/75, loss 0.6113\n",
            "epoch 3/250, batch 32/75, loss 0.5918\n",
            "epoch 3/250, batch 33/75, loss 0.6015\n",
            "epoch 3/250, batch 34/75, loss 0.6062\n",
            "epoch 3/250, batch 35/75, loss 0.6044\n",
            "epoch 3/250, batch 36/75, loss 3.4577\n",
            "epoch 3/250, batch 37/75, loss 0.7744\n",
            "epoch 3/250, batch 38/75, loss 0.5987\n",
            "epoch 3/250, batch 39/75, loss 0.6028\n",
            "epoch 3/250, batch 40/75, loss 0.6005\n",
            "epoch 3/250, batch 41/75, loss 0.5938\n",
            "epoch 3/250, batch 42/75, loss 0.6010\n",
            "epoch 3/250, batch 43/75, loss 0.5937\n",
            "epoch 3/250, batch 44/75, loss 0.5977\n",
            "epoch 3/250, batch 45/75, loss 0.5924\n",
            "epoch 3/250, batch 46/75, loss 0.5972\n",
            "epoch 3/250, batch 47/75, loss 0.5906\n",
            "epoch 3/250, batch 48/75, loss 0.5832\n",
            "epoch 3/250, batch 49/75, loss 0.5835\n",
            "epoch 3/250, batch 50/75, loss 0.5766\n",
            "epoch 3/250, batch 51/75, loss 0.5778\n",
            "epoch 3/250, batch 52/75, loss 0.5716\n",
            "epoch 3/250, batch 53/75, loss 11.4654\n",
            "epoch 3/250, batch 54/75, loss 1.1056\n",
            "epoch 3/250, batch 55/75, loss 0.5890\n",
            "epoch 3/250, batch 56/75, loss 0.5888\n",
            "epoch 3/250, batch 57/75, loss 0.5974\n",
            "epoch 3/250, batch 58/75, loss 0.6000\n",
            "epoch 3/250, batch 59/75, loss 0.6094\n",
            "epoch 3/250, batch 60/75, loss 0.6059\n",
            "epoch 3/250, batch 61/75, loss 0.6060\n",
            "epoch 3/250, batch 62/75, loss 0.6095\n",
            "epoch 3/250, batch 63/75, loss 0.6090\n",
            "epoch 3/250, batch 64/75, loss 0.6123\n",
            "epoch 3/250, batch 65/75, loss 0.6061\n",
            "epoch 3/250, batch 66/75, loss 0.5989\n",
            "epoch 3/250, batch 67/75, loss 0.6055\n",
            "epoch 3/250, batch 68/75, loss 0.5993\n",
            "epoch 3/250, batch 69/75, loss 0.6020\n",
            "epoch 3/250, batch 70/75, loss 0.5933\n",
            "epoch 3/250, batch 71/75, loss 0.5865\n",
            "epoch 3/250, batch 72/75, loss 0.5818\n",
            "epoch 3/250, batch 73/75, loss 0.5805\n",
            "epoch 3/250, batch 74/75, loss 0.5741\n",
            "epoch 3/250, batch 75/75, loss 0.5630\n",
            "epoch 3/250, training roc_auc_score 0.6618\n",
            "EarlyStopping counter: 2 out of 50\n",
            "epoch 3/250, validation roc_auc_score 0.7827, best validation roc_auc_score 0.7917\n",
            "epoch 4/250, batch 1/75, loss 1.1006\n",
            "epoch 4/250, batch 2/75, loss 0.8735\n",
            "epoch 4/250, batch 3/75, loss 0.6536\n",
            "epoch 4/250, batch 4/75, loss 0.7006\n",
            "epoch 4/250, batch 5/75, loss 0.5559\n",
            "epoch 4/250, batch 6/75, loss 0.5512\n",
            "epoch 4/250, batch 7/75, loss 0.5539\n",
            "epoch 4/250, batch 8/75, loss 1.3101\n",
            "epoch 4/250, batch 9/75, loss 0.5492\n",
            "epoch 4/250, batch 10/75, loss 1.6039\n",
            "epoch 4/250, batch 11/75, loss 0.8448\n",
            "epoch 4/250, batch 12/75, loss 0.5486\n",
            "epoch 4/250, batch 13/75, loss 0.6824\n",
            "epoch 4/250, batch 14/75, loss 0.5494\n",
            "epoch 4/250, batch 15/75, loss 3.2392\n",
            "epoch 4/250, batch 16/75, loss 0.5480\n",
            "epoch 4/250, batch 17/75, loss 0.5537\n",
            "epoch 4/250, batch 18/75, loss 0.5526\n",
            "epoch 4/250, batch 19/75, loss 0.5603\n",
            "epoch 4/250, batch 20/75, loss 2.4270\n",
            "epoch 4/250, batch 21/75, loss 0.5563\n",
            "epoch 4/250, batch 22/75, loss 0.5484\n",
            "epoch 4/250, batch 23/75, loss 0.5543\n",
            "epoch 4/250, batch 24/75, loss 0.5481\n",
            "epoch 4/250, batch 25/75, loss 0.5494\n",
            "epoch 4/250, batch 26/75, loss 3.9829\n",
            "epoch 4/250, batch 27/75, loss 0.5469\n",
            "epoch 4/250, batch 28/75, loss 0.5513\n",
            "epoch 4/250, batch 29/75, loss 0.5471\n",
            "epoch 4/250, batch 30/75, loss 0.5499\n",
            "epoch 4/250, batch 31/75, loss 0.5537\n",
            "epoch 4/250, batch 32/75, loss 0.5403\n",
            "epoch 4/250, batch 33/75, loss 0.5400\n",
            "epoch 4/250, batch 34/75, loss 0.5365\n",
            "epoch 4/250, batch 35/75, loss 0.5370\n",
            "epoch 4/250, batch 36/75, loss 4.0411\n",
            "epoch 4/250, batch 37/75, loss 0.7105\n",
            "epoch 4/250, batch 38/75, loss 0.5303\n",
            "epoch 4/250, batch 39/75, loss 0.5320\n",
            "epoch 4/250, batch 40/75, loss 0.5350\n",
            "epoch 4/250, batch 41/75, loss 0.5303\n",
            "epoch 4/250, batch 42/75, loss 0.5306\n",
            "epoch 4/250, batch 43/75, loss 0.5304\n",
            "epoch 4/250, batch 44/75, loss 0.5312\n",
            "epoch 4/250, batch 45/75, loss 0.5315\n",
            "epoch 4/250, batch 46/75, loss 0.5319\n",
            "epoch 4/250, batch 47/75, loss 0.5320\n",
            "epoch 4/250, batch 48/75, loss 0.5244\n",
            "epoch 4/250, batch 49/75, loss 0.5244\n",
            "epoch 4/250, batch 50/75, loss 0.5189\n",
            "epoch 4/250, batch 51/75, loss 0.5189\n",
            "epoch 4/250, batch 52/75, loss 0.5175\n",
            "epoch 4/250, batch 53/75, loss 12.8288\n",
            "epoch 4/250, batch 54/75, loss 1.2579\n",
            "epoch 4/250, batch 55/75, loss 0.5289\n",
            "epoch 4/250, batch 56/75, loss 0.5332\n",
            "epoch 4/250, batch 57/75, loss 0.5379\n",
            "epoch 4/250, batch 58/75, loss 0.5437\n",
            "epoch 4/250, batch 59/75, loss 0.5454\n",
            "epoch 4/250, batch 60/75, loss 0.5572\n",
            "epoch 4/250, batch 61/75, loss 0.5507\n",
            "epoch 4/250, batch 62/75, loss 0.5517\n",
            "epoch 4/250, batch 63/75, loss 0.5484\n",
            "epoch 4/250, batch 64/75, loss 0.5525\n",
            "epoch 4/250, batch 65/75, loss 0.5518\n",
            "epoch 4/250, batch 66/75, loss 0.5420\n",
            "epoch 4/250, batch 67/75, loss 0.5442\n",
            "epoch 4/250, batch 68/75, loss 0.5432\n",
            "epoch 4/250, batch 69/75, loss 0.5461\n",
            "epoch 4/250, batch 70/75, loss 0.5371\n",
            "epoch 4/250, batch 71/75, loss 0.5303\n",
            "epoch 4/250, batch 72/75, loss 0.5307\n",
            "epoch 4/250, batch 73/75, loss 0.5282\n",
            "epoch 4/250, batch 74/75, loss 0.5293\n",
            "epoch 4/250, batch 75/75, loss 0.5187\n",
            "epoch 4/250, training roc_auc_score 0.6704\n",
            "epoch 4/250, validation roc_auc_score 0.8380, best validation roc_auc_score 0.8380\n",
            "epoch 5/250, batch 1/75, loss 1.1177\n",
            "epoch 5/250, batch 2/75, loss 0.8844\n",
            "epoch 5/250, batch 3/75, loss 0.6035\n",
            "epoch 5/250, batch 4/75, loss 0.6656\n",
            "epoch 5/250, batch 5/75, loss 0.5125\n",
            "epoch 5/250, batch 6/75, loss 0.5049\n",
            "epoch 5/250, batch 7/75, loss 0.5125\n",
            "epoch 5/250, batch 8/75, loss 1.2332\n",
            "epoch 5/250, batch 9/75, loss 0.5065\n",
            "epoch 5/250, batch 10/75, loss 1.5748\n",
            "epoch 5/250, batch 11/75, loss 0.7962\n",
            "epoch 5/250, batch 12/75, loss 0.5048\n",
            "epoch 5/250, batch 13/75, loss 0.6357\n",
            "epoch 5/250, batch 14/75, loss 0.5057\n",
            "epoch 5/250, batch 15/75, loss 3.3126\n",
            "epoch 5/250, batch 16/75, loss 0.4991\n",
            "epoch 5/250, batch 17/75, loss 0.5036\n",
            "epoch 5/250, batch 18/75, loss 0.5016\n",
            "epoch 5/250, batch 19/75, loss 0.5094\n",
            "epoch 5/250, batch 20/75, loss 2.2034\n",
            "epoch 5/250, batch 21/75, loss 0.5113\n",
            "epoch 5/250, batch 22/75, loss 0.5028\n",
            "epoch 5/250, batch 23/75, loss 0.5116\n",
            "epoch 5/250, batch 24/75, loss 0.5186\n",
            "epoch 5/250, batch 25/75, loss 0.5148\n",
            "epoch 5/250, batch 26/75, loss 4.3591\n",
            "epoch 5/250, batch 27/75, loss 0.5102\n",
            "epoch 5/250, batch 28/75, loss 0.5205\n",
            "epoch 5/250, batch 29/75, loss 0.5077\n",
            "epoch 5/250, batch 30/75, loss 0.5137\n",
            "epoch 5/250, batch 31/75, loss 0.5330\n",
            "epoch 5/250, batch 32/75, loss 0.5126\n",
            "epoch 5/250, batch 33/75, loss 0.5197\n",
            "epoch 5/250, batch 34/75, loss 0.5403\n",
            "epoch 5/250, batch 35/75, loss 0.5332\n",
            "epoch 5/250, batch 36/75, loss 3.5560\n",
            "epoch 5/250, batch 37/75, loss 0.7613\n",
            "epoch 5/250, batch 38/75, loss 0.5166\n",
            "epoch 5/250, batch 39/75, loss 0.5294\n",
            "epoch 5/250, batch 40/75, loss 0.5302\n",
            "epoch 5/250, batch 41/75, loss 0.5156\n",
            "epoch 5/250, batch 42/75, loss 0.5264\n",
            "epoch 5/250, batch 43/75, loss 0.5212\n",
            "epoch 5/250, batch 44/75, loss 0.5265\n",
            "epoch 5/250, batch 45/75, loss 0.5263\n",
            "epoch 5/250, batch 46/75, loss 0.5232\n",
            "epoch 5/250, batch 47/75, loss 0.5248\n",
            "epoch 5/250, batch 48/75, loss 0.5183\n",
            "epoch 5/250, batch 49/75, loss 0.5178\n",
            "epoch 5/250, batch 50/75, loss 0.5043\n",
            "epoch 5/250, batch 51/75, loss 0.4975\n",
            "epoch 5/250, batch 52/75, loss 0.4993\n",
            "epoch 5/250, batch 53/75, loss 12.3515\n",
            "epoch 5/250, batch 54/75, loss 0.9814\n",
            "epoch 5/250, batch 55/75, loss 0.5130\n",
            "epoch 5/250, batch 56/75, loss 0.5103\n",
            "epoch 5/250, batch 57/75, loss 0.5209\n",
            "epoch 5/250, batch 58/75, loss 0.5310\n",
            "epoch 5/250, batch 59/75, loss 0.5345\n",
            "epoch 5/250, batch 60/75, loss 0.5313\n",
            "epoch 5/250, batch 61/75, loss 0.5248\n",
            "epoch 5/250, batch 62/75, loss 0.5352\n",
            "epoch 5/250, batch 63/75, loss 0.5324\n",
            "epoch 5/250, batch 64/75, loss 0.5362\n",
            "epoch 5/250, batch 65/75, loss 0.5319\n",
            "epoch 5/250, batch 66/75, loss 0.5241\n",
            "epoch 5/250, batch 67/75, loss 0.5250\n",
            "epoch 5/250, batch 68/75, loss 0.5262\n",
            "epoch 5/250, batch 69/75, loss 0.5291\n",
            "epoch 5/250, batch 70/75, loss 0.5189\n",
            "epoch 5/250, batch 71/75, loss 0.5094\n",
            "epoch 5/250, batch 72/75, loss 0.5102\n",
            "epoch 5/250, batch 73/75, loss 0.5055\n",
            "epoch 5/250, batch 74/75, loss 0.5088\n",
            "epoch 5/250, batch 75/75, loss 0.4946\n",
            "epoch 5/250, training roc_auc_score 0.6979\n",
            "EarlyStopping counter: 1 out of 50\n",
            "epoch 5/250, validation roc_auc_score 0.8355, best validation roc_auc_score 0.8380\n",
            "epoch 6/250, batch 1/75, loss 1.0521\n",
            "epoch 6/250, batch 2/75, loss 0.9294\n",
            "epoch 6/250, batch 3/75, loss 0.5602\n",
            "epoch 6/250, batch 4/75, loss 0.6367\n",
            "epoch 6/250, batch 5/75, loss 0.4848\n",
            "epoch 6/250, batch 6/75, loss 0.4699\n",
            "epoch 6/250, batch 7/75, loss 0.4803\n",
            "epoch 6/250, batch 8/75, loss 1.2071\n",
            "epoch 6/250, batch 9/75, loss 0.4726\n",
            "epoch 6/250, batch 10/75, loss 1.3784\n",
            "epoch 6/250, batch 11/75, loss 0.7295\n",
            "epoch 6/250, batch 12/75, loss 0.4712\n",
            "epoch 6/250, batch 13/75, loss 0.5575\n",
            "epoch 6/250, batch 14/75, loss 0.4625\n",
            "epoch 6/250, batch 15/75, loss 2.9949\n",
            "epoch 6/250, batch 16/75, loss 0.4646\n",
            "epoch 6/250, batch 17/75, loss 0.4690\n",
            "epoch 6/250, batch 18/75, loss 0.4698\n",
            "epoch 6/250, batch 19/75, loss 0.4750\n",
            "epoch 6/250, batch 20/75, loss 2.1570\n",
            "epoch 6/250, batch 21/75, loss 0.4853\n",
            "epoch 6/250, batch 22/75, loss 0.4697\n",
            "epoch 6/250, batch 23/75, loss 0.4913\n",
            "epoch 6/250, batch 24/75, loss 0.4920\n",
            "epoch 6/250, batch 25/75, loss 0.4937\n",
            "epoch 6/250, batch 26/75, loss 4.5383\n",
            "epoch 6/250, batch 27/75, loss 0.4882\n",
            "epoch 6/250, batch 28/75, loss 0.5042\n",
            "epoch 6/250, batch 29/75, loss 0.4817\n",
            "epoch 6/250, batch 30/75, loss 0.4986\n",
            "epoch 6/250, batch 31/75, loss 0.5125\n",
            "epoch 6/250, batch 32/75, loss 0.4808\n",
            "epoch 6/250, batch 33/75, loss 0.4993\n",
            "epoch 6/250, batch 34/75, loss 0.5161\n",
            "epoch 6/250, batch 35/75, loss 0.5042\n",
            "epoch 6/250, batch 36/75, loss 3.4452\n",
            "epoch 6/250, batch 37/75, loss 0.7505\n",
            "epoch 6/250, batch 38/75, loss 0.4948\n",
            "epoch 6/250, batch 39/75, loss 0.5064\n",
            "epoch 6/250, batch 40/75, loss 0.4999\n",
            "epoch 6/250, batch 41/75, loss 0.4814\n",
            "epoch 6/250, batch 42/75, loss 0.5035\n",
            "epoch 6/250, batch 43/75, loss 0.4850\n",
            "epoch 6/250, batch 44/75, loss 0.4942\n",
            "epoch 6/250, batch 45/75, loss 0.4966\n",
            "epoch 6/250, batch 46/75, loss 0.4895\n",
            "epoch 6/250, batch 47/75, loss 0.4917\n",
            "epoch 6/250, batch 48/75, loss 0.4862\n",
            "epoch 6/250, batch 49/75, loss 0.4817\n",
            "epoch 6/250, batch 50/75, loss 0.4708\n",
            "epoch 6/250, batch 51/75, loss 0.4623\n",
            "epoch 6/250, batch 52/75, loss 0.4637\n",
            "epoch 6/250, batch 53/75, loss 12.6921\n",
            "epoch 6/250, batch 54/75, loss 0.8252\n",
            "epoch 6/250, batch 55/75, loss 0.4765\n",
            "epoch 6/250, batch 56/75, loss 0.4751\n",
            "epoch 6/250, batch 57/75, loss 0.4844\n",
            "epoch 6/250, batch 58/75, loss 0.4877\n",
            "epoch 6/250, batch 59/75, loss 0.5042\n",
            "epoch 6/250, batch 60/75, loss 0.4943\n",
            "epoch 6/250, batch 61/75, loss 0.4832\n",
            "epoch 6/250, batch 62/75, loss 0.5029\n",
            "epoch 6/250, batch 63/75, loss 0.4979\n",
            "epoch 6/250, batch 64/75, loss 0.5035\n",
            "epoch 6/250, batch 65/75, loss 0.4951\n",
            "epoch 6/250, batch 66/75, loss 0.4866\n",
            "epoch 6/250, batch 67/75, loss 0.4910\n",
            "epoch 6/250, batch 68/75, loss 0.4891\n",
            "epoch 6/250, batch 69/75, loss 0.4968\n",
            "epoch 6/250, batch 70/75, loss 0.4810\n",
            "epoch 6/250, batch 71/75, loss 0.4767\n",
            "epoch 6/250, batch 72/75, loss 0.4750\n",
            "epoch 6/250, batch 73/75, loss 0.4728\n",
            "epoch 6/250, batch 74/75, loss 0.4772\n",
            "epoch 6/250, batch 75/75, loss 0.4593\n",
            "epoch 6/250, training roc_auc_score 0.7208\n",
            "epoch 6/250, validation roc_auc_score 0.8506, best validation roc_auc_score 0.8506\n",
            "epoch 7/250, batch 1/75, loss 0.9604\n",
            "epoch 7/250, batch 2/75, loss 0.8801\n",
            "epoch 7/250, batch 3/75, loss 0.4955\n",
            "epoch 7/250, batch 4/75, loss 0.6079\n",
            "epoch 7/250, batch 5/75, loss 0.4481\n",
            "epoch 7/250, batch 6/75, loss 0.4342\n",
            "epoch 7/250, batch 7/75, loss 0.4488\n",
            "epoch 7/250, batch 8/75, loss 1.2384\n",
            "epoch 7/250, batch 9/75, loss 0.4373\n",
            "epoch 7/250, batch 10/75, loss 1.0940\n",
            "epoch 7/250, batch 11/75, loss 0.6196\n",
            "epoch 7/250, batch 12/75, loss 0.4350\n",
            "epoch 7/250, batch 13/75, loss 0.4811\n",
            "epoch 7/250, batch 14/75, loss 0.4291\n",
            "epoch 7/250, batch 15/75, loss 2.8413\n",
            "epoch 7/250, batch 16/75, loss 0.4263\n",
            "epoch 7/250, batch 17/75, loss 0.4381\n",
            "epoch 7/250, batch 18/75, loss 0.4406\n",
            "epoch 7/250, batch 19/75, loss 0.4447\n",
            "epoch 7/250, batch 20/75, loss 1.7468\n",
            "epoch 7/250, batch 21/75, loss 0.4552\n",
            "epoch 7/250, batch 22/75, loss 0.4357\n",
            "epoch 7/250, batch 23/75, loss 0.4680\n",
            "epoch 7/250, batch 24/75, loss 0.4626\n",
            "epoch 7/250, batch 25/75, loss 0.4658\n",
            "epoch 7/250, batch 26/75, loss 4.1003\n",
            "epoch 7/250, batch 27/75, loss 0.4636\n",
            "epoch 7/250, batch 28/75, loss 0.4801\n",
            "epoch 7/250, batch 29/75, loss 0.4682\n",
            "epoch 7/250, batch 30/75, loss 0.4875\n",
            "epoch 7/250, batch 31/75, loss 0.4984\n",
            "epoch 7/250, batch 32/75, loss 0.4560\n",
            "epoch 7/250, batch 33/75, loss 0.4877\n",
            "epoch 7/250, batch 34/75, loss 0.5022\n",
            "epoch 7/250, batch 35/75, loss 0.4719\n",
            "epoch 7/250, batch 36/75, loss 3.3351\n",
            "epoch 7/250, batch 37/75, loss 0.7347\n",
            "epoch 7/250, batch 38/75, loss 0.4717\n",
            "epoch 7/250, batch 39/75, loss 0.4718\n",
            "epoch 7/250, batch 40/75, loss 0.4844\n",
            "epoch 7/250, batch 41/75, loss 0.4614\n",
            "epoch 7/250, batch 42/75, loss 0.4863\n",
            "epoch 7/250, batch 43/75, loss 0.4571\n",
            "epoch 7/250, batch 44/75, loss 0.4681\n",
            "epoch 7/250, batch 45/75, loss 0.4735\n",
            "epoch 7/250, batch 46/75, loss 0.4659\n",
            "epoch 7/250, batch 47/75, loss 0.4686\n",
            "epoch 7/250, batch 48/75, loss 0.4671\n",
            "epoch 7/250, batch 49/75, loss 0.4616\n",
            "epoch 7/250, batch 50/75, loss 0.4542\n",
            "epoch 7/250, batch 51/75, loss 0.4435\n",
            "epoch 7/250, batch 52/75, loss 0.4424\n",
            "epoch 7/250, batch 53/75, loss 12.9549\n",
            "epoch 7/250, batch 54/75, loss 0.7486\n",
            "epoch 7/250, batch 55/75, loss 0.4472\n",
            "epoch 7/250, batch 56/75, loss 0.4544\n",
            "epoch 7/250, batch 57/75, loss 0.4680\n",
            "epoch 7/250, batch 58/75, loss 0.4631\n",
            "epoch 7/250, batch 59/75, loss 0.4875\n",
            "epoch 7/250, batch 60/75, loss 0.4728\n",
            "epoch 7/250, batch 61/75, loss 0.4623\n",
            "epoch 7/250, batch 62/75, loss 0.4882\n",
            "epoch 7/250, batch 63/75, loss 0.4815\n",
            "epoch 7/250, batch 64/75, loss 0.4903\n",
            "epoch 7/250, batch 65/75, loss 0.4768\n",
            "epoch 7/250, batch 66/75, loss 0.4623\n",
            "epoch 7/250, batch 67/75, loss 0.4750\n",
            "epoch 7/250, batch 68/75, loss 0.4678\n",
            "epoch 7/250, batch 69/75, loss 0.4864\n",
            "epoch 7/250, batch 70/75, loss 0.4623\n",
            "epoch 7/250, batch 71/75, loss 0.4615\n",
            "epoch 7/250, batch 72/75, loss 0.4566\n",
            "epoch 7/250, batch 73/75, loss 0.4575\n",
            "epoch 7/250, batch 74/75, loss 0.4574\n",
            "epoch 7/250, batch 75/75, loss 0.4361\n",
            "epoch 7/250, training roc_auc_score 0.7564\n",
            "EarlyStopping counter: 1 out of 50\n",
            "epoch 7/250, validation roc_auc_score 0.8466, best validation roc_auc_score 0.8506\n",
            "epoch 8/250, batch 1/75, loss 0.8652\n",
            "epoch 8/250, batch 2/75, loss 0.9104\n",
            "epoch 8/250, batch 3/75, loss 0.4539\n",
            "epoch 8/250, batch 4/75, loss 0.5900\n",
            "epoch 8/250, batch 5/75, loss 0.4216\n",
            "epoch 8/250, batch 6/75, loss 0.4055\n",
            "epoch 8/250, batch 7/75, loss 0.4200\n",
            "epoch 8/250, batch 8/75, loss 1.2748\n",
            "epoch 8/250, batch 9/75, loss 0.4119\n",
            "epoch 8/250, batch 10/75, loss 1.0523\n",
            "epoch 8/250, batch 11/75, loss 0.5601\n",
            "epoch 8/250, batch 12/75, loss 0.4106\n",
            "epoch 8/250, batch 13/75, loss 0.4274\n",
            "epoch 8/250, batch 14/75, loss 0.4011\n",
            "epoch 8/250, batch 15/75, loss 2.6161\n",
            "epoch 8/250, batch 16/75, loss 0.4025\n",
            "epoch 8/250, batch 17/75, loss 0.4130\n",
            "epoch 8/250, batch 18/75, loss 0.4232\n",
            "epoch 8/250, batch 19/75, loss 0.4187\n",
            "epoch 8/250, batch 20/75, loss 1.5517\n",
            "epoch 8/250, batch 21/75, loss 0.4317\n",
            "epoch 8/250, batch 22/75, loss 0.4107\n",
            "epoch 8/250, batch 23/75, loss 0.4356\n",
            "epoch 8/250, batch 24/75, loss 0.4374\n",
            "epoch 8/250, batch 25/75, loss 0.4420\n",
            "epoch 8/250, batch 26/75, loss 4.1732\n",
            "epoch 8/250, batch 27/75, loss 0.4319\n",
            "epoch 8/250, batch 28/75, loss 0.4543\n",
            "epoch 8/250, batch 29/75, loss 0.4431\n",
            "epoch 8/250, batch 30/75, loss 0.4476\n",
            "epoch 8/250, batch 31/75, loss 0.4643\n",
            "epoch 8/250, batch 32/75, loss 0.4252\n",
            "epoch 8/250, batch 33/75, loss 0.4507\n",
            "epoch 8/250, batch 34/75, loss 0.4724\n",
            "epoch 8/250, batch 35/75, loss 0.4378\n",
            "epoch 8/250, batch 36/75, loss 3.1458\n",
            "epoch 8/250, batch 37/75, loss 0.7245\n",
            "epoch 8/250, batch 38/75, loss 0.4374\n",
            "epoch 8/250, batch 39/75, loss 0.4430\n",
            "epoch 8/250, batch 40/75, loss 0.4500\n",
            "epoch 8/250, batch 41/75, loss 0.4315\n",
            "epoch 8/250, batch 42/75, loss 0.4511\n",
            "epoch 8/250, batch 43/75, loss 0.4181\n",
            "epoch 8/250, batch 44/75, loss 0.4390\n",
            "epoch 8/250, batch 45/75, loss 0.4466\n",
            "epoch 8/250, batch 46/75, loss 0.4342\n",
            "epoch 8/250, batch 47/75, loss 0.4417\n",
            "epoch 8/250, batch 48/75, loss 0.4383\n",
            "epoch 8/250, batch 49/75, loss 0.4380\n",
            "epoch 8/250, batch 50/75, loss 0.4253\n",
            "epoch 8/250, batch 51/75, loss 0.4119\n",
            "epoch 8/250, batch 52/75, loss 0.4065\n",
            "epoch 8/250, batch 53/75, loss 13.4153\n",
            "epoch 8/250, batch 54/75, loss 0.6562\n",
            "epoch 8/250, batch 55/75, loss 0.4134\n",
            "epoch 8/250, batch 56/75, loss 0.4199\n",
            "epoch 8/250, batch 57/75, loss 0.4358\n",
            "epoch 8/250, batch 58/75, loss 0.4254\n",
            "epoch 8/250, batch 59/75, loss 0.4487\n",
            "epoch 8/250, batch 60/75, loss 0.4296\n",
            "epoch 8/250, batch 61/75, loss 0.4277\n",
            "epoch 8/250, batch 62/75, loss 0.4484\n",
            "epoch 8/250, batch 63/75, loss 0.4382\n",
            "epoch 8/250, batch 64/75, loss 0.4494\n",
            "epoch 8/250, batch 65/75, loss 0.4368\n",
            "epoch 8/250, batch 66/75, loss 0.4273\n",
            "epoch 8/250, batch 67/75, loss 0.4373\n",
            "epoch 8/250, batch 68/75, loss 0.4339\n",
            "epoch 8/250, batch 69/75, loss 0.4508\n",
            "epoch 8/250, batch 70/75, loss 0.4252\n",
            "epoch 8/250, batch 71/75, loss 0.4196\n",
            "epoch 8/250, batch 72/75, loss 0.4188\n",
            "epoch 8/250, batch 73/75, loss 0.4208\n",
            "epoch 8/250, batch 74/75, loss 0.4208\n",
            "epoch 8/250, batch 75/75, loss 0.3991\n",
            "epoch 8/250, training roc_auc_score 0.7720\n",
            "EarlyStopping counter: 2 out of 50\n",
            "epoch 8/250, validation roc_auc_score 0.8478, best validation roc_auc_score 0.8506\n",
            "epoch 9/250, batch 1/75, loss 0.8148\n",
            "epoch 9/250, batch 2/75, loss 0.9326\n",
            "epoch 9/250, batch 3/75, loss 0.4224\n",
            "epoch 9/250, batch 4/75, loss 0.5548\n",
            "epoch 9/250, batch 5/75, loss 0.3972\n",
            "epoch 9/250, batch 6/75, loss 0.3669\n",
            "epoch 9/250, batch 7/75, loss 0.3900\n",
            "epoch 9/250, batch 8/75, loss 1.2073\n",
            "epoch 9/250, batch 9/75, loss 0.3751\n",
            "epoch 9/250, batch 10/75, loss 0.9668\n",
            "epoch 9/250, batch 11/75, loss 0.5059\n",
            "epoch 9/250, batch 12/75, loss 0.3802\n",
            "epoch 9/250, batch 13/75, loss 0.3951\n",
            "epoch 9/250, batch 14/75, loss 0.3725\n",
            "epoch 9/250, batch 15/75, loss 2.7355\n",
            "epoch 9/250, batch 16/75, loss 0.3664\n",
            "epoch 9/250, batch 17/75, loss 0.3847\n",
            "epoch 9/250, batch 18/75, loss 0.3853\n",
            "epoch 9/250, batch 19/75, loss 0.3919\n",
            "epoch 9/250, batch 20/75, loss 1.5265\n",
            "epoch 9/250, batch 21/75, loss 0.3954\n",
            "epoch 9/250, batch 22/75, loss 0.3780\n",
            "epoch 9/250, batch 23/75, loss 0.4190\n",
            "epoch 9/250, batch 24/75, loss 0.4177\n",
            "epoch 9/250, batch 25/75, loss 0.4205\n",
            "epoch 9/250, batch 26/75, loss 3.9330\n",
            "epoch 9/250, batch 27/75, loss 0.4157\n",
            "epoch 9/250, batch 28/75, loss 0.4429\n",
            "epoch 9/250, batch 29/75, loss 0.4274\n",
            "epoch 9/250, batch 30/75, loss 0.4372\n",
            "epoch 9/250, batch 31/75, loss 0.4516\n",
            "epoch 9/250, batch 32/75, loss 0.4019\n",
            "epoch 9/250, batch 33/75, loss 0.4372\n",
            "epoch 9/250, batch 34/75, loss 0.4582\n",
            "epoch 9/250, batch 35/75, loss 0.4173\n",
            "epoch 9/250, batch 36/75, loss 2.9318\n",
            "epoch 9/250, batch 37/75, loss 0.6957\n",
            "epoch 9/250, batch 38/75, loss 0.4161\n",
            "epoch 9/250, batch 39/75, loss 0.4270\n",
            "epoch 9/250, batch 40/75, loss 0.4264\n",
            "epoch 9/250, batch 41/75, loss 0.4143\n",
            "epoch 9/250, batch 42/75, loss 0.4294\n",
            "epoch 9/250, batch 43/75, loss 0.4028\n",
            "epoch 9/250, batch 44/75, loss 0.4158\n",
            "epoch 9/250, batch 45/75, loss 0.4329\n",
            "epoch 9/250, batch 46/75, loss 0.4163\n",
            "epoch 9/250, batch 47/75, loss 0.4274\n",
            "epoch 9/250, batch 48/75, loss 0.4228\n",
            "epoch 9/250, batch 49/75, loss 0.4126\n",
            "epoch 9/250, batch 50/75, loss 0.4084\n",
            "epoch 9/250, batch 51/75, loss 0.3947\n",
            "epoch 9/250, batch 52/75, loss 0.3882\n",
            "epoch 9/250, batch 53/75, loss 13.5165\n",
            "epoch 9/250, batch 54/75, loss 0.6195\n",
            "epoch 9/250, batch 55/75, loss 0.3986\n",
            "epoch 9/250, batch 56/75, loss 0.4016\n",
            "epoch 9/250, batch 57/75, loss 0.4206\n",
            "epoch 9/250, batch 58/75, loss 0.4061\n",
            "epoch 9/250, batch 59/75, loss 0.4302\n",
            "epoch 9/250, batch 60/75, loss 0.4180\n",
            "epoch 9/250, batch 61/75, loss 0.4109\n",
            "epoch 9/250, batch 62/75, loss 0.4229\n",
            "epoch 9/250, batch 63/75, loss 0.4150\n",
            "epoch 9/250, batch 64/75, loss 0.4253\n",
            "epoch 9/250, batch 65/75, loss 0.4092\n",
            "epoch 9/250, batch 66/75, loss 0.4024\n",
            "epoch 9/250, batch 67/75, loss 0.4101\n",
            "epoch 9/250, batch 68/75, loss 0.3986\n",
            "epoch 9/250, batch 69/75, loss 0.4136\n",
            "epoch 9/250, batch 70/75, loss 0.3993\n",
            "epoch 9/250, batch 71/75, loss 0.3932\n",
            "epoch 9/250, batch 72/75, loss 0.3885\n",
            "epoch 9/250, batch 73/75, loss 0.3945\n",
            "epoch 9/250, batch 74/75, loss 0.3957\n",
            "epoch 9/250, batch 75/75, loss 0.3768\n",
            "epoch 9/250, training roc_auc_score 0.7920\n",
            "EarlyStopping counter: 3 out of 50\n",
            "epoch 9/250, validation roc_auc_score 0.8321, best validation roc_auc_score 0.8506\n",
            "epoch 10/250, batch 1/75, loss 0.7201\n",
            "epoch 10/250, batch 2/75, loss 0.9286\n",
            "epoch 10/250, batch 3/75, loss 0.4004\n",
            "epoch 10/250, batch 4/75, loss 0.5329\n",
            "epoch 10/250, batch 5/75, loss 0.3687\n",
            "epoch 10/250, batch 6/75, loss 0.3395\n",
            "epoch 10/250, batch 7/75, loss 0.3571\n",
            "epoch 10/250, batch 8/75, loss 1.0840\n",
            "epoch 10/250, batch 9/75, loss 0.3483\n",
            "epoch 10/250, batch 10/75, loss 1.0688\n",
            "epoch 10/250, batch 11/75, loss 0.4221\n",
            "epoch 10/250, batch 12/75, loss 0.3638\n",
            "epoch 10/250, batch 13/75, loss 0.3594\n",
            "epoch 10/250, batch 14/75, loss 0.3503\n",
            "epoch 10/250, batch 15/75, loss 2.2597\n",
            "epoch 10/250, batch 16/75, loss 0.3516\n",
            "epoch 10/250, batch 17/75, loss 0.3725\n",
            "epoch 10/250, batch 18/75, loss 0.3687\n",
            "epoch 10/250, batch 19/75, loss 0.3674\n",
            "epoch 10/250, batch 20/75, loss 1.4549\n",
            "epoch 10/250, batch 21/75, loss 0.3631\n",
            "epoch 10/250, batch 22/75, loss 0.3565\n",
            "epoch 10/250, batch 23/75, loss 0.3961\n",
            "epoch 10/250, batch 24/75, loss 0.3842\n",
            "epoch 10/250, batch 25/75, loss 0.3834\n",
            "epoch 10/250, batch 26/75, loss 3.6094\n",
            "epoch 10/250, batch 27/75, loss 0.3968\n",
            "epoch 10/250, batch 28/75, loss 0.4281\n",
            "epoch 10/250, batch 29/75, loss 0.4015\n",
            "epoch 10/250, batch 30/75, loss 0.4178\n",
            "epoch 10/250, batch 31/75, loss 0.4273\n",
            "epoch 10/250, batch 32/75, loss 0.3984\n",
            "epoch 10/250, batch 33/75, loss 0.4191\n",
            "epoch 10/250, batch 34/75, loss 0.4478\n",
            "epoch 10/250, batch 35/75, loss 0.3950\n",
            "epoch 10/250, batch 36/75, loss 3.2532\n",
            "epoch 10/250, batch 37/75, loss 0.6449\n",
            "epoch 10/250, batch 38/75, loss 0.4125\n",
            "epoch 10/250, batch 39/75, loss 0.4290\n",
            "epoch 10/250, batch 40/75, loss 0.4285\n",
            "epoch 10/250, batch 41/75, loss 0.4093\n",
            "epoch 10/250, batch 42/75, loss 0.4189\n",
            "epoch 10/250, batch 43/75, loss 0.4032\n",
            "epoch 10/250, batch 44/75, loss 0.4160\n",
            "epoch 10/250, batch 45/75, loss 0.4251\n",
            "epoch 10/250, batch 46/75, loss 0.4107\n",
            "epoch 10/250, batch 47/75, loss 0.4183\n",
            "epoch 10/250, batch 48/75, loss 0.4103\n",
            "epoch 10/250, batch 49/75, loss 0.4141\n",
            "epoch 10/250, batch 50/75, loss 0.4031\n",
            "epoch 10/250, batch 51/75, loss 0.3939\n",
            "epoch 10/250, batch 52/75, loss 0.3679\n",
            "epoch 10/250, batch 53/75, loss 14.1312\n",
            "epoch 10/250, batch 54/75, loss 0.5244\n",
            "epoch 10/250, batch 55/75, loss 0.3892\n",
            "epoch 10/250, batch 56/75, loss 0.3920\n",
            "epoch 10/250, batch 57/75, loss 0.4192\n",
            "epoch 10/250, batch 58/75, loss 0.3982\n",
            "epoch 10/250, batch 59/75, loss 0.4202\n",
            "epoch 10/250, batch 60/75, loss 0.4133\n",
            "epoch 10/250, batch 61/75, loss 0.4066\n",
            "epoch 10/250, batch 62/75, loss 0.4323\n",
            "epoch 10/250, batch 63/75, loss 0.4138\n",
            "epoch 10/250, batch 64/75, loss 0.4246\n",
            "epoch 10/250, batch 65/75, loss 0.4101\n",
            "epoch 10/250, batch 66/75, loss 0.3958\n",
            "epoch 10/250, batch 67/75, loss 0.4103\n",
            "epoch 10/250, batch 68/75, loss 0.4004\n",
            "epoch 10/250, batch 69/75, loss 0.4180\n",
            "epoch 10/250, batch 70/75, loss 0.3913\n",
            "epoch 10/250, batch 71/75, loss 0.3764\n",
            "epoch 10/250, batch 72/75, loss 0.3830\n",
            "epoch 10/250, batch 73/75, loss 0.3830\n",
            "epoch 10/250, batch 74/75, loss 0.3929\n",
            "epoch 10/250, batch 75/75, loss 0.3559\n",
            "epoch 10/250, training roc_auc_score 0.8067\n",
            "EarlyStopping counter: 4 out of 50\n",
            "epoch 10/250, validation roc_auc_score 0.8488, best validation roc_auc_score 0.8506\n",
            "epoch 11/250, batch 1/75, loss 0.6897\n",
            "epoch 11/250, batch 2/75, loss 0.8800\n",
            "epoch 11/250, batch 3/75, loss 0.3847\n",
            "epoch 11/250, batch 4/75, loss 0.4971\n",
            "epoch 11/250, batch 5/75, loss 0.3628\n",
            "epoch 11/250, batch 6/75, loss 0.3239\n",
            "epoch 11/250, batch 7/75, loss 0.3497\n",
            "epoch 11/250, batch 8/75, loss 1.1007\n",
            "epoch 11/250, batch 9/75, loss 0.3318\n",
            "epoch 11/250, batch 10/75, loss 0.7346\n",
            "epoch 11/250, batch 11/75, loss 0.4189\n",
            "epoch 11/250, batch 12/75, loss 0.3540\n",
            "epoch 11/250, batch 13/75, loss 0.3524\n",
            "epoch 11/250, batch 14/75, loss 0.3310\n",
            "epoch 11/250, batch 15/75, loss 2.5295\n",
            "epoch 11/250, batch 16/75, loss 0.3298\n",
            "epoch 11/250, batch 17/75, loss 0.3660\n",
            "epoch 11/250, batch 18/75, loss 0.3591\n",
            "epoch 11/250, batch 19/75, loss 0.3628\n",
            "epoch 11/250, batch 20/75, loss 1.3688\n",
            "epoch 11/250, batch 21/75, loss 0.3527\n",
            "epoch 11/250, batch 22/75, loss 0.3409\n",
            "epoch 11/250, batch 23/75, loss 0.3984\n",
            "epoch 11/250, batch 24/75, loss 0.3733\n",
            "epoch 11/250, batch 25/75, loss 0.3612\n",
            "epoch 11/250, batch 26/75, loss 3.3808\n",
            "epoch 11/250, batch 27/75, loss 0.3826\n",
            "epoch 11/250, batch 28/75, loss 0.4055\n",
            "epoch 11/250, batch 29/75, loss 0.3887\n",
            "epoch 11/250, batch 30/75, loss 0.3963\n",
            "epoch 11/250, batch 31/75, loss 0.4062\n",
            "epoch 11/250, batch 32/75, loss 0.3705\n",
            "epoch 11/250, batch 33/75, loss 0.4008\n",
            "epoch 11/250, batch 34/75, loss 0.4221\n",
            "epoch 11/250, batch 35/75, loss 0.3800\n",
            "epoch 11/250, batch 36/75, loss 2.7833\n",
            "epoch 11/250, batch 37/75, loss 0.6847\n",
            "epoch 11/250, batch 38/75, loss 0.3823\n",
            "epoch 11/250, batch 39/75, loss 0.3914\n",
            "epoch 11/250, batch 40/75, loss 0.3844\n",
            "epoch 11/250, batch 41/75, loss 0.3778\n",
            "epoch 11/250, batch 42/75, loss 0.3879\n",
            "epoch 11/250, batch 43/75, loss 0.3647\n",
            "epoch 11/250, batch 44/75, loss 0.3872\n",
            "epoch 11/250, batch 45/75, loss 0.3953\n",
            "epoch 11/250, batch 46/75, loss 0.3772\n",
            "epoch 11/250, batch 47/75, loss 0.3895\n",
            "epoch 11/250, batch 48/75, loss 0.3847\n",
            "epoch 11/250, batch 49/75, loss 0.3839\n",
            "epoch 11/250, batch 50/75, loss 0.3831\n",
            "epoch 11/250, batch 51/75, loss 0.3687\n",
            "epoch 11/250, batch 52/75, loss 0.3471\n",
            "epoch 11/250, batch 53/75, loss 14.0081\n",
            "epoch 11/250, batch 54/75, loss 0.4523\n",
            "epoch 11/250, batch 55/75, loss 0.3681\n",
            "epoch 11/250, batch 56/75, loss 0.3737\n",
            "epoch 11/250, batch 57/75, loss 0.3928\n",
            "epoch 11/250, batch 58/75, loss 0.3755\n",
            "epoch 11/250, batch 59/75, loss 0.3969\n",
            "epoch 11/250, batch 60/75, loss 0.3870\n",
            "epoch 11/250, batch 61/75, loss 0.3860\n",
            "epoch 11/250, batch 62/75, loss 0.4128\n",
            "epoch 11/250, batch 63/75, loss 0.3956\n",
            "epoch 11/250, batch 64/75, loss 0.4032\n",
            "epoch 11/250, batch 65/75, loss 0.3952\n",
            "epoch 11/250, batch 66/75, loss 0.3771\n",
            "epoch 11/250, batch 67/75, loss 0.3880\n",
            "epoch 11/250, batch 68/75, loss 0.3856\n",
            "epoch 11/250, batch 69/75, loss 0.3978\n",
            "epoch 11/250, batch 70/75, loss 0.3669\n",
            "epoch 11/250, batch 71/75, loss 0.3569\n",
            "epoch 11/250, batch 72/75, loss 0.3643\n",
            "epoch 11/250, batch 73/75, loss 0.3637\n",
            "epoch 11/250, batch 74/75, loss 0.3710\n",
            "epoch 11/250, batch 75/75, loss 0.3323\n",
            "epoch 11/250, training roc_auc_score 0.8276\n",
            "EarlyStopping counter: 5 out of 50\n",
            "epoch 11/250, validation roc_auc_score 0.8323, best validation roc_auc_score 0.8506\n",
            "epoch 12/250, batch 1/75, loss 0.6137\n",
            "epoch 12/250, batch 2/75, loss 0.8806\n",
            "epoch 12/250, batch 3/75, loss 0.3496\n",
            "epoch 12/250, batch 4/75, loss 0.4705\n",
            "epoch 12/250, batch 5/75, loss 0.3258\n",
            "epoch 12/250, batch 6/75, loss 0.3063\n",
            "epoch 12/250, batch 7/75, loss 0.3316\n",
            "epoch 12/250, batch 8/75, loss 0.9949\n",
            "epoch 12/250, batch 9/75, loss 0.3261\n",
            "epoch 12/250, batch 10/75, loss 0.8028\n",
            "epoch 12/250, batch 11/75, loss 0.4240\n",
            "epoch 12/250, batch 12/75, loss 0.3364\n",
            "epoch 12/250, batch 13/75, loss 0.3257\n",
            "epoch 12/250, batch 14/75, loss 0.3158\n",
            "epoch 12/250, batch 15/75, loss 2.6135\n",
            "epoch 12/250, batch 16/75, loss 0.3196\n",
            "epoch 12/250, batch 17/75, loss 0.3448\n",
            "epoch 12/250, batch 18/75, loss 0.3426\n",
            "epoch 12/250, batch 19/75, loss 0.3446\n",
            "epoch 12/250, batch 20/75, loss 1.2055\n",
            "epoch 12/250, batch 21/75, loss 0.3319\n",
            "epoch 12/250, batch 22/75, loss 0.3289\n",
            "epoch 12/250, batch 23/75, loss 0.3605\n",
            "epoch 12/250, batch 24/75, loss 0.3435\n",
            "epoch 12/250, batch 25/75, loss 0.3382\n",
            "epoch 12/250, batch 26/75, loss 3.1289\n",
            "epoch 12/250, batch 27/75, loss 0.3645\n",
            "epoch 12/250, batch 28/75, loss 0.3881\n",
            "epoch 12/250, batch 29/75, loss 0.3619\n",
            "epoch 12/250, batch 30/75, loss 0.3668\n",
            "epoch 12/250, batch 31/75, loss 0.3723\n",
            "epoch 12/250, batch 32/75, loss 0.3366\n",
            "epoch 12/250, batch 33/75, loss 0.3637\n",
            "epoch 12/250, batch 34/75, loss 0.3919\n",
            "epoch 12/250, batch 35/75, loss 0.3419\n",
            "epoch 12/250, batch 36/75, loss 2.7913\n",
            "epoch 12/250, batch 37/75, loss 0.6065\n",
            "epoch 12/250, batch 38/75, loss 0.3548\n",
            "epoch 12/250, batch 39/75, loss 0.3670\n",
            "epoch 12/250, batch 40/75, loss 0.3506\n",
            "epoch 12/250, batch 41/75, loss 0.3510\n",
            "epoch 12/250, batch 42/75, loss 0.3534\n",
            "epoch 12/250, batch 43/75, loss 0.3354\n",
            "epoch 12/250, batch 44/75, loss 0.3553\n",
            "epoch 12/250, batch 45/75, loss 0.3699\n",
            "epoch 12/250, batch 46/75, loss 0.3498\n",
            "epoch 12/250, batch 47/75, loss 0.3657\n",
            "epoch 12/250, batch 48/75, loss 0.3543\n",
            "epoch 12/250, batch 49/75, loss 0.3605\n",
            "epoch 12/250, batch 50/75, loss 0.3542\n",
            "epoch 12/250, batch 51/75, loss 0.3455\n",
            "epoch 12/250, batch 52/75, loss 0.3203\n",
            "epoch 12/250, batch 53/75, loss 14.5250\n",
            "epoch 12/250, batch 54/75, loss 0.4196\n",
            "epoch 12/250, batch 55/75, loss 0.3490\n",
            "epoch 12/250, batch 56/75, loss 0.3530\n",
            "epoch 12/250, batch 57/75, loss 0.3740\n",
            "epoch 12/250, batch 58/75, loss 0.3530\n",
            "epoch 12/250, batch 59/75, loss 0.3786\n",
            "epoch 12/250, batch 60/75, loss 0.3714\n",
            "epoch 12/250, batch 61/75, loss 0.3691\n",
            "epoch 12/250, batch 62/75, loss 0.3959\n",
            "epoch 12/250, batch 63/75, loss 0.3794\n",
            "epoch 12/250, batch 64/75, loss 0.3863\n",
            "epoch 12/250, batch 65/75, loss 0.3819\n",
            "epoch 12/250, batch 66/75, loss 0.3657\n",
            "epoch 12/250, batch 67/75, loss 0.3880\n",
            "epoch 12/250, batch 68/75, loss 0.3765\n",
            "epoch 12/250, batch 69/75, loss 0.4002\n",
            "epoch 12/250, batch 70/75, loss 0.3583\n",
            "epoch 12/250, batch 71/75, loss 0.3445\n",
            "epoch 12/250, batch 72/75, loss 0.3497\n",
            "epoch 12/250, batch 73/75, loss 0.3554\n",
            "epoch 12/250, batch 74/75, loss 0.3654\n",
            "epoch 12/250, batch 75/75, loss 0.3278\n",
            "epoch 12/250, training roc_auc_score 0.8398\n",
            "epoch 12/250, validation roc_auc_score 0.8516, best validation roc_auc_score 0.8516\n",
            "epoch 13/250, batch 1/75, loss 0.5169\n",
            "epoch 13/250, batch 2/75, loss 0.7226\n",
            "epoch 13/250, batch 3/75, loss 0.3538\n",
            "epoch 13/250, batch 4/75, loss 0.4749\n",
            "epoch 13/250, batch 5/75, loss 0.3272\n",
            "epoch 13/250, batch 6/75, loss 0.2997\n",
            "epoch 13/250, batch 7/75, loss 0.3222\n",
            "epoch 13/250, batch 8/75, loss 0.8453\n",
            "epoch 13/250, batch 9/75, loss 0.3181\n",
            "epoch 13/250, batch 10/75, loss 0.6198\n",
            "epoch 13/250, batch 11/75, loss 0.3513\n",
            "epoch 13/250, batch 12/75, loss 0.3363\n",
            "epoch 13/250, batch 13/75, loss 0.2978\n",
            "epoch 13/250, batch 14/75, loss 0.3017\n",
            "epoch 13/250, batch 15/75, loss 1.8486\n",
            "epoch 13/250, batch 16/75, loss 0.3076\n",
            "epoch 13/250, batch 17/75, loss 0.3341\n",
            "epoch 13/250, batch 18/75, loss 0.3305\n",
            "epoch 13/250, batch 19/75, loss 0.3332\n",
            "epoch 13/250, batch 20/75, loss 1.0106\n",
            "epoch 13/250, batch 21/75, loss 0.3064\n",
            "epoch 13/250, batch 22/75, loss 0.3162\n",
            "epoch 13/250, batch 23/75, loss 0.3320\n",
            "epoch 13/250, batch 24/75, loss 0.3282\n",
            "epoch 13/250, batch 25/75, loss 0.3014\n",
            "epoch 13/250, batch 26/75, loss 3.2421\n",
            "epoch 13/250, batch 27/75, loss 0.3472\n",
            "epoch 13/250, batch 28/75, loss 0.3695\n",
            "epoch 13/250, batch 29/75, loss 0.3428\n",
            "epoch 13/250, batch 30/75, loss 0.3446\n",
            "epoch 13/250, batch 31/75, loss 0.3477\n",
            "epoch 13/250, batch 32/75, loss 0.3168\n",
            "epoch 13/250, batch 33/75, loss 0.3569\n",
            "epoch 13/250, batch 34/75, loss 0.3907\n",
            "epoch 13/250, batch 35/75, loss 0.3396\n",
            "epoch 13/250, batch 36/75, loss 2.6432\n",
            "epoch 13/250, batch 37/75, loss 0.5628\n",
            "epoch 13/250, batch 38/75, loss 0.3351\n",
            "epoch 13/250, batch 39/75, loss 0.3499\n",
            "epoch 13/250, batch 40/75, loss 0.3300\n",
            "epoch 13/250, batch 41/75, loss 0.3378\n",
            "epoch 13/250, batch 42/75, loss 0.3378\n",
            "epoch 13/250, batch 43/75, loss 0.3167\n",
            "epoch 13/250, batch 44/75, loss 0.3439\n",
            "epoch 13/250, batch 45/75, loss 0.3624\n",
            "epoch 13/250, batch 46/75, loss 0.3383\n",
            "epoch 13/250, batch 47/75, loss 0.3527\n",
            "epoch 13/250, batch 48/75, loss 0.3439\n",
            "epoch 13/250, batch 49/75, loss 0.3446\n",
            "epoch 13/250, batch 50/75, loss 0.3334\n",
            "epoch 13/250, batch 51/75, loss 0.3379\n",
            "epoch 13/250, batch 52/75, loss 0.3042\n",
            "epoch 13/250, batch 53/75, loss 14.4437\n",
            "epoch 13/250, batch 54/75, loss 0.4398\n",
            "epoch 13/250, batch 55/75, loss 0.3299\n",
            "epoch 13/250, batch 56/75, loss 0.3378\n",
            "epoch 13/250, batch 57/75, loss 0.3567\n",
            "epoch 13/250, batch 58/75, loss 0.3393\n",
            "epoch 13/250, batch 59/75, loss 0.3668\n",
            "epoch 13/250, batch 60/75, loss 0.3629\n",
            "epoch 13/250, batch 61/75, loss 0.3665\n",
            "epoch 13/250, batch 62/75, loss 0.3866\n",
            "epoch 13/250, batch 63/75, loss 0.3661\n",
            "epoch 13/250, batch 64/75, loss 0.3778\n",
            "epoch 13/250, batch 65/75, loss 0.3704\n",
            "epoch 13/250, batch 66/75, loss 0.3594\n",
            "epoch 13/250, batch 67/75, loss 0.3652\n",
            "epoch 13/250, batch 68/75, loss 0.3560\n",
            "epoch 13/250, batch 69/75, loss 0.3783\n",
            "epoch 13/250, batch 70/75, loss 0.3383\n",
            "epoch 13/250, batch 71/75, loss 0.3287\n",
            "epoch 13/250, batch 72/75, loss 0.3342\n",
            "epoch 13/250, batch 73/75, loss 0.3428\n",
            "epoch 13/250, batch 74/75, loss 0.3476\n",
            "epoch 13/250, batch 75/75, loss 0.3147\n",
            "epoch 13/250, training roc_auc_score 0.8581\n",
            "EarlyStopping counter: 1 out of 50\n",
            "epoch 13/250, validation roc_auc_score 0.8337, best validation roc_auc_score 0.8516\n",
            "epoch 14/250, batch 1/75, loss 0.5983\n",
            "epoch 14/250, batch 2/75, loss 0.7546\n",
            "epoch 14/250, batch 3/75, loss 0.3456\n",
            "epoch 14/250, batch 4/75, loss 0.5031\n",
            "epoch 14/250, batch 5/75, loss 0.3116\n",
            "epoch 14/250, batch 6/75, loss 0.2788\n",
            "epoch 14/250, batch 7/75, loss 0.3085\n",
            "epoch 14/250, batch 8/75, loss 0.7541\n",
            "epoch 14/250, batch 9/75, loss 0.3011\n",
            "epoch 14/250, batch 10/75, loss 0.5040\n",
            "epoch 14/250, batch 11/75, loss 0.3580\n",
            "epoch 14/250, batch 12/75, loss 0.3186\n",
            "epoch 14/250, batch 13/75, loss 0.3060\n",
            "epoch 14/250, batch 14/75, loss 0.2902\n",
            "epoch 14/250, batch 15/75, loss 2.1355\n",
            "epoch 14/250, batch 16/75, loss 0.2960\n",
            "epoch 14/250, batch 17/75, loss 0.3358\n",
            "epoch 14/250, batch 18/75, loss 0.3205\n",
            "epoch 14/250, batch 19/75, loss 0.3291\n",
            "epoch 14/250, batch 20/75, loss 0.9303\n",
            "epoch 14/250, batch 21/75, loss 0.2944\n",
            "epoch 14/250, batch 22/75, loss 0.3001\n",
            "epoch 14/250, batch 23/75, loss 0.3334\n",
            "epoch 14/250, batch 24/75, loss 0.3203\n",
            "epoch 14/250, batch 25/75, loss 0.3024\n",
            "epoch 14/250, batch 26/75, loss 2.6759\n",
            "epoch 14/250, batch 27/75, loss 0.3396\n",
            "epoch 14/250, batch 28/75, loss 0.3717\n",
            "epoch 14/250, batch 29/75, loss 0.3522\n",
            "epoch 14/250, batch 30/75, loss 0.3623\n",
            "epoch 14/250, batch 31/75, loss 0.3376\n",
            "epoch 14/250, batch 32/75, loss 0.3094\n",
            "epoch 14/250, batch 33/75, loss 0.3344\n",
            "epoch 14/250, batch 34/75, loss 0.3766\n",
            "epoch 14/250, batch 35/75, loss 0.3464\n",
            "epoch 14/250, batch 36/75, loss 2.7011\n",
            "epoch 14/250, batch 37/75, loss 0.5744\n",
            "epoch 14/250, batch 38/75, loss 0.3318\n",
            "epoch 14/250, batch 39/75, loss 0.3473\n",
            "epoch 14/250, batch 40/75, loss 0.3387\n",
            "epoch 14/250, batch 41/75, loss 0.3424\n",
            "epoch 14/250, batch 42/75, loss 0.3300\n",
            "epoch 14/250, batch 43/75, loss 0.3075\n",
            "epoch 14/250, batch 44/75, loss 0.3415\n",
            "epoch 14/250, batch 45/75, loss 0.3464\n",
            "epoch 14/250, batch 46/75, loss 0.3208\n",
            "epoch 14/250, batch 47/75, loss 0.3460\n",
            "epoch 14/250, batch 48/75, loss 0.3314\n",
            "epoch 14/250, batch 49/75, loss 0.3413\n",
            "epoch 14/250, batch 50/75, loss 0.3371\n",
            "epoch 14/250, batch 51/75, loss 0.3298\n",
            "epoch 14/250, batch 52/75, loss 0.2932\n",
            "epoch 14/250, batch 53/75, loss 14.0555\n",
            "epoch 14/250, batch 54/75, loss 0.3985\n",
            "epoch 14/250, batch 55/75, loss 0.3296\n",
            "epoch 14/250, batch 56/75, loss 0.3180\n",
            "epoch 14/250, batch 57/75, loss 0.3346\n",
            "epoch 14/250, batch 58/75, loss 0.3199\n",
            "epoch 14/250, batch 59/75, loss 0.3444\n",
            "epoch 14/250, batch 60/75, loss 0.3436\n",
            "epoch 14/250, batch 61/75, loss 0.3422\n",
            "epoch 14/250, batch 62/75, loss 0.3677\n",
            "epoch 14/250, batch 63/75, loss 0.3451\n",
            "epoch 14/250, batch 64/75, loss 0.3592\n",
            "epoch 14/250, batch 65/75, loss 0.3565\n",
            "epoch 14/250, batch 66/75, loss 0.3362\n",
            "epoch 14/250, batch 67/75, loss 0.3437\n",
            "epoch 14/250, batch 68/75, loss 0.3389\n",
            "epoch 14/250, batch 69/75, loss 0.3680\n",
            "epoch 14/250, batch 70/75, loss 0.3274\n",
            "epoch 14/250, batch 71/75, loss 0.3157\n",
            "epoch 14/250, batch 72/75, loss 0.3237\n",
            "epoch 14/250, batch 73/75, loss 0.3307\n",
            "epoch 14/250, batch 74/75, loss 0.3406\n",
            "epoch 14/250, batch 75/75, loss 0.3062\n",
            "epoch 14/250, training roc_auc_score 0.8687\n",
            "EarlyStopping counter: 2 out of 50\n",
            "epoch 14/250, validation roc_auc_score 0.8301, best validation roc_auc_score 0.8516\n",
            "epoch 15/250, batch 1/75, loss 0.4890\n",
            "epoch 15/250, batch 2/75, loss 0.6984\n",
            "epoch 15/250, batch 3/75, loss 0.3232\n",
            "epoch 15/250, batch 4/75, loss 0.4588\n",
            "epoch 15/250, batch 5/75, loss 0.2886\n",
            "epoch 15/250, batch 6/75, loss 0.2677\n",
            "epoch 15/250, batch 7/75, loss 0.3117\n",
            "epoch 15/250, batch 8/75, loss 0.7048\n",
            "epoch 15/250, batch 9/75, loss 0.2897\n",
            "epoch 15/250, batch 10/75, loss 0.5169\n",
            "epoch 15/250, batch 11/75, loss 0.3327\n",
            "epoch 15/250, batch 12/75, loss 0.3097\n",
            "epoch 15/250, batch 13/75, loss 0.2834\n",
            "epoch 15/250, batch 14/75, loss 0.2771\n",
            "epoch 15/250, batch 15/75, loss 1.8071\n",
            "epoch 15/250, batch 16/75, loss 0.2905\n",
            "epoch 15/250, batch 17/75, loss 0.3145\n",
            "epoch 15/250, batch 18/75, loss 0.3156\n",
            "epoch 15/250, batch 19/75, loss 0.3173\n",
            "epoch 15/250, batch 20/75, loss 0.9603\n",
            "epoch 15/250, batch 21/75, loss 0.2712\n",
            "epoch 15/250, batch 22/75, loss 0.2857\n",
            "epoch 15/250, batch 23/75, loss 0.3095\n",
            "epoch 15/250, batch 24/75, loss 0.2824\n",
            "epoch 15/250, batch 25/75, loss 0.2737\n",
            "epoch 15/250, batch 26/75, loss 2.8129\n",
            "epoch 15/250, batch 27/75, loss 0.3194\n",
            "epoch 15/250, batch 28/75, loss 0.3501\n",
            "epoch 15/250, batch 29/75, loss 0.3337\n",
            "epoch 15/250, batch 30/75, loss 0.3375\n",
            "epoch 15/250, batch 31/75, loss 0.3282\n",
            "epoch 15/250, batch 32/75, loss 0.2961\n",
            "epoch 15/250, batch 33/75, loss 0.3291\n",
            "epoch 15/250, batch 34/75, loss 0.3762\n",
            "epoch 15/250, batch 35/75, loss 0.3284\n",
            "epoch 15/250, batch 36/75, loss 2.5444\n",
            "epoch 15/250, batch 37/75, loss 0.5215\n",
            "epoch 15/250, batch 38/75, loss 0.3208\n",
            "epoch 15/250, batch 39/75, loss 0.3385\n",
            "epoch 15/250, batch 40/75, loss 0.3285\n",
            "epoch 15/250, batch 41/75, loss 0.3394\n",
            "epoch 15/250, batch 42/75, loss 0.3071\n",
            "epoch 15/250, batch 43/75, loss 0.2903\n",
            "epoch 15/250, batch 44/75, loss 0.3291\n",
            "epoch 15/250, batch 45/75, loss 0.3429\n",
            "epoch 15/250, batch 46/75, loss 0.3045\n",
            "epoch 15/250, batch 47/75, loss 0.3283\n",
            "epoch 15/250, batch 48/75, loss 0.3169\n",
            "epoch 15/250, batch 49/75, loss 0.3229\n",
            "epoch 15/250, batch 50/75, loss 0.3174\n",
            "epoch 15/250, batch 51/75, loss 0.3249\n",
            "epoch 15/250, batch 52/75, loss 0.2768\n",
            "epoch 15/250, batch 53/75, loss 13.8956\n",
            "epoch 15/250, batch 54/75, loss 0.3695\n",
            "epoch 15/250, batch 55/75, loss 0.3133\n",
            "epoch 15/250, batch 56/75, loss 0.3086\n",
            "epoch 15/250, batch 57/75, loss 0.3199\n",
            "epoch 15/250, batch 58/75, loss 0.3141\n",
            "epoch 15/250, batch 59/75, loss 0.3388\n",
            "epoch 15/250, batch 60/75, loss 0.3346\n",
            "epoch 15/250, batch 61/75, loss 0.3410\n",
            "epoch 15/250, batch 62/75, loss 0.3730\n",
            "epoch 15/250, batch 63/75, loss 0.3411\n",
            "epoch 15/250, batch 64/75, loss 0.3627\n",
            "epoch 15/250, batch 65/75, loss 0.3584\n",
            "epoch 15/250, batch 66/75, loss 0.3443\n",
            "epoch 15/250, batch 67/75, loss 0.3422\n",
            "epoch 15/250, batch 68/75, loss 0.3304\n",
            "epoch 15/250, batch 69/75, loss 0.3703\n",
            "epoch 15/250, batch 70/75, loss 0.3213\n",
            "epoch 15/250, batch 71/75, loss 0.3152\n",
            "epoch 15/250, batch 72/75, loss 0.3276\n",
            "epoch 15/250, batch 73/75, loss 0.3205\n",
            "epoch 15/250, batch 74/75, loss 0.3322\n",
            "epoch 15/250, batch 75/75, loss 0.2918\n",
            "epoch 15/250, training roc_auc_score 0.8789\n",
            "EarlyStopping counter: 3 out of 50\n",
            "epoch 15/250, validation roc_auc_score 0.8225, best validation roc_auc_score 0.8516\n",
            "epoch 16/250, batch 1/75, loss 0.5574\n",
            "epoch 16/250, batch 2/75, loss 0.6927\n",
            "epoch 16/250, batch 3/75, loss 0.3216\n",
            "epoch 16/250, batch 4/75, loss 0.4455\n",
            "epoch 16/250, batch 5/75, loss 0.2707\n",
            "epoch 16/250, batch 6/75, loss 0.2538\n",
            "epoch 16/250, batch 7/75, loss 0.2967\n",
            "epoch 16/250, batch 8/75, loss 0.6243\n",
            "epoch 16/250, batch 9/75, loss 0.2722\n",
            "epoch 16/250, batch 10/75, loss 0.4734\n",
            "epoch 16/250, batch 11/75, loss 0.3447\n",
            "epoch 16/250, batch 12/75, loss 0.3051\n",
            "epoch 16/250, batch 13/75, loss 0.2844\n",
            "epoch 16/250, batch 14/75, loss 0.2561\n",
            "epoch 16/250, batch 15/75, loss 1.7629\n",
            "epoch 16/250, batch 16/75, loss 0.2827\n",
            "epoch 16/250, batch 17/75, loss 0.3162\n",
            "epoch 16/250, batch 18/75, loss 0.3135\n",
            "epoch 16/250, batch 19/75, loss 0.3092\n",
            "epoch 16/250, batch 20/75, loss 0.8306\n",
            "epoch 16/250, batch 21/75, loss 0.2601\n",
            "epoch 16/250, batch 22/75, loss 0.2733\n",
            "epoch 16/250, batch 23/75, loss 0.3013\n",
            "epoch 16/250, batch 24/75, loss 0.2666\n",
            "epoch 16/250, batch 25/75, loss 0.2624\n",
            "epoch 16/250, batch 26/75, loss 2.3224\n",
            "epoch 16/250, batch 27/75, loss 0.3067\n",
            "epoch 16/250, batch 28/75, loss 0.3328\n",
            "epoch 16/250, batch 29/75, loss 0.3027\n",
            "epoch 16/250, batch 30/75, loss 0.3102\n",
            "epoch 16/250, batch 31/75, loss 0.2952\n",
            "epoch 16/250, batch 32/75, loss 0.2728\n",
            "epoch 16/250, batch 33/75, loss 0.2990\n",
            "epoch 16/250, batch 34/75, loss 0.3400\n",
            "epoch 16/250, batch 35/75, loss 0.3005\n",
            "epoch 16/250, batch 36/75, loss 2.3825\n",
            "epoch 16/250, batch 37/75, loss 0.4703\n",
            "epoch 16/250, batch 38/75, loss 0.2942\n",
            "epoch 16/250, batch 39/75, loss 0.3176\n",
            "epoch 16/250, batch 40/75, loss 0.3158\n",
            "epoch 16/250, batch 41/75, loss 0.3144\n",
            "epoch 16/250, batch 42/75, loss 0.2900\n",
            "epoch 16/250, batch 43/75, loss 0.2684\n",
            "epoch 16/250, batch 44/75, loss 0.3117\n",
            "epoch 16/250, batch 45/75, loss 0.3140\n",
            "epoch 16/250, batch 46/75, loss 0.2815\n",
            "epoch 16/250, batch 47/75, loss 0.3050\n",
            "epoch 16/250, batch 48/75, loss 0.2927\n",
            "epoch 16/250, batch 49/75, loss 0.3009\n",
            "epoch 16/250, batch 50/75, loss 0.2863\n",
            "epoch 16/250, batch 51/75, loss 0.2996\n",
            "epoch 16/250, batch 52/75, loss 0.2546\n",
            "epoch 16/250, batch 53/75, loss 13.9242\n",
            "epoch 16/250, batch 54/75, loss 0.3401\n",
            "epoch 16/250, batch 55/75, loss 0.3006\n",
            "epoch 16/250, batch 56/75, loss 0.2981\n",
            "epoch 16/250, batch 57/75, loss 0.3088\n",
            "epoch 16/250, batch 58/75, loss 0.3050\n",
            "epoch 16/250, batch 59/75, loss 0.3228\n",
            "epoch 16/250, batch 60/75, loss 0.3293\n",
            "epoch 16/250, batch 61/75, loss 0.3332\n",
            "epoch 16/250, batch 62/75, loss 0.3571\n",
            "epoch 16/250, batch 63/75, loss 0.3311\n",
            "epoch 16/250, batch 64/75, loss 0.3512\n",
            "epoch 16/250, batch 65/75, loss 0.3553\n",
            "epoch 16/250, batch 66/75, loss 0.3382\n",
            "epoch 16/250, batch 67/75, loss 0.3391\n",
            "epoch 16/250, batch 68/75, loss 0.3258\n",
            "epoch 16/250, batch 69/75, loss 0.3696\n",
            "epoch 16/250, batch 70/75, loss 0.3141\n",
            "epoch 16/250, batch 71/75, loss 0.2983\n",
            "epoch 16/250, batch 72/75, loss 0.3204\n",
            "epoch 16/250, batch 73/75, loss 0.3258\n",
            "epoch 16/250, batch 74/75, loss 0.3382\n",
            "epoch 16/250, batch 75/75, loss 0.2969\n",
            "epoch 16/250, training roc_auc_score 0.8907\n",
            "EarlyStopping counter: 4 out of 50\n",
            "epoch 16/250, validation roc_auc_score 0.8196, best validation roc_auc_score 0.8516\n",
            "epoch 17/250, batch 1/75, loss 0.5041\n",
            "epoch 17/250, batch 2/75, loss 0.7437\n",
            "epoch 17/250, batch 3/75, loss 0.3147\n",
            "epoch 17/250, batch 4/75, loss 0.4418\n",
            "epoch 17/250, batch 5/75, loss 0.2570\n",
            "epoch 17/250, batch 6/75, loss 0.2432\n",
            "epoch 17/250, batch 7/75, loss 0.2819\n",
            "epoch 17/250, batch 8/75, loss 0.6554\n",
            "epoch 17/250, batch 9/75, loss 0.2692\n",
            "epoch 17/250, batch 10/75, loss 0.5530\n",
            "epoch 17/250, batch 11/75, loss 0.3833\n",
            "epoch 17/250, batch 12/75, loss 0.2907\n",
            "epoch 17/250, batch 13/75, loss 0.2968\n",
            "epoch 17/250, batch 14/75, loss 0.2433\n",
            "epoch 17/250, batch 15/75, loss 1.7638\n",
            "epoch 17/250, batch 16/75, loss 0.2783\n",
            "epoch 17/250, batch 17/75, loss 0.3250\n",
            "epoch 17/250, batch 18/75, loss 0.3263\n",
            "epoch 17/250, batch 19/75, loss 0.3198\n",
            "epoch 17/250, batch 20/75, loss 0.8426\n",
            "epoch 17/250, batch 21/75, loss 0.2618\n",
            "epoch 17/250, batch 22/75, loss 0.2760\n",
            "epoch 17/250, batch 23/75, loss 0.3074\n",
            "epoch 17/250, batch 24/75, loss 0.2746\n",
            "epoch 17/250, batch 25/75, loss 0.2641\n",
            "epoch 17/250, batch 26/75, loss 2.2113\n",
            "epoch 17/250, batch 27/75, loss 0.3082\n",
            "epoch 17/250, batch 28/75, loss 0.3350\n",
            "epoch 17/250, batch 29/75, loss 0.3127\n",
            "epoch 17/250, batch 30/75, loss 0.3385\n",
            "epoch 17/250, batch 31/75, loss 0.3028\n",
            "epoch 17/250, batch 32/75, loss 0.2782\n",
            "epoch 17/250, batch 33/75, loss 0.3001\n",
            "epoch 17/250, batch 34/75, loss 0.3603\n",
            "epoch 17/250, batch 35/75, loss 0.3212\n",
            "epoch 17/250, batch 36/75, loss 2.3444\n",
            "epoch 17/250, batch 37/75, loss 0.5030\n",
            "epoch 17/250, batch 38/75, loss 0.2904\n",
            "epoch 17/250, batch 39/75, loss 0.3050\n",
            "epoch 17/250, batch 40/75, loss 0.3005\n",
            "epoch 17/250, batch 41/75, loss 0.3128\n",
            "epoch 17/250, batch 42/75, loss 0.2940\n",
            "epoch 17/250, batch 43/75, loss 0.2611\n",
            "epoch 17/250, batch 44/75, loss 0.3070\n",
            "epoch 17/250, batch 45/75, loss 0.3071\n",
            "epoch 17/250, batch 46/75, loss 0.2639\n",
            "epoch 17/250, batch 47/75, loss 0.2980\n",
            "epoch 17/250, batch 48/75, loss 0.2789\n",
            "epoch 17/250, batch 49/75, loss 0.2966\n",
            "epoch 17/250, batch 50/75, loss 0.2806\n",
            "epoch 17/250, batch 51/75, loss 0.2781\n",
            "epoch 17/250, batch 52/75, loss 0.2373\n",
            "epoch 17/250, batch 53/75, loss 13.7630\n",
            "epoch 17/250, batch 54/75, loss 0.3422\n",
            "epoch 17/250, batch 55/75, loss 0.2936\n",
            "epoch 17/250, batch 56/75, loss 0.2802\n",
            "epoch 17/250, batch 57/75, loss 0.2927\n",
            "epoch 17/250, batch 58/75, loss 0.2890\n",
            "epoch 17/250, batch 59/75, loss 0.3091\n",
            "epoch 17/250, batch 60/75, loss 0.3180\n",
            "epoch 17/250, batch 61/75, loss 0.3250\n",
            "epoch 17/250, batch 62/75, loss 0.3565\n",
            "epoch 17/250, batch 63/75, loss 0.3046\n",
            "epoch 17/250, batch 64/75, loss 0.3386\n",
            "epoch 17/250, batch 65/75, loss 0.3378\n",
            "epoch 17/250, batch 66/75, loss 0.3298\n",
            "epoch 17/250, batch 67/75, loss 0.3182\n",
            "epoch 17/250, batch 68/75, loss 0.2994\n",
            "epoch 17/250, batch 69/75, loss 0.3463\n",
            "epoch 17/250, batch 70/75, loss 0.2913\n",
            "epoch 17/250, batch 71/75, loss 0.2911\n",
            "epoch 17/250, batch 72/75, loss 0.2951\n",
            "epoch 17/250, batch 73/75, loss 0.3182\n",
            "epoch 17/250, batch 74/75, loss 0.3304\n",
            "epoch 17/250, batch 75/75, loss 0.2840\n",
            "epoch 17/250, training roc_auc_score 0.8952\n",
            "EarlyStopping counter: 5 out of 50\n",
            "epoch 17/250, validation roc_auc_score 0.8062, best validation roc_auc_score 0.8516\n",
            "epoch 18/250, batch 1/75, loss 0.4252\n",
            "epoch 18/250, batch 2/75, loss 0.6115\n",
            "epoch 18/250, batch 3/75, loss 0.2798\n",
            "epoch 18/250, batch 4/75, loss 0.3456\n",
            "epoch 18/250, batch 5/75, loss 0.2311\n",
            "epoch 18/250, batch 6/75, loss 0.2373\n",
            "epoch 18/250, batch 7/75, loss 0.2747\n",
            "epoch 18/250, batch 8/75, loss 0.7025\n",
            "epoch 18/250, batch 9/75, loss 0.2531\n",
            "epoch 18/250, batch 10/75, loss 0.4433\n",
            "epoch 18/250, batch 11/75, loss 0.3134\n",
            "epoch 18/250, batch 12/75, loss 0.2936\n",
            "epoch 18/250, batch 13/75, loss 0.2850\n",
            "epoch 18/250, batch 14/75, loss 0.2360\n",
            "epoch 18/250, batch 15/75, loss 1.9556\n",
            "epoch 18/250, batch 16/75, loss 0.2807\n",
            "epoch 18/250, batch 17/75, loss 0.3236\n",
            "epoch 18/250, batch 18/75, loss 0.3316\n",
            "epoch 18/250, batch 19/75, loss 0.2946\n",
            "epoch 18/250, batch 20/75, loss 0.8210\n",
            "epoch 18/250, batch 21/75, loss 0.2375\n",
            "epoch 18/250, batch 22/75, loss 0.2723\n",
            "epoch 18/250, batch 23/75, loss 0.2727\n",
            "epoch 18/250, batch 24/75, loss 0.2459\n",
            "epoch 18/250, batch 25/75, loss 0.2398\n",
            "epoch 18/250, batch 26/75, loss 2.0218\n",
            "epoch 18/250, batch 27/75, loss 0.2835\n",
            "epoch 18/250, batch 28/75, loss 0.3161\n",
            "epoch 18/250, batch 29/75, loss 0.2918\n",
            "epoch 18/250, batch 30/75, loss 0.2933\n",
            "epoch 18/250, batch 31/75, loss 0.2738\n",
            "epoch 18/250, batch 32/75, loss 0.2512\n",
            "epoch 18/250, batch 33/75, loss 0.2827\n",
            "epoch 18/250, batch 34/75, loss 0.3236\n",
            "epoch 18/250, batch 35/75, loss 0.2987\n",
            "epoch 18/250, batch 36/75, loss 2.3437\n",
            "epoch 18/250, batch 37/75, loss 0.4313\n",
            "epoch 18/250, batch 38/75, loss 0.2705\n",
            "epoch 18/250, batch 39/75, loss 0.2928\n",
            "epoch 18/250, batch 40/75, loss 0.2784\n",
            "epoch 18/250, batch 41/75, loss 0.3172\n",
            "epoch 18/250, batch 42/75, loss 0.2644\n",
            "epoch 18/250, batch 43/75, loss 0.2480\n",
            "epoch 18/250, batch 44/75, loss 0.2932\n",
            "epoch 18/250, batch 45/75, loss 0.2866\n",
            "epoch 18/250, batch 46/75, loss 0.2509\n",
            "epoch 18/250, batch 47/75, loss 0.2760\n",
            "epoch 18/250, batch 48/75, loss 0.2558\n",
            "epoch 18/250, batch 49/75, loss 0.2718\n",
            "epoch 18/250, batch 50/75, loss 0.2605\n",
            "epoch 18/250, batch 51/75, loss 0.2627\n",
            "epoch 18/250, batch 52/75, loss 0.2176\n",
            "epoch 18/250, batch 53/75, loss 13.5169\n",
            "epoch 18/250, batch 54/75, loss 0.3544\n",
            "epoch 18/250, batch 55/75, loss 0.2776\n",
            "epoch 18/250, batch 56/75, loss 0.2651\n",
            "epoch 18/250, batch 57/75, loss 0.2753\n",
            "epoch 18/250, batch 58/75, loss 0.2826\n",
            "epoch 18/250, batch 59/75, loss 0.3012\n",
            "epoch 18/250, batch 60/75, loss 0.3097\n",
            "epoch 18/250, batch 61/75, loss 0.3085\n",
            "epoch 18/250, batch 62/75, loss 0.3423\n",
            "epoch 18/250, batch 63/75, loss 0.3047\n",
            "epoch 18/250, batch 64/75, loss 0.3415\n",
            "epoch 18/250, batch 65/75, loss 0.3402\n",
            "epoch 18/250, batch 66/75, loss 0.3211\n",
            "epoch 18/250, batch 67/75, loss 0.3147\n",
            "epoch 18/250, batch 68/75, loss 0.3009\n",
            "epoch 18/250, batch 69/75, loss 0.3606\n",
            "epoch 18/250, batch 70/75, loss 0.2951\n",
            "epoch 18/250, batch 71/75, loss 0.2862\n",
            "epoch 18/250, batch 72/75, loss 0.2996\n",
            "epoch 18/250, batch 73/75, loss 0.3141\n",
            "epoch 18/250, batch 74/75, loss 0.3130\n",
            "epoch 18/250, batch 75/75, loss 0.2710\n",
            "epoch 18/250, training roc_auc_score 0.9034\n",
            "EarlyStopping counter: 6 out of 50\n",
            "epoch 18/250, validation roc_auc_score 0.8282, best validation roc_auc_score 0.8516\n",
            "epoch 19/250, batch 1/75, loss 0.4325\n",
            "epoch 19/250, batch 2/75, loss 0.6315\n",
            "epoch 19/250, batch 3/75, loss 0.3060\n",
            "epoch 19/250, batch 4/75, loss 0.4157\n",
            "epoch 19/250, batch 5/75, loss 0.2207\n",
            "epoch 19/250, batch 6/75, loss 0.2179\n",
            "epoch 19/250, batch 7/75, loss 0.2577\n",
            "epoch 19/250, batch 8/75, loss 0.5425\n",
            "epoch 19/250, batch 9/75, loss 0.2459\n",
            "epoch 19/250, batch 10/75, loss 0.5099\n",
            "epoch 19/250, batch 11/75, loss 0.3665\n",
            "epoch 19/250, batch 12/75, loss 0.2817\n",
            "epoch 19/250, batch 13/75, loss 0.2641\n",
            "epoch 19/250, batch 14/75, loss 0.2205\n",
            "epoch 19/250, batch 15/75, loss 1.6721\n",
            "epoch 19/250, batch 16/75, loss 0.2624\n",
            "epoch 19/250, batch 17/75, loss 0.3049\n",
            "epoch 19/250, batch 18/75, loss 0.3107\n",
            "epoch 19/250, batch 19/75, loss 0.2983\n",
            "epoch 19/250, batch 20/75, loss 0.7006\n",
            "epoch 19/250, batch 21/75, loss 0.2337\n",
            "epoch 19/250, batch 22/75, loss 0.2602\n",
            "epoch 19/250, batch 23/75, loss 0.2655\n",
            "epoch 19/250, batch 24/75, loss 0.2398\n",
            "epoch 19/250, batch 25/75, loss 0.2374\n",
            "epoch 19/250, batch 26/75, loss 1.8524\n",
            "epoch 19/250, batch 27/75, loss 0.2734\n",
            "epoch 19/250, batch 28/75, loss 0.3114\n",
            "epoch 19/250, batch 29/75, loss 0.2898\n",
            "epoch 19/250, batch 30/75, loss 0.2939\n",
            "epoch 19/250, batch 31/75, loss 0.2797\n",
            "epoch 19/250, batch 32/75, loss 0.2522\n",
            "epoch 19/250, batch 33/75, loss 0.2930\n",
            "epoch 19/250, batch 34/75, loss 0.3378\n",
            "epoch 19/250, batch 35/75, loss 0.3080\n",
            "epoch 19/250, batch 36/75, loss 2.0026\n",
            "epoch 19/250, batch 37/75, loss 0.4213\n",
            "epoch 19/250, batch 38/75, loss 0.2700\n",
            "epoch 19/250, batch 39/75, loss 0.2889\n",
            "epoch 19/250, batch 40/75, loss 0.2750\n",
            "epoch 19/250, batch 41/75, loss 0.3061\n",
            "epoch 19/250, batch 42/75, loss 0.2738\n",
            "epoch 19/250, batch 43/75, loss 0.2384\n",
            "epoch 19/250, batch 44/75, loss 0.2899\n",
            "epoch 19/250, batch 45/75, loss 0.2827\n",
            "epoch 19/250, batch 46/75, loss 0.2366\n",
            "epoch 19/250, batch 47/75, loss 0.2704\n",
            "epoch 19/250, batch 48/75, loss 0.2415\n",
            "epoch 19/250, batch 49/75, loss 0.2602\n",
            "epoch 19/250, batch 50/75, loss 0.2416\n",
            "epoch 19/250, batch 51/75, loss 0.2547\n",
            "epoch 19/250, batch 52/75, loss 0.2071\n",
            "epoch 19/250, batch 53/75, loss 13.0328\n",
            "epoch 19/250, batch 54/75, loss 0.3197\n",
            "epoch 19/250, batch 55/75, loss 0.2651\n",
            "epoch 19/250, batch 56/75, loss 0.2590\n",
            "epoch 19/250, batch 57/75, loss 0.2567\n",
            "epoch 19/250, batch 58/75, loss 0.2690\n",
            "epoch 19/250, batch 59/75, loss 0.2849\n",
            "epoch 19/250, batch 60/75, loss 0.3126\n",
            "epoch 19/250, batch 61/75, loss 0.3043\n",
            "epoch 19/250, batch 62/75, loss 0.3375\n",
            "epoch 19/250, batch 63/75, loss 0.3035\n",
            "epoch 19/250, batch 64/75, loss 0.3290\n",
            "epoch 19/250, batch 65/75, loss 0.3358\n",
            "epoch 19/250, batch 66/75, loss 0.3103\n",
            "epoch 19/250, batch 67/75, loss 0.3124\n",
            "epoch 19/250, batch 68/75, loss 0.2906\n",
            "epoch 19/250, batch 69/75, loss 0.3611\n",
            "epoch 19/250, batch 70/75, loss 0.2826\n",
            "epoch 19/250, batch 71/75, loss 0.2908\n",
            "epoch 19/250, batch 72/75, loss 0.3066\n",
            "epoch 19/250, batch 73/75, loss 0.3011\n",
            "epoch 19/250, batch 74/75, loss 0.3125\n",
            "epoch 19/250, batch 75/75, loss 0.2599\n",
            "epoch 19/250, training roc_auc_score 0.9156\n",
            "EarlyStopping counter: 7 out of 50\n",
            "epoch 19/250, validation roc_auc_score 0.8021, best validation roc_auc_score 0.8516\n",
            "epoch 20/250, batch 1/75, loss 0.4060\n",
            "epoch 20/250, batch 2/75, loss 0.5817\n",
            "epoch 20/250, batch 3/75, loss 0.2902\n",
            "epoch 20/250, batch 4/75, loss 0.3457\n",
            "epoch 20/250, batch 5/75, loss 0.2200\n",
            "epoch 20/250, batch 6/75, loss 0.2136\n",
            "epoch 20/250, batch 7/75, loss 0.2718\n",
            "epoch 20/250, batch 8/75, loss 0.4850\n",
            "epoch 20/250, batch 9/75, loss 0.2343\n",
            "epoch 20/250, batch 10/75, loss 0.4906\n",
            "epoch 20/250, batch 11/75, loss 0.3182\n",
            "epoch 20/250, batch 12/75, loss 0.2631\n",
            "epoch 20/250, batch 13/75, loss 0.3281\n",
            "epoch 20/250, batch 14/75, loss 0.2109\n",
            "epoch 20/250, batch 15/75, loss 1.6112\n",
            "epoch 20/250, batch 16/75, loss 0.2678\n",
            "epoch 20/250, batch 17/75, loss 0.3086\n",
            "epoch 20/250, batch 18/75, loss 0.3155\n",
            "epoch 20/250, batch 19/75, loss 0.3054\n",
            "epoch 20/250, batch 20/75, loss 0.7205\n",
            "epoch 20/250, batch 21/75, loss 0.2201\n",
            "epoch 20/250, batch 22/75, loss 0.2661\n",
            "epoch 20/250, batch 23/75, loss 0.2653\n",
            "epoch 20/250, batch 24/75, loss 0.2299\n",
            "epoch 20/250, batch 25/75, loss 0.2262\n",
            "epoch 20/250, batch 26/75, loss 1.8797\n",
            "epoch 20/250, batch 27/75, loss 0.2792\n",
            "epoch 20/250, batch 28/75, loss 0.2858\n",
            "epoch 20/250, batch 29/75, loss 0.2646\n",
            "epoch 20/250, batch 30/75, loss 0.2811\n",
            "epoch 20/250, batch 31/75, loss 0.2570\n",
            "epoch 20/250, batch 32/75, loss 0.2399\n",
            "epoch 20/250, batch 33/75, loss 0.2693\n",
            "epoch 20/250, batch 34/75, loss 0.3205\n",
            "epoch 20/250, batch 35/75, loss 0.2908\n",
            "epoch 20/250, batch 36/75, loss 1.9944\n",
            "epoch 20/250, batch 37/75, loss 0.4684\n",
            "epoch 20/250, batch 38/75, loss 0.2465\n",
            "epoch 20/250, batch 39/75, loss 0.2826\n",
            "epoch 20/250, batch 40/75, loss 0.2669\n",
            "epoch 20/250, batch 41/75, loss 0.3048\n",
            "epoch 20/250, batch 42/75, loss 0.2518\n",
            "epoch 20/250, batch 43/75, loss 0.2251\n",
            "epoch 20/250, batch 44/75, loss 0.2857\n",
            "epoch 20/250, batch 45/75, loss 0.2819\n",
            "epoch 20/250, batch 46/75, loss 0.2262\n",
            "epoch 20/250, batch 47/75, loss 0.2590\n",
            "epoch 20/250, batch 48/75, loss 0.2342\n",
            "epoch 20/250, batch 49/75, loss 0.2583\n",
            "epoch 20/250, batch 50/75, loss 0.2359\n",
            "epoch 20/250, batch 51/75, loss 0.2453\n",
            "epoch 20/250, batch 52/75, loss 0.2012\n",
            "epoch 20/250, batch 53/75, loss 12.5588\n",
            "epoch 20/250, batch 54/75, loss 0.2891\n",
            "epoch 20/250, batch 55/75, loss 0.2645\n",
            "epoch 20/250, batch 56/75, loss 0.2467\n",
            "epoch 20/250, batch 57/75, loss 0.2517\n",
            "epoch 20/250, batch 58/75, loss 0.2613\n",
            "epoch 20/250, batch 59/75, loss 0.2771\n",
            "epoch 20/250, batch 60/75, loss 0.3001\n",
            "epoch 20/250, batch 61/75, loss 0.2984\n",
            "epoch 20/250, batch 62/75, loss 0.3358\n",
            "epoch 20/250, batch 63/75, loss 0.2781\n",
            "epoch 20/250, batch 64/75, loss 0.3295\n",
            "epoch 20/250, batch 65/75, loss 0.3184\n",
            "epoch 20/250, batch 66/75, loss 0.3051\n",
            "epoch 20/250, batch 67/75, loss 0.2934\n",
            "epoch 20/250, batch 68/75, loss 0.2731\n",
            "epoch 20/250, batch 69/75, loss 0.3277\n",
            "epoch 20/250, batch 70/75, loss 0.2609\n",
            "epoch 20/250, batch 71/75, loss 0.2835\n",
            "epoch 20/250, batch 72/75, loss 0.2915\n",
            "epoch 20/250, batch 73/75, loss 0.3024\n",
            "epoch 20/250, batch 74/75, loss 0.3146\n",
            "epoch 20/250, batch 75/75, loss 0.2595\n",
            "epoch 20/250, training roc_auc_score 0.9197\n",
            "EarlyStopping counter: 8 out of 50\n",
            "epoch 20/250, validation roc_auc_score 0.8039, best validation roc_auc_score 0.8516\n",
            "epoch 21/250, batch 1/75, loss 0.3502\n",
            "epoch 21/250, batch 2/75, loss 0.5298\n",
            "epoch 21/250, batch 3/75, loss 0.2823\n",
            "epoch 21/250, batch 4/75, loss 0.3828\n",
            "epoch 21/250, batch 5/75, loss 0.1994\n",
            "epoch 21/250, batch 6/75, loss 0.2101\n",
            "epoch 21/250, batch 7/75, loss 0.2668\n",
            "epoch 21/250, batch 8/75, loss 0.5129\n",
            "epoch 21/250, batch 9/75, loss 0.2192\n",
            "epoch 21/250, batch 10/75, loss 0.4288\n",
            "epoch 21/250, batch 11/75, loss 0.3215\n",
            "epoch 21/250, batch 12/75, loss 0.2648\n",
            "epoch 21/250, batch 13/75, loss 0.2964\n",
            "epoch 21/250, batch 14/75, loss 0.2073\n",
            "epoch 21/250, batch 15/75, loss 1.5709\n",
            "epoch 21/250, batch 16/75, loss 0.2590\n",
            "epoch 21/250, batch 17/75, loss 0.3111\n",
            "epoch 21/250, batch 18/75, loss 0.3147\n",
            "epoch 21/250, batch 19/75, loss 0.2828\n",
            "epoch 21/250, batch 20/75, loss 0.7049\n",
            "epoch 21/250, batch 21/75, loss 0.2157\n",
            "epoch 21/250, batch 22/75, loss 0.2510\n",
            "epoch 21/250, batch 23/75, loss 0.2686\n",
            "epoch 21/250, batch 24/75, loss 0.2218\n",
            "epoch 21/250, batch 25/75, loss 0.2158\n",
            "epoch 21/250, batch 26/75, loss 1.6718\n",
            "epoch 21/250, batch 27/75, loss 0.2732\n",
            "epoch 21/250, batch 28/75, loss 0.2990\n",
            "epoch 21/250, batch 29/75, loss 0.2771\n",
            "epoch 21/250, batch 30/75, loss 0.2870\n",
            "epoch 21/250, batch 31/75, loss 0.2688\n",
            "epoch 21/250, batch 32/75, loss 0.2419\n",
            "epoch 21/250, batch 33/75, loss 0.2731\n",
            "epoch 21/250, batch 34/75, loss 0.3179\n",
            "epoch 21/250, batch 35/75, loss 0.2855\n",
            "epoch 21/250, batch 36/75, loss 1.9283\n",
            "epoch 21/250, batch 37/75, loss 0.4056\n",
            "epoch 21/250, batch 38/75, loss 0.2428\n",
            "epoch 21/250, batch 39/75, loss 0.2679\n",
            "epoch 21/250, batch 40/75, loss 0.2540\n",
            "epoch 21/250, batch 41/75, loss 0.2933\n",
            "epoch 21/250, batch 42/75, loss 0.2571\n",
            "epoch 21/250, batch 43/75, loss 0.2173\n",
            "epoch 21/250, batch 44/75, loss 0.2803\n",
            "epoch 21/250, batch 45/75, loss 0.2697\n",
            "epoch 21/250, batch 46/75, loss 0.2188\n",
            "epoch 21/250, batch 47/75, loss 0.2679\n",
            "epoch 21/250, batch 48/75, loss 0.2261\n",
            "epoch 21/250, batch 49/75, loss 0.2567\n",
            "epoch 21/250, batch 50/75, loss 0.2314\n",
            "epoch 21/250, batch 51/75, loss 0.2344\n",
            "epoch 21/250, batch 52/75, loss 0.1957\n",
            "epoch 21/250, batch 53/75, loss 12.1234\n",
            "epoch 21/250, batch 54/75, loss 0.3055\n",
            "epoch 21/250, batch 55/75, loss 0.2552\n",
            "epoch 21/250, batch 56/75, loss 0.2425\n",
            "epoch 21/250, batch 57/75, loss 0.2437\n",
            "epoch 21/250, batch 58/75, loss 0.2581\n",
            "epoch 21/250, batch 59/75, loss 0.2769\n",
            "epoch 21/250, batch 60/75, loss 0.2965\n",
            "epoch 21/250, batch 61/75, loss 0.2858\n",
            "epoch 21/250, batch 62/75, loss 0.3347\n",
            "epoch 21/250, batch 63/75, loss 0.2728\n",
            "epoch 21/250, batch 64/75, loss 0.3195\n",
            "epoch 21/250, batch 65/75, loss 0.3143\n",
            "epoch 21/250, batch 66/75, loss 0.3131\n",
            "epoch 21/250, batch 67/75, loss 0.2876\n",
            "epoch 21/250, batch 68/75, loss 0.2794\n",
            "epoch 21/250, batch 69/75, loss 0.3587\n",
            "epoch 21/250, batch 70/75, loss 0.2642\n",
            "epoch 21/250, batch 71/75, loss 0.2878\n",
            "epoch 21/250, batch 72/75, loss 0.2860\n",
            "epoch 21/250, batch 73/75, loss 0.2888\n",
            "epoch 21/250, batch 74/75, loss 0.3151\n",
            "epoch 21/250, batch 75/75, loss 0.2549\n",
            "epoch 21/250, training roc_auc_score 0.9272\n",
            "EarlyStopping counter: 9 out of 50\n",
            "epoch 21/250, validation roc_auc_score 0.7972, best validation roc_auc_score 0.8516\n",
            "epoch 22/250, batch 1/75, loss 0.3528\n",
            "epoch 22/250, batch 2/75, loss 0.5083\n",
            "epoch 22/250, batch 3/75, loss 0.2696\n",
            "epoch 22/250, batch 4/75, loss 0.3314\n",
            "epoch 22/250, batch 5/75, loss 0.1847\n",
            "epoch 22/250, batch 6/75, loss 0.1932\n",
            "epoch 22/250, batch 7/75, loss 0.2597\n",
            "epoch 22/250, batch 8/75, loss 0.5923\n",
            "epoch 22/250, batch 9/75, loss 0.2177\n",
            "epoch 22/250, batch 10/75, loss 0.3272\n",
            "epoch 22/250, batch 11/75, loss 0.2993\n",
            "epoch 22/250, batch 12/75, loss 0.2626\n",
            "epoch 22/250, batch 13/75, loss 0.3181\n",
            "epoch 22/250, batch 14/75, loss 0.2064\n",
            "epoch 22/250, batch 15/75, loss 1.6343\n",
            "epoch 22/250, batch 16/75, loss 0.2612\n",
            "epoch 22/250, batch 17/75, loss 0.3242\n",
            "epoch 22/250, batch 18/75, loss 0.3489\n",
            "epoch 22/250, batch 19/75, loss 0.3083\n",
            "epoch 22/250, batch 20/75, loss 0.6096\n",
            "epoch 22/250, batch 21/75, loss 0.1951\n",
            "epoch 22/250, batch 22/75, loss 0.2570\n",
            "epoch 22/250, batch 23/75, loss 0.2802\n",
            "epoch 22/250, batch 24/75, loss 0.2124\n",
            "epoch 22/250, batch 25/75, loss 0.2035\n",
            "epoch 22/250, batch 26/75, loss 1.5357\n",
            "epoch 22/250, batch 27/75, loss 0.2603\n",
            "epoch 22/250, batch 28/75, loss 0.2662\n",
            "epoch 22/250, batch 29/75, loss 0.2566\n",
            "epoch 22/250, batch 30/75, loss 0.2670\n",
            "epoch 22/250, batch 31/75, loss 0.2399\n",
            "epoch 22/250, batch 32/75, loss 0.2138\n",
            "epoch 22/250, batch 33/75, loss 0.2496\n",
            "epoch 22/250, batch 34/75, loss 0.2911\n",
            "epoch 22/250, batch 35/75, loss 0.2886\n",
            "epoch 22/250, batch 36/75, loss 2.0323\n",
            "epoch 22/250, batch 37/75, loss 0.3957\n",
            "epoch 22/250, batch 38/75, loss 0.2157\n",
            "epoch 22/250, batch 39/75, loss 0.2812\n",
            "epoch 22/250, batch 40/75, loss 0.2600\n",
            "epoch 22/250, batch 41/75, loss 0.3041\n",
            "epoch 22/250, batch 42/75, loss 0.2413\n",
            "epoch 22/250, batch 43/75, loss 0.2190\n",
            "epoch 22/250, batch 44/75, loss 0.2823\n",
            "epoch 22/250, batch 45/75, loss 0.2733\n",
            "epoch 22/250, batch 46/75, loss 0.2152\n",
            "epoch 22/250, batch 47/75, loss 0.2577\n",
            "epoch 22/250, batch 48/75, loss 0.2231\n",
            "epoch 22/250, batch 49/75, loss 0.2477\n",
            "epoch 22/250, batch 50/75, loss 0.2191\n",
            "epoch 22/250, batch 51/75, loss 0.2249\n",
            "epoch 22/250, batch 52/75, loss 0.1866\n",
            "epoch 22/250, batch 53/75, loss 11.7394\n",
            "epoch 22/250, batch 54/75, loss 0.2886\n",
            "epoch 22/250, batch 55/75, loss 0.2493\n",
            "epoch 22/250, batch 56/75, loss 0.2301\n",
            "epoch 22/250, batch 57/75, loss 0.2330\n",
            "epoch 22/250, batch 58/75, loss 0.2487\n",
            "epoch 22/250, batch 59/75, loss 0.2643\n",
            "epoch 22/250, batch 60/75, loss 0.2827\n",
            "epoch 22/250, batch 61/75, loss 0.2708\n",
            "epoch 22/250, batch 62/75, loss 0.3195\n",
            "epoch 22/250, batch 63/75, loss 0.2762\n",
            "epoch 22/250, batch 64/75, loss 0.3169\n",
            "epoch 22/250, batch 65/75, loss 0.3107\n",
            "epoch 22/250, batch 66/75, loss 0.3054\n",
            "epoch 22/250, batch 67/75, loss 0.2940\n",
            "epoch 22/250, batch 68/75, loss 0.2677\n",
            "epoch 22/250, batch 69/75, loss 0.3362\n",
            "epoch 22/250, batch 70/75, loss 0.2567\n",
            "epoch 22/250, batch 71/75, loss 0.2842\n",
            "epoch 22/250, batch 72/75, loss 0.2665\n",
            "epoch 22/250, batch 73/75, loss 0.3045\n",
            "epoch 22/250, batch 74/75, loss 0.3049\n",
            "epoch 22/250, batch 75/75, loss 0.2544\n",
            "epoch 22/250, training roc_auc_score 0.9316\n",
            "EarlyStopping counter: 10 out of 50\n",
            "epoch 22/250, validation roc_auc_score 0.8264, best validation roc_auc_score 0.8516\n",
            "epoch 23/250, batch 1/75, loss 0.3743\n",
            "epoch 23/250, batch 2/75, loss 0.4775\n",
            "epoch 23/250, batch 3/75, loss 0.2456\n",
            "epoch 23/250, batch 4/75, loss 0.3395\n",
            "epoch 23/250, batch 5/75, loss 0.1883\n",
            "epoch 23/250, batch 6/75, loss 0.1941\n",
            "epoch 23/250, batch 7/75, loss 0.2520\n",
            "epoch 23/250, batch 8/75, loss 0.5017\n",
            "epoch 23/250, batch 9/75, loss 0.2165\n",
            "epoch 23/250, batch 10/75, loss 0.3412\n",
            "epoch 23/250, batch 11/75, loss 0.3084\n",
            "epoch 23/250, batch 12/75, loss 0.2636\n",
            "epoch 23/250, batch 13/75, loss 0.3033\n",
            "epoch 23/250, batch 14/75, loss 0.2044\n",
            "epoch 23/250, batch 15/75, loss 1.4144\n",
            "epoch 23/250, batch 16/75, loss 0.2439\n",
            "epoch 23/250, batch 17/75, loss 0.2962\n",
            "epoch 23/250, batch 18/75, loss 0.3092\n",
            "epoch 23/250, batch 19/75, loss 0.2850\n",
            "epoch 23/250, batch 20/75, loss 0.6020\n",
            "epoch 23/250, batch 21/75, loss 0.1928\n",
            "epoch 23/250, batch 22/75, loss 0.2250\n",
            "epoch 23/250, batch 23/75, loss 0.2488\n",
            "epoch 23/250, batch 24/75, loss 0.2082\n",
            "epoch 23/250, batch 25/75, loss 0.1867\n",
            "epoch 23/250, batch 26/75, loss 1.4297\n",
            "epoch 23/250, batch 27/75, loss 0.2642\n",
            "epoch 23/250, batch 28/75, loss 0.2764\n",
            "epoch 23/250, batch 29/75, loss 0.2458\n",
            "epoch 23/250, batch 30/75, loss 0.2504\n",
            "epoch 23/250, batch 31/75, loss 0.2445\n",
            "epoch 23/250, batch 32/75, loss 0.2208\n",
            "epoch 23/250, batch 33/75, loss 0.2494\n",
            "epoch 23/250, batch 34/75, loss 0.3002\n",
            "epoch 23/250, batch 35/75, loss 0.2772\n",
            "epoch 23/250, batch 36/75, loss 1.7336\n",
            "epoch 23/250, batch 37/75, loss 0.3748\n",
            "epoch 23/250, batch 38/75, loss 0.2219\n",
            "epoch 23/250, batch 39/75, loss 0.2505\n",
            "epoch 23/250, batch 40/75, loss 0.2429\n",
            "epoch 23/250, batch 41/75, loss 0.2889\n",
            "epoch 23/250, batch 42/75, loss 0.2410\n",
            "epoch 23/250, batch 43/75, loss 0.2144\n",
            "epoch 23/250, batch 44/75, loss 0.2619\n",
            "epoch 23/250, batch 45/75, loss 0.2589\n",
            "epoch 23/250, batch 46/75, loss 0.2051\n",
            "epoch 23/250, batch 47/75, loss 0.2455\n",
            "epoch 23/250, batch 48/75, loss 0.2079\n",
            "epoch 23/250, batch 49/75, loss 0.2417\n",
            "epoch 23/250, batch 50/75, loss 0.2166\n",
            "epoch 23/250, batch 51/75, loss 0.2174\n",
            "epoch 23/250, batch 52/75, loss 0.1717\n",
            "epoch 23/250, batch 53/75, loss 11.9543\n",
            "epoch 23/250, batch 54/75, loss 0.2892\n",
            "epoch 23/250, batch 55/75, loss 0.2336\n",
            "epoch 23/250, batch 56/75, loss 0.2271\n",
            "epoch 23/250, batch 57/75, loss 0.2308\n",
            "epoch 23/250, batch 58/75, loss 0.2388\n",
            "epoch 23/250, batch 59/75, loss 0.2617\n",
            "epoch 23/250, batch 60/75, loss 0.2856\n",
            "epoch 23/250, batch 61/75, loss 0.2924\n",
            "epoch 23/250, batch 62/75, loss 0.3378\n",
            "epoch 23/250, batch 63/75, loss 0.2615\n",
            "epoch 23/250, batch 64/75, loss 0.3197\n",
            "epoch 23/250, batch 65/75, loss 0.3012\n",
            "epoch 23/250, batch 66/75, loss 0.3129\n",
            "epoch 23/250, batch 67/75, loss 0.2801\n",
            "epoch 23/250, batch 68/75, loss 0.2513\n",
            "epoch 23/250, batch 69/75, loss 0.3379\n",
            "epoch 23/250, batch 70/75, loss 0.2531\n",
            "epoch 23/250, batch 71/75, loss 0.2709\n",
            "epoch 23/250, batch 72/75, loss 0.2954\n",
            "epoch 23/250, batch 73/75, loss 0.2969\n",
            "epoch 23/250, batch 74/75, loss 0.3130\n",
            "epoch 23/250, batch 75/75, loss 0.2453\n",
            "epoch 23/250, training roc_auc_score 0.9336\n",
            "EarlyStopping counter: 11 out of 50\n",
            "epoch 23/250, validation roc_auc_score 0.7631, best validation roc_auc_score 0.8516\n",
            "epoch 24/250, batch 1/75, loss 0.3777\n",
            "epoch 24/250, batch 2/75, loss 0.4858\n",
            "epoch 24/250, batch 3/75, loss 0.2585\n",
            "epoch 24/250, batch 4/75, loss 0.2799\n",
            "epoch 24/250, batch 5/75, loss 0.1695\n",
            "epoch 24/250, batch 6/75, loss 0.1847\n",
            "epoch 24/250, batch 7/75, loss 0.2242\n",
            "epoch 24/250, batch 8/75, loss 0.4944\n",
            "epoch 24/250, batch 9/75, loss 0.1881\n",
            "epoch 24/250, batch 10/75, loss 0.5109\n",
            "epoch 24/250, batch 11/75, loss 0.2895\n",
            "epoch 24/250, batch 12/75, loss 0.2506\n",
            "epoch 24/250, batch 13/75, loss 0.2773\n",
            "epoch 24/250, batch 14/75, loss 0.1806\n",
            "epoch 24/250, batch 15/75, loss 1.2635\n",
            "epoch 24/250, batch 16/75, loss 0.2493\n",
            "epoch 24/250, batch 17/75, loss 0.2898\n",
            "epoch 24/250, batch 18/75, loss 0.3313\n",
            "epoch 24/250, batch 19/75, loss 0.2681\n",
            "epoch 24/250, batch 20/75, loss 0.6291\n",
            "epoch 24/250, batch 21/75, loss 0.1817\n",
            "epoch 24/250, batch 22/75, loss 0.2527\n",
            "epoch 24/250, batch 23/75, loss 0.2205\n",
            "epoch 24/250, batch 24/75, loss 0.1967\n",
            "epoch 24/250, batch 25/75, loss 0.1841\n",
            "epoch 24/250, batch 26/75, loss 1.3073\n",
            "epoch 24/250, batch 27/75, loss 0.2415\n",
            "epoch 24/250, batch 28/75, loss 0.2592\n",
            "epoch 24/250, batch 29/75, loss 0.2274\n",
            "epoch 24/250, batch 30/75, loss 0.2423\n",
            "epoch 24/250, batch 31/75, loss 0.2318\n",
            "epoch 24/250, batch 32/75, loss 0.2186\n",
            "epoch 24/250, batch 33/75, loss 0.2382\n",
            "epoch 24/250, batch 34/75, loss 0.2786\n",
            "epoch 24/250, batch 35/75, loss 0.2934\n",
            "epoch 24/250, batch 36/75, loss 1.6018\n",
            "epoch 24/250, batch 37/75, loss 0.4172\n",
            "epoch 24/250, batch 38/75, loss 0.2107\n",
            "epoch 24/250, batch 39/75, loss 0.2552\n",
            "epoch 24/250, batch 40/75, loss 0.2319\n",
            "epoch 24/250, batch 41/75, loss 0.2898\n",
            "epoch 24/250, batch 42/75, loss 0.2297\n",
            "epoch 24/250, batch 43/75, loss 0.2084\n",
            "epoch 24/250, batch 44/75, loss 0.2541\n",
            "epoch 24/250, batch 45/75, loss 0.2423\n",
            "epoch 24/250, batch 46/75, loss 0.1887\n",
            "epoch 24/250, batch 47/75, loss 0.2365\n",
            "epoch 24/250, batch 48/75, loss 0.1938\n",
            "epoch 24/250, batch 49/75, loss 0.2225\n",
            "epoch 24/250, batch 50/75, loss 0.2058\n",
            "epoch 24/250, batch 51/75, loss 0.2060\n",
            "epoch 24/250, batch 52/75, loss 0.1633\n",
            "epoch 24/250, batch 53/75, loss 11.0538\n",
            "epoch 24/250, batch 54/75, loss 0.2539\n",
            "epoch 24/250, batch 55/75, loss 0.2349\n",
            "epoch 24/250, batch 56/75, loss 0.2116\n",
            "epoch 24/250, batch 57/75, loss 0.2107\n",
            "epoch 24/250, batch 58/75, loss 0.2326\n",
            "epoch 24/250, batch 59/75, loss 0.2481\n",
            "epoch 24/250, batch 60/75, loss 0.2717\n",
            "epoch 24/250, batch 61/75, loss 0.2699\n",
            "epoch 24/250, batch 62/75, loss 0.2978\n",
            "epoch 24/250, batch 63/75, loss 0.2334\n",
            "epoch 24/250, batch 64/75, loss 0.2765\n",
            "epoch 24/250, batch 65/75, loss 0.2834\n",
            "epoch 24/250, batch 66/75, loss 0.2780\n",
            "epoch 24/250, batch 67/75, loss 0.2693\n",
            "epoch 24/250, batch 68/75, loss 0.2492\n",
            "epoch 24/250, batch 69/75, loss 0.3045\n",
            "epoch 24/250, batch 70/75, loss 0.2276\n",
            "epoch 24/250, batch 71/75, loss 0.2546\n",
            "epoch 24/250, batch 72/75, loss 0.2611\n",
            "epoch 24/250, batch 73/75, loss 0.2746\n",
            "epoch 24/250, batch 74/75, loss 0.2865\n",
            "epoch 24/250, batch 75/75, loss 0.2323\n",
            "epoch 24/250, training roc_auc_score 0.9440\n",
            "EarlyStopping counter: 12 out of 50\n",
            "epoch 24/250, validation roc_auc_score 0.7946, best validation roc_auc_score 0.8516\n",
            "epoch 25/250, batch 1/75, loss 0.2750\n",
            "epoch 25/250, batch 2/75, loss 0.4883\n",
            "epoch 25/250, batch 3/75, loss 0.2397\n",
            "epoch 25/250, batch 4/75, loss 0.3580\n",
            "epoch 25/250, batch 5/75, loss 0.1757\n",
            "epoch 25/250, batch 6/75, loss 0.1816\n",
            "epoch 25/250, batch 7/75, loss 0.2812\n",
            "epoch 25/250, batch 8/75, loss 0.4459\n",
            "epoch 25/250, batch 9/75, loss 0.1937\n",
            "epoch 25/250, batch 10/75, loss 0.4262\n",
            "epoch 25/250, batch 11/75, loss 0.2765\n",
            "epoch 25/250, batch 12/75, loss 0.2323\n",
            "epoch 25/250, batch 13/75, loss 0.3706\n",
            "epoch 25/250, batch 14/75, loss 0.1780\n",
            "epoch 25/250, batch 15/75, loss 1.2912\n",
            "epoch 25/250, batch 16/75, loss 0.2371\n",
            "epoch 25/250, batch 17/75, loss 0.2897\n",
            "epoch 25/250, batch 18/75, loss 0.3359\n",
            "epoch 25/250, batch 19/75, loss 0.2764\n",
            "epoch 25/250, batch 20/75, loss 0.7409\n",
            "epoch 25/250, batch 21/75, loss 0.1808\n",
            "epoch 25/250, batch 22/75, loss 0.2244\n",
            "epoch 25/250, batch 23/75, loss 0.2456\n",
            "epoch 25/250, batch 24/75, loss 0.1876\n",
            "epoch 25/250, batch 25/75, loss 0.1847\n",
            "epoch 25/250, batch 26/75, loss 1.4278\n",
            "epoch 25/250, batch 27/75, loss 0.2567\n",
            "epoch 25/250, batch 28/75, loss 0.2630\n",
            "epoch 25/250, batch 29/75, loss 0.2206\n",
            "epoch 25/250, batch 30/75, loss 0.2287\n",
            "epoch 25/250, batch 31/75, loss 0.2109\n",
            "epoch 25/250, batch 32/75, loss 0.2002\n",
            "epoch 25/250, batch 33/75, loss 0.2275\n",
            "epoch 25/250, batch 34/75, loss 0.2524\n",
            "epoch 25/250, batch 35/75, loss 0.2334\n",
            "epoch 25/250, batch 36/75, loss 1.6149\n",
            "epoch 25/250, batch 37/75, loss 0.3304\n",
            "epoch 25/250, batch 38/75, loss 0.2001\n",
            "epoch 25/250, batch 39/75, loss 0.2651\n",
            "epoch 25/250, batch 40/75, loss 0.2336\n",
            "epoch 25/250, batch 41/75, loss 0.3038\n",
            "epoch 25/250, batch 42/75, loss 0.2169\n",
            "epoch 25/250, batch 43/75, loss 0.2027\n",
            "epoch 25/250, batch 44/75, loss 0.2705\n",
            "epoch 25/250, batch 45/75, loss 0.2544\n",
            "epoch 25/250, batch 46/75, loss 0.1925\n",
            "epoch 25/250, batch 47/75, loss 0.2241\n",
            "epoch 25/250, batch 48/75, loss 0.1959\n",
            "epoch 25/250, batch 49/75, loss 0.2369\n",
            "epoch 25/250, batch 50/75, loss 0.2103\n",
            "epoch 25/250, batch 51/75, loss 0.2035\n",
            "epoch 25/250, batch 52/75, loss 0.1685\n",
            "epoch 25/250, batch 53/75, loss 10.6094\n",
            "epoch 25/250, batch 54/75, loss 0.2424\n",
            "epoch 25/250, batch 55/75, loss 0.2351\n",
            "epoch 25/250, batch 56/75, loss 0.2179\n",
            "epoch 25/250, batch 57/75, loss 0.2205\n",
            "epoch 25/250, batch 58/75, loss 0.2340\n",
            "epoch 25/250, batch 59/75, loss 0.2554\n",
            "epoch 25/250, batch 60/75, loss 0.2735\n",
            "epoch 25/250, batch 61/75, loss 0.2802\n",
            "epoch 25/250, batch 62/75, loss 0.3188\n",
            "epoch 25/250, batch 63/75, loss 0.2314\n",
            "epoch 25/250, batch 64/75, loss 0.3072\n",
            "epoch 25/250, batch 65/75, loss 0.2716\n",
            "epoch 25/250, batch 66/75, loss 0.2908\n",
            "epoch 25/250, batch 67/75, loss 0.2490\n",
            "epoch 25/250, batch 68/75, loss 0.2363\n",
            "epoch 25/250, batch 69/75, loss 0.3011\n",
            "epoch 25/250, batch 70/75, loss 0.2295\n",
            "epoch 25/250, batch 71/75, loss 0.2605\n",
            "epoch 25/250, batch 72/75, loss 0.2509\n",
            "epoch 25/250, batch 73/75, loss 0.2701\n",
            "epoch 25/250, batch 74/75, loss 0.2901\n",
            "epoch 25/250, batch 75/75, loss 0.2436\n",
            "epoch 25/250, training roc_auc_score 0.9463\n",
            "EarlyStopping counter: 13 out of 50\n",
            "epoch 25/250, validation roc_auc_score 0.7668, best validation roc_auc_score 0.8516\n",
            "epoch 26/250, batch 1/75, loss 0.2863\n",
            "epoch 26/250, batch 2/75, loss 0.4293\n",
            "epoch 26/250, batch 3/75, loss 0.2405\n",
            "epoch 26/250, batch 4/75, loss 0.3158\n",
            "epoch 26/250, batch 5/75, loss 0.1517\n",
            "epoch 26/250, batch 6/75, loss 0.1626\n",
            "epoch 26/250, batch 7/75, loss 0.2147\n",
            "epoch 26/250, batch 8/75, loss 0.4500\n",
            "epoch 26/250, batch 9/75, loss 0.1806\n",
            "epoch 26/250, batch 10/75, loss 0.2871\n",
            "epoch 26/250, batch 11/75, loss 0.2495\n",
            "epoch 26/250, batch 12/75, loss 0.2410\n",
            "epoch 26/250, batch 13/75, loss 0.2720\n",
            "epoch 26/250, batch 14/75, loss 0.1643\n",
            "epoch 26/250, batch 15/75, loss 1.1727\n",
            "epoch 26/250, batch 16/75, loss 0.2227\n",
            "epoch 26/250, batch 17/75, loss 0.2728\n",
            "epoch 26/250, batch 18/75, loss 0.3202\n",
            "epoch 26/250, batch 19/75, loss 0.2615\n",
            "epoch 26/250, batch 20/75, loss 0.6120\n",
            "epoch 26/250, batch 21/75, loss 0.1691\n",
            "epoch 26/250, batch 22/75, loss 0.2221\n",
            "epoch 26/250, batch 23/75, loss 0.2039\n",
            "epoch 26/250, batch 24/75, loss 0.1657\n",
            "epoch 26/250, batch 25/75, loss 0.1703\n",
            "epoch 26/250, batch 26/75, loss 1.2420\n",
            "epoch 26/250, batch 27/75, loss 0.2444\n",
            "epoch 26/250, batch 28/75, loss 0.2451\n",
            "epoch 26/250, batch 29/75, loss 0.2232\n",
            "epoch 26/250, batch 30/75, loss 0.2424\n",
            "epoch 26/250, batch 31/75, loss 0.2178\n",
            "epoch 26/250, batch 32/75, loss 0.1992\n",
            "epoch 26/250, batch 33/75, loss 0.2449\n",
            "epoch 26/250, batch 34/75, loss 0.2614\n",
            "epoch 26/250, batch 35/75, loss 0.2885\n",
            "epoch 26/250, batch 36/75, loss 1.4436\n",
            "epoch 26/250, batch 37/75, loss 0.2491\n",
            "epoch 26/250, batch 38/75, loss 0.2089\n",
            "epoch 26/250, batch 39/75, loss 0.2460\n",
            "epoch 26/250, batch 40/75, loss 0.2443\n",
            "epoch 26/250, batch 41/75, loss 0.2847\n",
            "epoch 26/250, batch 42/75, loss 0.2264\n",
            "epoch 26/250, batch 43/75, loss 0.1939\n",
            "epoch 26/250, batch 44/75, loss 0.2496\n",
            "epoch 26/250, batch 45/75, loss 0.2397\n",
            "epoch 26/250, batch 46/75, loss 0.1721\n",
            "epoch 26/250, batch 47/75, loss 0.2141\n",
            "epoch 26/250, batch 48/75, loss 0.1874\n",
            "epoch 26/250, batch 49/75, loss 0.2106\n",
            "epoch 26/250, batch 50/75, loss 0.1865\n",
            "epoch 26/250, batch 51/75, loss 0.1895\n",
            "epoch 26/250, batch 52/75, loss 0.1536\n",
            "epoch 26/250, batch 53/75, loss 10.0561\n",
            "epoch 26/250, batch 54/75, loss 0.2557\n",
            "epoch 26/250, batch 55/75, loss 0.2100\n",
            "epoch 26/250, batch 56/75, loss 0.2036\n",
            "epoch 26/250, batch 57/75, loss 0.1992\n",
            "epoch 26/250, batch 58/75, loss 0.2231\n",
            "epoch 26/250, batch 59/75, loss 0.2342\n",
            "epoch 26/250, batch 60/75, loss 0.2802\n",
            "epoch 26/250, batch 61/75, loss 0.2611\n",
            "epoch 26/250, batch 62/75, loss 0.3135\n",
            "epoch 26/250, batch 63/75, loss 0.2435\n",
            "epoch 26/250, batch 64/75, loss 0.3001\n",
            "epoch 26/250, batch 65/75, loss 0.3075\n",
            "epoch 26/250, batch 66/75, loss 0.2708\n",
            "epoch 26/250, batch 67/75, loss 0.2648\n",
            "epoch 26/250, batch 68/75, loss 0.2513\n",
            "epoch 26/250, batch 69/75, loss 0.3035\n",
            "epoch 26/250, batch 70/75, loss 0.2282\n",
            "epoch 26/250, batch 71/75, loss 0.2685\n",
            "epoch 26/250, batch 72/75, loss 0.2763\n",
            "epoch 26/250, batch 73/75, loss 0.2890\n",
            "epoch 26/250, batch 74/75, loss 0.3008\n",
            "epoch 26/250, batch 75/75, loss 0.2058\n",
            "epoch 26/250, training roc_auc_score 0.9543\n",
            "EarlyStopping counter: 14 out of 50\n",
            "epoch 26/250, validation roc_auc_score 0.7850, best validation roc_auc_score 0.8516\n",
            "epoch 27/250, batch 1/75, loss 0.3531\n",
            "epoch 27/250, batch 2/75, loss 0.3425\n",
            "epoch 27/250, batch 3/75, loss 0.2104\n",
            "epoch 27/250, batch 4/75, loss 0.3115\n",
            "epoch 27/250, batch 5/75, loss 0.1548\n",
            "epoch 27/250, batch 6/75, loss 0.1737\n",
            "epoch 27/250, batch 7/75, loss 0.2424\n",
            "epoch 27/250, batch 8/75, loss 0.4645\n",
            "epoch 27/250, batch 9/75, loss 0.1870\n",
            "epoch 27/250, batch 10/75, loss 0.3206\n",
            "epoch 27/250, batch 11/75, loss 0.3057\n",
            "epoch 27/250, batch 12/75, loss 0.2275\n",
            "epoch 27/250, batch 13/75, loss 0.2903\n",
            "epoch 27/250, batch 14/75, loss 0.1709\n",
            "epoch 27/250, batch 15/75, loss 1.2722\n",
            "epoch 27/250, batch 16/75, loss 0.2205\n",
            "epoch 27/250, batch 17/75, loss 0.2667\n",
            "epoch 27/250, batch 18/75, loss 0.3115\n",
            "epoch 27/250, batch 19/75, loss 0.2603\n",
            "epoch 27/250, batch 20/75, loss 0.5800\n",
            "epoch 27/250, batch 21/75, loss 0.1609\n",
            "epoch 27/250, batch 22/75, loss 0.2094\n",
            "epoch 27/250, batch 23/75, loss 0.1985\n",
            "epoch 27/250, batch 24/75, loss 0.1687\n",
            "epoch 27/250, batch 25/75, loss 0.1737\n",
            "epoch 27/250, batch 26/75, loss 1.2490\n",
            "epoch 27/250, batch 27/75, loss 0.2365\n",
            "epoch 27/250, batch 28/75, loss 0.2519\n",
            "epoch 27/250, batch 29/75, loss 0.2141\n",
            "epoch 27/250, batch 30/75, loss 0.2322\n",
            "epoch 27/250, batch 31/75, loss 0.2005\n",
            "epoch 27/250, batch 32/75, loss 0.1932\n",
            "epoch 27/250, batch 33/75, loss 0.2081\n",
            "epoch 27/250, batch 34/75, loss 0.2744\n",
            "epoch 27/250, batch 35/75, loss 0.2404\n",
            "epoch 27/250, batch 36/75, loss 1.7640\n",
            "epoch 27/250, batch 37/75, loss 0.3719\n",
            "epoch 27/250, batch 38/75, loss 0.1962\n",
            "epoch 27/250, batch 39/75, loss 0.2406\n",
            "epoch 27/250, batch 40/75, loss 0.2142\n",
            "epoch 27/250, batch 41/75, loss 0.2888\n",
            "epoch 27/250, batch 42/75, loss 0.2129\n",
            "epoch 27/250, batch 43/75, loss 0.1827\n",
            "epoch 27/250, batch 44/75, loss 0.2473\n",
            "epoch 27/250, batch 45/75, loss 0.2406\n",
            "epoch 27/250, batch 46/75, loss 0.1753\n",
            "epoch 27/250, batch 47/75, loss 0.2124\n",
            "epoch 27/250, batch 48/75, loss 0.1858\n",
            "epoch 27/250, batch 49/75, loss 0.2140\n",
            "epoch 27/250, batch 50/75, loss 0.1872\n",
            "epoch 27/250, batch 51/75, loss 0.1808\n",
            "epoch 27/250, batch 52/75, loss 0.1564\n",
            "epoch 27/250, batch 53/75, loss 10.0380\n",
            "epoch 27/250, batch 54/75, loss 0.2533\n",
            "epoch 27/250, batch 55/75, loss 0.2085\n",
            "epoch 27/250, batch 56/75, loss 0.1969\n",
            "epoch 27/250, batch 57/75, loss 0.2200\n",
            "epoch 27/250, batch 58/75, loss 0.2228\n",
            "epoch 27/250, batch 59/75, loss 0.2447\n",
            "epoch 27/250, batch 60/75, loss 0.2463\n",
            "epoch 27/250, batch 61/75, loss 0.2558\n",
            "epoch 27/250, batch 62/75, loss 0.3182\n",
            "epoch 27/250, batch 63/75, loss 0.2169\n",
            "epoch 27/250, batch 64/75, loss 0.2932\n",
            "epoch 27/250, batch 65/75, loss 0.2841\n",
            "epoch 27/250, batch 66/75, loss 0.2771\n",
            "epoch 27/250, batch 67/75, loss 0.2331\n",
            "epoch 27/250, batch 68/75, loss 0.2346\n",
            "epoch 27/250, batch 69/75, loss 0.2984\n",
            "epoch 27/250, batch 70/75, loss 0.2165\n",
            "epoch 27/250, batch 71/75, loss 0.2526\n",
            "epoch 27/250, batch 72/75, loss 0.2399\n",
            "epoch 27/250, batch 73/75, loss 0.2769\n",
            "epoch 27/250, batch 74/75, loss 0.3034\n",
            "epoch 27/250, batch 75/75, loss 0.2290\n",
            "epoch 27/250, training roc_auc_score 0.9512\n",
            "EarlyStopping counter: 15 out of 50\n",
            "epoch 27/250, validation roc_auc_score 0.7583, best validation roc_auc_score 0.8516\n",
            "epoch 28/250, batch 1/75, loss 0.3761\n",
            "epoch 28/250, batch 2/75, loss 0.4067\n",
            "epoch 28/250, batch 3/75, loss 0.2429\n",
            "epoch 28/250, batch 4/75, loss 0.2625\n",
            "epoch 28/250, batch 5/75, loss 0.1452\n",
            "epoch 28/250, batch 6/75, loss 0.1477\n",
            "epoch 28/250, batch 7/75, loss 0.2395\n",
            "epoch 28/250, batch 8/75, loss 0.5813\n",
            "epoch 28/250, batch 9/75, loss 0.1767\n",
            "epoch 28/250, batch 10/75, loss 0.2884\n",
            "epoch 28/250, batch 11/75, loss 0.3212\n",
            "epoch 28/250, batch 12/75, loss 0.2219\n",
            "epoch 28/250, batch 13/75, loss 0.3384\n",
            "epoch 28/250, batch 14/75, loss 0.1851\n",
            "epoch 28/250, batch 15/75, loss 1.1247\n",
            "epoch 28/250, batch 16/75, loss 0.2377\n",
            "epoch 28/250, batch 17/75, loss 0.2613\n",
            "epoch 28/250, batch 18/75, loss 0.3247\n",
            "epoch 28/250, batch 19/75, loss 0.2564\n",
            "epoch 28/250, batch 20/75, loss 0.5891\n",
            "epoch 28/250, batch 21/75, loss 0.1648\n",
            "epoch 28/250, batch 22/75, loss 0.2235\n",
            "epoch 28/250, batch 23/75, loss 0.2082\n",
            "epoch 28/250, batch 24/75, loss 0.1680\n",
            "epoch 28/250, batch 25/75, loss 0.1812\n",
            "epoch 28/250, batch 26/75, loss 1.0650\n",
            "epoch 28/250, batch 27/75, loss 0.2571\n",
            "epoch 28/250, batch 28/75, loss 0.2369\n",
            "epoch 28/250, batch 29/75, loss 0.2129\n",
            "epoch 28/250, batch 30/75, loss 0.2343\n",
            "epoch 28/250, batch 31/75, loss 0.1889\n",
            "epoch 28/250, batch 32/75, loss 0.1850\n",
            "epoch 28/250, batch 33/75, loss 0.1961\n",
            "epoch 28/250, batch 34/75, loss 0.2669\n",
            "epoch 28/250, batch 35/75, loss 0.2452\n",
            "epoch 28/250, batch 36/75, loss 1.3276\n",
            "epoch 28/250, batch 37/75, loss 0.2665\n",
            "epoch 28/250, batch 38/75, loss 0.1996\n",
            "epoch 28/250, batch 39/75, loss 0.2290\n",
            "epoch 28/250, batch 40/75, loss 0.2137\n",
            "epoch 28/250, batch 41/75, loss 0.2710\n",
            "epoch 28/250, batch 42/75, loss 0.2102\n",
            "epoch 28/250, batch 43/75, loss 0.1839\n",
            "epoch 28/250, batch 44/75, loss 0.2277\n",
            "epoch 28/250, batch 45/75, loss 0.2085\n",
            "epoch 28/250, batch 46/75, loss 0.1567\n",
            "epoch 28/250, batch 47/75, loss 0.2118\n",
            "epoch 28/250, batch 48/75, loss 0.1642\n",
            "epoch 28/250, batch 49/75, loss 0.1951\n",
            "epoch 28/250, batch 50/75, loss 0.1799\n",
            "epoch 28/250, batch 51/75, loss 0.1688\n",
            "epoch 28/250, batch 52/75, loss 0.1439\n",
            "epoch 28/250, batch 53/75, loss 9.7707\n",
            "epoch 28/250, batch 54/75, loss 0.2645\n",
            "epoch 28/250, batch 55/75, loss 0.2007\n",
            "epoch 28/250, batch 56/75, loss 0.1828\n",
            "epoch 28/250, batch 57/75, loss 0.1870\n",
            "epoch 28/250, batch 58/75, loss 0.2275\n",
            "epoch 28/250, batch 59/75, loss 0.2251\n",
            "epoch 28/250, batch 60/75, loss 0.2447\n",
            "epoch 28/250, batch 61/75, loss 0.2611\n",
            "epoch 28/250, batch 62/75, loss 0.2974\n",
            "epoch 28/250, batch 63/75, loss 0.2174\n",
            "epoch 28/250, batch 64/75, loss 0.2579\n",
            "epoch 28/250, batch 65/75, loss 0.2646\n",
            "epoch 28/250, batch 66/75, loss 0.2822\n",
            "epoch 28/250, batch 67/75, loss 0.2355\n",
            "epoch 28/250, batch 68/75, loss 0.2090\n",
            "epoch 28/250, batch 69/75, loss 0.2759\n",
            "epoch 28/250, batch 70/75, loss 0.2079\n",
            "epoch 28/250, batch 71/75, loss 0.2416\n",
            "epoch 28/250, batch 72/75, loss 0.2265\n",
            "epoch 28/250, batch 73/75, loss 0.2497\n",
            "epoch 28/250, batch 74/75, loss 0.2776\n",
            "epoch 28/250, batch 75/75, loss 0.1942\n",
            "epoch 28/250, training roc_auc_score 0.9589\n",
            "EarlyStopping counter: 16 out of 50\n",
            "epoch 28/250, validation roc_auc_score 0.8196, best validation roc_auc_score 0.8516\n",
            "epoch 29/250, batch 1/75, loss 0.3098\n",
            "epoch 29/250, batch 2/75, loss 0.3634\n",
            "epoch 29/250, batch 3/75, loss 0.1889\n",
            "epoch 29/250, batch 4/75, loss 0.3364\n",
            "epoch 29/250, batch 5/75, loss 0.1302\n",
            "epoch 29/250, batch 6/75, loss 0.1416\n",
            "epoch 29/250, batch 7/75, loss 0.1997\n",
            "epoch 29/250, batch 8/75, loss 0.4337\n",
            "epoch 29/250, batch 9/75, loss 0.1519\n",
            "epoch 29/250, batch 10/75, loss 0.2226\n",
            "epoch 29/250, batch 11/75, loss 0.2361\n",
            "epoch 29/250, batch 12/75, loss 0.2130\n",
            "epoch 29/250, batch 13/75, loss 0.3111\n",
            "epoch 29/250, batch 14/75, loss 0.1695\n",
            "epoch 29/250, batch 15/75, loss 1.0023\n",
            "epoch 29/250, batch 16/75, loss 0.2413\n",
            "epoch 29/250, batch 17/75, loss 0.2704\n",
            "epoch 29/250, batch 18/75, loss 0.3652\n",
            "epoch 29/250, batch 19/75, loss 0.2516\n",
            "epoch 29/250, batch 20/75, loss 0.6340\n",
            "epoch 29/250, batch 21/75, loss 0.1655\n",
            "epoch 29/250, batch 22/75, loss 0.2205\n",
            "epoch 29/250, batch 23/75, loss 0.2100\n",
            "epoch 29/250, batch 24/75, loss 0.1476\n",
            "epoch 29/250, batch 25/75, loss 0.1573\n",
            "epoch 29/250, batch 26/75, loss 0.8032\n",
            "epoch 29/250, batch 27/75, loss 0.2275\n",
            "epoch 29/250, batch 28/75, loss 0.2386\n",
            "epoch 29/250, batch 29/75, loss 0.1924\n",
            "epoch 29/250, batch 30/75, loss 0.2137\n",
            "epoch 29/250, batch 31/75, loss 0.1840\n",
            "epoch 29/250, batch 32/75, loss 0.1835\n",
            "epoch 29/250, batch 33/75, loss 0.1957\n",
            "epoch 29/250, batch 34/75, loss 0.2335\n",
            "epoch 29/250, batch 35/75, loss 0.2502\n",
            "epoch 29/250, batch 36/75, loss 1.2409\n",
            "epoch 29/250, batch 37/75, loss 0.2542\n",
            "epoch 29/250, batch 38/75, loss 0.1800\n",
            "epoch 29/250, batch 39/75, loss 0.2484\n",
            "epoch 29/250, batch 40/75, loss 0.2248\n",
            "epoch 29/250, batch 41/75, loss 0.2821\n",
            "epoch 29/250, batch 42/75, loss 0.2180\n",
            "epoch 29/250, batch 43/75, loss 0.1684\n",
            "epoch 29/250, batch 44/75, loss 0.2276\n",
            "epoch 29/250, batch 45/75, loss 0.2217\n",
            "epoch 29/250, batch 46/75, loss 0.1556\n",
            "epoch 29/250, batch 47/75, loss 0.2104\n",
            "epoch 29/250, batch 48/75, loss 0.1703\n",
            "epoch 29/250, batch 49/75, loss 0.2046\n",
            "epoch 29/250, batch 50/75, loss 0.1660\n",
            "epoch 29/250, batch 51/75, loss 0.1582\n",
            "epoch 29/250, batch 52/75, loss 0.1491\n",
            "epoch 29/250, batch 53/75, loss 9.1963\n",
            "epoch 29/250, batch 54/75, loss 0.2428\n",
            "epoch 29/250, batch 55/75, loss 0.2019\n",
            "epoch 29/250, batch 56/75, loss 0.1817\n",
            "epoch 29/250, batch 57/75, loss 0.1758\n",
            "epoch 29/250, batch 58/75, loss 0.2208\n",
            "epoch 29/250, batch 59/75, loss 0.2232\n",
            "epoch 29/250, batch 60/75, loss 0.2385\n",
            "epoch 29/250, batch 61/75, loss 0.2463\n",
            "epoch 29/250, batch 62/75, loss 0.2882\n",
            "epoch 29/250, batch 63/75, loss 0.2211\n",
            "epoch 29/250, batch 64/75, loss 0.3050\n",
            "epoch 29/250, batch 65/75, loss 0.3050\n",
            "epoch 29/250, batch 66/75, loss 0.2605\n",
            "epoch 29/250, batch 67/75, loss 0.2591\n",
            "epoch 29/250, batch 68/75, loss 0.2424\n",
            "epoch 29/250, batch 69/75, loss 0.2874\n",
            "epoch 29/250, batch 70/75, loss 0.2125\n",
            "epoch 29/250, batch 71/75, loss 0.2190\n",
            "epoch 29/250, batch 72/75, loss 0.2368\n",
            "epoch 29/250, batch 73/75, loss 0.2524\n",
            "epoch 29/250, batch 74/75, loss 0.2810\n",
            "epoch 29/250, batch 75/75, loss 0.2013\n",
            "epoch 29/250, training roc_auc_score 0.9630\n",
            "EarlyStopping counter: 17 out of 50\n",
            "epoch 29/250, validation roc_auc_score 0.7970, best validation roc_auc_score 0.8516\n",
            "epoch 30/250, batch 1/75, loss 0.2854\n",
            "epoch 30/250, batch 2/75, loss 0.4498\n",
            "epoch 30/250, batch 3/75, loss 0.2586\n",
            "epoch 30/250, batch 4/75, loss 0.2518\n",
            "epoch 30/250, batch 5/75, loss 0.1215\n",
            "epoch 30/250, batch 6/75, loss 0.1432\n",
            "epoch 30/250, batch 7/75, loss 0.2359\n",
            "epoch 30/250, batch 8/75, loss 0.3406\n",
            "epoch 30/250, batch 9/75, loss 0.1597\n",
            "epoch 30/250, batch 10/75, loss 0.2968\n",
            "epoch 30/250, batch 11/75, loss 0.5021\n",
            "epoch 30/250, batch 12/75, loss 0.2132\n",
            "epoch 30/250, batch 13/75, loss 0.2824\n",
            "epoch 30/250, batch 14/75, loss 0.1516\n",
            "epoch 30/250, batch 15/75, loss 0.8054\n",
            "epoch 30/250, batch 16/75, loss 0.1831\n",
            "epoch 30/250, batch 17/75, loss 0.2635\n",
            "epoch 30/250, batch 18/75, loss 0.2990\n",
            "epoch 30/250, batch 19/75, loss 0.2522\n",
            "epoch 30/250, batch 20/75, loss 0.5126\n",
            "epoch 30/250, batch 21/75, loss 0.1551\n",
            "epoch 30/250, batch 22/75, loss 0.2177\n",
            "epoch 30/250, batch 23/75, loss 0.1967\n",
            "epoch 30/250, batch 24/75, loss 0.1358\n",
            "epoch 30/250, batch 25/75, loss 0.1636\n",
            "epoch 30/250, batch 26/75, loss 1.0633\n",
            "epoch 30/250, batch 27/75, loss 0.2454\n",
            "epoch 30/250, batch 28/75, loss 0.2395\n",
            "epoch 30/250, batch 29/75, loss 0.1725\n",
            "epoch 30/250, batch 30/75, loss 0.1959\n",
            "epoch 30/250, batch 31/75, loss 0.1813\n",
            "epoch 30/250, batch 32/75, loss 0.1842\n",
            "epoch 30/250, batch 33/75, loss 0.1921\n",
            "epoch 30/250, batch 34/75, loss 0.2144\n",
            "epoch 30/250, batch 35/75, loss 0.2337\n",
            "epoch 30/250, batch 36/75, loss 1.4010\n",
            "epoch 30/250, batch 37/75, loss 0.2699\n",
            "epoch 30/250, batch 38/75, loss 0.1714\n",
            "epoch 30/250, batch 39/75, loss 0.2315\n",
            "epoch 30/250, batch 40/75, loss 0.1878\n",
            "epoch 30/250, batch 41/75, loss 0.2828\n",
            "epoch 30/250, batch 42/75, loss 0.1934\n",
            "epoch 30/250, batch 43/75, loss 0.1733\n",
            "epoch 30/250, batch 44/75, loss 0.2376\n",
            "epoch 30/250, batch 45/75, loss 0.2112\n",
            "epoch 30/250, batch 46/75, loss 0.1502\n",
            "epoch 30/250, batch 47/75, loss 0.1943\n",
            "epoch 30/250, batch 48/75, loss 0.1501\n",
            "epoch 30/250, batch 49/75, loss 0.1947\n",
            "epoch 30/250, batch 50/75, loss 0.1581\n",
            "epoch 30/250, batch 51/75, loss 0.1607\n",
            "epoch 30/250, batch 52/75, loss 0.1338\n",
            "epoch 30/250, batch 53/75, loss 9.3154\n",
            "epoch 30/250, batch 54/75, loss 0.2232\n",
            "epoch 30/250, batch 55/75, loss 0.2096\n",
            "epoch 30/250, batch 56/75, loss 0.1732\n",
            "epoch 30/250, batch 57/75, loss 0.1942\n",
            "epoch 30/250, batch 58/75, loss 0.1961\n",
            "epoch 30/250, batch 59/75, loss 0.2133\n",
            "epoch 30/250, batch 60/75, loss 0.2209\n",
            "epoch 30/250, batch 61/75, loss 0.2422\n",
            "epoch 30/250, batch 62/75, loss 0.2678\n",
            "epoch 30/250, batch 63/75, loss 0.1863\n",
            "epoch 30/250, batch 64/75, loss 0.2609\n",
            "epoch 30/250, batch 65/75, loss 0.2377\n",
            "epoch 30/250, batch 66/75, loss 0.2672\n",
            "epoch 30/250, batch 67/75, loss 0.2012\n",
            "epoch 30/250, batch 68/75, loss 0.1981\n",
            "epoch 30/250, batch 69/75, loss 0.2592\n",
            "epoch 30/250, batch 70/75, loss 0.1879\n",
            "epoch 30/250, batch 71/75, loss 0.2139\n",
            "epoch 30/250, batch 72/75, loss 0.2180\n",
            "epoch 30/250, batch 73/75, loss 0.2437\n",
            "epoch 30/250, batch 74/75, loss 0.2685\n",
            "epoch 30/250, batch 75/75, loss 0.1817\n",
            "epoch 30/250, training roc_auc_score 0.9635\n",
            "EarlyStopping counter: 18 out of 50\n",
            "epoch 30/250, validation roc_auc_score 0.8014, best validation roc_auc_score 0.8516\n",
            "epoch 31/250, batch 1/75, loss 0.2543\n",
            "epoch 31/250, batch 2/75, loss 0.3213\n",
            "epoch 31/250, batch 3/75, loss 0.1818\n",
            "epoch 31/250, batch 4/75, loss 0.2456\n",
            "epoch 31/250, batch 5/75, loss 0.1246\n",
            "epoch 31/250, batch 6/75, loss 0.1256\n",
            "epoch 31/250, batch 7/75, loss 0.1907\n",
            "epoch 31/250, batch 8/75, loss 0.3648\n",
            "epoch 31/250, batch 9/75, loss 0.1384\n",
            "epoch 31/250, batch 10/75, loss 0.3581\n",
            "epoch 31/250, batch 11/75, loss 0.2105\n",
            "epoch 31/250, batch 12/75, loss 0.1974\n",
            "epoch 31/250, batch 13/75, loss 0.2599\n",
            "epoch 31/250, batch 14/75, loss 0.1355\n",
            "epoch 31/250, batch 15/75, loss 0.8726\n",
            "epoch 31/250, batch 16/75, loss 0.1963\n",
            "epoch 31/250, batch 17/75, loss 0.2168\n",
            "epoch 31/250, batch 18/75, loss 0.3088\n",
            "epoch 31/250, batch 19/75, loss 0.2072\n",
            "epoch 31/250, batch 20/75, loss 0.5323\n",
            "epoch 31/250, batch 21/75, loss 0.1386\n",
            "epoch 31/250, batch 22/75, loss 0.1878\n",
            "epoch 31/250, batch 23/75, loss 0.1670\n",
            "epoch 31/250, batch 24/75, loss 0.1335\n",
            "epoch 31/250, batch 25/75, loss 0.1423\n",
            "epoch 31/250, batch 26/75, loss 0.7765\n",
            "epoch 31/250, batch 27/75, loss 0.2327\n",
            "epoch 31/250, batch 28/75, loss 0.2145\n",
            "epoch 31/250, batch 29/75, loss 0.1663\n",
            "epoch 31/250, batch 30/75, loss 0.1916\n",
            "epoch 31/250, batch 31/75, loss 0.1509\n",
            "epoch 31/250, batch 32/75, loss 0.1662\n",
            "epoch 31/250, batch 33/75, loss 0.1909\n",
            "epoch 31/250, batch 34/75, loss 0.2062\n",
            "epoch 31/250, batch 35/75, loss 0.2278\n",
            "epoch 31/250, batch 36/75, loss 0.9815\n",
            "epoch 31/250, batch 37/75, loss 0.2441\n",
            "epoch 31/250, batch 38/75, loss 0.1699\n",
            "epoch 31/250, batch 39/75, loss 0.2327\n",
            "epoch 31/250, batch 40/75, loss 0.1963\n",
            "epoch 31/250, batch 41/75, loss 0.2662\n",
            "epoch 31/250, batch 42/75, loss 0.1913\n",
            "epoch 31/250, batch 43/75, loss 0.1790\n",
            "epoch 31/250, batch 44/75, loss 0.2105\n",
            "epoch 31/250, batch 45/75, loss 0.2003\n",
            "epoch 31/250, batch 46/75, loss 0.1361\n",
            "epoch 31/250, batch 47/75, loss 0.2003\n",
            "epoch 31/250, batch 48/75, loss 0.1532\n",
            "epoch 31/250, batch 49/75, loss 0.1873\n",
            "epoch 31/250, batch 50/75, loss 0.1544\n",
            "epoch 31/250, batch 51/75, loss 0.1508\n",
            "epoch 31/250, batch 52/75, loss 0.1287\n",
            "epoch 31/250, batch 53/75, loss 7.7039\n",
            "epoch 31/250, batch 54/75, loss 0.2240\n",
            "epoch 31/250, batch 55/75, loss 0.1876\n",
            "epoch 31/250, batch 56/75, loss 0.1601\n",
            "epoch 31/250, batch 57/75, loss 0.1711\n",
            "epoch 31/250, batch 58/75, loss 0.1954\n",
            "epoch 31/250, batch 59/75, loss 0.2031\n",
            "epoch 31/250, batch 60/75, loss 0.1938\n",
            "epoch 31/250, batch 61/75, loss 0.2152\n",
            "epoch 31/250, batch 62/75, loss 0.2444\n",
            "epoch 31/250, batch 63/75, loss 0.1833\n",
            "epoch 31/250, batch 64/75, loss 0.2338\n",
            "epoch 31/250, batch 65/75, loss 0.2107\n",
            "epoch 31/250, batch 66/75, loss 0.2346\n",
            "epoch 31/250, batch 67/75, loss 0.1844\n",
            "epoch 31/250, batch 68/75, loss 0.1929\n",
            "epoch 31/250, batch 69/75, loss 0.2416\n",
            "epoch 31/250, batch 70/75, loss 0.1752\n",
            "epoch 31/250, batch 71/75, loss 0.1951\n",
            "epoch 31/250, batch 72/75, loss 0.1733\n",
            "epoch 31/250, batch 73/75, loss 0.2111\n",
            "epoch 31/250, batch 74/75, loss 0.2309\n",
            "epoch 31/250, batch 75/75, loss 0.1667\n",
            "epoch 31/250, training roc_auc_score 0.9774\n",
            "EarlyStopping counter: 19 out of 50\n",
            "epoch 31/250, validation roc_auc_score 0.8187, best validation roc_auc_score 0.8516\n",
            "epoch 32/250, batch 1/75, loss 0.2326\n",
            "epoch 32/250, batch 2/75, loss 0.3048\n",
            "epoch 32/250, batch 3/75, loss 0.1803\n",
            "epoch 32/250, batch 4/75, loss 0.2757\n",
            "epoch 32/250, batch 5/75, loss 0.1137\n",
            "epoch 32/250, batch 6/75, loss 0.1179\n",
            "epoch 32/250, batch 7/75, loss 0.1917\n",
            "epoch 32/250, batch 8/75, loss 0.3325\n",
            "epoch 32/250, batch 9/75, loss 0.1420\n",
            "epoch 32/250, batch 10/75, loss 0.2407\n",
            "epoch 32/250, batch 11/75, loss 0.2365\n",
            "epoch 32/250, batch 12/75, loss 0.2048\n",
            "epoch 32/250, batch 13/75, loss 0.2972\n",
            "epoch 32/250, batch 14/75, loss 0.1328\n",
            "epoch 32/250, batch 15/75, loss 0.7047\n",
            "epoch 32/250, batch 16/75, loss 0.1968\n",
            "epoch 32/250, batch 17/75, loss 0.2320\n",
            "epoch 32/250, batch 18/75, loss 0.2951\n",
            "epoch 32/250, batch 19/75, loss 0.2090\n",
            "epoch 32/250, batch 20/75, loss 0.4605\n",
            "epoch 32/250, batch 21/75, loss 0.1354\n",
            "epoch 32/250, batch 22/75, loss 0.1744\n",
            "epoch 32/250, batch 23/75, loss 0.1626\n",
            "epoch 32/250, batch 24/75, loss 0.1194\n",
            "epoch 32/250, batch 25/75, loss 0.1323\n",
            "epoch 32/250, batch 26/75, loss 0.7159\n",
            "epoch 32/250, batch 27/75, loss 0.1929\n",
            "epoch 32/250, batch 28/75, loss 0.1918\n",
            "epoch 32/250, batch 29/75, loss 0.1506\n",
            "epoch 32/250, batch 30/75, loss 0.1759\n",
            "epoch 32/250, batch 31/75, loss 0.1355\n",
            "epoch 32/250, batch 32/75, loss 0.1456\n",
            "epoch 32/250, batch 33/75, loss 0.1580\n",
            "epoch 32/250, batch 34/75, loss 0.1778\n",
            "epoch 32/250, batch 35/75, loss 0.2024\n",
            "epoch 32/250, batch 36/75, loss 1.0599\n",
            "epoch 32/250, batch 37/75, loss 0.2008\n",
            "epoch 32/250, batch 38/75, loss 0.1468\n",
            "epoch 32/250, batch 39/75, loss 0.2015\n",
            "epoch 32/250, batch 40/75, loss 0.1764\n",
            "epoch 32/250, batch 41/75, loss 0.2425\n",
            "epoch 32/250, batch 42/75, loss 0.1669\n",
            "epoch 32/250, batch 43/75, loss 0.1447\n",
            "epoch 32/250, batch 44/75, loss 0.2014\n",
            "epoch 32/250, batch 45/75, loss 0.1888\n",
            "epoch 32/250, batch 46/75, loss 0.1227\n",
            "epoch 32/250, batch 47/75, loss 0.1748\n",
            "epoch 32/250, batch 48/75, loss 0.1376\n",
            "epoch 32/250, batch 49/75, loss 0.1744\n",
            "epoch 32/250, batch 50/75, loss 0.1395\n",
            "epoch 32/250, batch 51/75, loss 0.1357\n",
            "epoch 32/250, batch 52/75, loss 0.1198\n",
            "epoch 32/250, batch 53/75, loss 7.5049\n",
            "epoch 32/250, batch 54/75, loss 0.2166\n",
            "epoch 32/250, batch 55/75, loss 0.1622\n",
            "epoch 32/250, batch 56/75, loss 0.1446\n",
            "epoch 32/250, batch 57/75, loss 0.1451\n",
            "epoch 32/250, batch 58/75, loss 0.1720\n",
            "epoch 32/250, batch 59/75, loss 0.1756\n",
            "epoch 32/250, batch 60/75, loss 0.1852\n",
            "epoch 32/250, batch 61/75, loss 0.1884\n",
            "epoch 32/250, batch 62/75, loss 0.2256\n",
            "epoch 32/250, batch 63/75, loss 0.1763\n",
            "epoch 32/250, batch 64/75, loss 0.2043\n",
            "epoch 32/250, batch 65/75, loss 0.2164\n",
            "epoch 32/250, batch 66/75, loss 0.2142\n",
            "epoch 32/250, batch 67/75, loss 0.1834\n",
            "epoch 32/250, batch 68/75, loss 0.1876\n",
            "epoch 32/250, batch 69/75, loss 0.2488\n",
            "epoch 32/250, batch 70/75, loss 0.1569\n",
            "epoch 32/250, batch 71/75, loss 0.1916\n",
            "epoch 32/250, batch 72/75, loss 0.1787\n",
            "epoch 32/250, batch 73/75, loss 0.1925\n",
            "epoch 32/250, batch 74/75, loss 0.2186\n",
            "epoch 32/250, batch 75/75, loss 0.1454\n",
            "epoch 32/250, training roc_auc_score 0.9798\n",
            "EarlyStopping counter: 20 out of 50\n",
            "epoch 32/250, validation roc_auc_score 0.8112, best validation roc_auc_score 0.8516\n",
            "epoch 33/250, batch 1/75, loss 0.2466\n",
            "epoch 33/250, batch 2/75, loss 0.2632\n",
            "epoch 33/250, batch 3/75, loss 0.1680\n",
            "epoch 33/250, batch 4/75, loss 0.4713\n",
            "epoch 33/250, batch 5/75, loss 0.1086\n",
            "epoch 33/250, batch 6/75, loss 0.1081\n",
            "epoch 33/250, batch 7/75, loss 0.2499\n",
            "epoch 33/250, batch 8/75, loss 0.3428\n",
            "epoch 33/250, batch 9/75, loss 0.1363\n",
            "epoch 33/250, batch 10/75, loss 0.2317\n",
            "epoch 33/250, batch 11/75, loss 0.1737\n",
            "epoch 33/250, batch 12/75, loss 0.1903\n",
            "epoch 33/250, batch 13/75, loss 0.2803\n",
            "epoch 33/250, batch 14/75, loss 0.1229\n",
            "epoch 33/250, batch 15/75, loss 0.9078\n",
            "epoch 33/250, batch 16/75, loss 0.1797\n",
            "epoch 33/250, batch 17/75, loss 0.1925\n",
            "epoch 33/250, batch 18/75, loss 0.2869\n",
            "epoch 33/250, batch 19/75, loss 0.1945\n",
            "epoch 33/250, batch 20/75, loss 0.5249\n",
            "epoch 33/250, batch 21/75, loss 0.1277\n",
            "epoch 33/250, batch 22/75, loss 0.1498\n",
            "epoch 33/250, batch 23/75, loss 0.1978\n",
            "epoch 33/250, batch 24/75, loss 0.1270\n",
            "epoch 33/250, batch 25/75, loss 0.1258\n",
            "epoch 33/250, batch 26/75, loss 0.8439\n",
            "epoch 33/250, batch 27/75, loss 0.2009\n",
            "epoch 33/250, batch 28/75, loss 0.1943\n",
            "epoch 33/250, batch 29/75, loss 0.1511\n",
            "epoch 33/250, batch 30/75, loss 0.1593\n",
            "epoch 33/250, batch 31/75, loss 0.1307\n",
            "epoch 33/250, batch 32/75, loss 0.1599\n",
            "epoch 33/250, batch 33/75, loss 0.1862\n",
            "epoch 33/250, batch 34/75, loss 0.2016\n",
            "epoch 33/250, batch 35/75, loss 0.1967\n",
            "epoch 33/250, batch 36/75, loss 0.9604\n",
            "epoch 33/250, batch 37/75, loss 0.1767\n",
            "epoch 33/250, batch 38/75, loss 0.1641\n",
            "epoch 33/250, batch 39/75, loss 0.2275\n",
            "epoch 33/250, batch 40/75, loss 0.1980\n",
            "epoch 33/250, batch 41/75, loss 0.2450\n",
            "epoch 33/250, batch 42/75, loss 0.1803\n",
            "epoch 33/250, batch 43/75, loss 0.1598\n",
            "epoch 33/250, batch 44/75, loss 0.2062\n",
            "epoch 33/250, batch 45/75, loss 0.2021\n",
            "epoch 33/250, batch 46/75, loss 0.1248\n",
            "epoch 33/250, batch 47/75, loss 0.1713\n",
            "epoch 33/250, batch 48/75, loss 0.1422\n",
            "epoch 33/250, batch 49/75, loss 0.1698\n",
            "epoch 33/250, batch 50/75, loss 0.1398\n",
            "epoch 33/250, batch 51/75, loss 0.1334\n",
            "epoch 33/250, batch 52/75, loss 0.1152\n",
            "epoch 33/250, batch 53/75, loss 7.5879\n",
            "epoch 33/250, batch 54/75, loss 0.2194\n",
            "epoch 33/250, batch 55/75, loss 0.1665\n",
            "epoch 33/250, batch 56/75, loss 0.1479\n",
            "epoch 33/250, batch 57/75, loss 0.1605\n",
            "epoch 33/250, batch 58/75, loss 0.1838\n",
            "epoch 33/250, batch 59/75, loss 0.1691\n",
            "epoch 33/250, batch 60/75, loss 0.1846\n",
            "epoch 33/250, batch 61/75, loss 0.2158\n",
            "epoch 33/250, batch 62/75, loss 0.2282\n",
            "epoch 33/250, batch 63/75, loss 0.1569\n",
            "epoch 33/250, batch 64/75, loss 0.2424\n",
            "epoch 33/250, batch 65/75, loss 0.1983\n",
            "epoch 33/250, batch 66/75, loss 0.2335\n",
            "epoch 33/250, batch 67/75, loss 0.1749\n",
            "epoch 33/250, batch 68/75, loss 0.1651\n",
            "epoch 33/250, batch 69/75, loss 0.2497\n",
            "epoch 33/250, batch 70/75, loss 0.1663\n",
            "epoch 33/250, batch 71/75, loss 0.2061\n",
            "epoch 33/250, batch 72/75, loss 0.1813\n",
            "epoch 33/250, batch 73/75, loss 0.2133\n",
            "epoch 33/250, batch 74/75, loss 0.2436\n",
            "epoch 33/250, batch 75/75, loss 0.1567\n",
            "epoch 33/250, training roc_auc_score 0.9766\n",
            "EarlyStopping counter: 21 out of 50\n",
            "epoch 33/250, validation roc_auc_score 0.7781, best validation roc_auc_score 0.8516\n",
            "epoch 34/250, batch 1/75, loss 0.2378\n",
            "epoch 34/250, batch 2/75, loss 0.2504\n",
            "epoch 34/250, batch 3/75, loss 0.1734\n",
            "epoch 34/250, batch 4/75, loss 0.2044\n",
            "epoch 34/250, batch 5/75, loss 0.0964\n",
            "epoch 34/250, batch 6/75, loss 0.1103\n",
            "epoch 34/250, batch 7/75, loss 0.1633\n",
            "epoch 34/250, batch 8/75, loss 0.3630\n",
            "epoch 34/250, batch 9/75, loss 0.1219\n",
            "epoch 34/250, batch 10/75, loss 0.2370\n",
            "epoch 34/250, batch 11/75, loss 0.2553\n",
            "epoch 34/250, batch 12/75, loss 0.1686\n",
            "epoch 34/250, batch 13/75, loss 0.3513\n",
            "epoch 34/250, batch 14/75, loss 0.1432\n",
            "epoch 34/250, batch 15/75, loss 0.8671\n",
            "epoch 34/250, batch 16/75, loss 0.1836\n",
            "epoch 34/250, batch 17/75, loss 0.2195\n",
            "epoch 34/250, batch 18/75, loss 0.3222\n",
            "epoch 34/250, batch 19/75, loss 0.2125\n",
            "epoch 34/250, batch 20/75, loss 0.3326\n",
            "epoch 34/250, batch 21/75, loss 0.1289\n",
            "epoch 34/250, batch 22/75, loss 0.1752\n",
            "epoch 34/250, batch 23/75, loss 0.1889\n",
            "epoch 34/250, batch 24/75, loss 0.1063\n",
            "epoch 34/250, batch 25/75, loss 0.1412\n",
            "epoch 34/250, batch 26/75, loss 0.7047\n",
            "epoch 34/250, batch 27/75, loss 0.2182\n",
            "epoch 34/250, batch 28/75, loss 0.1945\n",
            "epoch 34/250, batch 29/75, loss 0.1441\n",
            "epoch 34/250, batch 30/75, loss 0.1771\n",
            "epoch 34/250, batch 31/75, loss 0.1401\n",
            "epoch 34/250, batch 32/75, loss 0.1537\n",
            "epoch 34/250, batch 33/75, loss 0.1498\n",
            "epoch 34/250, batch 34/75, loss 0.1697\n",
            "epoch 34/250, batch 35/75, loss 0.2059\n",
            "epoch 34/250, batch 36/75, loss 0.9149\n",
            "epoch 34/250, batch 37/75, loss 0.1946\n",
            "epoch 34/250, batch 38/75, loss 0.1442\n",
            "epoch 34/250, batch 39/75, loss 0.1800\n",
            "epoch 34/250, batch 40/75, loss 0.1575\n",
            "epoch 34/250, batch 41/75, loss 0.2549\n",
            "epoch 34/250, batch 42/75, loss 0.1694\n",
            "epoch 34/250, batch 43/75, loss 0.1404\n",
            "epoch 34/250, batch 44/75, loss 0.1962\n",
            "epoch 34/250, batch 45/75, loss 0.1810\n",
            "epoch 34/250, batch 46/75, loss 0.1094\n",
            "epoch 34/250, batch 47/75, loss 0.1662\n",
            "epoch 34/250, batch 48/75, loss 0.1231\n",
            "epoch 34/250, batch 49/75, loss 0.1502\n",
            "epoch 34/250, batch 50/75, loss 0.1373\n",
            "epoch 34/250, batch 51/75, loss 0.1284\n",
            "epoch 34/250, batch 52/75, loss 0.1018\n",
            "epoch 34/250, batch 53/75, loss 7.6146\n",
            "epoch 34/250, batch 54/75, loss 0.1923\n",
            "epoch 34/250, batch 55/75, loss 0.1539\n",
            "epoch 34/250, batch 56/75, loss 0.1334\n",
            "epoch 34/250, batch 57/75, loss 0.1453\n",
            "epoch 34/250, batch 58/75, loss 0.1683\n",
            "epoch 34/250, batch 59/75, loss 0.1781\n",
            "epoch 34/250, batch 60/75, loss 0.1951\n",
            "epoch 34/250, batch 61/75, loss 0.1847\n",
            "epoch 34/250, batch 62/75, loss 0.2300\n",
            "epoch 34/250, batch 63/75, loss 0.1539\n",
            "epoch 34/250, batch 64/75, loss 0.1976\n",
            "epoch 34/250, batch 65/75, loss 0.2448\n",
            "epoch 34/250, batch 66/75, loss 0.2073\n",
            "epoch 34/250, batch 67/75, loss 0.1680\n",
            "epoch 34/250, batch 68/75, loss 0.1429\n",
            "epoch 34/250, batch 69/75, loss 0.2166\n",
            "epoch 34/250, batch 70/75, loss 0.1497\n",
            "epoch 34/250, batch 71/75, loss 0.2298\n",
            "epoch 34/250, batch 72/75, loss 0.1957\n",
            "epoch 34/250, batch 73/75, loss 0.2089\n",
            "epoch 34/250, batch 74/75, loss 0.2521\n",
            "epoch 34/250, batch 75/75, loss 0.1572\n",
            "epoch 34/250, training roc_auc_score 0.9788\n",
            "EarlyStopping counter: 22 out of 50\n",
            "epoch 34/250, validation roc_auc_score 0.7549, best validation roc_auc_score 0.8516\n",
            "epoch 35/250, batch 1/75, loss 0.2555\n",
            "epoch 35/250, batch 2/75, loss 0.2216\n",
            "epoch 35/250, batch 3/75, loss 0.1551\n",
            "epoch 35/250, batch 4/75, loss 0.2093\n",
            "epoch 35/250, batch 5/75, loss 0.1061\n",
            "epoch 35/250, batch 6/75, loss 0.1162\n",
            "epoch 35/250, batch 7/75, loss 0.1819\n",
            "epoch 35/250, batch 8/75, loss 0.5838\n",
            "epoch 35/250, batch 9/75, loss 0.1286\n",
            "epoch 35/250, batch 10/75, loss 0.1622\n",
            "epoch 35/250, batch 11/75, loss 0.1868\n",
            "epoch 35/250, batch 12/75, loss 0.1784\n",
            "epoch 35/250, batch 13/75, loss 0.3144\n",
            "epoch 35/250, batch 14/75, loss 0.1287\n",
            "epoch 35/250, batch 15/75, loss 0.6560\n",
            "epoch 35/250, batch 16/75, loss 0.1692\n",
            "epoch 35/250, batch 17/75, loss 0.2200\n",
            "epoch 35/250, batch 18/75, loss 0.2955\n",
            "epoch 35/250, batch 19/75, loss 0.2225\n",
            "epoch 35/250, batch 20/75, loss 0.5661\n",
            "epoch 35/250, batch 21/75, loss 0.1129\n",
            "epoch 35/250, batch 22/75, loss 0.1461\n",
            "epoch 35/250, batch 23/75, loss 0.1745\n",
            "epoch 35/250, batch 24/75, loss 0.1220\n",
            "epoch 35/250, batch 25/75, loss 0.1149\n",
            "epoch 35/250, batch 26/75, loss 1.1865\n",
            "epoch 35/250, batch 27/75, loss 0.2125\n",
            "epoch 35/250, batch 28/75, loss 0.1824\n",
            "epoch 35/250, batch 29/75, loss 0.1335\n",
            "epoch 35/250, batch 30/75, loss 0.1579\n",
            "epoch 35/250, batch 31/75, loss 0.1398\n",
            "epoch 35/250, batch 32/75, loss 0.1520\n",
            "epoch 35/250, batch 33/75, loss 0.1853\n",
            "epoch 35/250, batch 34/75, loss 0.2364\n",
            "epoch 35/250, batch 35/75, loss 0.2433\n",
            "epoch 35/250, batch 36/75, loss 1.2111\n",
            "epoch 35/250, batch 37/75, loss 0.2481\n",
            "epoch 35/250, batch 38/75, loss 0.1437\n",
            "epoch 35/250, batch 39/75, loss 0.2046\n",
            "epoch 35/250, batch 40/75, loss 0.1750\n",
            "epoch 35/250, batch 41/75, loss 0.2454\n",
            "epoch 35/250, batch 42/75, loss 0.1665\n",
            "epoch 35/250, batch 43/75, loss 0.1627\n",
            "epoch 35/250, batch 44/75, loss 0.1858\n",
            "epoch 35/250, batch 45/75, loss 0.1835\n",
            "epoch 35/250, batch 46/75, loss 0.1001\n",
            "epoch 35/250, batch 47/75, loss 0.1613\n",
            "epoch 35/250, batch 48/75, loss 0.1251\n",
            "epoch 35/250, batch 49/75, loss 0.1675\n",
            "epoch 35/250, batch 50/75, loss 0.1259\n",
            "epoch 35/250, batch 51/75, loss 0.1365\n",
            "epoch 35/250, batch 52/75, loss 0.0965\n",
            "epoch 35/250, batch 53/75, loss 8.8885\n",
            "epoch 35/250, batch 54/75, loss 0.2029\n",
            "epoch 35/250, batch 55/75, loss 0.1571\n",
            "epoch 35/250, batch 56/75, loss 0.1310\n",
            "epoch 35/250, batch 57/75, loss 0.1443\n",
            "epoch 35/250, batch 58/75, loss 0.1819\n",
            "epoch 35/250, batch 59/75, loss 0.1744\n",
            "epoch 35/250, batch 60/75, loss 0.1858\n",
            "epoch 35/250, batch 61/75, loss 0.2127\n",
            "epoch 35/250, batch 62/75, loss 0.2774\n",
            "epoch 35/250, batch 63/75, loss 0.1709\n",
            "epoch 35/250, batch 64/75, loss 0.2899\n",
            "epoch 35/250, batch 65/75, loss 0.2394\n",
            "epoch 35/250, batch 66/75, loss 0.2691\n",
            "epoch 35/250, batch 67/75, loss 0.2127\n",
            "epoch 35/250, batch 68/75, loss 0.1956\n",
            "epoch 35/250, batch 69/75, loss 0.2617\n",
            "epoch 35/250, batch 70/75, loss 0.1571\n",
            "epoch 35/250, batch 71/75, loss 0.2173\n",
            "epoch 35/250, batch 72/75, loss 0.2012\n",
            "epoch 35/250, batch 73/75, loss 0.2232\n",
            "epoch 35/250, batch 74/75, loss 0.2232\n",
            "epoch 35/250, batch 75/75, loss 0.1692\n",
            "epoch 35/250, training roc_auc_score 0.9689\n",
            "EarlyStopping counter: 23 out of 50\n",
            "epoch 35/250, validation roc_auc_score 0.8094, best validation roc_auc_score 0.8516\n",
            "epoch 36/250, batch 1/75, loss 0.3197\n",
            "epoch 36/250, batch 2/75, loss 0.3862\n",
            "epoch 36/250, batch 3/75, loss 0.2066\n",
            "epoch 36/250, batch 4/75, loss 0.3172\n",
            "epoch 36/250, batch 5/75, loss 0.1065\n",
            "epoch 36/250, batch 6/75, loss 0.1261\n",
            "epoch 36/250, batch 7/75, loss 0.2064\n",
            "epoch 36/250, batch 8/75, loss 0.3067\n",
            "epoch 36/250, batch 9/75, loss 0.1346\n",
            "epoch 36/250, batch 10/75, loss 0.3098\n",
            "epoch 36/250, batch 11/75, loss 0.3351\n",
            "epoch 36/250, batch 12/75, loss 0.2063\n",
            "epoch 36/250, batch 13/75, loss 0.3274\n",
            "epoch 36/250, batch 14/75, loss 0.1089\n",
            "epoch 36/250, batch 15/75, loss 1.0676\n",
            "epoch 36/250, batch 16/75, loss 0.1897\n",
            "epoch 36/250, batch 17/75, loss 0.1882\n",
            "epoch 36/250, batch 18/75, loss 0.3101\n",
            "epoch 36/250, batch 19/75, loss 0.1909\n",
            "epoch 36/250, batch 20/75, loss 0.4555\n",
            "epoch 36/250, batch 21/75, loss 0.1236\n",
            "epoch 36/250, batch 22/75, loss 0.1772\n",
            "epoch 36/250, batch 23/75, loss 0.1598\n",
            "epoch 36/250, batch 24/75, loss 0.1000\n",
            "epoch 36/250, batch 25/75, loss 0.1304\n",
            "epoch 36/250, batch 26/75, loss 1.2870\n",
            "epoch 36/250, batch 27/75, loss 0.2033\n",
            "epoch 36/250, batch 28/75, loss 0.2049\n",
            "epoch 36/250, batch 29/75, loss 0.1451\n",
            "epoch 36/250, batch 30/75, loss 0.1697\n",
            "epoch 36/250, batch 31/75, loss 0.1281\n",
            "epoch 36/250, batch 32/75, loss 0.1581\n",
            "epoch 36/250, batch 33/75, loss 0.2062\n",
            "epoch 36/250, batch 34/75, loss 0.1714\n",
            "epoch 36/250, batch 35/75, loss 0.2296\n",
            "epoch 36/250, batch 36/75, loss 1.8181\n",
            "epoch 36/250, batch 37/75, loss 0.1957\n",
            "epoch 36/250, batch 38/75, loss 0.1428\n",
            "epoch 36/250, batch 39/75, loss 0.2373\n",
            "epoch 36/250, batch 40/75, loss 0.1680\n",
            "epoch 36/250, batch 41/75, loss 0.3389\n",
            "epoch 36/250, batch 42/75, loss 0.2053\n",
            "epoch 36/250, batch 43/75, loss 0.1531\n",
            "epoch 36/250, batch 44/75, loss 0.2418\n",
            "epoch 36/250, batch 45/75, loss 0.2214\n",
            "epoch 36/250, batch 46/75, loss 0.1494\n",
            "epoch 36/250, batch 47/75, loss 0.2031\n",
            "epoch 36/250, batch 48/75, loss 0.1533\n",
            "epoch 36/250, batch 49/75, loss 0.2293\n",
            "epoch 36/250, batch 50/75, loss 0.1691\n",
            "epoch 36/250, batch 51/75, loss 0.1572\n",
            "epoch 36/250, batch 52/75, loss 0.1296\n",
            "epoch 36/250, batch 53/75, loss 9.3420\n",
            "epoch 36/250, batch 54/75, loss 0.1546\n",
            "epoch 36/250, batch 55/75, loss 0.1805\n",
            "epoch 36/250, batch 56/75, loss 0.1500\n",
            "epoch 36/250, batch 57/75, loss 0.1471\n",
            "epoch 36/250, batch 58/75, loss 0.1694\n",
            "epoch 36/250, batch 59/75, loss 0.1637\n",
            "epoch 36/250, batch 60/75, loss 0.1922\n",
            "epoch 36/250, batch 61/75, loss 0.1776\n",
            "epoch 36/250, batch 62/75, loss 0.2174\n",
            "epoch 36/250, batch 63/75, loss 0.1694\n",
            "epoch 36/250, batch 64/75, loss 0.2404\n",
            "epoch 36/250, batch 65/75, loss 0.2213\n",
            "epoch 36/250, batch 66/75, loss 0.2227\n",
            "epoch 36/250, batch 67/75, loss 0.1736\n",
            "epoch 36/250, batch 68/75, loss 0.1612\n",
            "epoch 36/250, batch 69/75, loss 0.2335\n",
            "epoch 36/250, batch 70/75, loss 0.1633\n",
            "epoch 36/250, batch 71/75, loss 0.2427\n",
            "epoch 36/250, batch 72/75, loss 0.1732\n",
            "epoch 36/250, batch 73/75, loss 0.2128\n",
            "epoch 36/250, batch 74/75, loss 0.2455\n",
            "epoch 36/250, batch 75/75, loss 0.1862\n",
            "epoch 36/250, training roc_auc_score 0.9606\n",
            "EarlyStopping counter: 24 out of 50\n",
            "epoch 36/250, validation roc_auc_score 0.7360, best validation roc_auc_score 0.8516\n",
            "epoch 37/250, batch 1/75, loss 0.2411\n",
            "epoch 37/250, batch 2/75, loss 0.3636\n",
            "epoch 37/250, batch 3/75, loss 0.1458\n",
            "epoch 37/250, batch 4/75, loss 0.3778\n",
            "epoch 37/250, batch 5/75, loss 0.1226\n",
            "epoch 37/250, batch 6/75, loss 0.1213\n",
            "epoch 37/250, batch 7/75, loss 0.2088\n",
            "epoch 37/250, batch 8/75, loss 0.4416\n",
            "epoch 37/250, batch 9/75, loss 0.1438\n",
            "epoch 37/250, batch 10/75, loss 0.2202\n",
            "epoch 37/250, batch 11/75, loss 0.2638\n",
            "epoch 37/250, batch 12/75, loss 0.1959\n",
            "epoch 37/250, batch 13/75, loss 0.3433\n",
            "epoch 37/250, batch 14/75, loss 0.1135\n",
            "epoch 37/250, batch 15/75, loss 0.7993\n",
            "epoch 37/250, batch 16/75, loss 0.1763\n",
            "epoch 37/250, batch 17/75, loss 0.2456\n",
            "epoch 37/250, batch 18/75, loss 0.3095\n",
            "epoch 37/250, batch 19/75, loss 0.2045\n",
            "epoch 37/250, batch 20/75, loss 0.5019\n",
            "epoch 37/250, batch 21/75, loss 0.1073\n",
            "epoch 37/250, batch 22/75, loss 0.1560\n",
            "epoch 37/250, batch 23/75, loss 0.1360\n",
            "epoch 37/250, batch 24/75, loss 0.1090\n",
            "epoch 37/250, batch 25/75, loss 0.1075\n",
            "epoch 37/250, batch 26/75, loss 1.3421\n",
            "epoch 37/250, batch 27/75, loss 0.2269\n",
            "epoch 37/250, batch 28/75, loss 0.1945\n",
            "epoch 37/250, batch 29/75, loss 0.1457\n",
            "epoch 37/250, batch 30/75, loss 0.1618\n",
            "epoch 37/250, batch 31/75, loss 0.1358\n",
            "epoch 37/250, batch 32/75, loss 0.1379\n",
            "epoch 37/250, batch 33/75, loss 0.1914\n",
            "epoch 37/250, batch 34/75, loss 0.1864\n",
            "epoch 37/250, batch 35/75, loss 0.2302\n",
            "epoch 37/250, batch 36/75, loss 1.0145\n",
            "epoch 37/250, batch 37/75, loss 0.1753\n",
            "epoch 37/250, batch 38/75, loss 0.1603\n",
            "epoch 37/250, batch 39/75, loss 0.1915\n",
            "epoch 37/250, batch 40/75, loss 0.1812\n",
            "epoch 37/250, batch 41/75, loss 0.2354\n",
            "epoch 37/250, batch 42/75, loss 0.1703\n",
            "epoch 37/250, batch 43/75, loss 0.1293\n",
            "epoch 37/250, batch 44/75, loss 0.1985\n",
            "epoch 37/250, batch 45/75, loss 0.1789\n",
            "epoch 37/250, batch 46/75, loss 0.1020\n",
            "epoch 37/250, batch 47/75, loss 0.1753\n",
            "epoch 37/250, batch 48/75, loss 0.1365\n",
            "epoch 37/250, batch 49/75, loss 0.1639\n",
            "epoch 37/250, batch 50/75, loss 0.1240\n",
            "epoch 37/250, batch 51/75, loss 0.1307\n",
            "epoch 37/250, batch 52/75, loss 0.1035\n",
            "epoch 37/250, batch 53/75, loss 7.9851\n",
            "epoch 37/250, batch 54/75, loss 0.2946\n",
            "epoch 37/250, batch 55/75, loss 0.1702\n",
            "epoch 37/250, batch 56/75, loss 0.1509\n",
            "epoch 37/250, batch 57/75, loss 0.1442\n",
            "epoch 37/250, batch 58/75, loss 0.1911\n",
            "epoch 37/250, batch 59/75, loss 0.1711\n",
            "epoch 37/250, batch 60/75, loss 0.2060\n",
            "epoch 37/250, batch 61/75, loss 0.2256\n",
            "epoch 37/250, batch 62/75, loss 0.2991\n",
            "epoch 37/250, batch 63/75, loss 0.1684\n",
            "epoch 37/250, batch 64/75, loss 0.2834\n",
            "epoch 37/250, batch 65/75, loss 0.2584\n",
            "epoch 37/250, batch 66/75, loss 0.2713\n",
            "epoch 37/250, batch 67/75, loss 0.2226\n",
            "epoch 37/250, batch 68/75, loss 0.1978\n",
            "epoch 37/250, batch 69/75, loss 0.2625\n",
            "epoch 37/250, batch 70/75, loss 0.1769\n",
            "epoch 37/250, batch 71/75, loss 0.2383\n",
            "epoch 37/250, batch 72/75, loss 0.2108\n",
            "epoch 37/250, batch 73/75, loss 0.2429\n",
            "epoch 37/250, batch 74/75, loss 0.2607\n",
            "epoch 37/250, batch 75/75, loss 0.1678\n",
            "epoch 37/250, training roc_auc_score 0.9702\n",
            "EarlyStopping counter: 25 out of 50\n",
            "epoch 37/250, validation roc_auc_score 0.8063, best validation roc_auc_score 0.8516\n",
            "epoch 38/250, batch 1/75, loss 0.3152\n",
            "epoch 38/250, batch 2/75, loss 0.3072\n",
            "epoch 38/250, batch 3/75, loss 0.1687\n",
            "epoch 38/250, batch 4/75, loss 0.2937\n",
            "epoch 38/250, batch 5/75, loss 0.0943\n",
            "epoch 38/250, batch 6/75, loss 0.0963\n",
            "epoch 38/250, batch 7/75, loss 0.2412\n",
            "epoch 38/250, batch 8/75, loss 0.4502\n",
            "epoch 38/250, batch 9/75, loss 0.1354\n",
            "epoch 38/250, batch 10/75, loss 0.2065\n",
            "epoch 38/250, batch 11/75, loss 0.4172\n",
            "epoch 38/250, batch 12/75, loss 0.1691\n",
            "epoch 38/250, batch 13/75, loss 0.3781\n",
            "epoch 38/250, batch 14/75, loss 0.1390\n",
            "epoch 38/250, batch 15/75, loss 0.9103\n",
            "epoch 38/250, batch 16/75, loss 0.1838\n",
            "epoch 38/250, batch 17/75, loss 0.1967\n",
            "epoch 38/250, batch 18/75, loss 0.2526\n",
            "epoch 38/250, batch 19/75, loss 0.2151\n",
            "epoch 38/250, batch 20/75, loss 0.4026\n",
            "epoch 38/250, batch 21/75, loss 0.1113\n",
            "epoch 38/250, batch 22/75, loss 0.1478\n",
            "epoch 38/250, batch 23/75, loss 0.1578\n",
            "epoch 38/250, batch 24/75, loss 0.1049\n",
            "epoch 38/250, batch 25/75, loss 0.1195\n",
            "epoch 38/250, batch 26/75, loss 0.7467\n",
            "epoch 38/250, batch 27/75, loss 0.1805\n",
            "epoch 38/250, batch 28/75, loss 0.1879\n",
            "epoch 38/250, batch 29/75, loss 0.1509\n",
            "epoch 38/250, batch 30/75, loss 0.1338\n",
            "epoch 38/250, batch 31/75, loss 0.1283\n",
            "epoch 38/250, batch 32/75, loss 0.1378\n",
            "epoch 38/250, batch 33/75, loss 0.1403\n",
            "epoch 38/250, batch 34/75, loss 0.1617\n",
            "epoch 38/250, batch 35/75, loss 0.1567\n",
            "epoch 38/250, batch 36/75, loss 1.0361\n",
            "epoch 38/250, batch 37/75, loss 0.1772\n",
            "epoch 38/250, batch 38/75, loss 0.1265\n",
            "epoch 38/250, batch 39/75, loss 0.1798\n",
            "epoch 38/250, batch 40/75, loss 0.1686\n",
            "epoch 38/250, batch 41/75, loss 0.2435\n",
            "epoch 38/250, batch 42/75, loss 0.1592\n",
            "epoch 38/250, batch 43/75, loss 0.1406\n",
            "epoch 38/250, batch 44/75, loss 0.1650\n",
            "epoch 38/250, batch 45/75, loss 0.1641\n",
            "epoch 38/250, batch 46/75, loss 0.0853\n",
            "epoch 38/250, batch 47/75, loss 0.1412\n",
            "epoch 38/250, batch 48/75, loss 0.1105\n",
            "epoch 38/250, batch 49/75, loss 0.1458\n",
            "epoch 38/250, batch 50/75, loss 0.1120\n",
            "epoch 38/250, batch 51/75, loss 0.1260\n",
            "epoch 38/250, batch 52/75, loss 0.0863\n",
            "epoch 38/250, batch 53/75, loss 7.6626\n",
            "epoch 38/250, batch 54/75, loss 0.1872\n",
            "epoch 38/250, batch 55/75, loss 0.1537\n",
            "epoch 38/250, batch 56/75, loss 0.1337\n",
            "epoch 38/250, batch 57/75, loss 0.1569\n",
            "epoch 38/250, batch 58/75, loss 0.1948\n",
            "epoch 38/250, batch 59/75, loss 0.1664\n",
            "epoch 38/250, batch 60/75, loss 0.1679\n",
            "epoch 38/250, batch 61/75, loss 0.2328\n",
            "epoch 38/250, batch 62/75, loss 0.2434\n",
            "epoch 38/250, batch 63/75, loss 0.1687\n",
            "epoch 38/250, batch 64/75, loss 0.2377\n",
            "epoch 38/250, batch 65/75, loss 0.2108\n",
            "epoch 38/250, batch 66/75, loss 0.2623\n",
            "epoch 38/250, batch 67/75, loss 0.1507\n",
            "epoch 38/250, batch 68/75, loss 0.1477\n",
            "epoch 38/250, batch 69/75, loss 0.2603\n",
            "epoch 38/250, batch 70/75, loss 0.1730\n",
            "epoch 38/250, batch 71/75, loss 0.2138\n",
            "epoch 38/250, batch 72/75, loss 0.1881\n",
            "epoch 38/250, batch 73/75, loss 0.1695\n",
            "epoch 38/250, batch 74/75, loss 0.2065\n",
            "epoch 38/250, batch 75/75, loss 0.1401\n",
            "epoch 38/250, training roc_auc_score 0.9759\n",
            "EarlyStopping counter: 26 out of 50\n",
            "epoch 38/250, validation roc_auc_score 0.7754, best validation roc_auc_score 0.8516\n",
            "epoch 39/250, batch 1/75, loss 0.1893\n",
            "epoch 39/250, batch 2/75, loss 0.4151\n",
            "epoch 39/250, batch 3/75, loss 0.1352\n",
            "epoch 39/250, batch 4/75, loss 0.1712\n",
            "epoch 39/250, batch 5/75, loss 0.0843\n",
            "epoch 39/250, batch 6/75, loss 0.0789\n",
            "epoch 39/250, batch 7/75, loss 0.1762\n",
            "epoch 39/250, batch 8/75, loss 0.3147\n",
            "epoch 39/250, batch 9/75, loss 0.1017\n",
            "epoch 39/250, batch 10/75, loss 0.2343\n",
            "epoch 39/250, batch 11/75, loss 0.1609\n",
            "epoch 39/250, batch 12/75, loss 0.1569\n",
            "epoch 39/250, batch 13/75, loss 0.2878\n",
            "epoch 39/250, batch 14/75, loss 0.1034\n",
            "epoch 39/250, batch 15/75, loss 0.8461\n",
            "epoch 39/250, batch 16/75, loss 0.1771\n",
            "epoch 39/250, batch 17/75, loss 0.1910\n",
            "epoch 39/250, batch 18/75, loss 0.3009\n",
            "epoch 39/250, batch 19/75, loss 0.2314\n",
            "epoch 39/250, batch 20/75, loss 0.4602\n",
            "epoch 39/250, batch 21/75, loss 0.1171\n",
            "epoch 39/250, batch 22/75, loss 0.1662\n",
            "epoch 39/250, batch 23/75, loss 0.1362\n",
            "epoch 39/250, batch 24/75, loss 0.0878\n",
            "epoch 39/250, batch 25/75, loss 0.0999\n",
            "epoch 39/250, batch 26/75, loss 0.7170\n",
            "epoch 39/250, batch 27/75, loss 0.1944\n",
            "epoch 39/250, batch 28/75, loss 0.1734\n",
            "epoch 39/250, batch 29/75, loss 0.1403\n",
            "epoch 39/250, batch 30/75, loss 0.1556\n",
            "epoch 39/250, batch 31/75, loss 0.1044\n",
            "epoch 39/250, batch 32/75, loss 0.1490\n",
            "epoch 39/250, batch 33/75, loss 0.1382\n",
            "epoch 39/250, batch 34/75, loss 0.1522\n",
            "epoch 39/250, batch 35/75, loss 0.1886\n",
            "epoch 39/250, batch 36/75, loss 0.8137\n",
            "epoch 39/250, batch 37/75, loss 0.1600\n",
            "epoch 39/250, batch 38/75, loss 0.1353\n",
            "epoch 39/250, batch 39/75, loss 0.1969\n",
            "epoch 39/250, batch 40/75, loss 0.1956\n",
            "epoch 39/250, batch 41/75, loss 0.2784\n",
            "epoch 39/250, batch 42/75, loss 0.1791\n",
            "epoch 39/250, batch 43/75, loss 0.1359\n",
            "epoch 39/250, batch 44/75, loss 0.1820\n",
            "epoch 39/250, batch 45/75, loss 0.1730\n",
            "epoch 39/250, batch 46/75, loss 0.0831\n",
            "epoch 39/250, batch 47/75, loss 0.1625\n",
            "epoch 39/250, batch 48/75, loss 0.1087\n",
            "epoch 39/250, batch 49/75, loss 0.1479\n",
            "epoch 39/250, batch 50/75, loss 0.1059\n",
            "epoch 39/250, batch 51/75, loss 0.1171\n",
            "epoch 39/250, batch 52/75, loss 0.0895\n",
            "epoch 39/250, batch 53/75, loss 6.8931\n",
            "epoch 39/250, batch 54/75, loss 0.2161\n",
            "epoch 39/250, batch 55/75, loss 0.1647\n",
            "epoch 39/250, batch 56/75, loss 0.1255\n",
            "epoch 39/250, batch 57/75, loss 0.1235\n",
            "epoch 39/250, batch 58/75, loss 0.1686\n",
            "epoch 39/250, batch 59/75, loss 0.1535\n",
            "epoch 39/250, batch 60/75, loss 0.1515\n",
            "epoch 39/250, batch 61/75, loss 0.1657\n",
            "epoch 39/250, batch 62/75, loss 0.2414\n",
            "epoch 39/250, batch 63/75, loss 0.1451\n",
            "epoch 39/250, batch 64/75, loss 0.2099\n",
            "epoch 39/250, batch 65/75, loss 0.2049\n",
            "epoch 39/250, batch 66/75, loss 0.2011\n",
            "epoch 39/250, batch 67/75, loss 0.1703\n",
            "epoch 39/250, batch 68/75, loss 0.1908\n",
            "epoch 39/250, batch 69/75, loss 0.1931\n",
            "epoch 39/250, batch 70/75, loss 0.1518\n",
            "epoch 39/250, batch 71/75, loss 0.1917\n",
            "epoch 39/250, batch 72/75, loss 0.1708\n",
            "epoch 39/250, batch 73/75, loss 0.1699\n",
            "epoch 39/250, batch 74/75, loss 0.2206\n",
            "epoch 39/250, batch 75/75, loss 0.1344\n",
            "epoch 39/250, training roc_auc_score 0.9821\n",
            "EarlyStopping counter: 27 out of 50\n",
            "epoch 39/250, validation roc_auc_score 0.8122, best validation roc_auc_score 0.8516\n",
            "epoch 40/250, batch 1/75, loss 0.2592\n",
            "epoch 40/250, batch 2/75, loss 0.1711\n",
            "epoch 40/250, batch 3/75, loss 0.1303\n",
            "epoch 40/250, batch 4/75, loss 0.1704\n",
            "epoch 40/250, batch 5/75, loss 0.0891\n",
            "epoch 40/250, batch 6/75, loss 0.0946\n",
            "epoch 40/250, batch 7/75, loss 0.1915\n",
            "epoch 40/250, batch 8/75, loss 0.2143\n",
            "epoch 40/250, batch 9/75, loss 0.1099\n",
            "epoch 40/250, batch 10/75, loss 0.1617\n",
            "epoch 40/250, batch 11/75, loss 0.1904\n",
            "epoch 40/250, batch 12/75, loss 0.1754\n",
            "epoch 40/250, batch 13/75, loss 0.2792\n",
            "epoch 40/250, batch 14/75, loss 0.1167\n",
            "epoch 40/250, batch 15/75, loss 0.4851\n",
            "epoch 40/250, batch 16/75, loss 0.1453\n",
            "epoch 40/250, batch 17/75, loss 0.1502\n",
            "epoch 40/250, batch 18/75, loss 0.2226\n",
            "epoch 40/250, batch 19/75, loss 0.1823\n",
            "epoch 40/250, batch 20/75, loss 0.5441\n",
            "epoch 40/250, batch 21/75, loss 0.0940\n",
            "epoch 40/250, batch 22/75, loss 0.1265\n",
            "epoch 40/250, batch 23/75, loss 0.1268\n",
            "epoch 40/250, batch 24/75, loss 0.0917\n",
            "epoch 40/250, batch 25/75, loss 0.1012\n",
            "epoch 40/250, batch 26/75, loss 0.6684\n",
            "epoch 40/250, batch 27/75, loss 0.1807\n",
            "epoch 40/250, batch 28/75, loss 0.1516\n",
            "epoch 40/250, batch 29/75, loss 0.1106\n",
            "epoch 40/250, batch 30/75, loss 0.1175\n",
            "epoch 40/250, batch 31/75, loss 0.1005\n",
            "epoch 40/250, batch 32/75, loss 0.1190\n",
            "epoch 40/250, batch 33/75, loss 0.1313\n",
            "epoch 40/250, batch 34/75, loss 0.1594\n",
            "epoch 40/250, batch 35/75, loss 0.1596\n",
            "epoch 40/250, batch 36/75, loss 0.8335\n",
            "epoch 40/250, batch 37/75, loss 0.1579\n",
            "epoch 40/250, batch 38/75, loss 0.1270\n",
            "epoch 40/250, batch 39/75, loss 0.1669\n",
            "epoch 40/250, batch 40/75, loss 0.1525\n",
            "epoch 40/250, batch 41/75, loss 0.2097\n",
            "epoch 40/250, batch 42/75, loss 0.1424\n",
            "epoch 40/250, batch 43/75, loss 0.1222\n",
            "epoch 40/250, batch 44/75, loss 0.1602\n",
            "epoch 40/250, batch 45/75, loss 0.1649\n",
            "epoch 40/250, batch 46/75, loss 0.0907\n",
            "epoch 40/250, batch 47/75, loss 0.1400\n",
            "epoch 40/250, batch 48/75, loss 0.1010\n",
            "epoch 40/250, batch 49/75, loss 0.1234\n",
            "epoch 40/250, batch 50/75, loss 0.1051\n",
            "epoch 40/250, batch 51/75, loss 0.1160\n",
            "epoch 40/250, batch 52/75, loss 0.0925\n",
            "epoch 40/250, batch 53/75, loss 6.1324\n",
            "epoch 40/250, batch 54/75, loss 0.1721\n",
            "epoch 40/250, batch 55/75, loss 0.1456\n",
            "epoch 40/250, batch 56/75, loss 0.1170\n",
            "epoch 40/250, batch 57/75, loss 0.1289\n",
            "epoch 40/250, batch 58/75, loss 0.1735\n",
            "epoch 40/250, batch 59/75, loss 0.1437\n",
            "epoch 40/250, batch 60/75, loss 0.1455\n",
            "epoch 40/250, batch 61/75, loss 0.1963\n",
            "epoch 40/250, batch 62/75, loss 0.1934\n",
            "epoch 40/250, batch 63/75, loss 0.1251\n",
            "epoch 40/250, batch 64/75, loss 0.1861\n",
            "epoch 40/250, batch 65/75, loss 0.1878\n",
            "epoch 40/250, batch 66/75, loss 0.2141\n",
            "epoch 40/250, batch 67/75, loss 0.1334\n",
            "epoch 40/250, batch 68/75, loss 0.1361\n",
            "epoch 40/250, batch 69/75, loss 0.1968\n",
            "epoch 40/250, batch 70/75, loss 0.1398\n",
            "epoch 40/250, batch 71/75, loss 0.1920\n",
            "epoch 40/250, batch 72/75, loss 0.1538\n",
            "epoch 40/250, batch 73/75, loss 0.1500\n",
            "epoch 40/250, batch 74/75, loss 0.1915\n",
            "epoch 40/250, batch 75/75, loss 0.1337\n",
            "epoch 40/250, training roc_auc_score 0.9863\n",
            "EarlyStopping counter: 28 out of 50\n",
            "epoch 40/250, validation roc_auc_score 0.7871, best validation roc_auc_score 0.8516\n",
            "epoch 41/250, batch 1/75, loss 0.1801\n",
            "epoch 41/250, batch 2/75, loss 0.3176\n",
            "epoch 41/250, batch 3/75, loss 0.1315\n",
            "epoch 41/250, batch 4/75, loss 0.2869\n",
            "epoch 41/250, batch 5/75, loss 0.0751\n",
            "epoch 41/250, batch 6/75, loss 0.0765\n",
            "epoch 41/250, batch 7/75, loss 0.2120\n",
            "epoch 41/250, batch 8/75, loss 0.2544\n",
            "epoch 41/250, batch 9/75, loss 0.0942\n",
            "epoch 41/250, batch 10/75, loss 0.2260\n",
            "epoch 41/250, batch 11/75, loss 0.1555\n",
            "epoch 41/250, batch 12/75, loss 0.1642\n",
            "epoch 41/250, batch 13/75, loss 0.2683\n",
            "epoch 41/250, batch 14/75, loss 0.1063\n",
            "epoch 41/250, batch 15/75, loss 0.5340\n",
            "epoch 41/250, batch 16/75, loss 0.1502\n",
            "epoch 41/250, batch 17/75, loss 0.1625\n",
            "epoch 41/250, batch 18/75, loss 0.2865\n",
            "epoch 41/250, batch 19/75, loss 0.1824\n",
            "epoch 41/250, batch 20/75, loss 0.3829\n",
            "epoch 41/250, batch 21/75, loss 0.1116\n",
            "epoch 41/250, batch 22/75, loss 0.1316\n",
            "epoch 41/250, batch 23/75, loss 0.1231\n",
            "epoch 41/250, batch 24/75, loss 0.0760\n",
            "epoch 41/250, batch 25/75, loss 0.0920\n",
            "epoch 41/250, batch 26/75, loss 0.3931\n",
            "epoch 41/250, batch 27/75, loss 0.1820\n",
            "epoch 41/250, batch 28/75, loss 0.1567\n",
            "epoch 41/250, batch 29/75, loss 0.1102\n",
            "epoch 41/250, batch 30/75, loss 0.1241\n",
            "epoch 41/250, batch 31/75, loss 0.0925\n",
            "epoch 41/250, batch 32/75, loss 0.1177\n",
            "epoch 41/250, batch 33/75, loss 0.1236\n",
            "epoch 41/250, batch 34/75, loss 0.1326\n",
            "epoch 41/250, batch 35/75, loss 0.1623\n",
            "epoch 41/250, batch 36/75, loss 0.6315\n",
            "epoch 41/250, batch 37/75, loss 0.1496\n",
            "epoch 41/250, batch 38/75, loss 0.1164\n",
            "epoch 41/250, batch 39/75, loss 0.1622\n",
            "epoch 41/250, batch 40/75, loss 0.1621\n",
            "epoch 41/250, batch 41/75, loss 0.2333\n",
            "epoch 41/250, batch 42/75, loss 0.1486\n",
            "epoch 41/250, batch 43/75, loss 0.1087\n",
            "epoch 41/250, batch 44/75, loss 0.1528\n",
            "epoch 41/250, batch 45/75, loss 0.1574\n",
            "epoch 41/250, batch 46/75, loss 0.0739\n",
            "epoch 41/250, batch 47/75, loss 0.1368\n",
            "epoch 41/250, batch 48/75, loss 0.0958\n",
            "epoch 41/250, batch 49/75, loss 0.1266\n",
            "epoch 41/250, batch 50/75, loss 0.0930\n",
            "epoch 41/250, batch 51/75, loss 0.0932\n",
            "epoch 41/250, batch 52/75, loss 0.0770\n",
            "epoch 41/250, batch 53/75, loss 5.8469\n",
            "epoch 41/250, batch 54/75, loss 0.2046\n",
            "epoch 41/250, batch 55/75, loss 0.1362\n",
            "epoch 41/250, batch 56/75, loss 0.1159\n",
            "epoch 41/250, batch 57/75, loss 0.1168\n",
            "epoch 41/250, batch 58/75, loss 0.1664\n",
            "epoch 41/250, batch 59/75, loss 0.1395\n",
            "epoch 41/250, batch 60/75, loss 0.1566\n",
            "epoch 41/250, batch 61/75, loss 0.1701\n",
            "epoch 41/250, batch 62/75, loss 0.2612\n",
            "epoch 41/250, batch 63/75, loss 0.1387\n",
            "epoch 41/250, batch 64/75, loss 0.2130\n",
            "epoch 41/250, batch 65/75, loss 0.2008\n",
            "epoch 41/250, batch 66/75, loss 0.1964\n",
            "epoch 41/250, batch 67/75, loss 0.1724\n",
            "epoch 41/250, batch 68/75, loss 0.1822\n",
            "epoch 41/250, batch 69/75, loss 0.1878\n",
            "epoch 41/250, batch 70/75, loss 0.1366\n",
            "epoch 41/250, batch 71/75, loss 0.1857\n",
            "epoch 41/250, batch 72/75, loss 0.1638\n",
            "epoch 41/250, batch 73/75, loss 0.1780\n",
            "epoch 41/250, batch 74/75, loss 0.2347\n",
            "epoch 41/250, batch 75/75, loss 0.1256\n",
            "epoch 41/250, training roc_auc_score 0.9876\n",
            "EarlyStopping counter: 29 out of 50\n",
            "epoch 41/250, validation roc_auc_score 0.8304, best validation roc_auc_score 0.8516\n",
            "epoch 42/250, batch 1/75, loss 0.3282\n",
            "epoch 42/250, batch 2/75, loss 0.1402\n",
            "epoch 42/250, batch 3/75, loss 0.0964\n",
            "epoch 42/250, batch 4/75, loss 0.1675\n",
            "epoch 42/250, batch 5/75, loss 0.0771\n",
            "epoch 42/250, batch 6/75, loss 0.0874\n",
            "epoch 42/250, batch 7/75, loss 0.2060\n",
            "epoch 42/250, batch 8/75, loss 0.3477\n",
            "epoch 42/250, batch 9/75, loss 0.1138\n",
            "epoch 42/250, batch 10/75, loss 0.1250\n",
            "epoch 42/250, batch 11/75, loss 0.2170\n",
            "epoch 42/250, batch 12/75, loss 0.1840\n",
            "epoch 42/250, batch 13/75, loss 0.2252\n",
            "epoch 42/250, batch 14/75, loss 0.1078\n",
            "epoch 42/250, batch 15/75, loss 0.4570\n",
            "epoch 42/250, batch 16/75, loss 0.1273\n",
            "epoch 42/250, batch 17/75, loss 0.1298\n",
            "epoch 42/250, batch 18/75, loss 0.1890\n",
            "epoch 42/250, batch 19/75, loss 0.1679\n",
            "epoch 42/250, batch 20/75, loss 0.4672\n",
            "epoch 42/250, batch 21/75, loss 0.0742\n",
            "epoch 42/250, batch 22/75, loss 0.1082\n",
            "epoch 42/250, batch 23/75, loss 0.1047\n",
            "epoch 42/250, batch 24/75, loss 0.0818\n",
            "epoch 42/250, batch 25/75, loss 0.0831\n",
            "epoch 42/250, batch 26/75, loss 0.8454\n",
            "epoch 42/250, batch 27/75, loss 0.1668\n",
            "epoch 42/250, batch 28/75, loss 0.1464\n",
            "epoch 42/250, batch 29/75, loss 0.1006\n",
            "epoch 42/250, batch 30/75, loss 0.1027\n",
            "epoch 42/250, batch 31/75, loss 0.0882\n",
            "epoch 42/250, batch 32/75, loss 0.1176\n",
            "epoch 42/250, batch 33/75, loss 0.1251\n",
            "epoch 42/250, batch 34/75, loss 0.1306\n",
            "epoch 42/250, batch 35/75, loss 0.1291\n",
            "epoch 42/250, batch 36/75, loss 0.7793\n",
            "epoch 42/250, batch 37/75, loss 0.1527\n",
            "epoch 42/250, batch 38/75, loss 0.1090\n",
            "epoch 42/250, batch 39/75, loss 0.1557\n",
            "epoch 42/250, batch 40/75, loss 0.1412\n",
            "epoch 42/250, batch 41/75, loss 0.2140\n",
            "epoch 42/250, batch 42/75, loss 0.1291\n",
            "epoch 42/250, batch 43/75, loss 0.1076\n",
            "epoch 42/250, batch 44/75, loss 0.1517\n",
            "epoch 42/250, batch 45/75, loss 0.1659\n",
            "epoch 42/250, batch 46/75, loss 0.0890\n",
            "epoch 42/250, batch 47/75, loss 0.1458\n",
            "epoch 42/250, batch 48/75, loss 0.1030\n",
            "epoch 42/250, batch 49/75, loss 0.1331\n",
            "epoch 42/250, batch 50/75, loss 0.1067\n",
            "epoch 42/250, batch 51/75, loss 0.1197\n",
            "epoch 42/250, batch 52/75, loss 0.0896\n",
            "epoch 42/250, batch 53/75, loss 6.6486\n",
            "epoch 42/250, batch 54/75, loss 0.1511\n",
            "epoch 42/250, batch 55/75, loss 0.1383\n",
            "epoch 42/250, batch 56/75, loss 0.1124\n",
            "epoch 42/250, batch 57/75, loss 0.1211\n",
            "epoch 42/250, batch 58/75, loss 0.1550\n",
            "epoch 42/250, batch 59/75, loss 0.1450\n",
            "epoch 42/250, batch 60/75, loss 0.1527\n",
            "epoch 42/250, batch 61/75, loss 0.2148\n",
            "epoch 42/250, batch 62/75, loss 0.2154\n",
            "epoch 42/250, batch 63/75, loss 0.1321\n",
            "epoch 42/250, batch 64/75, loss 0.2047\n",
            "epoch 42/250, batch 65/75, loss 0.2166\n",
            "epoch 42/250, batch 66/75, loss 0.2050\n",
            "epoch 42/250, batch 67/75, loss 0.1308\n",
            "epoch 42/250, batch 68/75, loss 0.1650\n",
            "epoch 42/250, batch 69/75, loss 0.2061\n",
            "epoch 42/250, batch 70/75, loss 0.1406\n",
            "epoch 42/250, batch 71/75, loss 0.2156\n",
            "epoch 42/250, batch 72/75, loss 0.1885\n",
            "epoch 42/250, batch 73/75, loss 0.1578\n",
            "epoch 42/250, batch 74/75, loss 0.2113\n",
            "epoch 42/250, batch 75/75, loss 0.1436\n",
            "epoch 42/250, training roc_auc_score 0.9841\n",
            "EarlyStopping counter: 30 out of 50\n",
            "epoch 42/250, validation roc_auc_score 0.7602, best validation roc_auc_score 0.8516\n",
            "epoch 43/250, batch 1/75, loss 0.2415\n",
            "epoch 43/250, batch 2/75, loss 0.2870\n",
            "epoch 43/250, batch 3/75, loss 0.1419\n",
            "epoch 43/250, batch 4/75, loss 0.3293\n",
            "epoch 43/250, batch 5/75, loss 0.0773\n",
            "epoch 43/250, batch 6/75, loss 0.0762\n",
            "epoch 43/250, batch 7/75, loss 0.2644\n",
            "epoch 43/250, batch 8/75, loss 0.3119\n",
            "epoch 43/250, batch 9/75, loss 0.0974\n",
            "epoch 43/250, batch 10/75, loss 0.2094\n",
            "epoch 43/250, batch 11/75, loss 0.1605\n",
            "epoch 43/250, batch 12/75, loss 0.1410\n",
            "epoch 43/250, batch 13/75, loss 0.2627\n",
            "epoch 43/250, batch 14/75, loss 0.1073\n",
            "epoch 43/250, batch 15/75, loss 0.5850\n",
            "epoch 43/250, batch 16/75, loss 0.1457\n",
            "epoch 43/250, batch 17/75, loss 0.1422\n",
            "epoch 43/250, batch 18/75, loss 0.2902\n",
            "epoch 43/250, batch 19/75, loss 0.1914\n",
            "epoch 43/250, batch 20/75, loss 0.3050\n",
            "epoch 43/250, batch 21/75, loss 0.1036\n",
            "epoch 43/250, batch 22/75, loss 0.1342\n",
            "epoch 43/250, batch 23/75, loss 0.1137\n",
            "epoch 43/250, batch 24/75, loss 0.0705\n",
            "epoch 43/250, batch 25/75, loss 0.0880\n",
            "epoch 43/250, batch 26/75, loss 0.4888\n",
            "epoch 43/250, batch 27/75, loss 0.1523\n",
            "epoch 43/250, batch 28/75, loss 0.1580\n",
            "epoch 43/250, batch 29/75, loss 0.1121\n",
            "epoch 43/250, batch 30/75, loss 0.1421\n",
            "epoch 43/250, batch 31/75, loss 0.0928\n",
            "epoch 43/250, batch 32/75, loss 0.1188\n",
            "epoch 43/250, batch 33/75, loss 0.1335\n",
            "epoch 43/250, batch 34/75, loss 0.1196\n",
            "epoch 43/250, batch 35/75, loss 0.1542\n",
            "epoch 43/250, batch 36/75, loss 0.7254\n",
            "epoch 43/250, batch 37/75, loss 0.1509\n",
            "epoch 43/250, batch 38/75, loss 0.1085\n",
            "epoch 43/250, batch 39/75, loss 0.1397\n",
            "epoch 43/250, batch 40/75, loss 0.1332\n",
            "epoch 43/250, batch 41/75, loss 0.2082\n",
            "epoch 43/250, batch 42/75, loss 0.1463\n",
            "epoch 43/250, batch 43/75, loss 0.0961\n",
            "epoch 43/250, batch 44/75, loss 0.1618\n",
            "epoch 43/250, batch 45/75, loss 0.1453\n",
            "epoch 43/250, batch 46/75, loss 0.0632\n",
            "epoch 43/250, batch 47/75, loss 0.1350\n",
            "epoch 43/250, batch 48/75, loss 0.0912\n",
            "epoch 43/250, batch 49/75, loss 0.1355\n",
            "epoch 43/250, batch 50/75, loss 0.0917\n",
            "epoch 43/250, batch 51/75, loss 0.0879\n",
            "epoch 43/250, batch 52/75, loss 0.0745\n",
            "epoch 43/250, batch 53/75, loss 6.6639\n",
            "epoch 43/250, batch 54/75, loss 0.1816\n",
            "epoch 43/250, batch 55/75, loss 0.1256\n",
            "epoch 43/250, batch 56/75, loss 0.1089\n",
            "epoch 43/250, batch 57/75, loss 0.1115\n",
            "epoch 43/250, batch 58/75, loss 0.1583\n",
            "epoch 43/250, batch 59/75, loss 0.1278\n",
            "epoch 43/250, batch 60/75, loss 0.1456\n",
            "epoch 43/250, batch 61/75, loss 0.1481\n",
            "epoch 43/250, batch 62/75, loss 0.2554\n",
            "epoch 43/250, batch 63/75, loss 0.1329\n",
            "epoch 43/250, batch 64/75, loss 0.2184\n",
            "epoch 43/250, batch 65/75, loss 0.1884\n",
            "epoch 43/250, batch 66/75, loss 0.1937\n",
            "epoch 43/250, batch 67/75, loss 0.1693\n",
            "epoch 43/250, batch 68/75, loss 0.1719\n",
            "epoch 43/250, batch 69/75, loss 0.2079\n",
            "epoch 43/250, batch 70/75, loss 0.1258\n",
            "epoch 43/250, batch 71/75, loss 0.2001\n",
            "epoch 43/250, batch 72/75, loss 0.1611\n",
            "epoch 43/250, batch 73/75, loss 0.1821\n",
            "epoch 43/250, batch 74/75, loss 0.2357\n",
            "epoch 43/250, batch 75/75, loss 0.1330\n",
            "epoch 43/250, training roc_auc_score 0.9841\n",
            "EarlyStopping counter: 31 out of 50\n",
            "epoch 43/250, validation roc_auc_score 0.8095, best validation roc_auc_score 0.8516\n",
            "epoch 44/250, batch 1/75, loss 0.2701\n",
            "epoch 44/250, batch 2/75, loss 0.1587\n",
            "epoch 44/250, batch 3/75, loss 0.1199\n",
            "epoch 44/250, batch 4/75, loss 0.2438\n",
            "epoch 44/250, batch 5/75, loss 0.0790\n",
            "epoch 44/250, batch 6/75, loss 0.0832\n",
            "epoch 44/250, batch 7/75, loss 0.2105\n",
            "epoch 44/250, batch 8/75, loss 0.4484\n",
            "epoch 44/250, batch 9/75, loss 0.1376\n",
            "epoch 44/250, batch 10/75, loss 0.1380\n",
            "epoch 44/250, batch 11/75, loss 0.2014\n",
            "epoch 44/250, batch 12/75, loss 0.2036\n",
            "epoch 44/250, batch 13/75, loss 0.2545\n",
            "epoch 44/250, batch 14/75, loss 0.0993\n",
            "epoch 44/250, batch 15/75, loss 0.6159\n",
            "epoch 44/250, batch 16/75, loss 0.1362\n",
            "epoch 44/250, batch 17/75, loss 0.1447\n",
            "epoch 44/250, batch 18/75, loss 0.1693\n",
            "epoch 44/250, batch 19/75, loss 0.1668\n",
            "epoch 44/250, batch 20/75, loss 0.4769\n",
            "epoch 44/250, batch 21/75, loss 0.0799\n",
            "epoch 44/250, batch 22/75, loss 0.1017\n",
            "epoch 44/250, batch 23/75, loss 0.1343\n",
            "epoch 44/250, batch 24/75, loss 0.0629\n",
            "epoch 44/250, batch 25/75, loss 0.0718\n",
            "epoch 44/250, batch 26/75, loss 0.7959\n",
            "epoch 44/250, batch 27/75, loss 0.1499\n",
            "epoch 44/250, batch 28/75, loss 0.1501\n",
            "epoch 44/250, batch 29/75, loss 0.1049\n",
            "epoch 44/250, batch 30/75, loss 0.1027\n",
            "epoch 44/250, batch 31/75, loss 0.0953\n",
            "epoch 44/250, batch 32/75, loss 0.1074\n",
            "epoch 44/250, batch 33/75, loss 0.1252\n",
            "epoch 44/250, batch 34/75, loss 0.1365\n",
            "epoch 44/250, batch 35/75, loss 0.1253\n",
            "epoch 44/250, batch 36/75, loss 0.9249\n",
            "epoch 44/250, batch 37/75, loss 0.1508\n",
            "epoch 44/250, batch 38/75, loss 0.1059\n",
            "epoch 44/250, batch 39/75, loss 0.1333\n",
            "epoch 44/250, batch 40/75, loss 0.1406\n",
            "epoch 44/250, batch 41/75, loss 0.2113\n",
            "epoch 44/250, batch 42/75, loss 0.1365\n",
            "epoch 44/250, batch 43/75, loss 0.0958\n",
            "epoch 44/250, batch 44/75, loss 0.1242\n",
            "epoch 44/250, batch 45/75, loss 0.1422\n",
            "epoch 44/250, batch 46/75, loss 0.0773\n",
            "epoch 44/250, batch 47/75, loss 0.1362\n",
            "epoch 44/250, batch 48/75, loss 0.0870\n",
            "epoch 44/250, batch 49/75, loss 0.1294\n",
            "epoch 44/250, batch 50/75, loss 0.0905\n",
            "epoch 44/250, batch 51/75, loss 0.0964\n",
            "epoch 44/250, batch 52/75, loss 0.0726\n",
            "epoch 44/250, batch 53/75, loss 7.1301\n",
            "epoch 44/250, batch 54/75, loss 0.1495\n",
            "epoch 44/250, batch 55/75, loss 0.1269\n",
            "epoch 44/250, batch 56/75, loss 0.0998\n",
            "epoch 44/250, batch 57/75, loss 0.1004\n",
            "epoch 44/250, batch 58/75, loss 0.1394\n",
            "epoch 44/250, batch 59/75, loss 0.1345\n",
            "epoch 44/250, batch 60/75, loss 0.1451\n",
            "epoch 44/250, batch 61/75, loss 0.1679\n",
            "epoch 44/250, batch 62/75, loss 0.1990\n",
            "epoch 44/250, batch 63/75, loss 0.1272\n",
            "epoch 44/250, batch 64/75, loss 0.1892\n",
            "epoch 44/250, batch 65/75, loss 0.2242\n",
            "epoch 44/250, batch 66/75, loss 0.2113\n",
            "epoch 44/250, batch 67/75, loss 0.1276\n",
            "epoch 44/250, batch 68/75, loss 0.1461\n",
            "epoch 44/250, batch 69/75, loss 0.2172\n",
            "epoch 44/250, batch 70/75, loss 0.1467\n",
            "epoch 44/250, batch 71/75, loss 0.2177\n",
            "epoch 44/250, batch 72/75, loss 0.1727\n",
            "epoch 44/250, batch 73/75, loss 0.1935\n",
            "epoch 44/250, batch 74/75, loss 0.2905\n",
            "epoch 44/250, batch 75/75, loss 0.1531\n",
            "epoch 44/250, training roc_auc_score 0.9815\n",
            "EarlyStopping counter: 32 out of 50\n",
            "epoch 44/250, validation roc_auc_score 0.7886, best validation roc_auc_score 0.8516\n",
            "epoch 45/250, batch 1/75, loss 0.2723\n",
            "epoch 45/250, batch 2/75, loss 0.3820\n",
            "epoch 45/250, batch 3/75, loss 0.1319\n",
            "epoch 45/250, batch 4/75, loss 0.2505\n",
            "epoch 45/250, batch 5/75, loss 0.0765\n",
            "epoch 45/250, batch 6/75, loss 0.0749\n",
            "epoch 45/250, batch 7/75, loss 0.2016\n",
            "epoch 45/250, batch 8/75, loss 0.6218\n",
            "epoch 45/250, batch 9/75, loss 0.1031\n",
            "epoch 45/250, batch 10/75, loss 0.1337\n",
            "epoch 45/250, batch 11/75, loss 0.1307\n",
            "epoch 45/250, batch 12/75, loss 0.1368\n",
            "epoch 45/250, batch 13/75, loss 0.2717\n",
            "epoch 45/250, batch 14/75, loss 0.1115\n",
            "epoch 45/250, batch 15/75, loss 0.8882\n",
            "epoch 45/250, batch 16/75, loss 0.1400\n",
            "epoch 45/250, batch 17/75, loss 0.1821\n",
            "epoch 45/250, batch 18/75, loss 0.3330\n",
            "epoch 45/250, batch 19/75, loss 0.1995\n",
            "epoch 45/250, batch 20/75, loss 0.4238\n",
            "epoch 45/250, batch 21/75, loss 0.1113\n",
            "epoch 45/250, batch 22/75, loss 0.1526\n",
            "epoch 45/250, batch 23/75, loss 0.1351\n",
            "epoch 45/250, batch 24/75, loss 0.0801\n",
            "epoch 45/250, batch 25/75, loss 0.1076\n",
            "epoch 45/250, batch 26/75, loss 0.6881\n",
            "epoch 45/250, batch 27/75, loss 0.1792\n",
            "epoch 45/250, batch 28/75, loss 0.1733\n",
            "epoch 45/250, batch 29/75, loss 0.1665\n",
            "epoch 45/250, batch 30/75, loss 0.1419\n",
            "epoch 45/250, batch 31/75, loss 0.1114\n",
            "epoch 45/250, batch 32/75, loss 0.1212\n",
            "epoch 45/250, batch 33/75, loss 0.1434\n",
            "epoch 45/250, batch 34/75, loss 0.1616\n",
            "epoch 45/250, batch 35/75, loss 0.1921\n",
            "epoch 45/250, batch 36/75, loss 1.0165\n",
            "epoch 45/250, batch 37/75, loss 0.1975\n",
            "epoch 45/250, batch 38/75, loss 0.1191\n",
            "epoch 45/250, batch 39/75, loss 0.1670\n",
            "epoch 45/250, batch 40/75, loss 0.1316\n",
            "epoch 45/250, batch 41/75, loss 0.2680\n",
            "epoch 45/250, batch 42/75, loss 0.1516\n",
            "epoch 45/250, batch 43/75, loss 0.1119\n",
            "epoch 45/250, batch 44/75, loss 0.1705\n",
            "epoch 45/250, batch 45/75, loss 0.1599\n",
            "epoch 45/250, batch 46/75, loss 0.0750\n",
            "epoch 45/250, batch 47/75, loss 0.1483\n",
            "epoch 45/250, batch 48/75, loss 0.0919\n",
            "epoch 45/250, batch 49/75, loss 0.1212\n",
            "epoch 45/250, batch 50/75, loss 0.1019\n",
            "epoch 45/250, batch 51/75, loss 0.1057\n",
            "epoch 45/250, batch 52/75, loss 0.0712\n",
            "epoch 45/250, batch 53/75, loss 8.0421\n",
            "epoch 45/250, batch 54/75, loss 0.1834\n",
            "epoch 45/250, batch 55/75, loss 0.1393\n",
            "epoch 45/250, batch 56/75, loss 0.1077\n",
            "epoch 45/250, batch 57/75, loss 0.1128\n",
            "epoch 45/250, batch 58/75, loss 0.1484\n",
            "epoch 45/250, batch 59/75, loss 0.1351\n",
            "epoch 45/250, batch 60/75, loss 0.1359\n",
            "epoch 45/250, batch 61/75, loss 0.1598\n",
            "epoch 45/250, batch 62/75, loss 0.2465\n",
            "epoch 45/250, batch 63/75, loss 0.1101\n",
            "epoch 45/250, batch 64/75, loss 0.2058\n",
            "epoch 45/250, batch 65/75, loss 0.1740\n",
            "epoch 45/250, batch 66/75, loss 0.2033\n",
            "epoch 45/250, batch 67/75, loss 0.1266\n",
            "epoch 45/250, batch 68/75, loss 0.1211\n",
            "epoch 45/250, batch 69/75, loss 0.1964\n",
            "epoch 45/250, batch 70/75, loss 0.1127\n",
            "epoch 45/250, batch 71/75, loss 0.2016\n",
            "epoch 45/250, batch 72/75, loss 0.1351\n",
            "epoch 45/250, batch 73/75, loss 0.1598\n",
            "epoch 45/250, batch 74/75, loss 0.2101\n",
            "epoch 45/250, batch 75/75, loss 0.1271\n",
            "epoch 45/250, training roc_auc_score 0.9753\n",
            "EarlyStopping counter: 33 out of 50\n",
            "epoch 45/250, validation roc_auc_score 0.8146, best validation roc_auc_score 0.8516\n",
            "epoch 46/250, batch 1/75, loss 0.2547\n",
            "epoch 46/250, batch 2/75, loss 0.1567\n",
            "epoch 46/250, batch 3/75, loss 0.1093\n",
            "epoch 46/250, batch 4/75, loss 0.2137\n",
            "epoch 46/250, batch 5/75, loss 0.0626\n",
            "epoch 46/250, batch 6/75, loss 0.0778\n",
            "epoch 46/250, batch 7/75, loss 0.1569\n",
            "epoch 46/250, batch 8/75, loss 0.3324\n",
            "epoch 46/250, batch 9/75, loss 0.0958\n",
            "epoch 46/250, batch 10/75, loss 0.1184\n",
            "epoch 46/250, batch 11/75, loss 0.1786\n",
            "epoch 46/250, batch 12/75, loss 0.1782\n",
            "epoch 46/250, batch 13/75, loss 0.2908\n",
            "epoch 46/250, batch 14/75, loss 0.1068\n",
            "epoch 46/250, batch 15/75, loss 1.0480\n",
            "epoch 46/250, batch 16/75, loss 0.1582\n",
            "epoch 46/250, batch 17/75, loss 0.1841\n",
            "epoch 46/250, batch 18/75, loss 0.2702\n",
            "epoch 46/250, batch 19/75, loss 0.1764\n",
            "epoch 46/250, batch 20/75, loss 0.3022\n",
            "epoch 46/250, batch 21/75, loss 0.0768\n",
            "epoch 46/250, batch 22/75, loss 0.1152\n",
            "epoch 46/250, batch 23/75, loss 0.1121\n",
            "epoch 46/250, batch 24/75, loss 0.0626\n",
            "epoch 46/250, batch 25/75, loss 0.0731\n",
            "epoch 46/250, batch 26/75, loss 0.5189\n",
            "epoch 46/250, batch 27/75, loss 0.1540\n",
            "epoch 46/250, batch 28/75, loss 0.1421\n",
            "epoch 46/250, batch 29/75, loss 0.1039\n",
            "epoch 46/250, batch 30/75, loss 0.1138\n",
            "epoch 46/250, batch 31/75, loss 0.0915\n",
            "epoch 46/250, batch 32/75, loss 0.1117\n",
            "epoch 46/250, batch 33/75, loss 0.1063\n",
            "epoch 46/250, batch 34/75, loss 0.1329\n",
            "epoch 46/250, batch 35/75, loss 0.1172\n",
            "epoch 46/250, batch 36/75, loss 0.7443\n",
            "epoch 46/250, batch 37/75, loss 0.1660\n",
            "epoch 46/250, batch 38/75, loss 0.1039\n",
            "epoch 46/250, batch 39/75, loss 0.1424\n",
            "epoch 46/250, batch 40/75, loss 0.1270\n",
            "epoch 46/250, batch 41/75, loss 0.1996\n",
            "epoch 46/250, batch 42/75, loss 0.1332\n",
            "epoch 46/250, batch 43/75, loss 0.0973\n",
            "epoch 46/250, batch 44/75, loss 0.1299\n",
            "epoch 46/250, batch 45/75, loss 0.1450\n",
            "epoch 46/250, batch 46/75, loss 0.0772\n",
            "epoch 46/250, batch 47/75, loss 0.1284\n",
            "epoch 46/250, batch 48/75, loss 0.0955\n",
            "epoch 46/250, batch 49/75, loss 0.1163\n",
            "epoch 46/250, batch 50/75, loss 0.0968\n",
            "epoch 46/250, batch 51/75, loss 0.1049\n",
            "epoch 46/250, batch 52/75, loss 0.0751\n",
            "epoch 46/250, batch 53/75, loss 6.1515\n",
            "epoch 46/250, batch 54/75, loss 0.2066\n",
            "epoch 46/250, batch 55/75, loss 0.1303\n",
            "epoch 46/250, batch 56/75, loss 0.0991\n",
            "epoch 46/250, batch 57/75, loss 0.1040\n",
            "epoch 46/250, batch 58/75, loss 0.1419\n",
            "epoch 46/250, batch 59/75, loss 0.1091\n",
            "epoch 46/250, batch 60/75, loss 0.1236\n",
            "epoch 46/250, batch 61/75, loss 0.1599\n",
            "epoch 46/250, batch 62/75, loss 0.1749\n",
            "epoch 46/250, batch 63/75, loss 0.1035\n",
            "epoch 46/250, batch 64/75, loss 0.1827\n",
            "epoch 46/250, batch 65/75, loss 0.2080\n",
            "epoch 46/250, batch 66/75, loss 0.1665\n",
            "epoch 46/250, batch 67/75, loss 0.1192\n",
            "epoch 46/250, batch 68/75, loss 0.1256\n",
            "epoch 46/250, batch 69/75, loss 0.1800\n",
            "epoch 46/250, batch 70/75, loss 0.1273\n",
            "epoch 46/250, batch 71/75, loss 0.1883\n",
            "epoch 46/250, batch 72/75, loss 0.1403\n",
            "epoch 46/250, batch 73/75, loss 0.1577\n",
            "epoch 46/250, batch 74/75, loss 0.2137\n",
            "epoch 46/250, batch 75/75, loss 0.1348\n",
            "epoch 46/250, training roc_auc_score 0.9860\n",
            "EarlyStopping counter: 34 out of 50\n",
            "epoch 46/250, validation roc_auc_score 0.7414, best validation roc_auc_score 0.8516\n",
            "epoch 47/250, batch 1/75, loss 0.2240\n",
            "epoch 47/250, batch 2/75, loss 0.2758\n",
            "epoch 47/250, batch 3/75, loss 0.1269\n",
            "epoch 47/250, batch 4/75, loss 0.1866\n",
            "epoch 47/250, batch 5/75, loss 0.0729\n",
            "epoch 47/250, batch 6/75, loss 0.0949\n",
            "epoch 47/250, batch 7/75, loss 0.1633\n",
            "epoch 47/250, batch 8/75, loss 0.2455\n",
            "epoch 47/250, batch 9/75, loss 0.1014\n",
            "epoch 47/250, batch 10/75, loss 0.1343\n",
            "epoch 47/250, batch 11/75, loss 0.2344\n",
            "epoch 47/250, batch 12/75, loss 0.1380\n",
            "epoch 47/250, batch 13/75, loss 0.2982\n",
            "epoch 47/250, batch 14/75, loss 0.1021\n",
            "epoch 47/250, batch 15/75, loss 0.5712\n",
            "epoch 47/250, batch 16/75, loss 0.1226\n",
            "epoch 47/250, batch 17/75, loss 0.1439\n",
            "epoch 47/250, batch 18/75, loss 0.2720\n",
            "epoch 47/250, batch 19/75, loss 0.2164\n",
            "epoch 47/250, batch 20/75, loss 0.3524\n",
            "epoch 47/250, batch 21/75, loss 0.1054\n",
            "epoch 47/250, batch 22/75, loss 0.1160\n",
            "epoch 47/250, batch 23/75, loss 0.1035\n",
            "epoch 47/250, batch 24/75, loss 0.0668\n",
            "epoch 47/250, batch 25/75, loss 0.0830\n",
            "epoch 47/250, batch 26/75, loss 0.6461\n",
            "epoch 47/250, batch 27/75, loss 0.1836\n",
            "epoch 47/250, batch 28/75, loss 0.1617\n",
            "epoch 47/250, batch 29/75, loss 0.0980\n",
            "epoch 47/250, batch 30/75, loss 0.1289\n",
            "epoch 47/250, batch 31/75, loss 0.0887\n",
            "epoch 47/250, batch 32/75, loss 0.1175\n",
            "epoch 47/250, batch 33/75, loss 0.1026\n",
            "epoch 47/250, batch 34/75, loss 0.1311\n",
            "epoch 47/250, batch 35/75, loss 0.1301\n",
            "epoch 47/250, batch 36/75, loss 0.6861\n",
            "epoch 47/250, batch 37/75, loss 0.1383\n",
            "epoch 47/250, batch 38/75, loss 0.0954\n",
            "epoch 47/250, batch 39/75, loss 0.1280\n",
            "epoch 47/250, batch 40/75, loss 0.1164\n",
            "epoch 47/250, batch 41/75, loss 0.1687\n",
            "epoch 47/250, batch 42/75, loss 0.1156\n",
            "epoch 47/250, batch 43/75, loss 0.0793\n",
            "epoch 47/250, batch 44/75, loss 0.1419\n",
            "epoch 47/250, batch 45/75, loss 0.1459\n",
            "epoch 47/250, batch 46/75, loss 0.0613\n",
            "epoch 47/250, batch 47/75, loss 0.1289\n",
            "epoch 47/250, batch 48/75, loss 0.0913\n",
            "epoch 47/250, batch 49/75, loss 0.1168\n",
            "epoch 47/250, batch 50/75, loss 0.0863\n",
            "epoch 47/250, batch 51/75, loss 0.0880\n",
            "epoch 47/250, batch 52/75, loss 0.0688\n",
            "epoch 47/250, batch 53/75, loss 5.5246\n",
            "epoch 47/250, batch 54/75, loss 0.1843\n",
            "epoch 47/250, batch 55/75, loss 0.1193\n",
            "epoch 47/250, batch 56/75, loss 0.1015\n",
            "epoch 47/250, batch 57/75, loss 0.0924\n",
            "epoch 47/250, batch 58/75, loss 0.1362\n",
            "epoch 47/250, batch 59/75, loss 0.1168\n",
            "epoch 47/250, batch 60/75, loss 0.1409\n",
            "epoch 47/250, batch 61/75, loss 0.1719\n",
            "epoch 47/250, batch 62/75, loss 0.2407\n",
            "epoch 47/250, batch 63/75, loss 0.0962\n",
            "epoch 47/250, batch 64/75, loss 0.1792\n",
            "epoch 47/250, batch 65/75, loss 0.2060\n",
            "epoch 47/250, batch 66/75, loss 0.1784\n",
            "epoch 47/250, batch 67/75, loss 0.1288\n",
            "epoch 47/250, batch 68/75, loss 0.1319\n",
            "epoch 47/250, batch 69/75, loss 0.1815\n",
            "epoch 47/250, batch 70/75, loss 0.1076\n",
            "epoch 47/250, batch 71/75, loss 0.1649\n",
            "epoch 47/250, batch 72/75, loss 0.1677\n",
            "epoch 47/250, batch 73/75, loss 0.1533\n",
            "epoch 47/250, batch 74/75, loss 0.2139\n",
            "epoch 47/250, batch 75/75, loss 0.1002\n",
            "epoch 47/250, training roc_auc_score 0.9871\n",
            "EarlyStopping counter: 35 out of 50\n",
            "epoch 47/250, validation roc_auc_score 0.8198, best validation roc_auc_score 0.8516\n",
            "epoch 48/250, batch 1/75, loss 0.2221\n",
            "epoch 48/250, batch 2/75, loss 0.2010\n",
            "epoch 48/250, batch 3/75, loss 0.1029\n",
            "epoch 48/250, batch 4/75, loss 0.1627\n",
            "epoch 48/250, batch 5/75, loss 0.0501\n",
            "epoch 48/250, batch 6/75, loss 0.0549\n",
            "epoch 48/250, batch 7/75, loss 0.1497\n",
            "epoch 48/250, batch 8/75, loss 0.2671\n",
            "epoch 48/250, batch 9/75, loss 0.0679\n",
            "epoch 48/250, batch 10/75, loss 0.1526\n",
            "epoch 48/250, batch 11/75, loss 0.2285\n",
            "epoch 48/250, batch 12/75, loss 0.1314\n",
            "epoch 48/250, batch 13/75, loss 0.2567\n",
            "epoch 48/250, batch 14/75, loss 0.0984\n",
            "epoch 48/250, batch 15/75, loss 0.5664\n",
            "epoch 48/250, batch 16/75, loss 0.1323\n",
            "epoch 48/250, batch 17/75, loss 0.1433\n",
            "epoch 48/250, batch 18/75, loss 0.2082\n",
            "epoch 48/250, batch 19/75, loss 0.1596\n",
            "epoch 48/250, batch 20/75, loss 0.3468\n",
            "epoch 48/250, batch 21/75, loss 0.0622\n",
            "epoch 48/250, batch 22/75, loss 0.0998\n",
            "epoch 48/250, batch 23/75, loss 0.0920\n",
            "epoch 48/250, batch 24/75, loss 0.0555\n",
            "epoch 48/250, batch 25/75, loss 0.0688\n",
            "epoch 48/250, batch 26/75, loss 0.6643\n",
            "epoch 48/250, batch 27/75, loss 0.1421\n",
            "epoch 48/250, batch 28/75, loss 0.1228\n",
            "epoch 48/250, batch 29/75, loss 0.0790\n",
            "epoch 48/250, batch 30/75, loss 0.0929\n",
            "epoch 48/250, batch 31/75, loss 0.0773\n",
            "epoch 48/250, batch 32/75, loss 0.0989\n",
            "epoch 48/250, batch 33/75, loss 0.0928\n",
            "epoch 48/250, batch 34/75, loss 0.1169\n",
            "epoch 48/250, batch 35/75, loss 0.1076\n",
            "epoch 48/250, batch 36/75, loss 0.6843\n",
            "epoch 48/250, batch 37/75, loss 0.1314\n",
            "epoch 48/250, batch 38/75, loss 0.1039\n",
            "epoch 48/250, batch 39/75, loss 0.1154\n",
            "epoch 48/250, batch 40/75, loss 0.1100\n",
            "epoch 48/250, batch 41/75, loss 0.1731\n",
            "epoch 48/250, batch 42/75, loss 0.1286\n",
            "epoch 48/250, batch 43/75, loss 0.0840\n",
            "epoch 48/250, batch 44/75, loss 0.1213\n",
            "epoch 48/250, batch 45/75, loss 0.1297\n",
            "epoch 48/250, batch 46/75, loss 0.0596\n",
            "epoch 48/250, batch 47/75, loss 0.1238\n",
            "epoch 48/250, batch 48/75, loss 0.0874\n",
            "epoch 48/250, batch 49/75, loss 0.1093\n",
            "epoch 48/250, batch 50/75, loss 0.0907\n",
            "epoch 48/250, batch 51/75, loss 0.0962\n",
            "epoch 48/250, batch 52/75, loss 0.0685\n",
            "epoch 48/250, batch 53/75, loss 4.3654\n",
            "epoch 48/250, batch 54/75, loss 0.1912\n",
            "epoch 48/250, batch 55/75, loss 0.1161\n",
            "epoch 48/250, batch 56/75, loss 0.0894\n",
            "epoch 48/250, batch 57/75, loss 0.0922\n",
            "epoch 48/250, batch 58/75, loss 0.1445\n",
            "epoch 48/250, batch 59/75, loss 0.0943\n",
            "epoch 48/250, batch 60/75, loss 0.1140\n",
            "epoch 48/250, batch 61/75, loss 0.1519\n",
            "epoch 48/250, batch 62/75, loss 0.1452\n",
            "epoch 48/250, batch 63/75, loss 0.0956\n",
            "epoch 48/250, batch 64/75, loss 0.1698\n",
            "epoch 48/250, batch 65/75, loss 0.1440\n",
            "epoch 48/250, batch 66/75, loss 0.1576\n",
            "epoch 48/250, batch 67/75, loss 0.1071\n",
            "epoch 48/250, batch 68/75, loss 0.1375\n",
            "epoch 48/250, batch 69/75, loss 0.1675\n",
            "epoch 48/250, batch 70/75, loss 0.0924\n",
            "epoch 48/250, batch 71/75, loss 0.1615\n",
            "epoch 48/250, batch 72/75, loss 0.1195\n",
            "epoch 48/250, batch 73/75, loss 0.1405\n",
            "epoch 48/250, batch 74/75, loss 0.1657\n",
            "epoch 48/250, batch 75/75, loss 0.1132\n",
            "epoch 48/250, training roc_auc_score 0.9927\n",
            "EarlyStopping counter: 36 out of 50\n",
            "epoch 48/250, validation roc_auc_score 0.7666, best validation roc_auc_score 0.8516\n",
            "epoch 49/250, batch 1/75, loss 0.2330\n",
            "epoch 49/250, batch 2/75, loss 0.1836\n",
            "epoch 49/250, batch 3/75, loss 0.1231\n",
            "epoch 49/250, batch 4/75, loss 0.2251\n",
            "epoch 49/250, batch 5/75, loss 0.0604\n",
            "epoch 49/250, batch 6/75, loss 0.0715\n",
            "epoch 49/250, batch 7/75, loss 0.1582\n",
            "epoch 49/250, batch 8/75, loss 0.1831\n",
            "epoch 49/250, batch 9/75, loss 0.0809\n",
            "epoch 49/250, batch 10/75, loss 0.1539\n",
            "epoch 49/250, batch 11/75, loss 0.1864\n",
            "epoch 49/250, batch 12/75, loss 0.1227\n",
            "epoch 49/250, batch 13/75, loss 0.2138\n",
            "epoch 49/250, batch 14/75, loss 0.0659\n",
            "epoch 49/250, batch 15/75, loss 0.4261\n",
            "epoch 49/250, batch 16/75, loss 0.1070\n",
            "epoch 49/250, batch 17/75, loss 0.0945\n",
            "epoch 49/250, batch 18/75, loss 0.1828\n",
            "epoch 49/250, batch 19/75, loss 0.1592\n",
            "epoch 49/250, batch 20/75, loss 0.2662\n",
            "epoch 49/250, batch 21/75, loss 0.0735\n",
            "epoch 49/250, batch 22/75, loss 0.1154\n",
            "epoch 49/250, batch 23/75, loss 0.1058\n",
            "epoch 49/250, batch 24/75, loss 0.0499\n",
            "epoch 49/250, batch 25/75, loss 0.0686\n",
            "epoch 49/250, batch 26/75, loss 0.4308\n",
            "epoch 49/250, batch 27/75, loss 0.1396\n",
            "epoch 49/250, batch 28/75, loss 0.1314\n",
            "epoch 49/250, batch 29/75, loss 0.0879\n",
            "epoch 49/250, batch 30/75, loss 0.1103\n",
            "epoch 49/250, batch 31/75, loss 0.0681\n",
            "epoch 49/250, batch 32/75, loss 0.1131\n",
            "epoch 49/250, batch 33/75, loss 0.0901\n",
            "epoch 49/250, batch 34/75, loss 0.1143\n",
            "epoch 49/250, batch 35/75, loss 0.1120\n",
            "epoch 49/250, batch 36/75, loss 0.6549\n",
            "epoch 49/250, batch 37/75, loss 0.1089\n",
            "epoch 49/250, batch 38/75, loss 0.0956\n",
            "epoch 49/250, batch 39/75, loss 0.1172\n",
            "epoch 49/250, batch 40/75, loss 0.1135\n",
            "epoch 49/250, batch 41/75, loss 0.1820\n",
            "epoch 49/250, batch 42/75, loss 0.1188\n",
            "epoch 49/250, batch 43/75, loss 0.0880\n",
            "epoch 49/250, batch 44/75, loss 0.1156\n",
            "epoch 49/250, batch 45/75, loss 0.1392\n",
            "epoch 49/250, batch 46/75, loss 0.0569\n",
            "epoch 49/250, batch 47/75, loss 0.1216\n",
            "epoch 49/250, batch 48/75, loss 0.0760\n",
            "epoch 49/250, batch 49/75, loss 0.1105\n",
            "epoch 49/250, batch 50/75, loss 0.0834\n",
            "epoch 49/250, batch 51/75, loss 0.0787\n",
            "epoch 49/250, batch 52/75, loss 0.0619\n",
            "epoch 49/250, batch 53/75, loss 3.5981\n",
            "epoch 49/250, batch 54/75, loss 0.1762\n",
            "epoch 49/250, batch 55/75, loss 0.1112\n",
            "epoch 49/250, batch 56/75, loss 0.0805\n",
            "epoch 49/250, batch 57/75, loss 0.0844\n",
            "epoch 49/250, batch 58/75, loss 0.1189\n",
            "epoch 49/250, batch 59/75, loss 0.0960\n",
            "epoch 49/250, batch 60/75, loss 0.1030\n",
            "epoch 49/250, batch 61/75, loss 0.1347\n",
            "epoch 49/250, batch 62/75, loss 0.1722\n",
            "epoch 49/250, batch 63/75, loss 0.0838\n",
            "epoch 49/250, batch 64/75, loss 0.1372\n",
            "epoch 49/250, batch 65/75, loss 0.1323\n",
            "epoch 49/250, batch 66/75, loss 0.1603\n",
            "epoch 49/250, batch 67/75, loss 0.0979\n",
            "epoch 49/250, batch 68/75, loss 0.1271\n",
            "epoch 49/250, batch 69/75, loss 0.1578\n",
            "epoch 49/250, batch 70/75, loss 0.0852\n",
            "epoch 49/250, batch 71/75, loss 0.1428\n",
            "epoch 49/250, batch 72/75, loss 0.0991\n",
            "epoch 49/250, batch 73/75, loss 0.1304\n",
            "epoch 49/250, batch 74/75, loss 0.1637\n",
            "epoch 49/250, batch 75/75, loss 0.0907\n",
            "epoch 49/250, training roc_auc_score 0.9948\n",
            "EarlyStopping counter: 37 out of 50\n",
            "epoch 49/250, validation roc_auc_score 0.8366, best validation roc_auc_score 0.8516\n",
            "epoch 50/250, batch 1/75, loss 0.1864\n",
            "epoch 50/250, batch 2/75, loss 0.1666\n",
            "epoch 50/250, batch 3/75, loss 0.0827\n",
            "epoch 50/250, batch 4/75, loss 0.1660\n",
            "epoch 50/250, batch 5/75, loss 0.0450\n",
            "epoch 50/250, batch 6/75, loss 0.0458\n",
            "epoch 50/250, batch 7/75, loss 0.1333\n",
            "epoch 50/250, batch 8/75, loss 0.2610\n",
            "epoch 50/250, batch 9/75, loss 0.0606\n",
            "epoch 50/250, batch 10/75, loss 0.1437\n",
            "epoch 50/250, batch 11/75, loss 0.1362\n",
            "epoch 50/250, batch 12/75, loss 0.1332\n",
            "epoch 50/250, batch 13/75, loss 0.1734\n",
            "epoch 50/250, batch 14/75, loss 0.0569\n",
            "epoch 50/250, batch 15/75, loss 0.6483\n",
            "epoch 50/250, batch 16/75, loss 0.1111\n",
            "epoch 50/250, batch 17/75, loss 0.0949\n",
            "epoch 50/250, batch 18/75, loss 0.1758\n",
            "epoch 50/250, batch 19/75, loss 0.1174\n",
            "epoch 50/250, batch 20/75, loss 0.5016\n",
            "epoch 50/250, batch 21/75, loss 0.0589\n",
            "epoch 50/250, batch 22/75, loss 0.0875\n",
            "epoch 50/250, batch 23/75, loss 0.0758\n",
            "epoch 50/250, batch 24/75, loss 0.0544\n",
            "epoch 50/250, batch 25/75, loss 0.0544\n",
            "epoch 50/250, batch 26/75, loss 0.4262\n",
            "epoch 50/250, batch 27/75, loss 0.1436\n",
            "epoch 50/250, batch 28/75, loss 0.1183\n",
            "epoch 50/250, batch 29/75, loss 0.0676\n",
            "epoch 50/250, batch 30/75, loss 0.0838\n",
            "epoch 50/250, batch 31/75, loss 0.0613\n",
            "epoch 50/250, batch 32/75, loss 0.0888\n",
            "epoch 50/250, batch 33/75, loss 0.0896\n",
            "epoch 50/250, batch 34/75, loss 0.0967\n",
            "epoch 50/250, batch 35/75, loss 0.1050\n",
            "epoch 50/250, batch 36/75, loss 0.5276\n",
            "epoch 50/250, batch 37/75, loss 0.1273\n",
            "epoch 50/250, batch 38/75, loss 0.0960\n",
            "epoch 50/250, batch 39/75, loss 0.1159\n",
            "epoch 50/250, batch 40/75, loss 0.1039\n",
            "epoch 50/250, batch 41/75, loss 0.1844\n",
            "epoch 50/250, batch 42/75, loss 0.1114\n",
            "epoch 50/250, batch 43/75, loss 0.0793\n",
            "epoch 50/250, batch 44/75, loss 0.1098\n",
            "epoch 50/250, batch 45/75, loss 0.1280\n",
            "epoch 50/250, batch 46/75, loss 0.0560\n",
            "epoch 50/250, batch 47/75, loss 0.1062\n",
            "epoch 50/250, batch 48/75, loss 0.0772\n",
            "epoch 50/250, batch 49/75, loss 0.0923\n",
            "epoch 50/250, batch 50/75, loss 0.0727\n",
            "epoch 50/250, batch 51/75, loss 0.0756\n",
            "epoch 50/250, batch 52/75, loss 0.0565\n",
            "epoch 50/250, batch 53/75, loss 3.6449\n",
            "epoch 50/250, batch 54/75, loss 0.1743\n",
            "epoch 50/250, batch 55/75, loss 0.1052\n",
            "epoch 50/250, batch 56/75, loss 0.0748\n",
            "epoch 50/250, batch 57/75, loss 0.0763\n",
            "epoch 50/250, batch 58/75, loss 0.1115\n",
            "epoch 50/250, batch 59/75, loss 0.0879\n",
            "epoch 50/250, batch 60/75, loss 0.0852\n",
            "epoch 50/250, batch 61/75, loss 0.1398\n",
            "epoch 50/250, batch 62/75, loss 0.1261\n",
            "epoch 50/250, batch 63/75, loss 0.0708\n",
            "epoch 50/250, batch 64/75, loss 0.1315\n",
            "epoch 50/250, batch 65/75, loss 0.1142\n",
            "epoch 50/250, batch 66/75, loss 0.1425\n",
            "epoch 50/250, batch 67/75, loss 0.0790\n",
            "epoch 50/250, batch 68/75, loss 0.0978\n",
            "epoch 50/250, batch 69/75, loss 0.1355\n",
            "epoch 50/250, batch 70/75, loss 0.0713\n",
            "epoch 50/250, batch 71/75, loss 0.1304\n",
            "epoch 50/250, batch 72/75, loss 0.0784\n",
            "epoch 50/250, batch 73/75, loss 0.1206\n",
            "epoch 50/250, batch 74/75, loss 0.1306\n",
            "epoch 50/250, batch 75/75, loss 0.0916\n",
            "epoch 50/250, training roc_auc_score 0.9954\n",
            "EarlyStopping counter: 38 out of 50\n",
            "epoch 50/250, validation roc_auc_score 0.7727, best validation roc_auc_score 0.8516\n",
            "epoch 51/250, batch 1/75, loss 0.1776\n",
            "epoch 51/250, batch 2/75, loss 0.1841\n",
            "epoch 51/250, batch 3/75, loss 0.1321\n",
            "epoch 51/250, batch 4/75, loss 0.1633\n",
            "epoch 51/250, batch 5/75, loss 0.0434\n",
            "epoch 51/250, batch 6/75, loss 0.0459\n",
            "epoch 51/250, batch 7/75, loss 0.1406\n",
            "epoch 51/250, batch 8/75, loss 0.1255\n",
            "epoch 51/250, batch 9/75, loss 0.0660\n",
            "epoch 51/250, batch 10/75, loss 0.1174\n",
            "epoch 51/250, batch 11/75, loss 0.1336\n",
            "epoch 51/250, batch 12/75, loss 0.1094\n",
            "epoch 51/250, batch 13/75, loss 0.1997\n",
            "epoch 51/250, batch 14/75, loss 0.0580\n",
            "epoch 51/250, batch 15/75, loss 0.3348\n",
            "epoch 51/250, batch 16/75, loss 0.0943\n",
            "epoch 51/250, batch 17/75, loss 0.0944\n",
            "epoch 51/250, batch 18/75, loss 0.1915\n",
            "epoch 51/250, batch 19/75, loss 0.1159\n",
            "epoch 51/250, batch 20/75, loss 0.3408\n",
            "epoch 51/250, batch 21/75, loss 0.0629\n",
            "epoch 51/250, batch 22/75, loss 0.1000\n",
            "epoch 51/250, batch 23/75, loss 0.0873\n",
            "epoch 51/250, batch 24/75, loss 0.0420\n",
            "epoch 51/250, batch 25/75, loss 0.0602\n",
            "epoch 51/250, batch 26/75, loss 0.5504\n",
            "epoch 51/250, batch 27/75, loss 0.1255\n",
            "epoch 51/250, batch 28/75, loss 0.1123\n",
            "epoch 51/250, batch 29/75, loss 0.0805\n",
            "epoch 51/250, batch 30/75, loss 0.1082\n",
            "epoch 51/250, batch 31/75, loss 0.0639\n",
            "epoch 51/250, batch 32/75, loss 0.0956\n",
            "epoch 51/250, batch 33/75, loss 0.0859\n",
            "epoch 51/250, batch 34/75, loss 0.0980\n",
            "epoch 51/250, batch 35/75, loss 0.1185\n",
            "epoch 51/250, batch 36/75, loss 0.4338\n",
            "epoch 51/250, batch 37/75, loss 0.1170\n",
            "epoch 51/250, batch 38/75, loss 0.0754\n",
            "epoch 51/250, batch 39/75, loss 0.1093\n",
            "epoch 51/250, batch 40/75, loss 0.0944\n",
            "epoch 51/250, batch 41/75, loss 0.1731\n",
            "epoch 51/250, batch 42/75, loss 0.0956\n",
            "epoch 51/250, batch 43/75, loss 0.0917\n",
            "epoch 51/250, batch 44/75, loss 0.1067\n",
            "epoch 51/250, batch 45/75, loss 0.1089\n",
            "epoch 51/250, batch 46/75, loss 0.0456\n",
            "epoch 51/250, batch 47/75, loss 0.1029\n",
            "epoch 51/250, batch 48/75, loss 0.0656\n",
            "epoch 51/250, batch 49/75, loss 0.0932\n",
            "epoch 51/250, batch 50/75, loss 0.0747\n",
            "epoch 51/250, batch 51/75, loss 0.0690\n",
            "epoch 51/250, batch 52/75, loss 0.0568\n",
            "epoch 51/250, batch 53/75, loss 3.6756\n",
            "epoch 51/250, batch 54/75, loss 0.1397\n",
            "epoch 51/250, batch 55/75, loss 0.0903\n",
            "epoch 51/250, batch 56/75, loss 0.0637\n",
            "epoch 51/250, batch 57/75, loss 0.0686\n",
            "epoch 51/250, batch 58/75, loss 0.0927\n",
            "epoch 51/250, batch 59/75, loss 0.0879\n",
            "epoch 51/250, batch 60/75, loss 0.0859\n",
            "epoch 51/250, batch 61/75, loss 0.1116\n",
            "epoch 51/250, batch 62/75, loss 0.1575\n",
            "epoch 51/250, batch 63/75, loss 0.0707\n",
            "epoch 51/250, batch 64/75, loss 0.1279\n",
            "epoch 51/250, batch 65/75, loss 0.1166\n",
            "epoch 51/250, batch 66/75, loss 0.1459\n",
            "epoch 51/250, batch 67/75, loss 0.0788\n",
            "epoch 51/250, batch 68/75, loss 0.0869\n",
            "epoch 51/250, batch 69/75, loss 0.1288\n",
            "epoch 51/250, batch 70/75, loss 0.0702\n",
            "epoch 51/250, batch 71/75, loss 0.1311\n",
            "epoch 51/250, batch 72/75, loss 0.0792\n",
            "epoch 51/250, batch 73/75, loss 0.1128\n",
            "epoch 51/250, batch 74/75, loss 0.1597\n",
            "epoch 51/250, batch 75/75, loss 0.0700\n",
            "epoch 51/250, training roc_auc_score 0.9955\n",
            "EarlyStopping counter: 39 out of 50\n",
            "epoch 51/250, validation roc_auc_score 0.8292, best validation roc_auc_score 0.8516\n",
            "epoch 52/250, batch 1/75, loss 0.2160\n",
            "epoch 52/250, batch 2/75, loss 0.0984\n",
            "epoch 52/250, batch 3/75, loss 0.0859\n",
            "epoch 52/250, batch 4/75, loss 0.0956\n",
            "epoch 52/250, batch 5/75, loss 0.0389\n",
            "epoch 52/250, batch 6/75, loss 0.0502\n",
            "epoch 52/250, batch 7/75, loss 0.1034\n",
            "epoch 52/250, batch 8/75, loss 0.2749\n",
            "epoch 52/250, batch 9/75, loss 0.0541\n",
            "epoch 52/250, batch 10/75, loss 0.0774\n",
            "epoch 52/250, batch 11/75, loss 0.1313\n",
            "epoch 52/250, batch 12/75, loss 0.1187\n",
            "epoch 52/250, batch 13/75, loss 0.2085\n",
            "epoch 52/250, batch 14/75, loss 0.0646\n",
            "epoch 52/250, batch 15/75, loss 0.5471\n",
            "epoch 52/250, batch 16/75, loss 0.0949\n",
            "epoch 52/250, batch 17/75, loss 0.1004\n",
            "epoch 52/250, batch 18/75, loss 0.1851\n",
            "epoch 52/250, batch 19/75, loss 0.1211\n",
            "epoch 52/250, batch 20/75, loss 0.3116\n",
            "epoch 52/250, batch 21/75, loss 0.0506\n",
            "epoch 52/250, batch 22/75, loss 0.0803\n",
            "epoch 52/250, batch 23/75, loss 0.0897\n",
            "epoch 52/250, batch 24/75, loss 0.0415\n",
            "epoch 52/250, batch 25/75, loss 0.0522\n",
            "epoch 52/250, batch 26/75, loss 0.3996\n",
            "epoch 52/250, batch 27/75, loss 0.1270\n",
            "epoch 52/250, batch 28/75, loss 0.0983\n",
            "epoch 52/250, batch 29/75, loss 0.0911\n",
            "epoch 52/250, batch 30/75, loss 0.0856\n",
            "epoch 52/250, batch 31/75, loss 0.0555\n",
            "epoch 52/250, batch 32/75, loss 0.0781\n",
            "epoch 52/250, batch 33/75, loss 0.0657\n",
            "epoch 52/250, batch 34/75, loss 0.0952\n",
            "epoch 52/250, batch 35/75, loss 0.1066\n",
            "epoch 52/250, batch 36/75, loss 0.5689\n",
            "epoch 52/250, batch 37/75, loss 0.1317\n",
            "epoch 52/250, batch 38/75, loss 0.0733\n",
            "epoch 52/250, batch 39/75, loss 0.1153\n",
            "epoch 52/250, batch 40/75, loss 0.0814\n",
            "epoch 52/250, batch 41/75, loss 0.1918\n",
            "epoch 52/250, batch 42/75, loss 0.1138\n",
            "epoch 52/250, batch 43/75, loss 0.0804\n",
            "epoch 52/250, batch 44/75, loss 0.1024\n",
            "epoch 52/250, batch 45/75, loss 0.1063\n",
            "epoch 52/250, batch 46/75, loss 0.0480\n",
            "epoch 52/250, batch 47/75, loss 0.0962\n",
            "epoch 52/250, batch 48/75, loss 0.0685\n",
            "epoch 52/250, batch 49/75, loss 0.0945\n",
            "epoch 52/250, batch 50/75, loss 0.0685\n",
            "epoch 52/250, batch 51/75, loss 0.0739\n",
            "epoch 52/250, batch 52/75, loss 0.0580\n",
            "epoch 52/250, batch 53/75, loss 2.9781\n",
            "epoch 52/250, batch 54/75, loss 0.1306\n",
            "epoch 52/250, batch 55/75, loss 0.0920\n",
            "epoch 52/250, batch 56/75, loss 0.0654\n",
            "epoch 52/250, batch 57/75, loss 0.0696\n",
            "epoch 52/250, batch 58/75, loss 0.1130\n",
            "epoch 52/250, batch 59/75, loss 0.0815\n",
            "epoch 52/250, batch 60/75, loss 0.0894\n",
            "epoch 52/250, batch 61/75, loss 0.1301\n",
            "epoch 52/250, batch 62/75, loss 0.1365\n",
            "epoch 52/250, batch 63/75, loss 0.0695\n",
            "epoch 52/250, batch 64/75, loss 0.1203\n",
            "epoch 52/250, batch 65/75, loss 0.1438\n",
            "epoch 52/250, batch 66/75, loss 0.1207\n",
            "epoch 52/250, batch 67/75, loss 0.0904\n",
            "epoch 52/250, batch 68/75, loss 0.0905\n",
            "epoch 52/250, batch 69/75, loss 0.1342\n",
            "epoch 52/250, batch 70/75, loss 0.0793\n",
            "epoch 52/250, batch 71/75, loss 0.1197\n",
            "epoch 52/250, batch 72/75, loss 0.1011\n",
            "epoch 52/250, batch 73/75, loss 0.0849\n",
            "epoch 52/250, batch 74/75, loss 0.1466\n",
            "epoch 52/250, batch 75/75, loss 0.0857\n",
            "epoch 52/250, training roc_auc_score 0.9964\n",
            "EarlyStopping counter: 40 out of 50\n",
            "epoch 52/250, validation roc_auc_score 0.7959, best validation roc_auc_score 0.8516\n",
            "epoch 53/250, batch 1/75, loss 0.1826\n",
            "epoch 53/250, batch 2/75, loss 0.1115\n",
            "epoch 53/250, batch 3/75, loss 0.0646\n",
            "epoch 53/250, batch 4/75, loss 0.1650\n",
            "epoch 53/250, batch 5/75, loss 0.0362\n",
            "epoch 53/250, batch 6/75, loss 0.0418\n",
            "epoch 53/250, batch 7/75, loss 0.1351\n",
            "epoch 53/250, batch 8/75, loss 0.1868\n",
            "epoch 53/250, batch 9/75, loss 0.0613\n",
            "epoch 53/250, batch 10/75, loss 0.1173\n",
            "epoch 53/250, batch 11/75, loss 0.1508\n",
            "epoch 53/250, batch 12/75, loss 0.1236\n",
            "epoch 53/250, batch 13/75, loss 0.2504\n",
            "epoch 53/250, batch 14/75, loss 0.0692\n",
            "epoch 53/250, batch 15/75, loss 0.4052\n",
            "epoch 53/250, batch 16/75, loss 0.0816\n",
            "epoch 53/250, batch 17/75, loss 0.1032\n",
            "epoch 53/250, batch 18/75, loss 0.1927\n",
            "epoch 53/250, batch 19/75, loss 0.1342\n",
            "epoch 53/250, batch 20/75, loss 0.2139\n",
            "epoch 53/250, batch 21/75, loss 0.0532\n",
            "epoch 53/250, batch 22/75, loss 0.0696\n",
            "epoch 53/250, batch 23/75, loss 0.0743\n",
            "epoch 53/250, batch 24/75, loss 0.0363\n",
            "epoch 53/250, batch 25/75, loss 0.0497\n",
            "epoch 53/250, batch 26/75, loss 0.3629\n",
            "epoch 53/250, batch 27/75, loss 0.1102\n",
            "epoch 53/250, batch 28/75, loss 0.0977\n",
            "epoch 53/250, batch 29/75, loss 0.0646\n",
            "epoch 53/250, batch 30/75, loss 0.0861\n",
            "epoch 53/250, batch 31/75, loss 0.0641\n",
            "epoch 53/250, batch 32/75, loss 0.0791\n",
            "epoch 53/250, batch 33/75, loss 0.0691\n",
            "epoch 53/250, batch 34/75, loss 0.0729\n",
            "epoch 53/250, batch 35/75, loss 0.0739\n",
            "epoch 53/250, batch 36/75, loss 0.5230\n",
            "epoch 53/250, batch 37/75, loss 0.1013\n",
            "epoch 53/250, batch 38/75, loss 0.0695\n",
            "epoch 53/250, batch 39/75, loss 0.0897\n",
            "epoch 53/250, batch 40/75, loss 0.0923\n",
            "epoch 53/250, batch 41/75, loss 0.1364\n",
            "epoch 53/250, batch 42/75, loss 0.0981\n",
            "epoch 53/250, batch 43/75, loss 0.0607\n",
            "epoch 53/250, batch 44/75, loss 0.0834\n",
            "epoch 53/250, batch 45/75, loss 0.1009\n",
            "epoch 53/250, batch 46/75, loss 0.0414\n",
            "epoch 53/250, batch 47/75, loss 0.1027\n",
            "epoch 53/250, batch 48/75, loss 0.0551\n",
            "epoch 53/250, batch 49/75, loss 0.0834\n",
            "epoch 53/250, batch 50/75, loss 0.0651\n",
            "epoch 53/250, batch 51/75, loss 0.0549\n",
            "epoch 53/250, batch 52/75, loss 0.0453\n",
            "epoch 53/250, batch 53/75, loss 4.0225\n",
            "epoch 53/250, batch 54/75, loss 0.1826\n",
            "epoch 53/250, batch 55/75, loss 0.0835\n",
            "epoch 53/250, batch 56/75, loss 0.0742\n",
            "epoch 53/250, batch 57/75, loss 0.0739\n",
            "epoch 53/250, batch 58/75, loss 0.0955\n",
            "epoch 53/250, batch 59/75, loss 0.0867\n",
            "epoch 53/250, batch 60/75, loss 0.1107\n",
            "epoch 53/250, batch 61/75, loss 0.1186\n",
            "epoch 53/250, batch 62/75, loss 0.1837\n",
            "epoch 53/250, batch 63/75, loss 0.0835\n",
            "epoch 53/250, batch 64/75, loss 0.1793\n",
            "epoch 53/250, batch 65/75, loss 0.1845\n",
            "epoch 53/250, batch 66/75, loss 0.1206\n",
            "epoch 53/250, batch 67/75, loss 0.0871\n",
            "epoch 53/250, batch 68/75, loss 0.0768\n",
            "epoch 53/250, batch 69/75, loss 0.1655\n",
            "epoch 53/250, batch 70/75, loss 0.0792\n",
            "epoch 53/250, batch 71/75, loss 0.1933\n",
            "epoch 53/250, batch 72/75, loss 0.1000\n",
            "epoch 53/250, batch 73/75, loss 0.1368\n",
            "epoch 53/250, batch 74/75, loss 0.1673\n",
            "epoch 53/250, batch 75/75, loss 0.1014\n",
            "epoch 53/250, training roc_auc_score 0.9943\n",
            "EarlyStopping counter: 41 out of 50\n",
            "epoch 53/250, validation roc_auc_score 0.7254, best validation roc_auc_score 0.8516\n",
            "epoch 54/250, batch 1/75, loss 0.1360\n",
            "epoch 54/250, batch 2/75, loss 0.1296\n",
            "epoch 54/250, batch 3/75, loss 0.0839\n",
            "epoch 54/250, batch 4/75, loss 0.1160\n",
            "epoch 54/250, batch 5/75, loss 0.0408\n",
            "epoch 54/250, batch 6/75, loss 0.0541\n",
            "epoch 54/250, batch 7/75, loss 0.1086\n",
            "epoch 54/250, batch 8/75, loss 0.2984\n",
            "epoch 54/250, batch 9/75, loss 0.0752\n",
            "epoch 54/250, batch 10/75, loss 0.0938\n",
            "epoch 54/250, batch 11/75, loss 0.1550\n",
            "epoch 54/250, batch 12/75, loss 0.0957\n",
            "epoch 54/250, batch 13/75, loss 0.2979\n",
            "epoch 54/250, batch 14/75, loss 0.0884\n",
            "epoch 54/250, batch 15/75, loss 0.4645\n",
            "epoch 54/250, batch 16/75, loss 0.1249\n",
            "epoch 54/250, batch 17/75, loss 0.1512\n",
            "epoch 54/250, batch 18/75, loss 0.2262\n",
            "epoch 54/250, batch 19/75, loss 0.1463\n",
            "epoch 54/250, batch 20/75, loss 0.1967\n",
            "epoch 54/250, batch 21/75, loss 0.0516\n",
            "epoch 54/250, batch 22/75, loss 0.0891\n",
            "epoch 54/250, batch 23/75, loss 0.0692\n",
            "epoch 54/250, batch 24/75, loss 0.0317\n",
            "epoch 54/250, batch 25/75, loss 0.0482\n",
            "epoch 54/250, batch 26/75, loss 0.7448\n",
            "epoch 54/250, batch 27/75, loss 0.1473\n",
            "epoch 54/250, batch 28/75, loss 0.1122\n",
            "epoch 54/250, batch 29/75, loss 0.0584\n",
            "epoch 54/250, batch 30/75, loss 0.0758\n",
            "epoch 54/250, batch 31/75, loss 0.0526\n",
            "epoch 54/250, batch 32/75, loss 0.0811\n",
            "epoch 54/250, batch 33/75, loss 0.0817\n",
            "epoch 54/250, batch 34/75, loss 0.0941\n",
            "epoch 54/250, batch 35/75, loss 0.1092\n",
            "epoch 54/250, batch 36/75, loss 0.4392\n",
            "epoch 54/250, batch 37/75, loss 0.1173\n",
            "epoch 54/250, batch 38/75, loss 0.0728\n",
            "epoch 54/250, batch 39/75, loss 0.0910\n",
            "epoch 54/250, batch 40/75, loss 0.0891\n",
            "epoch 54/250, batch 41/75, loss 0.1467\n",
            "epoch 54/250, batch 42/75, loss 0.1099\n",
            "epoch 54/250, batch 43/75, loss 0.0665\n",
            "epoch 54/250, batch 44/75, loss 0.0977\n",
            "epoch 54/250, batch 45/75, loss 0.1147\n",
            "epoch 54/250, batch 46/75, loss 0.0409\n",
            "epoch 54/250, batch 47/75, loss 0.0936\n",
            "epoch 54/250, batch 48/75, loss 0.0648\n",
            "epoch 54/250, batch 49/75, loss 0.0812\n",
            "epoch 54/250, batch 50/75, loss 0.0642\n",
            "epoch 54/250, batch 51/75, loss 0.0590\n",
            "epoch 54/250, batch 52/75, loss 0.0503\n",
            "epoch 54/250, batch 53/75, loss 4.1873\n",
            "epoch 54/250, batch 54/75, loss 0.1585\n",
            "epoch 54/250, batch 55/75, loss 0.0804\n",
            "epoch 54/250, batch 56/75, loss 0.0653\n",
            "epoch 54/250, batch 57/75, loss 0.0735\n",
            "epoch 54/250, batch 58/75, loss 0.1080\n",
            "epoch 54/250, batch 59/75, loss 0.1009\n",
            "epoch 54/250, batch 60/75, loss 0.1215\n",
            "epoch 54/250, batch 61/75, loss 0.1530\n",
            "epoch 54/250, batch 62/75, loss 0.2193\n",
            "epoch 54/250, batch 63/75, loss 0.0788\n",
            "epoch 54/250, batch 64/75, loss 0.1584\n",
            "epoch 54/250, batch 65/75, loss 0.1937\n",
            "epoch 54/250, batch 66/75, loss 0.1677\n",
            "epoch 54/250, batch 67/75, loss 0.1395\n",
            "epoch 54/250, batch 68/75, loss 0.1109\n",
            "epoch 54/250, batch 69/75, loss 0.1905\n",
            "epoch 54/250, batch 70/75, loss 0.1023\n",
            "epoch 54/250, batch 71/75, loss 0.2035\n",
            "epoch 54/250, batch 72/75, loss 0.1545\n",
            "epoch 54/250, batch 73/75, loss 0.1419\n",
            "epoch 54/250, batch 74/75, loss 0.2094\n",
            "epoch 54/250, batch 75/75, loss 0.1086\n",
            "epoch 54/250, training roc_auc_score 0.9919\n",
            "EarlyStopping counter: 42 out of 50\n",
            "epoch 54/250, validation roc_auc_score 0.7894, best validation roc_auc_score 0.8516\n",
            "epoch 55/250, batch 1/75, loss 0.1873\n",
            "epoch 55/250, batch 2/75, loss 0.1631\n",
            "epoch 55/250, batch 3/75, loss 0.0801\n",
            "epoch 55/250, batch 4/75, loss 0.2736\n",
            "epoch 55/250, batch 5/75, loss 0.0415\n",
            "epoch 55/250, batch 6/75, loss 0.0432\n",
            "epoch 55/250, batch 7/75, loss 0.1747\n",
            "epoch 55/250, batch 8/75, loss 0.2749\n",
            "epoch 55/250, batch 9/75, loss 0.0638\n",
            "epoch 55/250, batch 10/75, loss 0.2649\n",
            "epoch 55/250, batch 11/75, loss 0.1550\n",
            "epoch 55/250, batch 12/75, loss 0.1071\n",
            "epoch 55/250, batch 13/75, loss 0.4047\n",
            "epoch 55/250, batch 14/75, loss 0.1215\n",
            "epoch 55/250, batch 15/75, loss 0.5749\n",
            "epoch 55/250, batch 16/75, loss 0.1349\n",
            "epoch 55/250, batch 17/75, loss 0.1519\n",
            "epoch 55/250, batch 18/75, loss 0.3071\n",
            "epoch 55/250, batch 19/75, loss 0.1829\n",
            "epoch 55/250, batch 20/75, loss 0.3802\n",
            "epoch 55/250, batch 21/75, loss 0.0496\n",
            "epoch 55/250, batch 22/75, loss 0.0621\n",
            "epoch 55/250, batch 23/75, loss 0.0892\n",
            "epoch 55/250, batch 24/75, loss 0.0385\n",
            "epoch 55/250, batch 25/75, loss 0.0654\n",
            "epoch 55/250, batch 26/75, loss 0.3727\n",
            "epoch 55/250, batch 27/75, loss 0.1637\n",
            "epoch 55/250, batch 28/75, loss 0.1078\n",
            "epoch 55/250, batch 29/75, loss 0.0768\n",
            "epoch 55/250, batch 30/75, loss 0.0775\n",
            "epoch 55/250, batch 31/75, loss 0.0677\n",
            "epoch 55/250, batch 32/75, loss 0.1086\n",
            "epoch 55/250, batch 33/75, loss 0.0771\n",
            "epoch 55/250, batch 34/75, loss 0.1240\n",
            "epoch 55/250, batch 35/75, loss 0.1010\n",
            "epoch 55/250, batch 36/75, loss 0.8144\n",
            "epoch 55/250, batch 37/75, loss 0.1012\n",
            "epoch 55/250, batch 38/75, loss 0.0922\n",
            "epoch 55/250, batch 39/75, loss 0.1094\n",
            "epoch 55/250, batch 40/75, loss 0.0989\n",
            "epoch 55/250, batch 41/75, loss 0.1782\n",
            "epoch 55/250, batch 42/75, loss 0.1086\n",
            "epoch 55/250, batch 43/75, loss 0.0746\n",
            "epoch 55/250, batch 44/75, loss 0.1063\n",
            "epoch 55/250, batch 45/75, loss 0.1325\n",
            "epoch 55/250, batch 46/75, loss 0.0492\n",
            "epoch 55/250, batch 47/75, loss 0.1078\n",
            "epoch 55/250, batch 48/75, loss 0.0740\n",
            "epoch 55/250, batch 49/75, loss 0.1246\n",
            "epoch 55/250, batch 50/75, loss 0.0852\n",
            "epoch 55/250, batch 51/75, loss 0.0650\n",
            "epoch 55/250, batch 52/75, loss 0.0591\n",
            "epoch 55/250, batch 53/75, loss 5.4396\n",
            "epoch 55/250, batch 54/75, loss 0.1304\n",
            "epoch 55/250, batch 55/75, loss 0.0988\n",
            "epoch 55/250, batch 56/75, loss 0.0686\n",
            "epoch 55/250, batch 57/75, loss 0.0783\n",
            "epoch 55/250, batch 58/75, loss 0.1360\n",
            "epoch 55/250, batch 59/75, loss 0.0909\n",
            "epoch 55/250, batch 60/75, loss 0.0941\n",
            "epoch 55/250, batch 61/75, loss 0.1237\n",
            "epoch 55/250, batch 62/75, loss 0.1940\n",
            "epoch 55/250, batch 63/75, loss 0.0997\n",
            "epoch 55/250, batch 64/75, loss 0.1868\n",
            "epoch 55/250, batch 65/75, loss 0.1240\n",
            "epoch 55/250, batch 66/75, loss 0.1746\n",
            "epoch 55/250, batch 67/75, loss 0.1060\n",
            "epoch 55/250, batch 68/75, loss 0.1413\n",
            "epoch 55/250, batch 69/75, loss 0.2111\n",
            "epoch 55/250, batch 70/75, loss 0.0974\n",
            "epoch 55/250, batch 71/75, loss 0.2264\n",
            "epoch 55/250, batch 72/75, loss 0.1053\n",
            "epoch 55/250, batch 73/75, loss 0.1808\n",
            "epoch 55/250, batch 74/75, loss 0.1614\n",
            "epoch 55/250, batch 75/75, loss 0.1106\n",
            "epoch 55/250, training roc_auc_score 0.9876\n",
            "EarlyStopping counter: 43 out of 50\n",
            "epoch 55/250, validation roc_auc_score 0.7926, best validation roc_auc_score 0.8516\n",
            "epoch 56/250, batch 1/75, loss 0.5689\n",
            "epoch 56/250, batch 2/75, loss 0.1626\n",
            "epoch 56/250, batch 3/75, loss 0.0871\n",
            "epoch 56/250, batch 4/75, loss 0.1896\n",
            "epoch 56/250, batch 5/75, loss 0.0562\n",
            "epoch 56/250, batch 6/75, loss 0.0554\n",
            "epoch 56/250, batch 7/75, loss 0.1701\n",
            "epoch 56/250, batch 8/75, loss 0.4190\n",
            "epoch 56/250, batch 9/75, loss 0.0717\n",
            "epoch 56/250, batch 10/75, loss 0.2070\n",
            "epoch 56/250, batch 11/75, loss 0.1663\n",
            "epoch 56/250, batch 12/75, loss 0.1533\n",
            "epoch 56/250, batch 13/75, loss 0.2150\n",
            "epoch 56/250, batch 14/75, loss 0.0472\n",
            "epoch 56/250, batch 15/75, loss 1.0277\n",
            "epoch 56/250, batch 16/75, loss 0.1132\n",
            "epoch 56/250, batch 17/75, loss 0.1252\n",
            "epoch 56/250, batch 18/75, loss 0.1009\n",
            "epoch 56/250, batch 19/75, loss 0.1026\n",
            "epoch 56/250, batch 20/75, loss 0.5369\n",
            "epoch 56/250, batch 21/75, loss 0.0545\n",
            "epoch 56/250, batch 22/75, loss 0.0854\n",
            "epoch 56/250, batch 23/75, loss 0.0936\n",
            "epoch 56/250, batch 24/75, loss 0.0382\n",
            "epoch 56/250, batch 25/75, loss 0.0482\n",
            "epoch 56/250, batch 26/75, loss 0.6228\n",
            "epoch 56/250, batch 27/75, loss 0.1473\n",
            "epoch 56/250, batch 28/75, loss 0.1333\n",
            "epoch 56/250, batch 29/75, loss 0.0879\n",
            "epoch 56/250, batch 30/75, loss 0.1074\n",
            "epoch 56/250, batch 31/75, loss 0.0915\n",
            "epoch 56/250, batch 32/75, loss 0.1225\n",
            "epoch 56/250, batch 33/75, loss 0.0995\n",
            "epoch 56/250, batch 34/75, loss 0.1405\n",
            "epoch 56/250, batch 35/75, loss 0.1005\n",
            "epoch 56/250, batch 36/75, loss 0.9027\n",
            "epoch 56/250, batch 37/75, loss 0.1208\n",
            "epoch 56/250, batch 38/75, loss 0.0909\n",
            "epoch 56/250, batch 39/75, loss 0.1471\n",
            "epoch 56/250, batch 40/75, loss 0.1109\n",
            "epoch 56/250, batch 41/75, loss 0.2106\n",
            "epoch 56/250, batch 42/75, loss 0.1338\n",
            "epoch 56/250, batch 43/75, loss 0.1018\n",
            "epoch 56/250, batch 44/75, loss 0.1073\n",
            "epoch 56/250, batch 45/75, loss 0.1514\n",
            "epoch 56/250, batch 46/75, loss 0.0546\n",
            "epoch 56/250, batch 47/75, loss 0.1339\n",
            "epoch 56/250, batch 48/75, loss 0.0798\n",
            "epoch 56/250, batch 49/75, loss 0.0966\n",
            "epoch 56/250, batch 50/75, loss 0.0905\n",
            "epoch 56/250, batch 51/75, loss 0.1079\n",
            "epoch 56/250, batch 52/75, loss 0.0530\n",
            "epoch 56/250, batch 53/75, loss 4.2072\n",
            "epoch 56/250, batch 54/75, loss 0.1511\n",
            "epoch 56/250, batch 55/75, loss 0.1247\n",
            "epoch 56/250, batch 56/75, loss 0.0819\n",
            "epoch 56/250, batch 57/75, loss 0.0737\n",
            "epoch 56/250, batch 58/75, loss 0.1064\n",
            "epoch 56/250, batch 59/75, loss 0.1185\n",
            "epoch 56/250, batch 60/75, loss 0.0881\n",
            "epoch 56/250, batch 61/75, loss 0.1364\n",
            "epoch 56/250, batch 62/75, loss 0.2163\n",
            "epoch 56/250, batch 63/75, loss 0.0850\n",
            "epoch 56/250, batch 64/75, loss 0.1539\n",
            "epoch 56/250, batch 65/75, loss 0.2140\n",
            "epoch 56/250, batch 66/75, loss 0.1869\n",
            "epoch 56/250, batch 67/75, loss 0.1136\n",
            "epoch 56/250, batch 68/75, loss 0.1278\n",
            "epoch 56/250, batch 69/75, loss 0.1697\n",
            "epoch 56/250, batch 70/75, loss 0.0741\n",
            "epoch 56/250, batch 71/75, loss 0.1694\n",
            "epoch 56/250, batch 72/75, loss 0.1519\n",
            "epoch 56/250, batch 73/75, loss 0.1351\n",
            "epoch 56/250, batch 74/75, loss 0.1798\n",
            "epoch 56/250, batch 75/75, loss 0.0914\n",
            "epoch 56/250, training roc_auc_score 0.9882\n",
            "EarlyStopping counter: 44 out of 50\n",
            "epoch 56/250, validation roc_auc_score 0.7816, best validation roc_auc_score 0.8516\n",
            "epoch 57/250, batch 1/75, loss 0.1747\n",
            "epoch 57/250, batch 2/75, loss 0.2253\n",
            "epoch 57/250, batch 3/75, loss 0.0939\n",
            "epoch 57/250, batch 4/75, loss 0.1697\n",
            "epoch 57/250, batch 5/75, loss 0.0444\n",
            "epoch 57/250, batch 6/75, loss 0.0417\n",
            "epoch 57/250, batch 7/75, loss 0.1606\n",
            "epoch 57/250, batch 8/75, loss 0.6487\n",
            "epoch 57/250, batch 9/75, loss 0.0529\n",
            "epoch 57/250, batch 10/75, loss 0.2332\n",
            "epoch 57/250, batch 11/75, loss 0.1647\n",
            "epoch 57/250, batch 12/75, loss 0.0809\n",
            "epoch 57/250, batch 13/75, loss 0.5431\n",
            "epoch 57/250, batch 14/75, loss 0.0888\n",
            "epoch 57/250, batch 15/75, loss 0.6041\n",
            "epoch 57/250, batch 16/75, loss 0.1009\n",
            "epoch 57/250, batch 17/75, loss 0.1053\n",
            "epoch 57/250, batch 18/75, loss 0.2581\n",
            "epoch 57/250, batch 19/75, loss 0.1899\n",
            "epoch 57/250, batch 20/75, loss 0.4112\n",
            "epoch 57/250, batch 21/75, loss 0.0544\n",
            "epoch 57/250, batch 22/75, loss 0.0856\n",
            "epoch 57/250, batch 23/75, loss 0.0778\n",
            "epoch 57/250, batch 24/75, loss 0.0461\n",
            "epoch 57/250, batch 25/75, loss 0.0881\n",
            "epoch 57/250, batch 26/75, loss 0.4178\n",
            "epoch 57/250, batch 27/75, loss 0.1790\n",
            "epoch 57/250, batch 28/75, loss 0.1342\n",
            "epoch 57/250, batch 29/75, loss 0.0768\n",
            "epoch 57/250, batch 30/75, loss 0.1146\n",
            "epoch 57/250, batch 31/75, loss 0.0760\n",
            "epoch 57/250, batch 32/75, loss 0.1016\n",
            "epoch 57/250, batch 33/75, loss 0.0894\n",
            "epoch 57/250, batch 34/75, loss 0.1046\n",
            "epoch 57/250, batch 35/75, loss 0.0945\n",
            "epoch 57/250, batch 36/75, loss 1.2690\n",
            "epoch 57/250, batch 37/75, loss 0.1139\n",
            "epoch 57/250, batch 38/75, loss 0.0909\n",
            "epoch 57/250, batch 39/75, loss 0.0965\n",
            "epoch 57/250, batch 40/75, loss 0.0881\n",
            "epoch 57/250, batch 41/75, loss 0.1887\n",
            "epoch 57/250, batch 42/75, loss 0.1026\n",
            "epoch 57/250, batch 43/75, loss 0.0745\n",
            "epoch 57/250, batch 44/75, loss 0.1005\n",
            "epoch 57/250, batch 45/75, loss 0.1044\n",
            "epoch 57/250, batch 46/75, loss 0.0387\n",
            "epoch 57/250, batch 47/75, loss 0.1041\n",
            "epoch 57/250, batch 48/75, loss 0.0505\n",
            "epoch 57/250, batch 49/75, loss 0.0982\n",
            "epoch 57/250, batch 50/75, loss 0.0793\n",
            "epoch 57/250, batch 51/75, loss 0.0676\n",
            "epoch 57/250, batch 52/75, loss 0.0469\n",
            "epoch 57/250, batch 53/75, loss 3.9817\n",
            "epoch 57/250, batch 54/75, loss 0.1575\n",
            "epoch 57/250, batch 55/75, loss 0.1165\n",
            "epoch 57/250, batch 56/75, loss 0.0638\n",
            "epoch 57/250, batch 57/75, loss 0.0707\n",
            "epoch 57/250, batch 58/75, loss 0.1057\n",
            "epoch 57/250, batch 59/75, loss 0.0919\n",
            "epoch 57/250, batch 60/75, loss 0.0817\n",
            "epoch 57/250, batch 61/75, loss 0.1055\n",
            "epoch 57/250, batch 62/75, loss 0.1582\n",
            "epoch 57/250, batch 63/75, loss 0.0728\n",
            "epoch 57/250, batch 64/75, loss 0.1203\n",
            "epoch 57/250, batch 65/75, loss 0.1160\n",
            "epoch 57/250, batch 66/75, loss 0.1116\n",
            "epoch 57/250, batch 67/75, loss 0.0683\n",
            "epoch 57/250, batch 68/75, loss 0.0631\n",
            "epoch 57/250, batch 69/75, loss 0.1600\n",
            "epoch 57/250, batch 70/75, loss 0.0730\n",
            "epoch 57/250, batch 71/75, loss 0.1314\n",
            "epoch 57/250, batch 72/75, loss 0.0904\n",
            "epoch 57/250, batch 73/75, loss 0.1111\n",
            "epoch 57/250, batch 74/75, loss 0.1286\n",
            "epoch 57/250, batch 75/75, loss 0.0859\n",
            "epoch 57/250, training roc_auc_score 0.9893\n",
            "EarlyStopping counter: 45 out of 50\n",
            "epoch 57/250, validation roc_auc_score 0.7507, best validation roc_auc_score 0.8516\n",
            "epoch 58/250, batch 1/75, loss 0.1376\n",
            "epoch 58/250, batch 2/75, loss 0.1611\n",
            "epoch 58/250, batch 3/75, loss 0.0786\n",
            "epoch 58/250, batch 4/75, loss 0.1367\n",
            "epoch 58/250, batch 5/75, loss 0.0394\n",
            "epoch 58/250, batch 6/75, loss 0.0417\n",
            "epoch 58/250, batch 7/75, loss 0.0896\n",
            "epoch 58/250, batch 8/75, loss 0.3360\n",
            "epoch 58/250, batch 9/75, loss 0.0566\n",
            "epoch 58/250, batch 10/75, loss 0.0977\n",
            "epoch 58/250, batch 11/75, loss 0.1099\n",
            "epoch 58/250, batch 12/75, loss 0.1191\n",
            "epoch 58/250, batch 13/75, loss 0.2313\n",
            "epoch 58/250, batch 14/75, loss 0.0464\n",
            "epoch 58/250, batch 15/75, loss 0.3940\n",
            "epoch 58/250, batch 16/75, loss 0.1162\n",
            "epoch 58/250, batch 17/75, loss 0.1236\n",
            "epoch 58/250, batch 18/75, loss 0.1741\n",
            "epoch 58/250, batch 19/75, loss 0.1361\n",
            "epoch 58/250, batch 20/75, loss 0.2436\n",
            "epoch 58/250, batch 21/75, loss 0.0428\n",
            "epoch 58/250, batch 22/75, loss 0.0707\n",
            "epoch 58/250, batch 23/75, loss 0.0767\n",
            "epoch 58/250, batch 24/75, loss 0.0300\n",
            "epoch 58/250, batch 25/75, loss 0.0409\n",
            "epoch 58/250, batch 26/75, loss 0.5550\n",
            "epoch 58/250, batch 27/75, loss 0.1327\n",
            "epoch 58/250, batch 28/75, loss 0.0962\n",
            "epoch 58/250, batch 29/75, loss 0.0426\n",
            "epoch 58/250, batch 30/75, loss 0.0796\n",
            "epoch 58/250, batch 31/75, loss 0.0478\n",
            "epoch 58/250, batch 32/75, loss 0.0793\n",
            "epoch 58/250, batch 33/75, loss 0.0660\n",
            "epoch 58/250, batch 34/75, loss 0.0831\n",
            "epoch 58/250, batch 35/75, loss 0.0658\n",
            "epoch 58/250, batch 36/75, loss 0.8853\n",
            "epoch 58/250, batch 37/75, loss 0.0924\n",
            "epoch 58/250, batch 38/75, loss 0.0539\n",
            "epoch 58/250, batch 39/75, loss 0.0933\n",
            "epoch 58/250, batch 40/75, loss 0.0761\n",
            "epoch 58/250, batch 41/75, loss 0.1533\n",
            "epoch 58/250, batch 42/75, loss 0.0877\n",
            "epoch 58/250, batch 43/75, loss 0.0661\n",
            "epoch 58/250, batch 44/75, loss 0.0767\n",
            "epoch 58/250, batch 45/75, loss 0.1206\n",
            "epoch 58/250, batch 46/75, loss 0.0422\n",
            "epoch 58/250, batch 47/75, loss 0.1001\n",
            "epoch 58/250, batch 48/75, loss 0.0499\n",
            "epoch 58/250, batch 49/75, loss 0.0832\n",
            "epoch 58/250, batch 50/75, loss 0.0578\n",
            "epoch 58/250, batch 51/75, loss 0.0747\n",
            "epoch 58/250, batch 52/75, loss 0.0488\n",
            "epoch 58/250, batch 53/75, loss 4.1242\n",
            "epoch 58/250, batch 54/75, loss 0.1331\n",
            "epoch 58/250, batch 55/75, loss 0.1049\n",
            "epoch 58/250, batch 56/75, loss 0.0618\n",
            "epoch 58/250, batch 57/75, loss 0.0540\n",
            "epoch 58/250, batch 58/75, loss 0.0888\n",
            "epoch 58/250, batch 59/75, loss 0.0976\n",
            "epoch 58/250, batch 60/75, loss 0.0959\n",
            "epoch 58/250, batch 61/75, loss 0.1204\n",
            "epoch 58/250, batch 62/75, loss 0.1519\n",
            "epoch 58/250, batch 63/75, loss 0.0764\n",
            "epoch 58/250, batch 64/75, loss 0.1408\n",
            "epoch 58/250, batch 65/75, loss 0.1643\n",
            "epoch 58/250, batch 66/75, loss 0.1545\n",
            "epoch 58/250, batch 67/75, loss 0.0850\n",
            "epoch 58/250, batch 68/75, loss 0.1106\n",
            "epoch 58/250, batch 69/75, loss 0.1571\n",
            "epoch 58/250, batch 70/75, loss 0.0826\n",
            "epoch 58/250, batch 71/75, loss 0.1675\n",
            "epoch 58/250, batch 72/75, loss 0.1051\n",
            "epoch 58/250, batch 73/75, loss 0.1428\n",
            "epoch 58/250, batch 74/75, loss 0.1437\n",
            "epoch 58/250, batch 75/75, loss 0.1000\n",
            "epoch 58/250, training roc_auc_score 0.9929\n",
            "EarlyStopping counter: 46 out of 50\n",
            "epoch 58/250, validation roc_auc_score 0.7401, best validation roc_auc_score 0.8516\n",
            "epoch 59/250, batch 1/75, loss 0.1423\n",
            "epoch 59/250, batch 2/75, loss 0.1280\n",
            "epoch 59/250, batch 3/75, loss 0.0722\n",
            "epoch 59/250, batch 4/75, loss 0.3167\n",
            "epoch 59/250, batch 5/75, loss 0.0386\n",
            "epoch 59/250, batch 6/75, loss 0.0466\n",
            "epoch 59/250, batch 7/75, loss 0.1764\n",
            "epoch 59/250, batch 8/75, loss 0.5147\n",
            "epoch 59/250, batch 9/75, loss 0.0776\n",
            "epoch 59/250, batch 10/75, loss 0.2234\n",
            "epoch 59/250, batch 11/75, loss 0.1087\n",
            "epoch 59/250, batch 12/75, loss 0.1043\n",
            "epoch 59/250, batch 13/75, loss 0.2126\n",
            "epoch 59/250, batch 14/75, loss 0.0420\n",
            "epoch 59/250, batch 15/75, loss 1.0842\n",
            "epoch 59/250, batch 16/75, loss 0.1071\n",
            "epoch 59/250, batch 17/75, loss 0.1245\n",
            "epoch 59/250, batch 18/75, loss 0.1816\n",
            "epoch 59/250, batch 19/75, loss 0.1582\n",
            "epoch 59/250, batch 20/75, loss 0.1759\n",
            "epoch 59/250, batch 21/75, loss 0.0650\n",
            "epoch 59/250, batch 22/75, loss 0.0982\n",
            "epoch 59/250, batch 23/75, loss 0.0726\n",
            "epoch 59/250, batch 24/75, loss 0.0368\n",
            "epoch 59/250, batch 25/75, loss 0.0546\n",
            "epoch 59/250, batch 26/75, loss 1.7436\n",
            "epoch 59/250, batch 27/75, loss 0.1641\n",
            "epoch 59/250, batch 28/75, loss 0.1014\n",
            "epoch 59/250, batch 29/75, loss 0.1263\n",
            "epoch 59/250, batch 30/75, loss 0.1247\n",
            "epoch 59/250, batch 31/75, loss 0.0675\n",
            "epoch 59/250, batch 32/75, loss 0.1196\n",
            "epoch 59/250, batch 33/75, loss 0.0956\n",
            "epoch 59/250, batch 34/75, loss 0.1450\n",
            "epoch 59/250, batch 35/75, loss 0.2025\n",
            "epoch 59/250, batch 36/75, loss 0.9213\n",
            "epoch 59/250, batch 37/75, loss 0.1351\n",
            "epoch 59/250, batch 38/75, loss 0.1271\n",
            "epoch 59/250, batch 39/75, loss 0.1761\n",
            "epoch 59/250, batch 40/75, loss 0.1504\n",
            "epoch 59/250, batch 41/75, loss 0.2400\n",
            "epoch 59/250, batch 42/75, loss 0.1301\n",
            "epoch 59/250, batch 43/75, loss 0.1004\n",
            "epoch 59/250, batch 44/75, loss 0.1467\n",
            "epoch 59/250, batch 45/75, loss 0.1626\n",
            "epoch 59/250, batch 46/75, loss 0.0499\n",
            "epoch 59/250, batch 47/75, loss 0.1785\n",
            "epoch 59/250, batch 48/75, loss 0.0938\n",
            "epoch 59/250, batch 49/75, loss 0.1085\n",
            "epoch 59/250, batch 50/75, loss 0.0942\n",
            "epoch 59/250, batch 51/75, loss 0.0856\n",
            "epoch 59/250, batch 52/75, loss 0.0502\n",
            "epoch 59/250, batch 53/75, loss 4.8786\n",
            "epoch 59/250, batch 54/75, loss 0.1882\n",
            "epoch 59/250, batch 55/75, loss 0.1064\n",
            "epoch 59/250, batch 56/75, loss 0.0751\n",
            "epoch 59/250, batch 57/75, loss 0.0612\n",
            "epoch 59/250, batch 58/75, loss 0.0997\n",
            "epoch 59/250, batch 59/75, loss 0.0799\n",
            "epoch 59/250, batch 60/75, loss 0.1035\n",
            "epoch 59/250, batch 61/75, loss 0.1244\n",
            "epoch 59/250, batch 62/75, loss 0.1445\n",
            "epoch 59/250, batch 63/75, loss 0.0639\n",
            "epoch 59/250, batch 64/75, loss 0.1257\n",
            "epoch 59/250, batch 65/75, loss 0.1540\n",
            "epoch 59/250, batch 66/75, loss 0.1610\n",
            "epoch 59/250, batch 67/75, loss 0.0838\n",
            "epoch 59/250, batch 68/75, loss 0.0984\n",
            "epoch 59/250, batch 69/75, loss 0.1283\n",
            "epoch 59/250, batch 70/75, loss 0.0931\n",
            "epoch 59/250, batch 71/75, loss 0.1973\n",
            "epoch 59/250, batch 72/75, loss 0.1134\n",
            "epoch 59/250, batch 73/75, loss 0.1287\n",
            "epoch 59/250, batch 74/75, loss 0.2195\n",
            "epoch 59/250, batch 75/75, loss 0.1068\n",
            "epoch 59/250, training roc_auc_score 0.9844\n",
            "EarlyStopping counter: 47 out of 50\n",
            "epoch 59/250, validation roc_auc_score 0.8222, best validation roc_auc_score 0.8516\n",
            "epoch 60/250, batch 1/75, loss 0.1838\n",
            "epoch 60/250, batch 2/75, loss 0.2243\n",
            "epoch 60/250, batch 3/75, loss 0.1014\n",
            "epoch 60/250, batch 4/75, loss 0.1448\n",
            "epoch 60/250, batch 5/75, loss 0.0353\n",
            "epoch 60/250, batch 6/75, loss 0.0417\n",
            "epoch 60/250, batch 7/75, loss 0.1709\n",
            "epoch 60/250, batch 8/75, loss 0.2561\n",
            "epoch 60/250, batch 9/75, loss 0.0725\n",
            "epoch 60/250, batch 10/75, loss 0.1699\n",
            "epoch 60/250, batch 11/75, loss 0.1577\n",
            "epoch 60/250, batch 12/75, loss 0.1000\n",
            "epoch 60/250, batch 13/75, loss 0.3045\n",
            "epoch 60/250, batch 14/75, loss 0.1135\n",
            "epoch 60/250, batch 15/75, loss 0.8738\n",
            "epoch 60/250, batch 16/75, loss 0.1118\n",
            "epoch 60/250, batch 17/75, loss 0.1445\n",
            "epoch 60/250, batch 18/75, loss 0.2330\n",
            "epoch 60/250, batch 19/75, loss 0.1726\n",
            "epoch 60/250, batch 20/75, loss 0.1752\n",
            "epoch 60/250, batch 21/75, loss 0.0367\n",
            "epoch 60/250, batch 22/75, loss 0.0660\n",
            "epoch 60/250, batch 23/75, loss 0.0771\n",
            "epoch 60/250, batch 24/75, loss 0.0346\n",
            "epoch 60/250, batch 25/75, loss 0.0549\n",
            "epoch 60/250, batch 26/75, loss 0.4936\n",
            "epoch 60/250, batch 27/75, loss 0.1289\n",
            "epoch 60/250, batch 28/75, loss 0.1299\n",
            "epoch 60/250, batch 29/75, loss 0.0610\n",
            "epoch 60/250, batch 30/75, loss 0.0589\n",
            "epoch 60/250, batch 31/75, loss 0.0535\n",
            "epoch 60/250, batch 32/75, loss 0.0760\n",
            "epoch 60/250, batch 33/75, loss 0.0725\n",
            "epoch 60/250, batch 34/75, loss 0.0958\n",
            "epoch 60/250, batch 35/75, loss 0.0973\n",
            "epoch 60/250, batch 36/75, loss 0.8988\n",
            "epoch 60/250, batch 37/75, loss 0.0920\n",
            "epoch 60/250, batch 38/75, loss 0.0692\n",
            "epoch 60/250, batch 39/75, loss 0.0984\n",
            "epoch 60/250, batch 40/75, loss 0.0716\n",
            "epoch 60/250, batch 41/75, loss 0.2201\n",
            "epoch 60/250, batch 42/75, loss 0.0960\n",
            "epoch 60/250, batch 43/75, loss 0.0754\n",
            "epoch 60/250, batch 44/75, loss 0.1062\n",
            "epoch 60/250, batch 45/75, loss 0.1076\n",
            "epoch 60/250, batch 46/75, loss 0.0383\n",
            "epoch 60/250, batch 47/75, loss 0.0886\n",
            "epoch 60/250, batch 48/75, loss 0.0518\n",
            "epoch 60/250, batch 49/75, loss 0.0914\n",
            "epoch 60/250, batch 50/75, loss 0.0502\n",
            "epoch 60/250, batch 51/75, loss 0.0601\n",
            "epoch 60/250, batch 52/75, loss 0.0392\n",
            "epoch 60/250, batch 53/75, loss 4.0523\n",
            "epoch 60/250, batch 54/75, loss 0.1530\n",
            "epoch 60/250, batch 55/75, loss 0.0955\n",
            "epoch 60/250, batch 56/75, loss 0.0701\n",
            "epoch 60/250, batch 57/75, loss 0.0709\n",
            "epoch 60/250, batch 58/75, loss 0.1111\n",
            "epoch 60/250, batch 59/75, loss 0.0735\n",
            "epoch 60/250, batch 60/75, loss 0.0975\n",
            "epoch 60/250, batch 61/75, loss 0.1443\n",
            "epoch 60/250, batch 62/75, loss 0.1741\n",
            "epoch 60/250, batch 63/75, loss 0.0713\n",
            "epoch 60/250, batch 64/75, loss 0.1464\n",
            "epoch 60/250, batch 65/75, loss 0.1197\n",
            "epoch 60/250, batch 66/75, loss 0.1656\n",
            "epoch 60/250, batch 67/75, loss 0.0665\n",
            "epoch 60/250, batch 68/75, loss 0.0889\n",
            "epoch 60/250, batch 69/75, loss 0.1204\n",
            "epoch 60/250, batch 70/75, loss 0.0677\n",
            "epoch 60/250, batch 71/75, loss 0.1293\n",
            "epoch 60/250, batch 72/75, loss 0.1024\n",
            "epoch 60/250, batch 73/75, loss 0.1374\n",
            "epoch 60/250, batch 74/75, loss 0.1648\n",
            "epoch 60/250, batch 75/75, loss 0.0990\n",
            "epoch 60/250, training roc_auc_score 0.9917\n",
            "EarlyStopping counter: 48 out of 50\n",
            "epoch 60/250, validation roc_auc_score 0.7988, best validation roc_auc_score 0.8516\n",
            "epoch 61/250, batch 1/75, loss 0.1660\n",
            "epoch 61/250, batch 2/75, loss 0.3585\n",
            "epoch 61/250, batch 3/75, loss 0.1017\n",
            "epoch 61/250, batch 4/75, loss 0.1658\n",
            "epoch 61/250, batch 5/75, loss 0.0320\n",
            "epoch 61/250, batch 6/75, loss 0.0348\n",
            "epoch 61/250, batch 7/75, loss 0.1320\n",
            "epoch 61/250, batch 8/75, loss 0.1765\n",
            "epoch 61/250, batch 9/75, loss 0.0610\n",
            "epoch 61/250, batch 10/75, loss 0.4510\n",
            "epoch 61/250, batch 11/75, loss 0.1282\n",
            "epoch 61/250, batch 12/75, loss 0.1388\n",
            "epoch 61/250, batch 13/75, loss 0.1976\n",
            "epoch 61/250, batch 14/75, loss 0.0402\n",
            "epoch 61/250, batch 15/75, loss 0.4900\n",
            "epoch 61/250, batch 16/75, loss 0.1038\n",
            "epoch 61/250, batch 17/75, loss 0.0826\n",
            "epoch 61/250, batch 18/75, loss 0.1641\n",
            "epoch 61/250, batch 19/75, loss 0.1149\n",
            "epoch 61/250, batch 20/75, loss 0.5031\n",
            "epoch 61/250, batch 21/75, loss 0.0558\n",
            "epoch 61/250, batch 22/75, loss 0.1163\n",
            "epoch 61/250, batch 23/75, loss 0.0666\n",
            "epoch 61/250, batch 24/75, loss 0.0303\n",
            "epoch 61/250, batch 25/75, loss 0.0556\n",
            "epoch 61/250, batch 26/75, loss 0.3919\n",
            "epoch 61/250, batch 27/75, loss 0.1652\n",
            "epoch 61/250, batch 28/75, loss 0.1232\n",
            "epoch 61/250, batch 29/75, loss 0.0638\n",
            "epoch 61/250, batch 30/75, loss 0.0922\n",
            "epoch 61/250, batch 31/75, loss 0.0581\n",
            "epoch 61/250, batch 32/75, loss 0.0962\n",
            "epoch 61/250, batch 33/75, loss 0.1145\n",
            "epoch 61/250, batch 34/75, loss 0.0964\n",
            "epoch 61/250, batch 35/75, loss 0.1070\n",
            "epoch 61/250, batch 36/75, loss 0.4098\n",
            "epoch 61/250, batch 37/75, loss 0.1122\n",
            "epoch 61/250, batch 38/75, loss 0.0775\n",
            "epoch 61/250, batch 39/75, loss 0.1159\n",
            "epoch 61/250, batch 40/75, loss 0.0640\n",
            "epoch 61/250, batch 41/75, loss 0.1711\n",
            "epoch 61/250, batch 42/75, loss 0.1125\n",
            "epoch 61/250, batch 43/75, loss 0.0637\n",
            "epoch 61/250, batch 44/75, loss 0.0753\n",
            "epoch 61/250, batch 45/75, loss 0.1056\n",
            "epoch 61/250, batch 46/75, loss 0.0371\n",
            "epoch 61/250, batch 47/75, loss 0.0933\n",
            "epoch 61/250, batch 48/75, loss 0.0612\n",
            "epoch 61/250, batch 49/75, loss 0.0841\n",
            "epoch 61/250, batch 50/75, loss 0.0602\n",
            "epoch 61/250, batch 51/75, loss 0.0671\n",
            "epoch 61/250, batch 52/75, loss 0.0461\n",
            "epoch 61/250, batch 53/75, loss 2.6990\n",
            "epoch 61/250, batch 54/75, loss 0.1197\n",
            "epoch 61/250, batch 55/75, loss 0.0944\n",
            "epoch 61/250, batch 56/75, loss 0.0553\n",
            "epoch 61/250, batch 57/75, loss 0.0522\n",
            "epoch 61/250, batch 58/75, loss 0.0769\n",
            "epoch 61/250, batch 59/75, loss 0.0678\n",
            "epoch 61/250, batch 60/75, loss 0.0669\n",
            "epoch 61/250, batch 61/75, loss 0.0911\n",
            "epoch 61/250, batch 62/75, loss 0.1124\n",
            "epoch 61/250, batch 63/75, loss 0.0522\n",
            "epoch 61/250, batch 64/75, loss 0.0993\n",
            "epoch 61/250, batch 65/75, loss 0.0912\n",
            "epoch 61/250, batch 66/75, loss 0.1053\n",
            "epoch 61/250, batch 67/75, loss 0.0583\n",
            "epoch 61/250, batch 68/75, loss 0.0680\n",
            "epoch 61/250, batch 69/75, loss 0.0947\n",
            "epoch 61/250, batch 70/75, loss 0.0433\n",
            "epoch 61/250, batch 71/75, loss 0.1028\n",
            "epoch 61/250, batch 72/75, loss 0.0797\n",
            "epoch 61/250, batch 73/75, loss 0.0795\n",
            "epoch 61/250, batch 74/75, loss 0.1351\n",
            "epoch 61/250, batch 75/75, loss 0.0566\n",
            "epoch 61/250, training roc_auc_score 0.9953\n",
            "EarlyStopping counter: 49 out of 50\n",
            "epoch 61/250, validation roc_auc_score 0.8214, best validation roc_auc_score 0.8516\n",
            "epoch 62/250, batch 1/75, loss 0.1262\n",
            "epoch 62/250, batch 2/75, loss 0.0829\n",
            "epoch 62/250, batch 3/75, loss 0.0757\n",
            "epoch 62/250, batch 4/75, loss 0.1674\n",
            "epoch 62/250, batch 5/75, loss 0.0319\n",
            "epoch 62/250, batch 6/75, loss 0.0342\n",
            "epoch 62/250, batch 7/75, loss 0.1074\n",
            "epoch 62/250, batch 8/75, loss 0.1438\n",
            "epoch 62/250, batch 9/75, loss 0.0446\n",
            "epoch 62/250, batch 10/75, loss 0.0630\n",
            "epoch 62/250, batch 11/75, loss 0.1528\n",
            "epoch 62/250, batch 12/75, loss 0.1118\n",
            "epoch 62/250, batch 13/75, loss 0.1574\n",
            "epoch 62/250, batch 14/75, loss 0.0463\n",
            "epoch 62/250, batch 15/75, loss 0.2906\n",
            "epoch 62/250, batch 16/75, loss 0.0715\n",
            "epoch 62/250, batch 17/75, loss 0.0647\n",
            "epoch 62/250, batch 18/75, loss 0.1254\n",
            "epoch 62/250, batch 19/75, loss 0.1031\n",
            "epoch 62/250, batch 20/75, loss 0.1870\n",
            "epoch 62/250, batch 21/75, loss 0.0378\n",
            "epoch 62/250, batch 22/75, loss 0.0860\n",
            "epoch 62/250, batch 23/75, loss 0.0545\n",
            "epoch 62/250, batch 24/75, loss 0.0307\n",
            "epoch 62/250, batch 25/75, loss 0.0352\n",
            "epoch 62/250, batch 26/75, loss 0.4023\n",
            "epoch 62/250, batch 27/75, loss 0.1039\n",
            "epoch 62/250, batch 28/75, loss 0.0964\n",
            "epoch 62/250, batch 29/75, loss 0.0479\n",
            "epoch 62/250, batch 30/75, loss 0.0579\n",
            "epoch 62/250, batch 31/75, loss 0.0425\n",
            "epoch 62/250, batch 32/75, loss 0.0539\n",
            "epoch 62/250, batch 33/75, loss 0.0877\n",
            "epoch 62/250, batch 34/75, loss 0.0610\n",
            "epoch 62/250, batch 35/75, loss 0.0648\n",
            "epoch 62/250, batch 36/75, loss 0.4930\n",
            "epoch 62/250, batch 37/75, loss 0.0902\n",
            "epoch 62/250, batch 38/75, loss 0.0512\n",
            "epoch 62/250, batch 39/75, loss 0.0769\n",
            "epoch 62/250, batch 40/75, loss 0.0649\n",
            "epoch 62/250, batch 41/75, loss 0.1304\n",
            "epoch 62/250, batch 42/75, loss 0.0787\n",
            "epoch 62/250, batch 43/75, loss 0.0537\n",
            "epoch 62/250, batch 44/75, loss 0.0729\n",
            "epoch 62/250, batch 45/75, loss 0.0895\n",
            "epoch 62/250, batch 46/75, loss 0.0326\n",
            "epoch 62/250, batch 47/75, loss 0.0670\n",
            "epoch 62/250, batch 48/75, loss 0.0480\n",
            "epoch 62/250, batch 49/75, loss 0.0820\n",
            "epoch 62/250, batch 50/75, loss 0.0525\n",
            "epoch 62/250, batch 51/75, loss 0.0534\n",
            "epoch 62/250, batch 52/75, loss 0.0335\n",
            "epoch 62/250, batch 53/75, loss 2.6025\n",
            "epoch 62/250, batch 54/75, loss 0.0967\n",
            "epoch 62/250, batch 55/75, loss 0.0763\n",
            "epoch 62/250, batch 56/75, loss 0.0557\n",
            "epoch 62/250, batch 57/75, loss 0.0558\n",
            "epoch 62/250, batch 58/75, loss 0.0873\n",
            "epoch 62/250, batch 59/75, loss 0.0709\n",
            "epoch 62/250, batch 60/75, loss 0.0722\n",
            "epoch 62/250, batch 61/75, loss 0.1096\n",
            "epoch 62/250, batch 62/75, loss 0.1497\n",
            "epoch 62/250, batch 63/75, loss 0.0559\n",
            "epoch 62/250, batch 64/75, loss 0.1088\n",
            "epoch 62/250, batch 65/75, loss 0.1054\n",
            "epoch 62/250, batch 66/75, loss 0.0912\n",
            "epoch 62/250, batch 67/75, loss 0.0519\n",
            "epoch 62/250, batch 68/75, loss 0.0578\n",
            "epoch 62/250, batch 69/75, loss 0.1298\n",
            "epoch 62/250, batch 70/75, loss 0.0664\n",
            "epoch 62/250, batch 71/75, loss 0.1519\n",
            "epoch 62/250, batch 72/75, loss 0.0983\n",
            "epoch 62/250, batch 73/75, loss 0.1128\n",
            "epoch 62/250, batch 74/75, loss 0.1354\n",
            "epoch 62/250, batch 75/75, loss 0.0793\n",
            "epoch 62/250, training roc_auc_score 0.9976\n",
            "EarlyStopping counter: 50 out of 50\n",
            "epoch 62/250, validation roc_auc_score 0.7332, best validation roc_auc_score 0.8516\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YaWpHl2Gnr-",
        "colab_type": "code",
        "outputId": "692eea70-5f78-46ea-d6e8-bfb7b07bf0f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "stopper.load_checkpoint(model)\n",
        "test_score = run_an_eval_epoch(args, model, test_loader)\n",
        "print('test {} {:.4f}'.format(args['metric_name'], test_score))\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test roc_auc_score 0.7059\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD-OkX5yOhC4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "6f1e01a6-eb64-4c3a-9af4-3540b5010c96"
      },
      "source": [
        "print(type(test_fold_loader_1))\n",
        "print(type(test_fold_loader_2))\n",
        "print(type(test_fold_loader_3))\n",
        "print(type(test_fold_loader_4))\n",
        "print(type(test_fold_loader_5))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.utils.data.dataloader.DataLoader'>\n",
            "<class 'torch.utils.data.dataloader.DataLoader'>\n",
            "<class 'torch.utils.data.dataloader.DataLoader'>\n",
            "<class 'torch.utils.data.dataloader.DataLoader'>\n",
            "<class 'torch.utils.data.dataloader.DataLoader'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLkl062SLtaF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3a271aad-da2c-4e93-bbbe-14bb8078be99"
      },
      "source": [
        "stopper.load_checkpoint(model)\n",
        "test_score = run_an_eval_epoch(args, model, test_fold_loader_1)\n",
        "print('test_fold_1 {} {:.4f}'.format(args['metric_name'], test_score))\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_fold_1 roc_auc_score 0.7348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8vN_qzeOaFl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e8654017-67dd-4a12-a416-459e97672423"
      },
      "source": [
        "stopper.load_checkpoint(model)\n",
        "test_score = run_an_eval_epoch(args, model, test_fold_loader_2)\n",
        "print('test_fold_2 {} {:.4f}'.format(args['metric_name'], test_score))\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_fold_2 roc_auc_score 0.5212\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrOwszjOPNQ3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "88f67417-3a3b-403d-a0a6-a90b902d4830"
      },
      "source": [
        "stopper.load_checkpoint(model)\n",
        "test_score = run_an_eval_epoch(args, model, test_fold_loader_3)\n",
        "print('test_fold_3 {} {:.4f}'.format(args['metric_name'], test_score))\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_fold_3 roc_auc_score 0.6725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV4ywP5IPOB1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8831321e-58f6-4ad4-f14e-2b2a019217c6"
      },
      "source": [
        "stopper.load_checkpoint(model)\n",
        "test_score = run_an_eval_epoch(args, model, test_fold_loader_4)\n",
        "print('test_fold_4 {} {:.4f}'.format(args['metric_name'], test_score))\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_fold_4 roc_auc_score 0.8573\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yqQtlwDPPZV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d85615e2-1991-443d-bb2a-3ba3d92dcfeb"
      },
      "source": [
        "stopper.load_checkpoint(model)\n",
        "test_score = run_an_eval_epoch(args, model, test_fold_loader_5)\n",
        "print('test_fold_5 {} {:.4f}'.format(args['metric_name'], test_score))\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_fold_5 roc_auc_score 0.7088\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw7lfMPURlMH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "741a0ac6-ac41-4774-9d6a-30d1b69cb4c6"
      },
      "source": [
        "stopper.load_checkpoint(model)\n",
        "test_score = run_an_eval_epoch(args, model, test_fold_loader_6)\n",
        "print('test_fold_6 {} {:.4f}'.format(args['metric_name'], test_score))\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_fold_6 roc_auc_score 0.6881\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJPUlmPFRlKd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5420871f-ad6e-4cd3-9de1-d7350083a3c7"
      },
      "source": [
        "stopper.load_checkpoint(model)\n",
        "test_score = run_an_eval_epoch(args, model, test_fold_loader_7)\n",
        "print('test_fold_7 {} {:.4f}'.format(args['metric_name'], test_score))\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_fold_7 roc_auc_score 0.6790\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA9UEclTRqhy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "46791b87-ba00-4bba-9638-dce9efcd289b"
      },
      "source": [
        "stopper.load_checkpoint(model)\n",
        "test_score = run_an_eval_epoch(args, model, test_fold_loader_8)\n",
        "print('test_fold_8 {} {:.4f}'.format(args['metric_name'], test_score))\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_fold_8 roc_auc_score 0.7943\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjzKedbJRvS6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3bcdb562-11b6-4999-9398-05b4ab71d608"
      },
      "source": [
        "stopper.load_checkpoint(model)\n",
        "test_score = run_an_eval_epoch(args, model, test_fold_loader_9)\n",
        "print('test_fold_9 {} {:.4f}'.format(args['metric_name'], test_score))\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_fold_9 roc_auc_score 0.7337\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2kTmqvHRv5B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7fe870f3-0b85-40a6-b274-268e9e1fcf42"
      },
      "source": [
        "stopper.load_checkpoint(model)\n",
        "test_score = run_an_eval_epoch(args, model, test_fold_loader_10)\n",
        "print('test_fold_10 {} {:.4f}'.format(args['metric_name'], test_score))\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_fold_10 roc_auc_score 0.6269\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}