{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scaffold_GAT_ncov.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1IR5L6vLnP13En-KHYxyvIsl9QZ55xzwV",
      "authorship_tag": "ABX9TyP8DM4Piw4++iTDewrebCF/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seyonechithrananda/ncov-ligand-protein/blob/master/Scaffold_GAT_ncov.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnhZ7HBLDf73",
        "colab_type": "code",
        "outputId": "8878468d-5845-461d-9571-6c27fc33275e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!wget -c https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!conda install -q -y -c conda-forge rdkit\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-05 13:05:30--  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8303, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 85055499 (81M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-latest-Linux-x86_64.sh’\n",
            "\n",
            "\r          Miniconda   0%[                    ]       0  --.-KB/s               \r         Miniconda3  38%[======>             ]  31.39M   155MB/s               \r        Miniconda3-  77%[==============>     ]  63.15M   157MB/s               \rMiniconda3-latest-L 100%[===================>]  81.12M   167MB/s    in 0.5s    \n",
            "\n",
            "2020-05-05 13:05:30 (167 MB/s) - ‘Miniconda3-latest-Linux-x86_64.sh’ saved [85055499/85055499]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - _libgcc_mutex==0.1=main\n",
            "    - asn1crypto==1.3.0=py37_0\n",
            "    - ca-certificates==2020.1.1=0\n",
            "    - certifi==2019.11.28=py37_0\n",
            "    - cffi==1.14.0=py37h2e261b9_0\n",
            "    - chardet==3.0.4=py37_1003\n",
            "    - conda-package-handling==1.6.0=py37h7b6447c_0\n",
            "    - conda==4.8.2=py37_0\n",
            "    - cryptography==2.8=py37h1ba5d50_0\n",
            "    - idna==2.8=py37_0\n",
            "    - ld_impl_linux-64==2.33.1=h53a641e_7\n",
            "    - libedit==3.1.20181209=hc058e9b_0\n",
            "    - libffi==3.2.1=hd88cf55_4\n",
            "    - libgcc-ng==9.1.0=hdf63c60_0\n",
            "    - libstdcxx-ng==9.1.0=hdf63c60_0\n",
            "    - ncurses==6.2=he6710b0_0\n",
            "    - openssl==1.1.1d=h7b6447c_4\n",
            "    - pip==20.0.2=py37_1\n",
            "    - pycosat==0.6.3=py37h7b6447c_0\n",
            "    - pycparser==2.19=py37_0\n",
            "    - pyopenssl==19.1.0=py37_0\n",
            "    - pysocks==1.7.1=py37_0\n",
            "    - python==3.7.6=h0371630_2\n",
            "    - readline==7.0=h7b6447c_5\n",
            "    - requests==2.22.0=py37_1\n",
            "    - ruamel_yaml==0.15.87=py37h7b6447c_0\n",
            "    - setuptools==45.2.0=py37_0\n",
            "    - six==1.14.0=py37_0\n",
            "    - sqlite==3.31.1=h7b6447c_0\n",
            "    - tk==8.6.8=hbc83047_0\n",
            "    - tqdm==4.42.1=py_0\n",
            "    - urllib3==1.25.8=py37_0\n",
            "    - wheel==0.34.2=py37_0\n",
            "    - xz==5.2.4=h14c3975_4\n",
            "    - yaml==0.1.7=had09818_2\n",
            "    - zlib==1.2.11=h7b6447c_3\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
            "  asn1crypto         pkgs/main/linux-64::asn1crypto-1.3.0-py37_0\n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2020.1.1-0\n",
            "  certifi            pkgs/main/linux-64::certifi-2019.11.28-py37_0\n",
            "  cffi               pkgs/main/linux-64::cffi-1.14.0-py37h2e261b9_0\n",
            "  chardet            pkgs/main/linux-64::chardet-3.0.4-py37_1003\n",
            "  conda              pkgs/main/linux-64::conda-4.8.2-py37_0\n",
            "  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1.6.0-py37h7b6447c_0\n",
            "  cryptography       pkgs/main/linux-64::cryptography-2.8-py37h1ba5d50_0\n",
            "  idna               pkgs/main/linux-64::idna-2.8-py37_0\n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.33.1-h53a641e_7\n",
            "  libedit            pkgs/main/linux-64::libedit-3.1.20181209-hc058e9b_0\n",
            "  libffi             pkgs/main/linux-64::libffi-3.2.1-hd88cf55_4\n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-9.1.0-hdf63c60_0\n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-9.1.0-hdf63c60_0\n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.2-he6710b0_0\n",
            "  openssl            pkgs/main/linux-64::openssl-1.1.1d-h7b6447c_4\n",
            "  pip                pkgs/main/linux-64::pip-20.0.2-py37_1\n",
            "  pycosat            pkgs/main/linux-64::pycosat-0.6.3-py37h7b6447c_0\n",
            "  pycparser          pkgs/main/linux-64::pycparser-2.19-py37_0\n",
            "  pyopenssl          pkgs/main/linux-64::pyopenssl-19.1.0-py37_0\n",
            "  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py37_0\n",
            "  python             pkgs/main/linux-64::python-3.7.6-h0371630_2\n",
            "  readline           pkgs/main/linux-64::readline-7.0-h7b6447c_5\n",
            "  requests           pkgs/main/linux-64::requests-2.22.0-py37_1\n",
            "  ruamel_yaml        pkgs/main/linux-64::ruamel_yaml-0.15.87-py37h7b6447c_0\n",
            "  setuptools         pkgs/main/linux-64::setuptools-45.2.0-py37_0\n",
            "  six                pkgs/main/linux-64::six-1.14.0-py37_0\n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.31.1-h7b6447c_0\n",
            "  tk                 pkgs/main/linux-64::tk-8.6.8-hbc83047_0\n",
            "  tqdm               pkgs/main/noarch::tqdm-4.42.1-py_0\n",
            "  urllib3            pkgs/main/linux-64::urllib3-1.25.8-py37_0\n",
            "  wheel              pkgs/main/linux-64::wheel-0.34.2-py37_0\n",
            "  xz                 pkgs/main/linux-64::xz-5.2.4-h14c3975_4\n",
            "  yaml               pkgs/main/linux-64::yaml-0.1.7-had09818_2\n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.11-h7b6447c_3\n",
            "\n",
            "\n",
            "Preparing transaction: / \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - rdkit\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    boost-1.72.0               |   py37h9de70de_0         316 KB  conda-forge\n",
            "    boost-cpp-1.72.0           |       h8e57a91_0        21.8 MB  conda-forge\n",
            "    bzip2-1.0.8                |       h516909a_2         396 KB  conda-forge\n",
            "    ca-certificates-2020.4.5.1 |       hecc5488_0         146 KB  conda-forge\n",
            "    cairo-1.16.0               |    hcf35c78_1003         1.5 MB  conda-forge\n",
            "    certifi-2020.4.5.1         |   py37hc8dfbb8_0         151 KB  conda-forge\n",
            "    conda-4.8.3                |   py37hc8dfbb8_1         3.0 MB  conda-forge\n",
            "    fontconfig-2.13.1          |    h86ecdb6_1001         340 KB  conda-forge\n",
            "    freetype-2.10.1            |       he06d7ca_0         877 KB  conda-forge\n",
            "    gettext-0.19.8.1           |    hc5be6a0_1002         3.6 MB  conda-forge\n",
            "    glib-2.64.2                |       h6f030ca_0         3.4 MB  conda-forge\n",
            "    icu-64.2                   |       he1b5a44_1        12.6 MB  conda-forge\n",
            "    jpeg-9c                    |    h14c3975_1001         251 KB  conda-forge\n",
            "    libblas-3.8.0              |      14_openblas          10 KB  conda-forge\n",
            "    libcblas-3.8.0             |      14_openblas          10 KB  conda-forge\n",
            "    libgfortran-ng-7.3.0       |       hdf63c60_5         1.7 MB  conda-forge\n",
            "    libiconv-1.15              |    h516909a_1006         2.0 MB  conda-forge\n",
            "    liblapack-3.8.0            |      14_openblas          10 KB  conda-forge\n",
            "    libopenblas-0.3.7          |       h5ec1e0e_6         7.6 MB  conda-forge\n",
            "    libpng-1.6.37              |       hed695b0_1         308 KB  conda-forge\n",
            "    libtiff-4.1.0              |       hc7e4089_6         668 KB  conda-forge\n",
            "    libuuid-2.32.1             |    h14c3975_1000          26 KB  conda-forge\n",
            "    libwebp-base-1.1.0         |       h516909a_3         845 KB  conda-forge\n",
            "    libxcb-1.13                |    h14c3975_1002         396 KB  conda-forge\n",
            "    libxml2-2.9.10             |       hee79883_0         1.3 MB  conda-forge\n",
            "    lz4-c-1.8.3                |    he1b5a44_1001         187 KB  conda-forge\n",
            "    numpy-1.18.4               |   py37h8960a57_0         5.2 MB  conda-forge\n",
            "    olefile-0.46               |             py_0          31 KB  conda-forge\n",
            "    openssl-1.1.1g             |       h516909a_0         2.1 MB  conda-forge\n",
            "    pandas-1.0.3               |   py37h0da4684_1        11.1 MB  conda-forge\n",
            "    pcre-8.44                  |       he1b5a44_0         261 KB  conda-forge\n",
            "    pillow-7.1.2               |   py37hb39fc2d_0         603 KB\n",
            "    pixman-0.38.0              |    h516909a_1003         594 KB  conda-forge\n",
            "    pthread-stubs-0.4          |    h14c3975_1001           5 KB  conda-forge\n",
            "    pycairo-1.19.1             |   py37h01af8b0_3          77 KB  conda-forge\n",
            "    python-dateutil-2.8.1      |             py_0         220 KB  conda-forge\n",
            "    python_abi-3.7             |          1_cp37m           4 KB  conda-forge\n",
            "    pytz-2020.1                |     pyh9f0ad1d_0         227 KB  conda-forge\n",
            "    rdkit-2020.03.1            |   py37hdd87690_3        24.6 MB  conda-forge\n",
            "    xorg-kbproto-1.0.7         |    h14c3975_1002          26 KB  conda-forge\n",
            "    xorg-libice-1.0.10         |       h516909a_0          57 KB  conda-forge\n",
            "    xorg-libsm-1.2.3           |    h84519dc_1000          25 KB  conda-forge\n",
            "    xorg-libx11-1.6.9          |       h516909a_0         918 KB  conda-forge\n",
            "    xorg-libxau-1.0.9          |       h14c3975_0          13 KB  conda-forge\n",
            "    xorg-libxdmcp-1.1.3        |       h516909a_0          18 KB  conda-forge\n",
            "    xorg-libxext-1.3.4         |       h516909a_0          51 KB  conda-forge\n",
            "    xorg-libxrender-0.9.10     |    h516909a_1002          31 KB  conda-forge\n",
            "    xorg-renderproto-0.11.1    |    h14c3975_1002           8 KB  conda-forge\n",
            "    xorg-xextproto-7.3.0       |    h14c3975_1002          27 KB  conda-forge\n",
            "    xorg-xproto-7.0.31         |    h14c3975_1007          72 KB  conda-forge\n",
            "    zstd-1.4.4                 |       h3b9ef0a_2         982 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       110.7 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  boost              conda-forge/linux-64::boost-1.72.0-py37h9de70de_0\n",
            "  boost-cpp          conda-forge/linux-64::boost-cpp-1.72.0-h8e57a91_0\n",
            "  bzip2              conda-forge/linux-64::bzip2-1.0.8-h516909a_2\n",
            "  cairo              conda-forge/linux-64::cairo-1.16.0-hcf35c78_1003\n",
            "  fontconfig         conda-forge/linux-64::fontconfig-2.13.1-h86ecdb6_1001\n",
            "  freetype           conda-forge/linux-64::freetype-2.10.1-he06d7ca_0\n",
            "  gettext            conda-forge/linux-64::gettext-0.19.8.1-hc5be6a0_1002\n",
            "  glib               conda-forge/linux-64::glib-2.64.2-h6f030ca_0\n",
            "  icu                conda-forge/linux-64::icu-64.2-he1b5a44_1\n",
            "  jpeg               conda-forge/linux-64::jpeg-9c-h14c3975_1001\n",
            "  libblas            conda-forge/linux-64::libblas-3.8.0-14_openblas\n",
            "  libcblas           conda-forge/linux-64::libcblas-3.8.0-14_openblas\n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-7.3.0-hdf63c60_5\n",
            "  libiconv           conda-forge/linux-64::libiconv-1.15-h516909a_1006\n",
            "  liblapack          conda-forge/linux-64::liblapack-3.8.0-14_openblas\n",
            "  libopenblas        conda-forge/linux-64::libopenblas-0.3.7-h5ec1e0e_6\n",
            "  libpng             conda-forge/linux-64::libpng-1.6.37-hed695b0_1\n",
            "  libtiff            conda-forge/linux-64::libtiff-4.1.0-hc7e4089_6\n",
            "  libuuid            conda-forge/linux-64::libuuid-2.32.1-h14c3975_1000\n",
            "  libwebp-base       conda-forge/linux-64::libwebp-base-1.1.0-h516909a_3\n",
            "  libxcb             conda-forge/linux-64::libxcb-1.13-h14c3975_1002\n",
            "  libxml2            conda-forge/linux-64::libxml2-2.9.10-hee79883_0\n",
            "  lz4-c              conda-forge/linux-64::lz4-c-1.8.3-he1b5a44_1001\n",
            "  numpy              conda-forge/linux-64::numpy-1.18.4-py37h8960a57_0\n",
            "  olefile            conda-forge/noarch::olefile-0.46-py_0\n",
            "  pandas             conda-forge/linux-64::pandas-1.0.3-py37h0da4684_1\n",
            "  pcre               conda-forge/linux-64::pcre-8.44-he1b5a44_0\n",
            "  pillow             pkgs/main/linux-64::pillow-7.1.2-py37hb39fc2d_0\n",
            "  pixman             conda-forge/linux-64::pixman-0.38.0-h516909a_1003\n",
            "  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-h14c3975_1001\n",
            "  pycairo            conda-forge/linux-64::pycairo-1.19.1-py37h01af8b0_3\n",
            "  python-dateutil    conda-forge/noarch::python-dateutil-2.8.1-py_0\n",
            "  python_abi         conda-forge/linux-64::python_abi-3.7-1_cp37m\n",
            "  pytz               conda-forge/noarch::pytz-2020.1-pyh9f0ad1d_0\n",
            "  rdkit              conda-forge/linux-64::rdkit-2020.03.1-py37hdd87690_3\n",
            "  xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-h14c3975_1002\n",
            "  xorg-libice        conda-forge/linux-64::xorg-libice-1.0.10-h516909a_0\n",
            "  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.3-h84519dc_1000\n",
            "  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.6.9-h516909a_0\n",
            "  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.9-h14c3975_0\n",
            "  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.3-h516909a_0\n",
            "  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.4-h516909a_0\n",
            "  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.10-h516909a_1002\n",
            "  xorg-renderproto   conda-forge/linux-64::xorg-renderproto-0.11.1-h14c3975_1002\n",
            "  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h14c3975_1002\n",
            "  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h14c3975_1007\n",
            "  zstd               conda-forge/linux-64::zstd-1.4.4-h3b9ef0a_2\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates     pkgs/main::ca-certificates-2020.1.1-0 --> conda-forge::ca-certificates-2020.4.5.1-hecc5488_0\n",
            "  certifi              pkgs/main::certifi-2019.11.28-py37_0 --> conda-forge::certifi-2020.4.5.1-py37hc8dfbb8_0\n",
            "  conda                       pkgs/main::conda-4.8.2-py37_0 --> conda-forge::conda-4.8.3-py37hc8dfbb8_1\n",
            "  openssl              pkgs/main::openssl-1.1.1d-h7b6447c_4 --> conda-forge::openssl-1.1.1g-h516909a_0\n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxPjaPjsGNGb",
        "colab_type": "code",
        "outputId": "2d4a7aeb-b217-47e2-d5e2-8290cf0546ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        }
      },
      "source": [
        "!conda install -c dglteam dgl-cuda10.1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Solving environment: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - dgl-cuda10.1\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    blas-1.0                   |         openblas          46 KB\n",
            "    certifi-2020.4.5.1         |           py37_0         155 KB\n",
            "    decorator-4.4.2            |             py_0          14 KB\n",
            "    dgl-cuda10.1-0.4.3post2    |           py37_0        11.2 MB  dglteam\n",
            "    networkx-2.4               |             py_0         1.2 MB\n",
            "    scipy-1.4.1                |   py37habc2bb6_0        14.6 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        27.1 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  blas               pkgs/main/linux-64::blas-1.0-openblas\n",
            "  decorator          pkgs/main/noarch::decorator-4.4.2-py_0\n",
            "  dgl-cuda10.1       dglteam/linux-64::dgl-cuda10.1-0.4.3post2-py37_0\n",
            "  networkx           pkgs/main/noarch::networkx-2.4-py_0\n",
            "  scipy              pkgs/main/linux-64::scipy-1.4.1-py37habc2bb6_0\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  certifi            conda-forge::certifi-2020.4.5.1-py37h~ --> pkgs/main::certifi-2020.4.5.1-py37_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "certifi-2020.4.5.1   | 155 KB    | : 100% 1.0/1 [00:00<00:00, 12.35it/s]\n",
            "blas-1.0             | 46 KB     | : 100% 1.0/1 [00:00<00:00, 24.11it/s]\n",
            "decorator-4.4.2      | 14 KB     | : 100% 1.0/1 [00:00<00:00, 23.58it/s]\n",
            "scipy-1.4.1          | 14.6 MB   | : 100% 1.0/1 [00:00<00:00,  2.23it/s]               \n",
            "networkx-2.4         | 1.2 MB    | : 100% 1.0/1 [00:00<00:00,  3.49it/s]\n",
            "dgl-cuda10.1-0.4.3po | 11.2 MB   | : 100% 1.0/1 [00:09<00:00,  9.18s/it]               \n",
            "Preparing transaction: / \b\b- \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ba7Nw5eGUTr",
        "colab_type": "code",
        "outputId": "0f5aa50b-775e-46f8-c5c0-b2caf5f4f1b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        }
      },
      "source": [
        "!conda install -c dglteam dgllife\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - dgllife\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    dgllife-0.2.1              |           py37_0         132 KB  dglteam\n",
            "    joblib-0.14.1              |             py_0         201 KB\n",
            "    scikit-learn-0.22.1        |   py37h22eb022_0         5.3 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         5.6 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  dgllife            dglteam/linux-64::dgllife-0.2.1-py37_0\n",
            "  joblib             pkgs/main/noarch::joblib-0.14.1-py_0\n",
            "  scikit-learn       pkgs/main/linux-64::scikit-learn-0.22.1-py37h22eb022_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "scikit-learn-0.22.1  | 5.3 MB    | : 100% 1.0/1 [00:00<00:00,  3.25it/s]               \n",
            "dgllife-0.2.1        | 132 KB    | : 100% 1.0/1 [00:02<00:00,  2.25s/it]               \n",
            "joblib-0.14.1        | 201 KB    | : 100% 1.0/1 [00:00<00:00, 18.27it/s]\n",
            "Preparing transaction: | \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqRn8KexGdiL",
        "colab_type": "code",
        "outputId": "32df2295-19fe-4774-ff3d-c88a31741c27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        }
      },
      "source": [
        "!conda install pandas"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - pandas\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    conda-4.8.3                |           py37_0         2.8 MB\n",
            "    openssl-1.1.1g             |       h7b6447c_0         2.5 MB\n",
            "    pandas-1.0.3               |   py37h0573a6f_0         8.6 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        13.9 MB\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  ca-certificates    conda-forge::ca-certificates-2020.4.5~ --> pkgs/main::ca-certificates-2020.1.1-0\n",
            "  conda              conda-forge::conda-4.8.3-py37hc8dfbb8~ --> pkgs/main::conda-4.8.3-py37_0\n",
            "  openssl            conda-forge::openssl-1.1.1g-h516909a_0 --> pkgs/main::openssl-1.1.1g-h7b6447c_0\n",
            "  pandas             conda-forge::pandas-1.0.3-py37h0da468~ --> pkgs/main::pandas-1.0.3-py37h0573a6f_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "conda-4.8.3          | 2.8 MB    | : 100% 1.0/1 [00:00<00:00,  4.86it/s]\n",
            "openssl-1.1.1g       | 2.5 MB    | : 100% 1.0/1 [00:00<00:00,  7.83it/s]\n",
            "pandas-1.0.3         | 8.6 MB    | : 100% 1.0/1 [00:00<00:00,  2.16it/s]               \n",
            "Preparing transaction: \\ \b\b| \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\bdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVKD7kwtHDUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "import sys \n",
        "import pandas as pd\n",
        "\n",
        "# train --> balanced dataset\n",
        "dataset_train_file = \"/content/drive/My Drive/Project De Novo/AID1706_binarized_sars_full_eval_actives_12k_samples.csv\"\n",
        "dataset_eval_file = \"/content/drive/My Drive/Project De Novo/mpro_xchem.csv\"\n",
        "dataset_train = pd.read_csv(dataset_train_file)\n",
        "dataset_eval = pd.read_csv(dataset_eval_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy7_zDz9LCq2",
        "colab_type": "code",
        "outputId": "bff2dfa2-279c-4262-aa60-a49ba7f62e09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "dataset_train.head"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                                   smiles  activity\n",
              "0      C1CC(C1)C(=O)NC2=CC=C(C=C2)N(C(C3=CC(=CC=C3)F)...         1\n",
              "1      CC(C)(C)C1=CC=C(C=C1)N(C(C2=CN=NC=C2)C(=O)NC(C...         1\n",
              "2      CC(C)(C)NC(=O)C(C1=CSC=C1)N(C2=CC=C(C=C2)N)C(=...         1\n",
              "3      CC(C)C(=O)NC1=CC=C(C=C1)N(C(C2=CSC=C2)C(=O)NC(...         1\n",
              "4      CC(C)C(=O)NC1=CC=C(C=C1)N(CC2=CSC=C2)C(=O)CN3C...         1\n",
              "...                                                  ...       ...\n",
              "11994                               C1=CC2=C(C=C1N)NN=C2         0\n",
              "11995  CC(=O)[C@H]1CC[C@@H]2[C@@]1(CC(=O)[C@H]3[C@H]2...         0\n",
              "11996                       C1CN(CCN1CC(CO)O)C2=CC=CC=C2         0\n",
              "11997  CCOC(=O)N1CCC(=C2C3=C(CCC4=C2N=CC=C4)C=C(C=C3)...         0\n",
              "11998                    C1=CC2=C(C=C1OC(F)(F)F)SC(=N2)N         0\n",
              "\n",
              "[11999 rows x 2 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBnPl4isM1FQ",
        "colab_type": "code",
        "outputId": "bb481753-9d96-4e3a-af75-5653767bf809",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "dataset_eval.head"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                 smiles  activity\n",
              "0      OC=1C=CC=CC1CNC2=NC=3C=CC=CC3N2         1\n",
              "1        CC(=O)NCCC1=CNC=2C=CC(F)=CC12         1\n",
              "2    O=C([C@@H]1[C@H](C2=CSC=C2)CCC1)N         1\n",
              "3       CN1CCCC=2C=CC(=CC12)S(=O)(=O)N         1\n",
              "4     CC(=O)NC=1C=CC(OC=2N=CC=CN2)=CC1         1\n",
              "..                                 ...       ...\n",
              "875   CC(C)C=1C=CC(NC(=O)N2CCOCC2)=CC1         0\n",
              "876        CN(CC(=O)O)C(=O)C=1C=CC=CN1         0\n",
              "877  CN1CCN(CC1)C(=O)C=2C=CC(F)=C(F)C2         0\n",
              "878      FC=1C=CC=C(F)C1C(=O)N2CCCCCC2         0\n",
              "879             FC=1C=CC=NC1NCC2CCOCC2         0\n",
              "\n",
              "[880 rows x 2 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVrUoqtTHGy8",
        "colab_type": "code",
        "outputId": "f76156c6-9f57-408e-d21f-f39f6f8e9159",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from dgllife.data import MoleculeCSVDataset\n",
        "from dgllife.data.csv_dataset import *\n",
        "from dgllife.utils.featurizers import *\n",
        "from dgllife.utils.mol_to_graph import *\n",
        "\n",
        "# featurize bigraph/molecular graph set for train (SARS-COV-1) set\n",
        "train_set = MoleculeCSVDataset(dataset_train, smiles_to_graph=smiles_to_bigraph, node_featurizer=CanonicalAtomFeaturizer(),\n",
        "                               edge_featurizer=CanonicalBondFeaturizer(), smiles_column='smiles', cache_file_path='/content/drive/My Drive/Project De Novo/graph_featurizer/train.bin', task_names=['activity'])"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading previously saved dgl graphs...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofwpNYUXJASi",
        "colab_type": "code",
        "outputId": "1395eed1-5463-48c9-a95d-3a6764ce1def",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# featurize bigraph/molecular graph set for test (SARS-COV-2) set\n",
        "test_set = MoleculeCSVDataset(dataset_eval, smiles_to_graph=smiles_to_bigraph, node_featurizer=CanonicalAtomFeaturizer(),\n",
        "                               edge_featurizer=CanonicalBondFeaturizer(), smiles_column='smiles', cache_file_path='/content/drive/My Drive/Project De Novo/graph_featurizer/test.bin', task_names=['activity'])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading previously saved dgl graphs...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1TtgCu8U26K",
        "colab_type": "code",
        "outputId": "9c735f30-af1f-462b-8e05-5bff0db3b85f",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7118ef45-b79b-4e5e-8fdc-c311df6ca683\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-7118ef45-b79b-4e5e-8fdc-c311df6ca683\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving utils.py to utils.py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'utils.py': b'import dgl\\nimport numpy as np\\nimport random\\nimport torch\\n\\nfrom dgllife.utils.featurizers import one_hot_encoding\\nfrom dgllife.utils.splitters import RandomSplitter\\n\\ndef set_random_seed(seed=0):\\n    \"\"\"Set random seed.\\n    Parameters\\n    ----------\\n    seed : int\\n        Random seed to use\\n    \"\"\"\\n    random.seed(seed)\\n    np.random.seed(seed)\\n    torch.manual_seed(seed)\\n    if torch.cuda.is_available():\\n        torch.cuda.manual_seed(seed)\\n\\n\\ndef load_dataset_for_classification(args):\\n    \"\"\"Load dataset for classification tasks.\\n    Parameters\\n    ----------\\n    args : dict\\n        Configurations.\\n    Returns\\n    -------\\n    dataset\\n        The whole dataset.\\n    train_set\\n        Subset for training.\\n    val_set\\n        Subset for validation.\\n    test_set\\n        Subset for test.\\n    \"\"\"\\n    assert args[\\'dataset\\'] in [\\'Tox21\\']\\n    if args[\\'dataset\\'] == \\'Tox21\\':\\n        from dgllife.data import Tox21\\n        dataset = Tox21(smiles_to_graph=args[\\'smiles_to_graph\\'],\\n                        node_featurizer=args.get(\\'node_featurizer\\', None),\\n                        edge_featurizer=args.get(\\'edge_featurizer\\', None))\\n        train_set, val_set, test_set = RandomSplitter.train_val_test_split(\\n            dataset, frac_train=args[\\'frac_train\\'], frac_val=args[\\'frac_val\\'],\\n            frac_test=args[\\'frac_test\\'], random_state=args[\\'random_seed\\'])\\n\\n    return dataset, train_set, val_set, test_set\\n\\n\\ndef load_dataset_for_regression(args):\\n    \"\"\"Load dataset for regression tasks.\\n    Parameters\\n    ----------\\n    args : dict\\n        Configurations.\\n    Returns\\n    -------\\n    train_set\\n        Subset for training.\\n    val_set\\n        Subset for validation.\\n    test_set\\n        Subset for test.\\n    \"\"\"\\n    assert args[\\'dataset\\'] in [\\'Alchemy\\', \\'Aromaticity\\']\\n\\n    if args[\\'dataset\\'] == \\'Alchemy\\':\\n        from dgllife.data import TencentAlchemyDataset\\n        train_set = TencentAlchemyDataset(mode=\\'dev\\')\\n        val_set = TencentAlchemyDataset(mode=\\'valid\\')\\n        test_set = None\\n\\n    if args[\\'dataset\\'] == \\'Aromaticity\\':\\n        from dgllife.data import PubChemBioAssayAromaticity\\n        dataset = PubChemBioAssayAromaticity(smiles_to_graph=args[\\'smiles_to_graph\\'],\\n                                             node_featurizer=args.get(\\'node_featurizer\\', None),\\n                                             edge_featurizer=args.get(\\'edge_featurizer\\', None))\\n        train_set, val_set, test_set = RandomSplitter.train_val_test_split(\\n            dataset, frac_train=args[\\'frac_train\\'], frac_val=args[\\'frac_val\\'],\\n            frac_test=args[\\'frac_test\\'], random_state=args[\\'random_seed\\'])\\n\\n    return train_set, val_set, test_set\\n\\n\\ndef collate_molgraphs(data):\\n    \"\"\"Batching a list of datapoints for dataloader.\\n    Parameters\\n    ----------\\n    data : list of 3-tuples or 4-tuples.\\n        Each tuple is for a single datapoint, consisting of\\n        a SMILES, a DGLGraph, all-task labels and optionally\\n        a binary mask indicating the existence of labels.\\n    Returns\\n    -------\\n    smiles : list\\n        List of smiles\\n    bg : DGLGraph\\n        The batched DGLGraph.\\n    labels : Tensor of dtype float32 and shape (B, T)\\n        Batched datapoint labels. B is len(data) and\\n        T is the number of total tasks.\\n    masks : Tensor of dtype float32 and shape (B, T)\\n        Batched datapoint binary mask, indicating the\\n        existence of labels. If binary masks are not\\n        provided, return a tensor with ones.\\n    \"\"\"\\n    assert len(data[0]) in [3, 4], \\\\\\n        \\'Expect the tuple to be of length 3 or 4, got {:d}\\'.format(len(data[0]))\\n    if len(data[0]) == 3:\\n        smiles, graphs, labels = map(list, zip(*data))\\n        masks = None\\n    else:\\n        smiles, graphs, labels, masks = map(list, zip(*data))\\n\\n    bg = dgl.batch(graphs)\\n    bg.set_n_initializer(dgl.init.zero_initializer)\\n    bg.set_e_initializer(dgl.init.zero_initializer)\\n    labels = torch.stack(labels, dim=0)\\n\\n    if masks is None:\\n        masks = torch.ones(labels.shape)\\n    else:\\n        masks = torch.stack(masks, dim=0)\\n    return smiles, bg, labels, masks\\n\\n\\ndef load_model(args):\\n    if args[\\'model\\'] == \\'GCN\\':\\n        from dgllife.model import GCNPredictor\\n        model = GCNPredictor(in_feats=args[\\'node_featurizer\\'].feat_size(),\\n                             hidden_feats=args[\\'gcn_hidden_feats\\'],\\n                             classifier_hidden_feats=args[\\'classifier_hidden_feats\\'],\\n                             n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'GAT\\':\\n        from dgllife.model import GATPredictor\\n        model = GATPredictor(in_feats=args[\\'node_featurizer\\'].feat_size(),\\n                             hidden_feats=args[\\'gat_hidden_feats\\'],\\n                             num_heads=args[\\'num_heads\\'],\\n                             classifier_hidden_feats=args[\\'classifier_hidden_feats\\'],\\n                             n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'Weave\\':\\n        from dgllife.model import WeavePredictor\\n        model = WeavePredictor(node_in_feats=args[\\'node_featurizer\\'].feat_size(),\\n                               edge_in_feats=args[\\'edge_featurizer\\'].feat_size(),\\n                               num_gnn_layers=args[\\'num_gnn_layers\\'],\\n                               gnn_hidden_feats=args[\\'gnn_hidden_feats\\'],\\n                               graph_feats=args[\\'graph_feats\\'],\\n                               n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'AttentiveFP\\':\\n        from dgllife.model import AttentiveFPPredictor\\n        model = AttentiveFPPredictor(node_feat_size=args[\\'node_featurizer\\'].feat_size(),\\n                                     edge_feat_size=args[\\'edge_featurizer\\'].feat_size(),\\n                                     num_layers=args[\\'num_layers\\'],\\n                                     num_timesteps=args[\\'num_timesteps\\'],\\n                                     graph_feat_size=args[\\'graph_feat_size\\'],\\n                                     n_tasks=args[\\'n_tasks\\'],\\n                                     dropout=args[\\'dropout\\'])\\n\\n    if args[\\'model\\'] == \\'SchNet\\':\\n        from dgllife.model import SchNetPredictor\\n        model = SchNetPredictor(node_feats=args[\\'node_feats\\'],\\n                                hidden_feats=args[\\'hidden_feats\\'],\\n                                classifier_hidden_feats=args[\\'classifier_hidden_feats\\'],\\n                                n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'MGCN\\':\\n        from dgllife.model import MGCNPredictor\\n        model = MGCNPredictor(feats=args[\\'feats\\'],\\n                              n_layers=args[\\'n_layers\\'],\\n                              classifier_hidden_feats=args[\\'classifier_hidden_feats\\'],\\n                              n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'MPNN\\':\\n        from dgllife.model import MPNNPredictor\\n        model = MPNNPredictor(node_in_feats=args[\\'node_in_feats\\'],\\n                              edge_in_feats=args[\\'edge_in_feats\\'],\\n                              node_out_feats=args[\\'node_out_feats\\'],\\n                              edge_hidden_feats=args[\\'edge_hidden_feats\\'],\\n                              n_tasks=args[\\'n_tasks\\'])\\n\\n    return model\\n\\n\\ndef chirality(atom):\\n    try:\\n        return one_hot_encoding(atom.GetProp(\\'_CIPCode\\'), [\\'R\\', \\'S\\']) + \\\\\\n               [atom.HasProp(\\'_ChiralityPossible\\')]\\n    except:\\n        return [False, False] + [atom.HasProp(\\'_ChiralityPossible\\')]\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKMgchzflhiW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = {\n",
        "    'random_seed': 2,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-3,\n",
        "    'num_epochs': 100,\n",
        "    'node_data_field': 'h',\n",
        "    'frac_train': 0.8,\n",
        "    'frac_val': 0.1,\n",
        "    'frac_test': 0.1,\n",
        "    'in_feats': 74,\n",
        "    'gat_hidden_feats': [32, 32],\n",
        "    'classifier_hidden_feats': 64,\n",
        "    'num_heads': [4, 4],\n",
        "    'patience': 10,\n",
        "    'smiles_to_graph': smiles_to_bigraph,\n",
        "    'node_featurizer': CanonicalAtomFeaturizer(),\n",
        "    'metric_name': 'roc_auc_score',\n",
        "    'model': 'GAT'\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuZkFAz-PDvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import utils\n",
        "\n",
        "from dgllife.model import load_pretrained\n",
        "from dgllife.utils import EarlyStopping, Meter\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "from utils import set_random_seed, load_dataset_for_classification, collate_molgraphs, load_model\n",
        "\n",
        "from dgllife.model import GATPredictor\n",
        "\n",
        "args['device'] = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "set_random_seed(args['random_seed'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJf2RePJHUcx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "3e573e0d-c327-4857-c244-5fd24632010f"
      },
      "source": [
        "from dgllife.utils.splitters import ScaffoldSplitter\n",
        "\n",
        "train_scaffold_set, val_set, test_scaffold_set = ScaffoldSplitter.train_val_test_split(train_set, frac_train=0.8, frac_val=0.2,frac_test=0.0)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start initializing RDKit molecule instances...\n",
            "Creating RDKit molecule instance 1000/11999\n",
            "Creating RDKit molecule instance 2000/11999\n",
            "Creating RDKit molecule instance 3000/11999\n",
            "Creating RDKit molecule instance 4000/11999\n",
            "Creating RDKit molecule instance 5000/11999\n",
            "Creating RDKit molecule instance 6000/11999\n",
            "Creating RDKit molecule instance 7000/11999\n",
            "Creating RDKit molecule instance 8000/11999\n",
            "Creating RDKit molecule instance 9000/11999\n",
            "Creating RDKit molecule instance 10000/11999\n",
            "Creating RDKit molecule instance 11000/11999\n",
            "Start computing Bemis-Murcko scaffolds.\n",
            "Computing Bemis-Murcko for compound 1000/11999\n",
            "Computing Bemis-Murcko for compound 2000/11999\n",
            "Computing Bemis-Murcko for compound 3000/11999\n",
            "Computing Bemis-Murcko for compound 4000/11999\n",
            "Computing Bemis-Murcko for compound 5000/11999\n",
            "Computing Bemis-Murcko for compound 6000/11999\n",
            "Computing Bemis-Murcko for compound 7000/11999\n",
            "Computing Bemis-Murcko for compound 8000/11999\n",
            "Computing Bemis-Murcko for compound 9000/11999\n",
            "Computing Bemis-Murcko for compound 10000/11999\n",
            "Computing Bemis-Murcko for compound 11000/11999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCmuQegvJMcq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "c2bba880-976a-4e5f-bb93-8a8a0812e41a"
      },
      "source": [
        "print (len(train_set))\n",
        "print(len(train_scaffold_set))\n",
        "print (len(val_set))\n",
        "print (len(test_set))\n",
        "print(train_set[1])"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11999\n",
            "9599\n",
            "2400\n",
            "880\n",
            "('CC(C)(C)C1=CC=C(C=C1)N(C(C2=CN=NC=C2)C(=O)NC(C)(C)C)C(=O)C3=CC=CO3', DGLGraph(num_nodes=32, num_edges=68,\n",
            "         ndata_schemes={'h': Scheme(shape=(74,), dtype=torch.float32)}\n",
            "         edata_schemes={'e': Scheme(shape=(12,), dtype=torch.float32)}), tensor([1.]), tensor([1.]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YjAjoUyLEjT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4bae6d18-6ab3-4ed2-b259-11bddec9cb2f"
      },
      "source": [
        "print(type(train_set))\n",
        "print(type(train_scaffold_set))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'dgllife.data.csv_dataset.MoleculeCSVDataset'>\n",
            "<class 'dgl.data.utils.Subset'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYfHiYl0T8V4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_scaffold_set,  batch_size=args['batch_size'],\n",
        "                              collate_fn=collate_molgraphs)\n",
        "val_loader = DataLoader(val_set,  batch_size=args['batch_size'],\n",
        "                              collate_fn=collate_molgraphs)\n",
        "\n",
        "test_loader = DataLoader(test_set,  batch_size=args['batch_size'],\n",
        "                          collate_fn=collate_molgraphs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzUMvJO8Qx85",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "daa89caf-592c-4da1-ba7a-2d43455b6c15"
      },
      "source": [
        "print(len(test_loader))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGAUIdpRUAUu",
        "colab_type": "code",
        "outputId": "b2bbbebe-223d-41fd-946e-eda39d94fb6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 725
        }
      },
      "source": [
        "args['n_tasks'] = 1\n",
        "model = GATPredictor(in_feats=args['node_featurizer'].feat_size('h'),\n",
        "                             hidden_feats=args['gat_hidden_feats'],\n",
        "                             num_heads=args['num_heads'],\n",
        "                             classifier_hidden_feats=args['classifier_hidden_feats'],\n",
        "                             n_tasks=args['n_tasks'])\n",
        "\n",
        "import dgl.backend as F\n",
        "\n",
        "train_num_pos = F.sum(train_set.labels, dim=0)\n",
        "train_num_indices = F.sum(train_set.mask, dim=0)\n",
        "train_task_pos_weights = (train_num_indices - train_num_pos) / train_num_pos\n",
        "\n",
        "loss_criterion = BCEWithLogitsLoss(pos_weight=train_task_pos_weights.to(args['device']),\n",
        "                                    reduction='none')\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=args['lr'])\n",
        "stopper = EarlyStopping(patience=args['patience'], mode='higher', filename='/content/checkpoint/train.pth')\n",
        "model.to(args['device'])\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GATPredictor(\n",
              "  (gnn): GAT(\n",
              "    (gnn_layers): ModuleList(\n",
              "      (0): GATLayer(\n",
              "        (gat_conv): GATConv(\n",
              "          (fc): Linear(in_features=74, out_features=128, bias=False)\n",
              "          (feat_drop): Dropout(p=0.0, inplace=False)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
              "          (res_fc): Linear(in_features=74, out_features=128, bias=False)\n",
              "        )\n",
              "      )\n",
              "      (1): GATLayer(\n",
              "        (gat_conv): GATConv(\n",
              "          (fc): Linear(in_features=128, out_features=128, bias=False)\n",
              "          (feat_drop): Dropout(p=0.0, inplace=False)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
              "          (res_fc): Linear(in_features=128, out_features=128, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (readout): WeightedSumAndMax(\n",
              "    (weight_and_sum): WeightAndSum(\n",
              "      (atom_weighting): Sequential(\n",
              "        (0): Linear(in_features=32, out_features=1, bias=True)\n",
              "        (1): Sigmoid()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (predict): MLPPredictor(\n",
              "    (predict): Sequential(\n",
              "      (0): Dropout(p=0.0, inplace=False)\n",
              "      (1): Linear(in_features=64, out_features=64, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (4): Linear(in_features=64, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7Fdo_NxLPWz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3fa4ceee-f4c8-42f9-9b6e-7c0b8977af67"
      },
      "source": [
        "print(train_task_pos_weights)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([25.9036])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C941OCzRuXH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def predict(args, model, bg):\n",
        "    node_feats = bg.ndata.pop(args['node_data_field']).to(args['device'])\n",
        "    if args.get('edge_featurizer', None) is not None:\n",
        "        edge_feats = bg.edata.pop(args['edge_data_field']).to(args['device'])\n",
        "        return model(bg, node_feats, edge_feats)\n",
        "    else:\n",
        "        return model(bg, node_feats)\n",
        "\n",
        "def run_a_train_epoch(args, epoch, model, data_loader, loss_criterion, optimizer):\n",
        "    model.train()\n",
        "    train_meter = Meter()\n",
        "    for batch_id, batch_data in enumerate(data_loader):\n",
        "        smiles, bg, labels, masks = batch_data\n",
        "        labels, masks = labels.to(args['device']), masks.to(args['device'])\n",
        "        logits = predict(args, model, bg)\n",
        "        # Mask non-existing labels\n",
        "        loss = (loss_criterion(logits, labels) * (masks != 0).float()).mean()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print('epoch {:d}/{:d}, batch {:d}/{:d}, loss {:.4f}'.format(\n",
        "            epoch + 1, args['num_epochs'], batch_id + 1, len(data_loader), loss.item()))\n",
        "        train_meter.update(logits, labels, masks)\n",
        "    train_score = np.mean(train_meter.compute_metric(args['metric_name']))\n",
        "    print('epoch {:d}/{:d}, training {} {:.4f}'.format(\n",
        "        epoch + 1, args['num_epochs'], args['metric_name'], train_score))\n",
        "\n",
        "def run_an_eval_epoch(args, model, data_loader):\n",
        "    model.eval()\n",
        "    eval_meter = Meter()\n",
        "    with torch.no_grad():\n",
        "        for batch_id, batch_data in enumerate(data_loader):\n",
        "            smiles, bg, labels, masks = batch_data\n",
        "            labels = labels.to(args['device'])\n",
        "            logits = predict(args, model, bg)\n",
        "            eval_meter.update(logits, labels, masks)\n",
        "    return np.mean(eval_meter.compute_metric(args['metric_name']))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73ZBLfQxuHQA",
        "colab_type": "code",
        "outputId": "e1f73ba9-db56-4bba-e3ec-5ba90f7c551f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(args['num_epochs']):\n",
        "        # Train\n",
        "        run_a_train_epoch(args, epoch, model, train_loader, loss_criterion, optimizer)\n",
        "\n",
        "        # Validation and early stop\n",
        "        val_score = run_an_eval_epoch(args, model, val_loader)\n",
        "        early_stop = stopper.step(val_score, model)\n",
        "        print('epoch {:d}/{:d}, validation {} {:.4f}, best validation {} {:.4f}'.format(\n",
        "            epoch + 1, args['num_epochs'], args['metric_name'],\n",
        "            val_score, args['metric_name'], stopper.best_score))\n",
        "        if early_stop:\n",
        "            break"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1/100, batch 1/75, loss 1.2572\n",
            "epoch 1/100, batch 2/75, loss 1.1322\n",
            "epoch 1/100, batch 3/75, loss 0.9232\n",
            "epoch 1/100, batch 4/75, loss 0.8452\n",
            "epoch 1/100, batch 5/75, loss 0.7506\n",
            "epoch 1/100, batch 6/75, loss 0.7520\n",
            "epoch 1/100, batch 7/75, loss 0.7477\n",
            "epoch 1/100, batch 8/75, loss 1.4198\n",
            "epoch 1/100, batch 9/75, loss 0.7345\n",
            "epoch 1/100, batch 10/75, loss 2.3942\n",
            "epoch 1/100, batch 11/75, loss 0.9633\n",
            "epoch 1/100, batch 12/75, loss 0.7256\n",
            "epoch 1/100, batch 13/75, loss 0.8439\n",
            "epoch 1/100, batch 14/75, loss 0.7235\n",
            "epoch 1/100, batch 15/75, loss 2.8578\n",
            "epoch 1/100, batch 16/75, loss 0.7214\n",
            "epoch 1/100, batch 17/75, loss 0.7184\n",
            "epoch 1/100, batch 18/75, loss 0.7246\n",
            "epoch 1/100, batch 19/75, loss 0.7242\n",
            "epoch 1/100, batch 20/75, loss 2.2671\n",
            "epoch 1/100, batch 21/75, loss 0.7202\n",
            "epoch 1/100, batch 22/75, loss 0.7195\n",
            "epoch 1/100, batch 23/75, loss 0.7118\n",
            "epoch 1/100, batch 24/75, loss 0.7139\n",
            "epoch 1/100, batch 25/75, loss 0.7093\n",
            "epoch 1/100, batch 26/75, loss 3.5776\n",
            "epoch 1/100, batch 27/75, loss 0.6985\n",
            "epoch 1/100, batch 28/75, loss 0.7025\n",
            "epoch 1/100, batch 29/75, loss 0.7046\n",
            "epoch 1/100, batch 30/75, loss 0.7040\n",
            "epoch 1/100, batch 31/75, loss 0.7150\n",
            "epoch 1/100, batch 32/75, loss 0.7075\n",
            "epoch 1/100, batch 33/75, loss 0.7078\n",
            "epoch 1/100, batch 34/75, loss 0.7078\n",
            "epoch 1/100, batch 35/75, loss 0.7101\n",
            "epoch 1/100, batch 36/75, loss 3.3330\n",
            "epoch 1/100, batch 37/75, loss 0.8569\n",
            "epoch 1/100, batch 38/75, loss 0.6966\n",
            "epoch 1/100, batch 39/75, loss 0.6973\n",
            "epoch 1/100, batch 40/75, loss 0.6966\n",
            "epoch 1/100, batch 41/75, loss 0.6937\n",
            "epoch 1/100, batch 42/75, loss 0.6959\n",
            "epoch 1/100, batch 43/75, loss 0.6993\n",
            "epoch 1/100, batch 44/75, loss 0.6958\n",
            "epoch 1/100, batch 45/75, loss 0.6959\n",
            "epoch 1/100, batch 46/75, loss 0.6974\n",
            "epoch 1/100, batch 47/75, loss 0.6946\n",
            "epoch 1/100, batch 48/75, loss 0.6871\n",
            "epoch 1/100, batch 49/75, loss 0.6849\n",
            "epoch 1/100, batch 50/75, loss 0.6816\n",
            "epoch 1/100, batch 51/75, loss 0.6809\n",
            "epoch 1/100, batch 52/75, loss 0.6775\n",
            "epoch 1/100, batch 53/75, loss 10.2827\n",
            "epoch 1/100, batch 54/75, loss 1.1672\n",
            "epoch 1/100, batch 55/75, loss 0.6921\n",
            "epoch 1/100, batch 56/75, loss 0.6907\n",
            "epoch 1/100, batch 57/75, loss 0.7012\n",
            "epoch 1/100, batch 58/75, loss 0.7120\n",
            "epoch 1/100, batch 59/75, loss 0.7071\n",
            "epoch 1/100, batch 60/75, loss 0.7084\n",
            "epoch 1/100, batch 61/75, loss 0.7183\n",
            "epoch 1/100, batch 62/75, loss 0.7131\n",
            "epoch 1/100, batch 63/75, loss 0.7103\n",
            "epoch 1/100, batch 64/75, loss 0.7104\n",
            "epoch 1/100, batch 65/75, loss 0.7134\n",
            "epoch 1/100, batch 66/75, loss 0.7079\n",
            "epoch 1/100, batch 67/75, loss 0.7044\n",
            "epoch 1/100, batch 68/75, loss 0.7077\n",
            "epoch 1/100, batch 69/75, loss 0.7052\n",
            "epoch 1/100, batch 70/75, loss 0.6990\n",
            "epoch 1/100, batch 71/75, loss 0.6916\n",
            "epoch 1/100, batch 72/75, loss 0.6883\n",
            "epoch 1/100, batch 73/75, loss 0.6838\n",
            "epoch 1/100, batch 74/75, loss 0.6840\n",
            "epoch 1/100, batch 75/75, loss 0.6873\n",
            "epoch 1/100, training roc_auc_score 0.5768\n",
            "epoch 1/100, validation roc_auc_score 0.8157, best validation roc_auc_score 0.8157\n",
            "epoch 2/100, batch 1/75, loss 1.2400\n",
            "epoch 2/100, batch 2/75, loss 0.9803\n",
            "epoch 2/100, batch 3/75, loss 0.7637\n",
            "epoch 2/100, batch 4/75, loss 0.8179\n",
            "epoch 2/100, batch 5/75, loss 0.6560\n",
            "epoch 2/100, batch 6/75, loss 0.6592\n",
            "epoch 2/100, batch 7/75, loss 0.6592\n",
            "epoch 2/100, batch 8/75, loss 1.2234\n",
            "epoch 2/100, batch 9/75, loss 0.6499\n",
            "epoch 2/100, batch 10/75, loss 1.6285\n",
            "epoch 2/100, batch 11/75, loss 0.9598\n",
            "epoch 2/100, batch 12/75, loss 0.6485\n",
            "epoch 2/100, batch 13/75, loss 0.7805\n",
            "epoch 2/100, batch 14/75, loss 0.6484\n",
            "epoch 2/100, batch 15/75, loss 2.9555\n",
            "epoch 2/100, batch 16/75, loss 0.6472\n",
            "epoch 2/100, batch 17/75, loss 0.6437\n",
            "epoch 2/100, batch 18/75, loss 0.6495\n",
            "epoch 2/100, batch 19/75, loss 0.6477\n",
            "epoch 2/100, batch 20/75, loss 2.3394\n",
            "epoch 2/100, batch 21/75, loss 0.6462\n",
            "epoch 2/100, batch 22/75, loss 0.6459\n",
            "epoch 2/100, batch 23/75, loss 0.6441\n",
            "epoch 2/100, batch 24/75, loss 0.6475\n",
            "epoch 2/100, batch 25/75, loss 0.6462\n",
            "epoch 2/100, batch 26/75, loss 3.4738\n",
            "epoch 2/100, batch 27/75, loss 0.6376\n",
            "epoch 2/100, batch 28/75, loss 0.6444\n",
            "epoch 2/100, batch 29/75, loss 0.6463\n",
            "epoch 2/100, batch 30/75, loss 0.6461\n",
            "epoch 2/100, batch 31/75, loss 0.6605\n",
            "epoch 2/100, batch 32/75, loss 0.6538\n",
            "epoch 2/100, batch 33/75, loss 0.6516\n",
            "epoch 2/100, batch 34/75, loss 0.6570\n",
            "epoch 2/100, batch 35/75, loss 0.6589\n",
            "epoch 2/100, batch 36/75, loss 3.3268\n",
            "epoch 2/100, batch 37/75, loss 0.8229\n",
            "epoch 2/100, batch 38/75, loss 0.6386\n",
            "epoch 2/100, batch 39/75, loss 0.6436\n",
            "epoch 2/100, batch 40/75, loss 0.6473\n",
            "epoch 2/100, batch 41/75, loss 0.6383\n",
            "epoch 2/100, batch 42/75, loss 0.6399\n",
            "epoch 2/100, batch 43/75, loss 0.6484\n",
            "epoch 2/100, batch 44/75, loss 0.6445\n",
            "epoch 2/100, batch 45/75, loss 0.6398\n",
            "epoch 2/100, batch 46/75, loss 0.6409\n",
            "epoch 2/100, batch 47/75, loss 0.6450\n",
            "epoch 2/100, batch 48/75, loss 0.6343\n",
            "epoch 2/100, batch 49/75, loss 0.6344\n",
            "epoch 2/100, batch 50/75, loss 0.6310\n",
            "epoch 2/100, batch 51/75, loss 0.6273\n",
            "epoch 2/100, batch 52/75, loss 0.6251\n",
            "epoch 2/100, batch 53/75, loss 10.7523\n",
            "epoch 2/100, batch 54/75, loss 1.1066\n",
            "epoch 2/100, batch 55/75, loss 0.6371\n",
            "epoch 2/100, batch 56/75, loss 0.6365\n",
            "epoch 2/100, batch 57/75, loss 0.6414\n",
            "epoch 2/100, batch 58/75, loss 0.6491\n",
            "epoch 2/100, batch 59/75, loss 0.6471\n",
            "epoch 2/100, batch 60/75, loss 0.6452\n",
            "epoch 2/100, batch 61/75, loss 0.6528\n",
            "epoch 2/100, batch 62/75, loss 0.6469\n",
            "epoch 2/100, batch 63/75, loss 0.6465\n",
            "epoch 2/100, batch 64/75, loss 0.6482\n",
            "epoch 2/100, batch 65/75, loss 0.6461\n",
            "epoch 2/100, batch 66/75, loss 0.6389\n",
            "epoch 2/100, batch 67/75, loss 0.6411\n",
            "epoch 2/100, batch 68/75, loss 0.6388\n",
            "epoch 2/100, batch 69/75, loss 0.6395\n",
            "epoch 2/100, batch 70/75, loss 0.6325\n",
            "epoch 2/100, batch 71/75, loss 0.6274\n",
            "epoch 2/100, batch 72/75, loss 0.6262\n",
            "epoch 2/100, batch 73/75, loss 0.6223\n",
            "epoch 2/100, batch 74/75, loss 0.6204\n",
            "epoch 2/100, batch 75/75, loss 0.6190\n",
            "epoch 2/100, training roc_auc_score 0.6651\n",
            "epoch 2/100, validation roc_auc_score 0.8258, best validation roc_auc_score 0.8258\n",
            "epoch 3/100, batch 1/75, loss 1.1930\n",
            "epoch 3/100, batch 2/75, loss 0.9436\n",
            "epoch 3/100, batch 3/75, loss 0.7177\n",
            "epoch 3/100, batch 4/75, loss 0.7655\n",
            "epoch 3/100, batch 5/75, loss 0.5993\n",
            "epoch 3/100, batch 6/75, loss 0.5996\n",
            "epoch 3/100, batch 7/75, loss 0.6010\n",
            "epoch 3/100, batch 8/75, loss 1.2435\n",
            "epoch 3/100, batch 9/75, loss 0.5930\n",
            "epoch 3/100, batch 10/75, loss 1.6070\n",
            "epoch 3/100, batch 11/75, loss 0.8998\n",
            "epoch 3/100, batch 12/75, loss 0.5938\n",
            "epoch 3/100, batch 13/75, loss 0.7366\n",
            "epoch 3/100, batch 14/75, loss 0.5921\n",
            "epoch 3/100, batch 15/75, loss 3.0619\n",
            "epoch 3/100, batch 16/75, loss 0.5932\n",
            "epoch 3/100, batch 17/75, loss 0.5902\n",
            "epoch 3/100, batch 18/75, loss 0.5941\n",
            "epoch 3/100, batch 19/75, loss 0.5917\n",
            "epoch 3/100, batch 20/75, loss 2.3218\n",
            "epoch 3/100, batch 21/75, loss 0.5920\n",
            "epoch 3/100, batch 22/75, loss 0.5939\n",
            "epoch 3/100, batch 23/75, loss 0.5936\n",
            "epoch 3/100, batch 24/75, loss 0.5995\n",
            "epoch 3/100, batch 25/75, loss 0.5952\n",
            "epoch 3/100, batch 26/75, loss 3.6303\n",
            "epoch 3/100, batch 27/75, loss 0.5862\n",
            "epoch 3/100, batch 28/75, loss 0.5971\n",
            "epoch 3/100, batch 29/75, loss 0.5969\n",
            "epoch 3/100, batch 30/75, loss 0.5966\n",
            "epoch 3/100, batch 31/75, loss 0.6130\n",
            "epoch 3/100, batch 32/75, loss 0.6065\n",
            "epoch 3/100, batch 33/75, loss 0.6041\n",
            "epoch 3/100, batch 34/75, loss 0.6134\n",
            "epoch 3/100, batch 35/75, loss 0.6113\n",
            "epoch 3/100, batch 36/75, loss 3.3732\n",
            "epoch 3/100, batch 37/75, loss 0.7952\n",
            "epoch 3/100, batch 38/75, loss 0.5885\n",
            "epoch 3/100, batch 39/75, loss 0.6003\n",
            "epoch 3/100, batch 40/75, loss 0.6028\n",
            "epoch 3/100, batch 41/75, loss 0.5907\n",
            "epoch 3/100, batch 42/75, loss 0.5923\n",
            "epoch 3/100, batch 43/75, loss 0.6063\n",
            "epoch 3/100, batch 44/75, loss 0.5984\n",
            "epoch 3/100, batch 45/75, loss 0.5902\n",
            "epoch 3/100, batch 46/75, loss 0.5964\n",
            "epoch 3/100, batch 47/75, loss 0.5983\n",
            "epoch 3/100, batch 48/75, loss 0.5850\n",
            "epoch 3/100, batch 49/75, loss 0.5847\n",
            "epoch 3/100, batch 50/75, loss 0.5808\n",
            "epoch 3/100, batch 51/75, loss 0.5764\n",
            "epoch 3/100, batch 52/75, loss 0.5722\n",
            "epoch 3/100, batch 53/75, loss 11.6565\n",
            "epoch 3/100, batch 54/75, loss 1.1104\n",
            "epoch 3/100, batch 55/75, loss 0.5858\n",
            "epoch 3/100, batch 56/75, loss 0.5891\n",
            "epoch 3/100, batch 57/75, loss 0.5963\n",
            "epoch 3/100, batch 58/75, loss 0.6088\n",
            "epoch 3/100, batch 59/75, loss 0.6104\n",
            "epoch 3/100, batch 60/75, loss 0.6074\n",
            "epoch 3/100, batch 61/75, loss 0.6141\n",
            "epoch 3/100, batch 62/75, loss 0.6139\n",
            "epoch 3/100, batch 63/75, loss 0.6081\n",
            "epoch 3/100, batch 64/75, loss 0.6106\n",
            "epoch 3/100, batch 65/75, loss 0.6082\n",
            "epoch 3/100, batch 66/75, loss 0.5992\n",
            "epoch 3/100, batch 67/75, loss 0.5962\n",
            "epoch 3/100, batch 68/75, loss 0.5955\n",
            "epoch 3/100, batch 69/75, loss 0.5988\n",
            "epoch 3/100, batch 70/75, loss 0.5879\n",
            "epoch 3/100, batch 71/75, loss 0.5789\n",
            "epoch 3/100, batch 72/75, loss 0.5791\n",
            "epoch 3/100, batch 73/75, loss 0.5750\n",
            "epoch 3/100, batch 74/75, loss 0.5702\n",
            "epoch 3/100, batch 75/75, loss 0.5731\n",
            "epoch 3/100, training roc_auc_score 0.6697\n",
            "EarlyStopping counter: 1 out of 10\n",
            "epoch 3/100, validation roc_auc_score 0.8185, best validation roc_auc_score 0.8258\n",
            "epoch 4/100, batch 1/75, loss 1.0772\n",
            "epoch 4/100, batch 2/75, loss 0.9550\n",
            "epoch 4/100, batch 3/75, loss 0.6727\n",
            "epoch 4/100, batch 4/75, loss 0.7115\n",
            "epoch 4/100, batch 5/75, loss 0.5525\n",
            "epoch 4/100, batch 6/75, loss 0.5527\n",
            "epoch 4/100, batch 7/75, loss 0.5521\n",
            "epoch 4/100, batch 8/75, loss 1.2252\n",
            "epoch 4/100, batch 9/75, loss 0.5436\n",
            "epoch 4/100, batch 10/75, loss 1.5765\n",
            "epoch 4/100, batch 11/75, loss 0.8707\n",
            "epoch 4/100, batch 12/75, loss 0.5453\n",
            "epoch 4/100, batch 13/75, loss 0.6761\n",
            "epoch 4/100, batch 14/75, loss 0.5432\n",
            "epoch 4/100, batch 15/75, loss 3.0341\n",
            "epoch 4/100, batch 16/75, loss 0.5415\n",
            "epoch 4/100, batch 17/75, loss 0.5427\n",
            "epoch 4/100, batch 18/75, loss 0.5475\n",
            "epoch 4/100, batch 19/75, loss 0.5445\n",
            "epoch 4/100, batch 20/75, loss 2.3921\n",
            "epoch 4/100, batch 21/75, loss 0.5437\n",
            "epoch 4/100, batch 22/75, loss 0.5378\n",
            "epoch 4/100, batch 23/75, loss 0.5383\n",
            "epoch 4/100, batch 24/75, loss 0.5486\n",
            "epoch 4/100, batch 25/75, loss 0.5463\n",
            "epoch 4/100, batch 26/75, loss 4.0536\n",
            "epoch 4/100, batch 27/75, loss 0.5352\n",
            "epoch 4/100, batch 28/75, loss 0.5482\n",
            "epoch 4/100, batch 29/75, loss 0.5453\n",
            "epoch 4/100, batch 30/75, loss 0.5443\n",
            "epoch 4/100, batch 31/75, loss 0.5614\n",
            "epoch 4/100, batch 32/75, loss 0.5451\n",
            "epoch 4/100, batch 33/75, loss 0.5505\n",
            "epoch 4/100, batch 34/75, loss 0.5641\n",
            "epoch 4/100, batch 35/75, loss 0.5583\n",
            "epoch 4/100, batch 36/75, loss 3.4936\n",
            "epoch 4/100, batch 37/75, loss 0.7472\n",
            "epoch 4/100, batch 38/75, loss 0.5407\n",
            "epoch 4/100, batch 39/75, loss 0.5490\n",
            "epoch 4/100, batch 40/75, loss 0.5554\n",
            "epoch 4/100, batch 41/75, loss 0.5411\n",
            "epoch 4/100, batch 42/75, loss 0.5455\n",
            "epoch 4/100, batch 43/75, loss 0.5604\n",
            "epoch 4/100, batch 44/75, loss 0.5531\n",
            "epoch 4/100, batch 45/75, loss 0.5458\n",
            "epoch 4/100, batch 46/75, loss 0.5528\n",
            "epoch 4/100, batch 47/75, loss 0.5554\n",
            "epoch 4/100, batch 48/75, loss 0.5405\n",
            "epoch 4/100, batch 49/75, loss 0.5398\n",
            "epoch 4/100, batch 50/75, loss 0.5403\n",
            "epoch 4/100, batch 51/75, loss 0.5362\n",
            "epoch 4/100, batch 52/75, loss 0.5319\n",
            "epoch 4/100, batch 53/75, loss 12.1180\n",
            "epoch 4/100, batch 54/75, loss 0.9697\n",
            "epoch 4/100, batch 55/75, loss 0.5452\n",
            "epoch 4/100, batch 56/75, loss 0.5463\n",
            "epoch 4/100, batch 57/75, loss 0.5520\n",
            "epoch 4/100, batch 58/75, loss 0.5553\n",
            "epoch 4/100, batch 59/75, loss 0.5513\n",
            "epoch 4/100, batch 60/75, loss 0.5502\n",
            "epoch 4/100, batch 61/75, loss 0.5570\n",
            "epoch 4/100, batch 62/75, loss 0.5536\n",
            "epoch 4/100, batch 63/75, loss 0.5528\n",
            "epoch 4/100, batch 64/75, loss 0.5532\n",
            "epoch 4/100, batch 65/75, loss 0.5546\n",
            "epoch 4/100, batch 66/75, loss 0.5488\n",
            "epoch 4/100, batch 67/75, loss 0.5476\n",
            "epoch 4/100, batch 68/75, loss 0.5465\n",
            "epoch 4/100, batch 69/75, loss 0.5520\n",
            "epoch 4/100, batch 70/75, loss 0.5423\n",
            "epoch 4/100, batch 71/75, loss 0.5345\n",
            "epoch 4/100, batch 72/75, loss 0.5345\n",
            "epoch 4/100, batch 73/75, loss 0.5306\n",
            "epoch 4/100, batch 74/75, loss 0.5282\n",
            "epoch 4/100, batch 75/75, loss 0.5329\n",
            "epoch 4/100, training roc_auc_score 0.6868\n",
            "epoch 4/100, validation roc_auc_score 0.8280, best validation roc_auc_score 0.8280\n",
            "epoch 5/100, batch 1/75, loss 0.9908\n",
            "epoch 5/100, batch 2/75, loss 0.9301\n",
            "epoch 5/100, batch 3/75, loss 0.6328\n",
            "epoch 5/100, batch 4/75, loss 0.6614\n",
            "epoch 5/100, batch 5/75, loss 0.5126\n",
            "epoch 5/100, batch 6/75, loss 0.5138\n",
            "epoch 5/100, batch 7/75, loss 0.5127\n",
            "epoch 5/100, batch 8/75, loss 1.2309\n",
            "epoch 5/100, batch 9/75, loss 0.5012\n",
            "epoch 5/100, batch 10/75, loss 1.5474\n",
            "epoch 5/100, batch 11/75, loss 0.8434\n",
            "epoch 5/100, batch 12/75, loss 0.5067\n",
            "epoch 5/100, batch 13/75, loss 0.6038\n",
            "epoch 5/100, batch 14/75, loss 0.5012\n",
            "epoch 5/100, batch 15/75, loss 2.8696\n",
            "epoch 5/100, batch 16/75, loss 0.5001\n",
            "epoch 5/100, batch 17/75, loss 0.5016\n",
            "epoch 5/100, batch 18/75, loss 0.5077\n",
            "epoch 5/100, batch 19/75, loss 0.5053\n",
            "epoch 5/100, batch 20/75, loss 2.3260\n",
            "epoch 5/100, batch 21/75, loss 0.5076\n",
            "epoch 5/100, batch 22/75, loss 0.5056\n",
            "epoch 5/100, batch 23/75, loss 0.5061\n",
            "epoch 5/100, batch 24/75, loss 0.5173\n",
            "epoch 5/100, batch 25/75, loss 0.5182\n",
            "epoch 5/100, batch 26/75, loss 3.7548\n",
            "epoch 5/100, batch 27/75, loss 0.4998\n",
            "epoch 5/100, batch 28/75, loss 0.5205\n",
            "epoch 5/100, batch 29/75, loss 0.5149\n",
            "epoch 5/100, batch 30/75, loss 0.5193\n",
            "epoch 5/100, batch 31/75, loss 0.5375\n",
            "epoch 5/100, batch 32/75, loss 0.5156\n",
            "epoch 5/100, batch 33/75, loss 0.5201\n",
            "epoch 5/100, batch 34/75, loss 0.5348\n",
            "epoch 5/100, batch 35/75, loss 0.5297\n",
            "epoch 5/100, batch 36/75, loss 3.5516\n",
            "epoch 5/100, batch 37/75, loss 0.7310\n",
            "epoch 5/100, batch 38/75, loss 0.4927\n",
            "epoch 5/100, batch 39/75, loss 0.4998\n",
            "epoch 5/100, batch 40/75, loss 0.5059\n",
            "epoch 5/100, batch 41/75, loss 0.4957\n",
            "epoch 5/100, batch 42/75, loss 0.4979\n",
            "epoch 5/100, batch 43/75, loss 0.5088\n",
            "epoch 5/100, batch 44/75, loss 0.5042\n",
            "epoch 5/100, batch 45/75, loss 0.4968\n",
            "epoch 5/100, batch 46/75, loss 0.5009\n",
            "epoch 5/100, batch 47/75, loss 0.5083\n",
            "epoch 5/100, batch 48/75, loss 0.4890\n",
            "epoch 5/100, batch 49/75, loss 0.4901\n",
            "epoch 5/100, batch 50/75, loss 0.4913\n",
            "epoch 5/100, batch 51/75, loss 0.4844\n",
            "epoch 5/100, batch 52/75, loss 0.4811\n",
            "epoch 5/100, batch 53/75, loss 13.0460\n",
            "epoch 5/100, batch 54/75, loss 1.0167\n",
            "epoch 5/100, batch 55/75, loss 0.4984\n",
            "epoch 5/100, batch 56/75, loss 0.5053\n",
            "epoch 5/100, batch 57/75, loss 0.5146\n",
            "epoch 5/100, batch 58/75, loss 0.5203\n",
            "epoch 5/100, batch 59/75, loss 0.5265\n",
            "epoch 5/100, batch 60/75, loss 0.5184\n",
            "epoch 5/100, batch 61/75, loss 0.5261\n",
            "epoch 5/100, batch 62/75, loss 0.5298\n",
            "epoch 5/100, batch 63/75, loss 0.5249\n",
            "epoch 5/100, batch 64/75, loss 0.5315\n",
            "epoch 5/100, batch 65/75, loss 0.5257\n",
            "epoch 5/100, batch 66/75, loss 0.5206\n",
            "epoch 5/100, batch 67/75, loss 0.5193\n",
            "epoch 5/100, batch 68/75, loss 0.5198\n",
            "epoch 5/100, batch 69/75, loss 0.5280\n",
            "epoch 5/100, batch 70/75, loss 0.5118\n",
            "epoch 5/100, batch 71/75, loss 0.5017\n",
            "epoch 5/100, batch 72/75, loss 0.5066\n",
            "epoch 5/100, batch 73/75, loss 0.4976\n",
            "epoch 5/100, batch 74/75, loss 0.4960\n",
            "epoch 5/100, batch 75/75, loss 0.4969\n",
            "epoch 5/100, training roc_auc_score 0.7088\n",
            "epoch 5/100, validation roc_auc_score 0.8332, best validation roc_auc_score 0.8332\n",
            "epoch 6/100, batch 1/75, loss 0.8101\n",
            "epoch 6/100, batch 2/75, loss 0.9193\n",
            "epoch 6/100, batch 3/75, loss 0.6039\n",
            "epoch 6/100, batch 4/75, loss 0.6237\n",
            "epoch 6/100, batch 5/75, loss 0.4723\n",
            "epoch 6/100, batch 6/75, loss 0.4704\n",
            "epoch 6/100, batch 7/75, loss 0.4671\n",
            "epoch 6/100, batch 8/75, loss 1.3013\n",
            "epoch 6/100, batch 9/75, loss 0.4582\n",
            "epoch 6/100, batch 10/75, loss 1.8247\n",
            "epoch 6/100, batch 11/75, loss 0.7510\n",
            "epoch 6/100, batch 12/75, loss 0.4688\n",
            "epoch 6/100, batch 13/75, loss 0.5353\n",
            "epoch 6/100, batch 14/75, loss 0.4605\n",
            "epoch 6/100, batch 15/75, loss 2.8686\n",
            "epoch 6/100, batch 16/75, loss 0.4638\n",
            "epoch 6/100, batch 17/75, loss 0.4647\n",
            "epoch 6/100, batch 18/75, loss 0.4714\n",
            "epoch 6/100, batch 19/75, loss 0.4692\n",
            "epoch 6/100, batch 20/75, loss 2.1498\n",
            "epoch 6/100, batch 21/75, loss 0.4739\n",
            "epoch 6/100, batch 22/75, loss 0.4753\n",
            "epoch 6/100, batch 23/75, loss 0.4757\n",
            "epoch 6/100, batch 24/75, loss 0.4995\n",
            "epoch 6/100, batch 25/75, loss 0.5028\n",
            "epoch 6/100, batch 26/75, loss 3.7330\n",
            "epoch 6/100, batch 27/75, loss 0.4751\n",
            "epoch 6/100, batch 28/75, loss 0.5069\n",
            "epoch 6/100, batch 29/75, loss 0.4878\n",
            "epoch 6/100, batch 30/75, loss 0.5031\n",
            "epoch 6/100, batch 31/75, loss 0.5240\n",
            "epoch 6/100, batch 32/75, loss 0.4903\n",
            "epoch 6/100, batch 33/75, loss 0.4945\n",
            "epoch 6/100, batch 34/75, loss 0.5234\n",
            "epoch 6/100, batch 35/75, loss 0.5064\n",
            "epoch 6/100, batch 36/75, loss 3.3741\n",
            "epoch 6/100, batch 37/75, loss 0.7476\n",
            "epoch 6/100, batch 38/75, loss 0.4761\n",
            "epoch 6/100, batch 39/75, loss 0.4919\n",
            "epoch 6/100, batch 40/75, loss 0.4934\n",
            "epoch 6/100, batch 41/75, loss 0.4846\n",
            "epoch 6/100, batch 42/75, loss 0.4862\n",
            "epoch 6/100, batch 43/75, loss 0.4976\n",
            "epoch 6/100, batch 44/75, loss 0.4937\n",
            "epoch 6/100, batch 45/75, loss 0.4867\n",
            "epoch 6/100, batch 46/75, loss 0.4869\n",
            "epoch 6/100, batch 47/75, loss 0.5006\n",
            "epoch 6/100, batch 48/75, loss 0.4745\n",
            "epoch 6/100, batch 49/75, loss 0.4694\n",
            "epoch 6/100, batch 50/75, loss 0.4757\n",
            "epoch 6/100, batch 51/75, loss 0.4628\n",
            "epoch 6/100, batch 52/75, loss 0.4564\n",
            "epoch 6/100, batch 53/75, loss 13.1245\n",
            "epoch 6/100, batch 54/75, loss 0.8796\n",
            "epoch 6/100, batch 55/75, loss 0.4657\n",
            "epoch 6/100, batch 56/75, loss 0.4792\n",
            "epoch 6/100, batch 57/75, loss 0.4863\n",
            "epoch 6/100, batch 58/75, loss 0.4826\n",
            "epoch 6/100, batch 59/75, loss 0.4941\n",
            "epoch 6/100, batch 60/75, loss 0.4829\n",
            "epoch 6/100, batch 61/75, loss 0.4893\n",
            "epoch 6/100, batch 62/75, loss 0.4962\n",
            "epoch 6/100, batch 63/75, loss 0.4892\n",
            "epoch 6/100, batch 64/75, loss 0.5002\n",
            "epoch 6/100, batch 65/75, loss 0.4920\n",
            "epoch 6/100, batch 66/75, loss 0.4872\n",
            "epoch 6/100, batch 67/75, loss 0.4881\n",
            "epoch 6/100, batch 68/75, loss 0.4860\n",
            "epoch 6/100, batch 69/75, loss 0.4929\n",
            "epoch 6/100, batch 70/75, loss 0.4763\n",
            "epoch 6/100, batch 71/75, loss 0.4697\n",
            "epoch 6/100, batch 72/75, loss 0.4756\n",
            "epoch 6/100, batch 73/75, loss 0.4653\n",
            "epoch 6/100, batch 74/75, loss 0.4642\n",
            "epoch 6/100, batch 75/75, loss 0.4646\n",
            "epoch 6/100, training roc_auc_score 0.7288\n",
            "EarlyStopping counter: 1 out of 10\n",
            "epoch 6/100, validation roc_auc_score 0.8323, best validation roc_auc_score 0.8332\n",
            "epoch 7/100, batch 1/75, loss 0.7334\n",
            "epoch 7/100, batch 2/75, loss 0.8756\n",
            "epoch 7/100, batch 3/75, loss 0.5467\n",
            "epoch 7/100, batch 4/75, loss 0.5842\n",
            "epoch 7/100, batch 5/75, loss 0.4446\n",
            "epoch 7/100, batch 6/75, loss 0.4390\n",
            "epoch 7/100, batch 7/75, loss 0.4503\n",
            "epoch 7/100, batch 8/75, loss 1.2587\n",
            "epoch 7/100, batch 9/75, loss 0.4283\n",
            "epoch 7/100, batch 10/75, loss 1.3422\n",
            "epoch 7/100, batch 11/75, loss 0.6829\n",
            "epoch 7/100, batch 12/75, loss 0.4362\n",
            "epoch 7/100, batch 13/75, loss 0.4690\n",
            "epoch 7/100, batch 14/75, loss 0.4282\n",
            "epoch 7/100, batch 15/75, loss 2.4845\n",
            "epoch 7/100, batch 16/75, loss 0.4338\n",
            "epoch 7/100, batch 17/75, loss 0.4379\n",
            "epoch 7/100, batch 18/75, loss 0.4457\n",
            "epoch 7/100, batch 19/75, loss 0.4436\n",
            "epoch 7/100, batch 20/75, loss 1.8595\n",
            "epoch 7/100, batch 21/75, loss 0.4515\n",
            "epoch 7/100, batch 22/75, loss 0.4407\n",
            "epoch 7/100, batch 23/75, loss 0.4526\n",
            "epoch 7/100, batch 24/75, loss 0.4732\n",
            "epoch 7/100, batch 25/75, loss 0.4812\n",
            "epoch 7/100, batch 26/75, loss 3.6658\n",
            "epoch 7/100, batch 27/75, loss 0.4523\n",
            "epoch 7/100, batch 28/75, loss 0.4934\n",
            "epoch 7/100, batch 29/75, loss 0.4696\n",
            "epoch 7/100, batch 30/75, loss 0.4992\n",
            "epoch 7/100, batch 31/75, loss 0.4986\n",
            "epoch 7/100, batch 32/75, loss 0.4664\n",
            "epoch 7/100, batch 33/75, loss 0.4723\n",
            "epoch 7/100, batch 34/75, loss 0.4989\n",
            "epoch 7/100, batch 35/75, loss 0.4813\n",
            "epoch 7/100, batch 36/75, loss 3.4112\n",
            "epoch 7/100, batch 37/75, loss 0.7211\n",
            "epoch 7/100, batch 38/75, loss 0.4594\n",
            "epoch 7/100, batch 39/75, loss 0.4800\n",
            "epoch 7/100, batch 40/75, loss 0.4684\n",
            "epoch 7/100, batch 41/75, loss 0.4776\n",
            "epoch 7/100, batch 42/75, loss 0.4774\n",
            "epoch 7/100, batch 43/75, loss 0.4652\n",
            "epoch 7/100, batch 44/75, loss 0.4756\n",
            "epoch 7/100, batch 45/75, loss 0.4727\n",
            "epoch 7/100, batch 46/75, loss 0.4657\n",
            "epoch 7/100, batch 47/75, loss 0.4858\n",
            "epoch 7/100, batch 48/75, loss 0.4610\n",
            "epoch 7/100, batch 49/75, loss 0.4462\n",
            "epoch 7/100, batch 50/75, loss 0.4566\n",
            "epoch 7/100, batch 51/75, loss 0.4456\n",
            "epoch 7/100, batch 52/75, loss 0.4369\n",
            "epoch 7/100, batch 53/75, loss 13.4526\n",
            "epoch 7/100, batch 54/75, loss 0.7298\n",
            "epoch 7/100, batch 55/75, loss 0.4469\n",
            "epoch 7/100, batch 56/75, loss 0.4698\n",
            "epoch 7/100, batch 57/75, loss 0.4735\n",
            "epoch 7/100, batch 58/75, loss 0.4734\n",
            "epoch 7/100, batch 59/75, loss 0.4866\n",
            "epoch 7/100, batch 60/75, loss 0.4765\n",
            "epoch 7/100, batch 61/75, loss 0.4771\n",
            "epoch 7/100, batch 62/75, loss 0.4912\n",
            "epoch 7/100, batch 63/75, loss 0.4772\n",
            "epoch 7/100, batch 64/75, loss 0.4940\n",
            "epoch 7/100, batch 65/75, loss 0.4760\n",
            "epoch 7/100, batch 66/75, loss 0.4696\n",
            "epoch 7/100, batch 67/75, loss 0.4760\n",
            "epoch 7/100, batch 68/75, loss 0.4711\n",
            "epoch 7/100, batch 69/75, loss 0.4779\n",
            "epoch 7/100, batch 70/75, loss 0.4549\n",
            "epoch 7/100, batch 71/75, loss 0.4538\n",
            "epoch 7/100, batch 72/75, loss 0.4604\n",
            "epoch 7/100, batch 73/75, loss 0.4447\n",
            "epoch 7/100, batch 74/75, loss 0.4471\n",
            "epoch 7/100, batch 75/75, loss 0.4430\n",
            "epoch 7/100, training roc_auc_score 0.7592\n",
            "epoch 7/100, validation roc_auc_score 0.8334, best validation roc_auc_score 0.8334\n",
            "epoch 8/100, batch 1/75, loss 0.6485\n",
            "epoch 8/100, batch 2/75, loss 0.8859\n",
            "epoch 8/100, batch 3/75, loss 0.4965\n",
            "epoch 8/100, batch 4/75, loss 0.5461\n",
            "epoch 8/100, batch 5/75, loss 0.4234\n",
            "epoch 8/100, batch 6/75, loss 0.4127\n",
            "epoch 8/100, batch 7/75, loss 0.4285\n",
            "epoch 8/100, batch 8/75, loss 1.2387\n",
            "epoch 8/100, batch 9/75, loss 0.4034\n",
            "epoch 8/100, batch 10/75, loss 1.3163\n",
            "epoch 8/100, batch 11/75, loss 0.6733\n",
            "epoch 8/100, batch 12/75, loss 0.4088\n",
            "epoch 8/100, batch 13/75, loss 0.4445\n",
            "epoch 8/100, batch 14/75, loss 0.4040\n",
            "epoch 8/100, batch 15/75, loss 2.3632\n",
            "epoch 8/100, batch 16/75, loss 0.4081\n",
            "epoch 8/100, batch 17/75, loss 0.4198\n",
            "epoch 8/100, batch 18/75, loss 0.4264\n",
            "epoch 8/100, batch 19/75, loss 0.4215\n",
            "epoch 8/100, batch 20/75, loss 1.8624\n",
            "epoch 8/100, batch 21/75, loss 0.4239\n",
            "epoch 8/100, batch 22/75, loss 0.4194\n",
            "epoch 8/100, batch 23/75, loss 0.4364\n",
            "epoch 8/100, batch 24/75, loss 0.4598\n",
            "epoch 8/100, batch 25/75, loss 0.4676\n",
            "epoch 8/100, batch 26/75, loss 3.4595\n",
            "epoch 8/100, batch 27/75, loss 0.4403\n",
            "epoch 8/100, batch 28/75, loss 0.4741\n",
            "epoch 8/100, batch 29/75, loss 0.4725\n",
            "epoch 8/100, batch 30/75, loss 0.5078\n",
            "epoch 8/100, batch 31/75, loss 0.4985\n",
            "epoch 8/100, batch 32/75, loss 0.4521\n",
            "epoch 8/100, batch 33/75, loss 0.4788\n",
            "epoch 8/100, batch 34/75, loss 0.5179\n",
            "epoch 8/100, batch 35/75, loss 0.4809\n",
            "epoch 8/100, batch 36/75, loss 2.8576\n",
            "epoch 8/100, batch 37/75, loss 0.6514\n",
            "epoch 8/100, batch 38/75, loss 0.4508\n",
            "epoch 8/100, batch 39/75, loss 0.4660\n",
            "epoch 8/100, batch 40/75, loss 0.4540\n",
            "epoch 8/100, batch 41/75, loss 0.4662\n",
            "epoch 8/100, batch 42/75, loss 0.4687\n",
            "epoch 8/100, batch 43/75, loss 0.4403\n",
            "epoch 8/100, batch 44/75, loss 0.4520\n",
            "epoch 8/100, batch 45/75, loss 0.4550\n",
            "epoch 8/100, batch 46/75, loss 0.4444\n",
            "epoch 8/100, batch 47/75, loss 0.4602\n",
            "epoch 8/100, batch 48/75, loss 0.4365\n",
            "epoch 8/100, batch 49/75, loss 0.4227\n",
            "epoch 8/100, batch 50/75, loss 0.4398\n",
            "epoch 8/100, batch 51/75, loss 0.4206\n",
            "epoch 8/100, batch 52/75, loss 0.4095\n",
            "epoch 8/100, batch 53/75, loss 13.6171\n",
            "epoch 8/100, batch 54/75, loss 0.6429\n",
            "epoch 8/100, batch 55/75, loss 0.4187\n",
            "epoch 8/100, batch 56/75, loss 0.4342\n",
            "epoch 8/100, batch 57/75, loss 0.4386\n",
            "epoch 8/100, batch 58/75, loss 0.4352\n",
            "epoch 8/100, batch 59/75, loss 0.4519\n",
            "epoch 8/100, batch 60/75, loss 0.4396\n",
            "epoch 8/100, batch 61/75, loss 0.4428\n",
            "epoch 8/100, batch 62/75, loss 0.4561\n",
            "epoch 8/100, batch 63/75, loss 0.4430\n",
            "epoch 8/100, batch 64/75, loss 0.4556\n",
            "epoch 8/100, batch 65/75, loss 0.4420\n",
            "epoch 8/100, batch 66/75, loss 0.4372\n",
            "epoch 8/100, batch 67/75, loss 0.4471\n",
            "epoch 8/100, batch 68/75, loss 0.4383\n",
            "epoch 8/100, batch 69/75, loss 0.4487\n",
            "epoch 8/100, batch 70/75, loss 0.4275\n",
            "epoch 8/100, batch 71/75, loss 0.4197\n",
            "epoch 8/100, batch 72/75, loss 0.4293\n",
            "epoch 8/100, batch 73/75, loss 0.4187\n",
            "epoch 8/100, batch 74/75, loss 0.4254\n",
            "epoch 8/100, batch 75/75, loss 0.4130\n",
            "epoch 8/100, training roc_auc_score 0.7863\n",
            "epoch 8/100, validation roc_auc_score 0.8350, best validation roc_auc_score 0.8350\n",
            "epoch 9/100, batch 1/75, loss 0.5588\n",
            "epoch 9/100, batch 2/75, loss 0.8629\n",
            "epoch 9/100, batch 3/75, loss 0.4532\n",
            "epoch 9/100, batch 4/75, loss 0.5598\n",
            "epoch 9/100, batch 5/75, loss 0.3971\n",
            "epoch 9/100, batch 6/75, loss 0.3784\n",
            "epoch 9/100, batch 7/75, loss 0.4094\n",
            "epoch 9/100, batch 8/75, loss 1.1446\n",
            "epoch 9/100, batch 9/75, loss 0.3753\n",
            "epoch 9/100, batch 10/75, loss 0.9892\n",
            "epoch 9/100, batch 11/75, loss 0.5876\n",
            "epoch 9/100, batch 12/75, loss 0.3783\n",
            "epoch 9/100, batch 13/75, loss 0.3979\n",
            "epoch 9/100, batch 14/75, loss 0.3795\n",
            "epoch 9/100, batch 15/75, loss 2.0421\n",
            "epoch 9/100, batch 16/75, loss 0.3867\n",
            "epoch 9/100, batch 17/75, loss 0.3954\n",
            "epoch 9/100, batch 18/75, loss 0.4068\n",
            "epoch 9/100, batch 19/75, loss 0.4056\n",
            "epoch 9/100, batch 20/75, loss 1.6291\n",
            "epoch 9/100, batch 21/75, loss 0.3992\n",
            "epoch 9/100, batch 22/75, loss 0.3898\n",
            "epoch 9/100, batch 23/75, loss 0.4077\n",
            "epoch 9/100, batch 24/75, loss 0.4352\n",
            "epoch 9/100, batch 25/75, loss 0.4379\n",
            "epoch 9/100, batch 26/75, loss 3.2009\n",
            "epoch 9/100, batch 27/75, loss 0.4161\n",
            "epoch 9/100, batch 28/75, loss 0.4679\n",
            "epoch 9/100, batch 29/75, loss 0.4688\n",
            "epoch 9/100, batch 30/75, loss 0.4914\n",
            "epoch 9/100, batch 31/75, loss 0.4624\n",
            "epoch 9/100, batch 32/75, loss 0.4310\n",
            "epoch 9/100, batch 33/75, loss 0.4667\n",
            "epoch 9/100, batch 34/75, loss 0.4873\n",
            "epoch 9/100, batch 35/75, loss 0.4440\n",
            "epoch 9/100, batch 36/75, loss 2.6922\n",
            "epoch 9/100, batch 37/75, loss 0.5881\n",
            "epoch 9/100, batch 38/75, loss 0.4229\n",
            "epoch 9/100, batch 39/75, loss 0.4397\n",
            "epoch 9/100, batch 40/75, loss 0.4317\n",
            "epoch 9/100, batch 41/75, loss 0.4361\n",
            "epoch 9/100, batch 42/75, loss 0.4393\n",
            "epoch 9/100, batch 43/75, loss 0.4169\n",
            "epoch 9/100, batch 44/75, loss 0.4329\n",
            "epoch 9/100, batch 45/75, loss 0.4386\n",
            "epoch 9/100, batch 46/75, loss 0.4262\n",
            "epoch 9/100, batch 47/75, loss 0.4475\n",
            "epoch 9/100, batch 48/75, loss 0.4155\n",
            "epoch 9/100, batch 49/75, loss 0.3924\n",
            "epoch 9/100, batch 50/75, loss 0.4256\n",
            "epoch 9/100, batch 51/75, loss 0.4005\n",
            "epoch 9/100, batch 52/75, loss 0.3871\n",
            "epoch 9/100, batch 53/75, loss 13.7119\n",
            "epoch 9/100, batch 54/75, loss 0.6108\n",
            "epoch 9/100, batch 55/75, loss 0.4035\n",
            "epoch 9/100, batch 56/75, loss 0.4116\n",
            "epoch 9/100, batch 57/75, loss 0.4145\n",
            "epoch 9/100, batch 58/75, loss 0.4103\n",
            "epoch 9/100, batch 59/75, loss 0.4320\n",
            "epoch 9/100, batch 60/75, loss 0.4176\n",
            "epoch 9/100, batch 61/75, loss 0.4126\n",
            "epoch 9/100, batch 62/75, loss 0.4305\n",
            "epoch 9/100, batch 63/75, loss 0.4203\n",
            "epoch 9/100, batch 64/75, loss 0.4355\n",
            "epoch 9/100, batch 65/75, loss 0.4179\n",
            "epoch 9/100, batch 66/75, loss 0.4160\n",
            "epoch 9/100, batch 67/75, loss 0.4282\n",
            "epoch 9/100, batch 68/75, loss 0.4130\n",
            "epoch 9/100, batch 69/75, loss 0.4258\n",
            "epoch 9/100, batch 70/75, loss 0.4010\n",
            "epoch 9/100, batch 71/75, loss 0.4037\n",
            "epoch 9/100, batch 72/75, loss 0.4050\n",
            "epoch 9/100, batch 73/75, loss 0.3956\n",
            "epoch 9/100, batch 74/75, loss 0.4118\n",
            "epoch 9/100, batch 75/75, loss 0.3823\n",
            "epoch 9/100, training roc_auc_score 0.8098\n",
            "EarlyStopping counter: 1 out of 10\n",
            "epoch 9/100, validation roc_auc_score 0.8303, best validation roc_auc_score 0.8350\n",
            "epoch 10/100, batch 1/75, loss 0.5617\n",
            "epoch 10/100, batch 2/75, loss 0.8831\n",
            "epoch 10/100, batch 3/75, loss 0.4314\n",
            "epoch 10/100, batch 4/75, loss 0.5480\n",
            "epoch 10/100, batch 5/75, loss 0.3802\n",
            "epoch 10/100, batch 6/75, loss 0.3502\n",
            "epoch 10/100, batch 7/75, loss 0.3944\n",
            "epoch 10/100, batch 8/75, loss 1.1457\n",
            "epoch 10/100, batch 9/75, loss 0.3522\n",
            "epoch 10/100, batch 10/75, loss 0.8982\n",
            "epoch 10/100, batch 11/75, loss 0.5528\n",
            "epoch 10/100, batch 12/75, loss 0.3500\n",
            "epoch 10/100, batch 13/75, loss 0.3901\n",
            "epoch 10/100, batch 14/75, loss 0.3576\n",
            "epoch 10/100, batch 15/75, loss 2.3128\n",
            "epoch 10/100, batch 16/75, loss 0.3693\n",
            "epoch 10/100, batch 17/75, loss 0.3766\n",
            "epoch 10/100, batch 18/75, loss 0.3831\n",
            "epoch 10/100, batch 19/75, loss 0.3812\n",
            "epoch 10/100, batch 20/75, loss 1.8060\n",
            "epoch 10/100, batch 21/75, loss 0.3772\n",
            "epoch 10/100, batch 22/75, loss 0.3643\n",
            "epoch 10/100, batch 23/75, loss 0.3747\n",
            "epoch 10/100, batch 24/75, loss 0.4045\n",
            "epoch 10/100, batch 25/75, loss 0.4121\n",
            "epoch 10/100, batch 26/75, loss 2.9344\n",
            "epoch 10/100, batch 27/75, loss 0.3892\n",
            "epoch 10/100, batch 28/75, loss 0.4444\n",
            "epoch 10/100, batch 29/75, loss 0.4249\n",
            "epoch 10/100, batch 30/75, loss 0.4504\n",
            "epoch 10/100, batch 31/75, loss 0.4209\n",
            "epoch 10/100, batch 32/75, loss 0.3893\n",
            "epoch 10/100, batch 33/75, loss 0.4300\n",
            "epoch 10/100, batch 34/75, loss 0.4651\n",
            "epoch 10/100, batch 35/75, loss 0.4265\n",
            "epoch 10/100, batch 36/75, loss 2.7307\n",
            "epoch 10/100, batch 37/75, loss 0.5287\n",
            "epoch 10/100, batch 38/75, loss 0.4000\n",
            "epoch 10/100, batch 39/75, loss 0.4197\n",
            "epoch 10/100, batch 40/75, loss 0.4126\n",
            "epoch 10/100, batch 41/75, loss 0.4234\n",
            "epoch 10/100, batch 42/75, loss 0.4133\n",
            "epoch 10/100, batch 43/75, loss 0.3828\n",
            "epoch 10/100, batch 44/75, loss 0.4025\n",
            "epoch 10/100, batch 45/75, loss 0.4152\n",
            "epoch 10/100, batch 46/75, loss 0.4032\n",
            "epoch 10/100, batch 47/75, loss 0.4199\n",
            "epoch 10/100, batch 48/75, loss 0.3831\n",
            "epoch 10/100, batch 49/75, loss 0.3747\n",
            "epoch 10/100, batch 50/75, loss 0.4007\n",
            "epoch 10/100, batch 51/75, loss 0.3757\n",
            "epoch 10/100, batch 52/75, loss 0.3585\n",
            "epoch 10/100, batch 53/75, loss 13.9687\n",
            "epoch 10/100, batch 54/75, loss 0.5896\n",
            "epoch 10/100, batch 55/75, loss 0.3850\n",
            "epoch 10/100, batch 56/75, loss 0.4012\n",
            "epoch 10/100, batch 57/75, loss 0.4086\n",
            "epoch 10/100, batch 58/75, loss 0.4105\n",
            "epoch 10/100, batch 59/75, loss 0.4249\n",
            "epoch 10/100, batch 60/75, loss 0.4348\n",
            "epoch 10/100, batch 61/75, loss 0.4250\n",
            "epoch 10/100, batch 62/75, loss 0.4538\n",
            "epoch 10/100, batch 63/75, loss 0.4385\n",
            "epoch 10/100, batch 64/75, loss 0.4626\n",
            "epoch 10/100, batch 65/75, loss 0.4408\n",
            "epoch 10/100, batch 66/75, loss 0.4344\n",
            "epoch 10/100, batch 67/75, loss 0.4508\n",
            "epoch 10/100, batch 68/75, loss 0.4291\n",
            "epoch 10/100, batch 69/75, loss 0.4433\n",
            "epoch 10/100, batch 70/75, loss 0.4123\n",
            "epoch 10/100, batch 71/75, loss 0.4102\n",
            "epoch 10/100, batch 72/75, loss 0.4103\n",
            "epoch 10/100, batch 73/75, loss 0.3977\n",
            "epoch 10/100, batch 74/75, loss 0.4186\n",
            "epoch 10/100, batch 75/75, loss 0.3789\n",
            "epoch 10/100, training roc_auc_score 0.8162\n",
            "epoch 10/100, validation roc_auc_score 0.8547, best validation roc_auc_score 0.8547\n",
            "epoch 11/100, batch 1/75, loss 0.5704\n",
            "epoch 11/100, batch 2/75, loss 0.8893\n",
            "epoch 11/100, batch 3/75, loss 0.4151\n",
            "epoch 11/100, batch 4/75, loss 0.5751\n",
            "epoch 11/100, batch 5/75, loss 0.3826\n",
            "epoch 11/100, batch 6/75, loss 0.3376\n",
            "epoch 11/100, batch 7/75, loss 0.3854\n",
            "epoch 11/100, batch 8/75, loss 1.0579\n",
            "epoch 11/100, batch 9/75, loss 0.3400\n",
            "epoch 11/100, batch 10/75, loss 0.7274\n",
            "epoch 11/100, batch 11/75, loss 0.5238\n",
            "epoch 11/100, batch 12/75, loss 0.3439\n",
            "epoch 11/100, batch 13/75, loss 0.3928\n",
            "epoch 11/100, batch 14/75, loss 0.3567\n",
            "epoch 11/100, batch 15/75, loss 1.9995\n",
            "epoch 11/100, batch 16/75, loss 0.3650\n",
            "epoch 11/100, batch 17/75, loss 0.3792\n",
            "epoch 11/100, batch 18/75, loss 0.3852\n",
            "epoch 11/100, batch 19/75, loss 0.3715\n",
            "epoch 11/100, batch 20/75, loss 1.6085\n",
            "epoch 11/100, batch 21/75, loss 0.3660\n",
            "epoch 11/100, batch 22/75, loss 0.3403\n",
            "epoch 11/100, batch 23/75, loss 0.3529\n",
            "epoch 11/100, batch 24/75, loss 0.3965\n",
            "epoch 11/100, batch 25/75, loss 0.3984\n",
            "epoch 11/100, batch 26/75, loss 2.7410\n",
            "epoch 11/100, batch 27/75, loss 0.3589\n",
            "epoch 11/100, batch 28/75, loss 0.4155\n",
            "epoch 11/100, batch 29/75, loss 0.3982\n",
            "epoch 11/100, batch 30/75, loss 0.4195\n",
            "epoch 11/100, batch 31/75, loss 0.3823\n",
            "epoch 11/100, batch 32/75, loss 0.3687\n",
            "epoch 11/100, batch 33/75, loss 0.3937\n",
            "epoch 11/100, batch 34/75, loss 0.4291\n",
            "epoch 11/100, batch 35/75, loss 0.3992\n",
            "epoch 11/100, batch 36/75, loss 2.4049\n",
            "epoch 11/100, batch 37/75, loss 0.5255\n",
            "epoch 11/100, batch 38/75, loss 0.3773\n",
            "epoch 11/100, batch 39/75, loss 0.3844\n",
            "epoch 11/100, batch 40/75, loss 0.3865\n",
            "epoch 11/100, batch 41/75, loss 0.4107\n",
            "epoch 11/100, batch 42/75, loss 0.4149\n",
            "epoch 11/100, batch 43/75, loss 0.3765\n",
            "epoch 11/100, batch 44/75, loss 0.3935\n",
            "epoch 11/100, batch 45/75, loss 0.4077\n",
            "epoch 11/100, batch 46/75, loss 0.3978\n",
            "epoch 11/100, batch 47/75, loss 0.4204\n",
            "epoch 11/100, batch 48/75, loss 0.3765\n",
            "epoch 11/100, batch 49/75, loss 0.3662\n",
            "epoch 11/100, batch 50/75, loss 0.4003\n",
            "epoch 11/100, batch 51/75, loss 0.3635\n",
            "epoch 11/100, batch 52/75, loss 0.3526\n",
            "epoch 11/100, batch 53/75, loss 13.3159\n",
            "epoch 11/100, batch 54/75, loss 0.5312\n",
            "epoch 11/100, batch 55/75, loss 0.3777\n",
            "epoch 11/100, batch 56/75, loss 0.3797\n",
            "epoch 11/100, batch 57/75, loss 0.3807\n",
            "epoch 11/100, batch 58/75, loss 0.3735\n",
            "epoch 11/100, batch 59/75, loss 0.3881\n",
            "epoch 11/100, batch 60/75, loss 0.3872\n",
            "epoch 11/100, batch 61/75, loss 0.3855\n",
            "epoch 11/100, batch 62/75, loss 0.4146\n",
            "epoch 11/100, batch 63/75, loss 0.3943\n",
            "epoch 11/100, batch 64/75, loss 0.4122\n",
            "epoch 11/100, batch 65/75, loss 0.4018\n",
            "epoch 11/100, batch 66/75, loss 0.3898\n",
            "epoch 11/100, batch 67/75, loss 0.4040\n",
            "epoch 11/100, batch 68/75, loss 0.3793\n",
            "epoch 11/100, batch 69/75, loss 0.3922\n",
            "epoch 11/100, batch 70/75, loss 0.3680\n",
            "epoch 11/100, batch 71/75, loss 0.3710\n",
            "epoch 11/100, batch 72/75, loss 0.3689\n",
            "epoch 11/100, batch 73/75, loss 0.3597\n",
            "epoch 11/100, batch 74/75, loss 0.3859\n",
            "epoch 11/100, batch 75/75, loss 0.3400\n",
            "epoch 11/100, training roc_auc_score 0.8482\n",
            "EarlyStopping counter: 1 out of 10\n",
            "epoch 11/100, validation roc_auc_score 0.8310, best validation roc_auc_score 0.8547\n",
            "epoch 12/100, batch 1/75, loss 0.5650\n",
            "epoch 12/100, batch 2/75, loss 0.7836\n",
            "epoch 12/100, batch 3/75, loss 0.3844\n",
            "epoch 12/100, batch 4/75, loss 0.5111\n",
            "epoch 12/100, batch 5/75, loss 0.3494\n",
            "epoch 12/100, batch 6/75, loss 0.3158\n",
            "epoch 12/100, batch 7/75, loss 0.3544\n",
            "epoch 12/100, batch 8/75, loss 0.9747\n",
            "epoch 12/100, batch 9/75, loss 0.3129\n",
            "epoch 12/100, batch 10/75, loss 0.6856\n",
            "epoch 12/100, batch 11/75, loss 0.5112\n",
            "epoch 12/100, batch 12/75, loss 0.3078\n",
            "epoch 12/100, batch 13/75, loss 0.3754\n",
            "epoch 12/100, batch 14/75, loss 0.3267\n",
            "epoch 12/100, batch 15/75, loss 1.9231\n",
            "epoch 12/100, batch 16/75, loss 0.3383\n",
            "epoch 12/100, batch 17/75, loss 0.3504\n",
            "epoch 12/100, batch 18/75, loss 0.3653\n",
            "epoch 12/100, batch 19/75, loss 0.3544\n",
            "epoch 12/100, batch 20/75, loss 1.6548\n",
            "epoch 12/100, batch 21/75, loss 0.3441\n",
            "epoch 12/100, batch 22/75, loss 0.3248\n",
            "epoch 12/100, batch 23/75, loss 0.3309\n",
            "epoch 12/100, batch 24/75, loss 0.3768\n",
            "epoch 12/100, batch 25/75, loss 0.3788\n",
            "epoch 12/100, batch 26/75, loss 2.4140\n",
            "epoch 12/100, batch 27/75, loss 0.3422\n",
            "epoch 12/100, batch 28/75, loss 0.4021\n",
            "epoch 12/100, batch 29/75, loss 0.3844\n",
            "epoch 12/100, batch 30/75, loss 0.4161\n",
            "epoch 12/100, batch 31/75, loss 0.3728\n",
            "epoch 12/100, batch 32/75, loss 0.3570\n",
            "epoch 12/100, batch 33/75, loss 0.3840\n",
            "epoch 12/100, batch 34/75, loss 0.4303\n",
            "epoch 12/100, batch 35/75, loss 0.3965\n",
            "epoch 12/100, batch 36/75, loss 2.3650\n",
            "epoch 12/100, batch 37/75, loss 0.5177\n",
            "epoch 12/100, batch 38/75, loss 0.3559\n",
            "epoch 12/100, batch 39/75, loss 0.3674\n",
            "epoch 12/100, batch 40/75, loss 0.3686\n",
            "epoch 12/100, batch 41/75, loss 0.3988\n",
            "epoch 12/100, batch 42/75, loss 0.3863\n",
            "epoch 12/100, batch 43/75, loss 0.3440\n",
            "epoch 12/100, batch 44/75, loss 0.3656\n",
            "epoch 12/100, batch 45/75, loss 0.3748\n",
            "epoch 12/100, batch 46/75, loss 0.3694\n",
            "epoch 12/100, batch 47/75, loss 0.3997\n",
            "epoch 12/100, batch 48/75, loss 0.3450\n",
            "epoch 12/100, batch 49/75, loss 0.3421\n",
            "epoch 12/100, batch 50/75, loss 0.3706\n",
            "epoch 12/100, batch 51/75, loss 0.3410\n",
            "epoch 12/100, batch 52/75, loss 0.3213\n",
            "epoch 12/100, batch 53/75, loss 13.3353\n",
            "epoch 12/100, batch 54/75, loss 0.4592\n",
            "epoch 12/100, batch 55/75, loss 0.3598\n",
            "epoch 12/100, batch 56/75, loss 0.3735\n",
            "epoch 12/100, batch 57/75, loss 0.3726\n",
            "epoch 12/100, batch 58/75, loss 0.3644\n",
            "epoch 12/100, batch 59/75, loss 0.3788\n",
            "epoch 12/100, batch 60/75, loss 0.3874\n",
            "epoch 12/100, batch 61/75, loss 0.3886\n",
            "epoch 12/100, batch 62/75, loss 0.4190\n",
            "epoch 12/100, batch 63/75, loss 0.3848\n",
            "epoch 12/100, batch 64/75, loss 0.4071\n",
            "epoch 12/100, batch 65/75, loss 0.4038\n",
            "epoch 12/100, batch 66/75, loss 0.3810\n",
            "epoch 12/100, batch 67/75, loss 0.3991\n",
            "epoch 12/100, batch 68/75, loss 0.3857\n",
            "epoch 12/100, batch 69/75, loss 0.3877\n",
            "epoch 12/100, batch 70/75, loss 0.3684\n",
            "epoch 12/100, batch 71/75, loss 0.3574\n",
            "epoch 12/100, batch 72/75, loss 0.3617\n",
            "epoch 12/100, batch 73/75, loss 0.3516\n",
            "epoch 12/100, batch 74/75, loss 0.3879\n",
            "epoch 12/100, batch 75/75, loss 0.3340\n",
            "epoch 12/100, training roc_auc_score 0.8628\n",
            "EarlyStopping counter: 2 out of 10\n",
            "epoch 12/100, validation roc_auc_score 0.8048, best validation roc_auc_score 0.8547\n",
            "epoch 13/100, batch 1/75, loss 0.4945\n",
            "epoch 13/100, batch 2/75, loss 0.7498\n",
            "epoch 13/100, batch 3/75, loss 0.3818\n",
            "epoch 13/100, batch 4/75, loss 0.4945\n",
            "epoch 13/100, batch 5/75, loss 0.3430\n",
            "epoch 13/100, batch 6/75, loss 0.3031\n",
            "epoch 13/100, batch 7/75, loss 0.3478\n",
            "epoch 13/100, batch 8/75, loss 0.8836\n",
            "epoch 13/100, batch 9/75, loss 0.3074\n",
            "epoch 13/100, batch 10/75, loss 0.5954\n",
            "epoch 13/100, batch 11/75, loss 0.4502\n",
            "epoch 13/100, batch 12/75, loss 0.3061\n",
            "epoch 13/100, batch 13/75, loss 0.3457\n",
            "epoch 13/100, batch 14/75, loss 0.3274\n",
            "epoch 13/100, batch 15/75, loss 1.5626\n",
            "epoch 13/100, batch 16/75, loss 0.3372\n",
            "epoch 13/100, batch 17/75, loss 0.3613\n",
            "epoch 13/100, batch 18/75, loss 0.3777\n",
            "epoch 13/100, batch 19/75, loss 0.3502\n",
            "epoch 13/100, batch 20/75, loss 1.3510\n",
            "epoch 13/100, batch 21/75, loss 0.3230\n",
            "epoch 13/100, batch 22/75, loss 0.3093\n",
            "epoch 13/100, batch 23/75, loss 0.3213\n",
            "epoch 13/100, batch 24/75, loss 0.3528\n",
            "epoch 13/100, batch 25/75, loss 0.3445\n",
            "epoch 13/100, batch 26/75, loss 2.2973\n",
            "epoch 13/100, batch 27/75, loss 0.3267\n",
            "epoch 13/100, batch 28/75, loss 0.3831\n",
            "epoch 13/100, batch 29/75, loss 0.3565\n",
            "epoch 13/100, batch 30/75, loss 0.3831\n",
            "epoch 13/100, batch 31/75, loss 0.3456\n",
            "epoch 13/100, batch 32/75, loss 0.3388\n",
            "epoch 13/100, batch 33/75, loss 0.3544\n",
            "epoch 13/100, batch 34/75, loss 0.4046\n",
            "epoch 13/100, batch 35/75, loss 0.3684\n",
            "epoch 13/100, batch 36/75, loss 2.2066\n",
            "epoch 13/100, batch 37/75, loss 0.4917\n",
            "epoch 13/100, batch 38/75, loss 0.3234\n",
            "epoch 13/100, batch 39/75, loss 0.3338\n",
            "epoch 13/100, batch 40/75, loss 0.3423\n",
            "epoch 13/100, batch 41/75, loss 0.3566\n",
            "epoch 13/100, batch 42/75, loss 0.3367\n",
            "epoch 13/100, batch 43/75, loss 0.3137\n",
            "epoch 13/100, batch 44/75, loss 0.3259\n",
            "epoch 13/100, batch 45/75, loss 0.3419\n",
            "epoch 13/100, batch 46/75, loss 0.3326\n",
            "epoch 13/100, batch 47/75, loss 0.3744\n",
            "epoch 13/100, batch 48/75, loss 0.3107\n",
            "epoch 13/100, batch 49/75, loss 0.3121\n",
            "epoch 13/100, batch 50/75, loss 0.3475\n",
            "epoch 13/100, batch 51/75, loss 0.3271\n",
            "epoch 13/100, batch 52/75, loss 0.2970\n",
            "epoch 13/100, batch 53/75, loss 13.7053\n",
            "epoch 13/100, batch 54/75, loss 0.4732\n",
            "epoch 13/100, batch 55/75, loss 0.3389\n",
            "epoch 13/100, batch 56/75, loss 0.3503\n",
            "epoch 13/100, batch 57/75, loss 0.3429\n",
            "epoch 13/100, batch 58/75, loss 0.3417\n",
            "epoch 13/100, batch 59/75, loss 0.3601\n",
            "epoch 13/100, batch 60/75, loss 0.3660\n",
            "epoch 13/100, batch 61/75, loss 0.3673\n",
            "epoch 13/100, batch 62/75, loss 0.3887\n",
            "epoch 13/100, batch 63/75, loss 0.3665\n",
            "epoch 13/100, batch 64/75, loss 0.3910\n",
            "epoch 13/100, batch 65/75, loss 0.3777\n",
            "epoch 13/100, batch 66/75, loss 0.3603\n",
            "epoch 13/100, batch 67/75, loss 0.3920\n",
            "epoch 13/100, batch 68/75, loss 0.3640\n",
            "epoch 13/100, batch 69/75, loss 0.3779\n",
            "epoch 13/100, batch 70/75, loss 0.3480\n",
            "epoch 13/100, batch 71/75, loss 0.3503\n",
            "epoch 13/100, batch 72/75, loss 0.3414\n",
            "epoch 13/100, batch 73/75, loss 0.3385\n",
            "epoch 13/100, batch 74/75, loss 0.3740\n",
            "epoch 13/100, batch 75/75, loss 0.3160\n",
            "epoch 13/100, training roc_auc_score 0.8724\n",
            "EarlyStopping counter: 3 out of 10\n",
            "epoch 13/100, validation roc_auc_score 0.8003, best validation roc_auc_score 0.8547\n",
            "epoch 14/100, batch 1/75, loss 0.5202\n",
            "epoch 14/100, batch 2/75, loss 0.6016\n",
            "epoch 14/100, batch 3/75, loss 0.3502\n",
            "epoch 14/100, batch 4/75, loss 0.4539\n",
            "epoch 14/100, batch 5/75, loss 0.3257\n",
            "epoch 14/100, batch 6/75, loss 0.2824\n",
            "epoch 14/100, batch 7/75, loss 0.3314\n",
            "epoch 14/100, batch 8/75, loss 1.0199\n",
            "epoch 14/100, batch 9/75, loss 0.2898\n",
            "epoch 14/100, batch 10/75, loss 0.5173\n",
            "epoch 14/100, batch 11/75, loss 0.5127\n",
            "epoch 14/100, batch 12/75, loss 0.2874\n",
            "epoch 14/100, batch 13/75, loss 0.3582\n",
            "epoch 14/100, batch 14/75, loss 0.3080\n",
            "epoch 14/100, batch 15/75, loss 1.5824\n",
            "epoch 14/100, batch 16/75, loss 0.3249\n",
            "epoch 14/100, batch 17/75, loss 0.3346\n",
            "epoch 14/100, batch 18/75, loss 0.3565\n",
            "epoch 14/100, batch 19/75, loss 0.3276\n",
            "epoch 14/100, batch 20/75, loss 1.4191\n",
            "epoch 14/100, batch 21/75, loss 0.2995\n",
            "epoch 14/100, batch 22/75, loss 0.2910\n",
            "epoch 14/100, batch 23/75, loss 0.3021\n",
            "epoch 14/100, batch 24/75, loss 0.3322\n",
            "epoch 14/100, batch 25/75, loss 0.3479\n",
            "epoch 14/100, batch 26/75, loss 1.9137\n",
            "epoch 14/100, batch 27/75, loss 0.3123\n",
            "epoch 14/100, batch 28/75, loss 0.3621\n",
            "epoch 14/100, batch 29/75, loss 0.3375\n",
            "epoch 14/100, batch 30/75, loss 0.3610\n",
            "epoch 14/100, batch 31/75, loss 0.3204\n",
            "epoch 14/100, batch 32/75, loss 0.3281\n",
            "epoch 14/100, batch 33/75, loss 0.3283\n",
            "epoch 14/100, batch 34/75, loss 0.3656\n",
            "epoch 14/100, batch 35/75, loss 0.3366\n",
            "epoch 14/100, batch 36/75, loss 2.0945\n",
            "epoch 14/100, batch 37/75, loss 0.4867\n",
            "epoch 14/100, batch 38/75, loss 0.3078\n",
            "epoch 14/100, batch 39/75, loss 0.3168\n",
            "epoch 14/100, batch 40/75, loss 0.3342\n",
            "epoch 14/100, batch 41/75, loss 0.3524\n",
            "epoch 14/100, batch 42/75, loss 0.3324\n",
            "epoch 14/100, batch 43/75, loss 0.3027\n",
            "epoch 14/100, batch 44/75, loss 0.3202\n",
            "epoch 14/100, batch 45/75, loss 0.3322\n",
            "epoch 14/100, batch 46/75, loss 0.3354\n",
            "epoch 14/100, batch 47/75, loss 0.3660\n",
            "epoch 14/100, batch 48/75, loss 0.3122\n",
            "epoch 14/100, batch 49/75, loss 0.3011\n",
            "epoch 14/100, batch 50/75, loss 0.3513\n",
            "epoch 14/100, batch 51/75, loss 0.3254\n",
            "epoch 14/100, batch 52/75, loss 0.3021\n",
            "epoch 14/100, batch 53/75, loss 13.3951\n",
            "epoch 14/100, batch 54/75, loss 0.4385\n",
            "epoch 14/100, batch 55/75, loss 0.3357\n",
            "epoch 14/100, batch 56/75, loss 0.3432\n",
            "epoch 14/100, batch 57/75, loss 0.3348\n",
            "epoch 14/100, batch 58/75, loss 0.3247\n",
            "epoch 14/100, batch 59/75, loss 0.3408\n",
            "epoch 14/100, batch 60/75, loss 0.3547\n",
            "epoch 14/100, batch 61/75, loss 0.3528\n",
            "epoch 14/100, batch 62/75, loss 0.3838\n",
            "epoch 14/100, batch 63/75, loss 0.3541\n",
            "epoch 14/100, batch 64/75, loss 0.3803\n",
            "epoch 14/100, batch 65/75, loss 0.3878\n",
            "epoch 14/100, batch 66/75, loss 0.3624\n",
            "epoch 14/100, batch 67/75, loss 0.3684\n",
            "epoch 14/100, batch 68/75, loss 0.3665\n",
            "epoch 14/100, batch 69/75, loss 0.3709\n",
            "epoch 14/100, batch 70/75, loss 0.3366\n",
            "epoch 14/100, batch 71/75, loss 0.3317\n",
            "epoch 14/100, batch 72/75, loss 0.3285\n",
            "epoch 14/100, batch 73/75, loss 0.3252\n",
            "epoch 14/100, batch 74/75, loss 0.3564\n",
            "epoch 14/100, batch 75/75, loss 0.2973\n",
            "epoch 14/100, training roc_auc_score 0.8843\n",
            "EarlyStopping counter: 4 out of 10\n",
            "epoch 14/100, validation roc_auc_score 0.8255, best validation roc_auc_score 0.8547\n",
            "epoch 15/100, batch 1/75, loss 0.5053\n",
            "epoch 15/100, batch 2/75, loss 0.5795\n",
            "epoch 15/100, batch 3/75, loss 0.3463\n",
            "epoch 15/100, batch 4/75, loss 0.4979\n",
            "epoch 15/100, batch 5/75, loss 0.3063\n",
            "epoch 15/100, batch 6/75, loss 0.2631\n",
            "epoch 15/100, batch 7/75, loss 0.3125\n",
            "epoch 15/100, batch 8/75, loss 0.7761\n",
            "epoch 15/100, batch 9/75, loss 0.2617\n",
            "epoch 15/100, batch 10/75, loss 0.4889\n",
            "epoch 15/100, batch 11/75, loss 0.4456\n",
            "epoch 15/100, batch 12/75, loss 0.2718\n",
            "epoch 15/100, batch 13/75, loss 0.3355\n",
            "epoch 15/100, batch 14/75, loss 0.3058\n",
            "epoch 15/100, batch 15/75, loss 1.5662\n",
            "epoch 15/100, batch 16/75, loss 0.3147\n",
            "epoch 15/100, batch 17/75, loss 0.3239\n",
            "epoch 15/100, batch 18/75, loss 0.3450\n",
            "epoch 15/100, batch 19/75, loss 0.2970\n",
            "epoch 15/100, batch 20/75, loss 1.2140\n",
            "epoch 15/100, batch 21/75, loss 0.2712\n",
            "epoch 15/100, batch 22/75, loss 0.2707\n",
            "epoch 15/100, batch 23/75, loss 0.2754\n",
            "epoch 15/100, batch 24/75, loss 0.2983\n",
            "epoch 15/100, batch 25/75, loss 0.3010\n",
            "epoch 15/100, batch 26/75, loss 1.9422\n",
            "epoch 15/100, batch 27/75, loss 0.2947\n",
            "epoch 15/100, batch 28/75, loss 0.3388\n",
            "epoch 15/100, batch 29/75, loss 0.3123\n",
            "epoch 15/100, batch 30/75, loss 0.3551\n",
            "epoch 15/100, batch 31/75, loss 0.3160\n",
            "epoch 15/100, batch 32/75, loss 0.3060\n",
            "epoch 15/100, batch 33/75, loss 0.3239\n",
            "epoch 15/100, batch 34/75, loss 0.3878\n",
            "epoch 15/100, batch 35/75, loss 0.3383\n",
            "epoch 15/100, batch 36/75, loss 1.9715\n",
            "epoch 15/100, batch 37/75, loss 0.4230\n",
            "epoch 15/100, batch 38/75, loss 0.2850\n",
            "epoch 15/100, batch 39/75, loss 0.3144\n",
            "epoch 15/100, batch 40/75, loss 0.3209\n",
            "epoch 15/100, batch 41/75, loss 0.3576\n",
            "epoch 15/100, batch 42/75, loss 0.3239\n",
            "epoch 15/100, batch 43/75, loss 0.2903\n",
            "epoch 15/100, batch 44/75, loss 0.3272\n",
            "epoch 15/100, batch 45/75, loss 0.3298\n",
            "epoch 15/100, batch 46/75, loss 0.3239\n",
            "epoch 15/100, batch 47/75, loss 0.3637\n",
            "epoch 15/100, batch 48/75, loss 0.2950\n",
            "epoch 15/100, batch 49/75, loss 0.3021\n",
            "epoch 15/100, batch 50/75, loss 0.3325\n",
            "epoch 15/100, batch 51/75, loss 0.3167\n",
            "epoch 15/100, batch 52/75, loss 0.2871\n",
            "epoch 15/100, batch 53/75, loss 12.7616\n",
            "epoch 15/100, batch 54/75, loss 0.3908\n",
            "epoch 15/100, batch 55/75, loss 0.3302\n",
            "epoch 15/100, batch 56/75, loss 0.3437\n",
            "epoch 15/100, batch 57/75, loss 0.3377\n",
            "epoch 15/100, batch 58/75, loss 0.3300\n",
            "epoch 15/100, batch 59/75, loss 0.3422\n",
            "epoch 15/100, batch 60/75, loss 0.3736\n",
            "epoch 15/100, batch 61/75, loss 0.3680\n",
            "epoch 15/100, batch 62/75, loss 0.4098\n",
            "epoch 15/100, batch 63/75, loss 0.3780\n",
            "epoch 15/100, batch 64/75, loss 0.4041\n",
            "epoch 15/100, batch 65/75, loss 0.4124\n",
            "epoch 15/100, batch 66/75, loss 0.3725\n",
            "epoch 15/100, batch 67/75, loss 0.3790\n",
            "epoch 15/100, batch 68/75, loss 0.3777\n",
            "epoch 15/100, batch 69/75, loss 0.3777\n",
            "epoch 15/100, batch 70/75, loss 0.3335\n",
            "epoch 15/100, batch 71/75, loss 0.3300\n",
            "epoch 15/100, batch 72/75, loss 0.3372\n",
            "epoch 15/100, batch 73/75, loss 0.3220\n",
            "epoch 15/100, batch 74/75, loss 0.3596\n",
            "epoch 15/100, batch 75/75, loss 0.2960\n",
            "epoch 15/100, training roc_auc_score 0.8964\n",
            "EarlyStopping counter: 5 out of 10\n",
            "epoch 15/100, validation roc_auc_score 0.8091, best validation roc_auc_score 0.8547\n",
            "epoch 16/100, batch 1/75, loss 0.4394\n",
            "epoch 16/100, batch 2/75, loss 0.5591\n",
            "epoch 16/100, batch 3/75, loss 0.3623\n",
            "epoch 16/100, batch 4/75, loss 0.4571\n",
            "epoch 16/100, batch 5/75, loss 0.3086\n",
            "epoch 16/100, batch 6/75, loss 0.2689\n",
            "epoch 16/100, batch 7/75, loss 0.3092\n",
            "epoch 16/100, batch 8/75, loss 0.8775\n",
            "epoch 16/100, batch 9/75, loss 0.2675\n",
            "epoch 16/100, batch 10/75, loss 0.4500\n",
            "epoch 16/100, batch 11/75, loss 0.4389\n",
            "epoch 16/100, batch 12/75, loss 0.2718\n",
            "epoch 16/100, batch 13/75, loss 0.3147\n",
            "epoch 16/100, batch 14/75, loss 0.2903\n",
            "epoch 16/100, batch 15/75, loss 1.5409\n",
            "epoch 16/100, batch 16/75, loss 0.3062\n",
            "epoch 16/100, batch 17/75, loss 0.3047\n",
            "epoch 16/100, batch 18/75, loss 0.3237\n",
            "epoch 16/100, batch 19/75, loss 0.2952\n",
            "epoch 16/100, batch 20/75, loss 1.2326\n",
            "epoch 16/100, batch 21/75, loss 0.2609\n",
            "epoch 16/100, batch 22/75, loss 0.2709\n",
            "epoch 16/100, batch 23/75, loss 0.2655\n",
            "epoch 16/100, batch 24/75, loss 0.2875\n",
            "epoch 16/100, batch 25/75, loss 0.2897\n",
            "epoch 16/100, batch 26/75, loss 1.6776\n",
            "epoch 16/100, batch 27/75, loss 0.2865\n",
            "epoch 16/100, batch 28/75, loss 0.3356\n",
            "epoch 16/100, batch 29/75, loss 0.3008\n",
            "epoch 16/100, batch 30/75, loss 0.3218\n",
            "epoch 16/100, batch 31/75, loss 0.2860\n",
            "epoch 16/100, batch 32/75, loss 0.2881\n",
            "epoch 16/100, batch 33/75, loss 0.3040\n",
            "epoch 16/100, batch 34/75, loss 0.3522\n",
            "epoch 16/100, batch 35/75, loss 0.3231\n",
            "epoch 16/100, batch 36/75, loss 1.8089\n",
            "epoch 16/100, batch 37/75, loss 0.4543\n",
            "epoch 16/100, batch 38/75, loss 0.2703\n",
            "epoch 16/100, batch 39/75, loss 0.2858\n",
            "epoch 16/100, batch 40/75, loss 0.2892\n",
            "epoch 16/100, batch 41/75, loss 0.3303\n",
            "epoch 16/100, batch 42/75, loss 0.2959\n",
            "epoch 16/100, batch 43/75, loss 0.2645\n",
            "epoch 16/100, batch 44/75, loss 0.2940\n",
            "epoch 16/100, batch 45/75, loss 0.2972\n",
            "epoch 16/100, batch 46/75, loss 0.2950\n",
            "epoch 16/100, batch 47/75, loss 0.3310\n",
            "epoch 16/100, batch 48/75, loss 0.2762\n",
            "epoch 16/100, batch 49/75, loss 0.2860\n",
            "epoch 16/100, batch 50/75, loss 0.3304\n",
            "epoch 16/100, batch 51/75, loss 0.2998\n",
            "epoch 16/100, batch 52/75, loss 0.2693\n",
            "epoch 16/100, batch 53/75, loss 12.3136\n",
            "epoch 16/100, batch 54/75, loss 0.3809\n",
            "epoch 16/100, batch 55/75, loss 0.3140\n",
            "epoch 16/100, batch 56/75, loss 0.3054\n",
            "epoch 16/100, batch 57/75, loss 0.3078\n",
            "epoch 16/100, batch 58/75, loss 0.2931\n",
            "epoch 16/100, batch 59/75, loss 0.3045\n",
            "epoch 16/100, batch 60/75, loss 0.3195\n",
            "epoch 16/100, batch 61/75, loss 0.3145\n",
            "epoch 16/100, batch 62/75, loss 0.3499\n",
            "epoch 16/100, batch 63/75, loss 0.3185\n",
            "epoch 16/100, batch 64/75, loss 0.3349\n",
            "epoch 16/100, batch 65/75, loss 0.3636\n",
            "epoch 16/100, batch 66/75, loss 0.3239\n",
            "epoch 16/100, batch 67/75, loss 0.3364\n",
            "epoch 16/100, batch 68/75, loss 0.3345\n",
            "epoch 16/100, batch 69/75, loss 0.3451\n",
            "epoch 16/100, batch 70/75, loss 0.2988\n",
            "epoch 16/100, batch 71/75, loss 0.3063\n",
            "epoch 16/100, batch 72/75, loss 0.2906\n",
            "epoch 16/100, batch 73/75, loss 0.2865\n",
            "epoch 16/100, batch 74/75, loss 0.3239\n",
            "epoch 16/100, batch 75/75, loss 0.2683\n",
            "epoch 16/100, training roc_auc_score 0.9105\n",
            "EarlyStopping counter: 6 out of 10\n",
            "epoch 16/100, validation roc_auc_score 0.8197, best validation roc_auc_score 0.8547\n",
            "epoch 17/100, batch 1/75, loss 0.4308\n",
            "epoch 17/100, batch 2/75, loss 0.5330\n",
            "epoch 17/100, batch 3/75, loss 0.3121\n",
            "epoch 17/100, batch 4/75, loss 0.4691\n",
            "epoch 17/100, batch 5/75, loss 0.2776\n",
            "epoch 17/100, batch 6/75, loss 0.2427\n",
            "epoch 17/100, batch 7/75, loss 0.2877\n",
            "epoch 17/100, batch 8/75, loss 1.0174\n",
            "epoch 17/100, batch 9/75, loss 0.2348\n",
            "epoch 17/100, batch 10/75, loss 0.4664\n",
            "epoch 17/100, batch 11/75, loss 0.3517\n",
            "epoch 17/100, batch 12/75, loss 0.2617\n",
            "epoch 17/100, batch 13/75, loss 0.2975\n",
            "epoch 17/100, batch 14/75, loss 0.2647\n",
            "epoch 17/100, batch 15/75, loss 1.2964\n",
            "epoch 17/100, batch 16/75, loss 0.2984\n",
            "epoch 17/100, batch 17/75, loss 0.2996\n",
            "epoch 17/100, batch 18/75, loss 0.3232\n",
            "epoch 17/100, batch 19/75, loss 0.2647\n",
            "epoch 17/100, batch 20/75, loss 0.9654\n",
            "epoch 17/100, batch 21/75, loss 0.2381\n",
            "epoch 17/100, batch 22/75, loss 0.2481\n",
            "epoch 17/100, batch 23/75, loss 0.2414\n",
            "epoch 17/100, batch 24/75, loss 0.2672\n",
            "epoch 17/100, batch 25/75, loss 0.2598\n",
            "epoch 17/100, batch 26/75, loss 1.6266\n",
            "epoch 17/100, batch 27/75, loss 0.2675\n",
            "epoch 17/100, batch 28/75, loss 0.3177\n",
            "epoch 17/100, batch 29/75, loss 0.2627\n",
            "epoch 17/100, batch 30/75, loss 0.2935\n",
            "epoch 17/100, batch 31/75, loss 0.2563\n",
            "epoch 17/100, batch 32/75, loss 0.2850\n",
            "epoch 17/100, batch 33/75, loss 0.2746\n",
            "epoch 17/100, batch 34/75, loss 0.3235\n",
            "epoch 17/100, batch 35/75, loss 0.2905\n",
            "epoch 17/100, batch 36/75, loss 1.8167\n",
            "epoch 17/100, batch 37/75, loss 0.4419\n",
            "epoch 17/100, batch 38/75, loss 0.2562\n",
            "epoch 17/100, batch 39/75, loss 0.2806\n",
            "epoch 17/100, batch 40/75, loss 0.2689\n",
            "epoch 17/100, batch 41/75, loss 0.3130\n",
            "epoch 17/100, batch 42/75, loss 0.2789\n",
            "epoch 17/100, batch 43/75, loss 0.2493\n",
            "epoch 17/100, batch 44/75, loss 0.2920\n",
            "epoch 17/100, batch 45/75, loss 0.3029\n",
            "epoch 17/100, batch 46/75, loss 0.2783\n",
            "epoch 17/100, batch 47/75, loss 0.3218\n",
            "epoch 17/100, batch 48/75, loss 0.2549\n",
            "epoch 17/100, batch 49/75, loss 0.2902\n",
            "epoch 17/100, batch 50/75, loss 0.3147\n",
            "epoch 17/100, batch 51/75, loss 0.2909\n",
            "epoch 17/100, batch 52/75, loss 0.2550\n",
            "epoch 17/100, batch 53/75, loss 12.3009\n",
            "epoch 17/100, batch 54/75, loss 0.3468\n",
            "epoch 17/100, batch 55/75, loss 0.2999\n",
            "epoch 17/100, batch 56/75, loss 0.2971\n",
            "epoch 17/100, batch 57/75, loss 0.2933\n",
            "epoch 17/100, batch 58/75, loss 0.2844\n",
            "epoch 17/100, batch 59/75, loss 0.3073\n",
            "epoch 17/100, batch 60/75, loss 0.3280\n",
            "epoch 17/100, batch 61/75, loss 0.3318\n",
            "epoch 17/100, batch 62/75, loss 0.3655\n",
            "epoch 17/100, batch 63/75, loss 0.3354\n",
            "epoch 17/100, batch 64/75, loss 0.3289\n",
            "epoch 17/100, batch 65/75, loss 0.3736\n",
            "epoch 17/100, batch 66/75, loss 0.3231\n",
            "epoch 17/100, batch 67/75, loss 0.3414\n",
            "epoch 17/100, batch 68/75, loss 0.3397\n",
            "epoch 17/100, batch 69/75, loss 0.3449\n",
            "epoch 17/100, batch 70/75, loss 0.3047\n",
            "epoch 17/100, batch 71/75, loss 0.3084\n",
            "epoch 17/100, batch 72/75, loss 0.2912\n",
            "epoch 17/100, batch 73/75, loss 0.3120\n",
            "epoch 17/100, batch 74/75, loss 0.3372\n",
            "epoch 17/100, batch 75/75, loss 0.2814\n",
            "epoch 17/100, training roc_auc_score 0.9176\n",
            "EarlyStopping counter: 7 out of 10\n",
            "epoch 17/100, validation roc_auc_score 0.7869, best validation roc_auc_score 0.8547\n",
            "epoch 18/100, batch 1/75, loss 0.4115\n",
            "epoch 18/100, batch 2/75, loss 0.3992\n",
            "epoch 18/100, batch 3/75, loss 0.3127\n",
            "epoch 18/100, batch 4/75, loss 0.4018\n",
            "epoch 18/100, batch 5/75, loss 0.2626\n",
            "epoch 18/100, batch 6/75, loss 0.2370\n",
            "epoch 18/100, batch 7/75, loss 0.2628\n",
            "epoch 18/100, batch 8/75, loss 0.8232\n",
            "epoch 18/100, batch 9/75, loss 0.2322\n",
            "epoch 18/100, batch 10/75, loss 0.3698\n",
            "epoch 18/100, batch 11/75, loss 0.3285\n",
            "epoch 18/100, batch 12/75, loss 0.2469\n",
            "epoch 18/100, batch 13/75, loss 0.2813\n",
            "epoch 18/100, batch 14/75, loss 0.2602\n",
            "epoch 18/100, batch 15/75, loss 1.2255\n",
            "epoch 18/100, batch 16/75, loss 0.2816\n",
            "epoch 18/100, batch 17/75, loss 0.2848\n",
            "epoch 18/100, batch 18/75, loss 0.3202\n",
            "epoch 18/100, batch 19/75, loss 0.2551\n",
            "epoch 18/100, batch 20/75, loss 1.1725\n",
            "epoch 18/100, batch 21/75, loss 0.2197\n",
            "epoch 18/100, batch 22/75, loss 0.2304\n",
            "epoch 18/100, batch 23/75, loss 0.2222\n",
            "epoch 18/100, batch 24/75, loss 0.2326\n",
            "epoch 18/100, batch 25/75, loss 0.2513\n",
            "epoch 18/100, batch 26/75, loss 1.6524\n",
            "epoch 18/100, batch 27/75, loss 0.2472\n",
            "epoch 18/100, batch 28/75, loss 0.2904\n",
            "epoch 18/100, batch 29/75, loss 0.2524\n",
            "epoch 18/100, batch 30/75, loss 0.2709\n",
            "epoch 18/100, batch 31/75, loss 0.2405\n",
            "epoch 18/100, batch 32/75, loss 0.2691\n",
            "epoch 18/100, batch 33/75, loss 0.2757\n",
            "epoch 18/100, batch 34/75, loss 0.3169\n",
            "epoch 18/100, batch 35/75, loss 0.2746\n",
            "epoch 18/100, batch 36/75, loss 2.0052\n",
            "epoch 18/100, batch 37/75, loss 0.3583\n",
            "epoch 18/100, batch 38/75, loss 0.2491\n",
            "epoch 18/100, batch 39/75, loss 0.2748\n",
            "epoch 18/100, batch 40/75, loss 0.2645\n",
            "epoch 18/100, batch 41/75, loss 0.3143\n",
            "epoch 18/100, batch 42/75, loss 0.2883\n",
            "epoch 18/100, batch 43/75, loss 0.2502\n",
            "epoch 18/100, batch 44/75, loss 0.2935\n",
            "epoch 18/100, batch 45/75, loss 0.3145\n",
            "epoch 18/100, batch 46/75, loss 0.2637\n",
            "epoch 18/100, batch 47/75, loss 0.3216\n",
            "epoch 18/100, batch 48/75, loss 0.2593\n",
            "epoch 18/100, batch 49/75, loss 0.2848\n",
            "epoch 18/100, batch 50/75, loss 0.2952\n",
            "epoch 18/100, batch 51/75, loss 0.2897\n",
            "epoch 18/100, batch 52/75, loss 0.2418\n",
            "epoch 18/100, batch 53/75, loss 12.1064\n",
            "epoch 18/100, batch 54/75, loss 0.3294\n",
            "epoch 18/100, batch 55/75, loss 0.2839\n",
            "epoch 18/100, batch 56/75, loss 0.2921\n",
            "epoch 18/100, batch 57/75, loss 0.2938\n",
            "epoch 18/100, batch 58/75, loss 0.2772\n",
            "epoch 18/100, batch 59/75, loss 0.2810\n",
            "epoch 18/100, batch 60/75, loss 0.3233\n",
            "epoch 18/100, batch 61/75, loss 0.3085\n",
            "epoch 18/100, batch 62/75, loss 0.3235\n",
            "epoch 18/100, batch 63/75, loss 0.2974\n",
            "epoch 18/100, batch 64/75, loss 0.3214\n",
            "epoch 18/100, batch 65/75, loss 0.3618\n",
            "epoch 18/100, batch 66/75, loss 0.2922\n",
            "epoch 18/100, batch 67/75, loss 0.3095\n",
            "epoch 18/100, batch 68/75, loss 0.3174\n",
            "epoch 18/100, batch 69/75, loss 0.3147\n",
            "epoch 18/100, batch 70/75, loss 0.2905\n",
            "epoch 18/100, batch 71/75, loss 0.2853\n",
            "epoch 18/100, batch 72/75, loss 0.2728\n",
            "epoch 18/100, batch 73/75, loss 0.2707\n",
            "epoch 18/100, batch 74/75, loss 0.3053\n",
            "epoch 18/100, batch 75/75, loss 0.2517\n",
            "epoch 18/100, training roc_auc_score 0.9219\n",
            "EarlyStopping counter: 8 out of 10\n",
            "epoch 18/100, validation roc_auc_score 0.7886, best validation roc_auc_score 0.8547\n",
            "epoch 19/100, batch 1/75, loss 0.3990\n",
            "epoch 19/100, batch 2/75, loss 0.4207\n",
            "epoch 19/100, batch 3/75, loss 0.2925\n",
            "epoch 19/100, batch 4/75, loss 0.3612\n",
            "epoch 19/100, batch 5/75, loss 0.2722\n",
            "epoch 19/100, batch 6/75, loss 0.2222\n",
            "epoch 19/100, batch 7/75, loss 0.2384\n",
            "epoch 19/100, batch 8/75, loss 0.8967\n",
            "epoch 19/100, batch 9/75, loss 0.2168\n",
            "epoch 19/100, batch 10/75, loss 0.5000\n",
            "epoch 19/100, batch 11/75, loss 0.3476\n",
            "epoch 19/100, batch 12/75, loss 0.2271\n",
            "epoch 19/100, batch 13/75, loss 0.3132\n",
            "epoch 19/100, batch 14/75, loss 0.2972\n",
            "epoch 19/100, batch 15/75, loss 1.5089\n",
            "epoch 19/100, batch 16/75, loss 0.2957\n",
            "epoch 19/100, batch 17/75, loss 0.2746\n",
            "epoch 19/100, batch 18/75, loss 0.2963\n",
            "epoch 19/100, batch 19/75, loss 0.2589\n",
            "epoch 19/100, batch 20/75, loss 0.8816\n",
            "epoch 19/100, batch 21/75, loss 0.2075\n",
            "epoch 19/100, batch 22/75, loss 0.2470\n",
            "epoch 19/100, batch 23/75, loss 0.2275\n",
            "epoch 19/100, batch 24/75, loss 0.2630\n",
            "epoch 19/100, batch 25/75, loss 0.2607\n",
            "epoch 19/100, batch 26/75, loss 1.3791\n",
            "epoch 19/100, batch 27/75, loss 0.2591\n",
            "epoch 19/100, batch 28/75, loss 0.3253\n",
            "epoch 19/100, batch 29/75, loss 0.2507\n",
            "epoch 19/100, batch 30/75, loss 0.2811\n",
            "epoch 19/100, batch 31/75, loss 0.2413\n",
            "epoch 19/100, batch 32/75, loss 0.2541\n",
            "epoch 19/100, batch 33/75, loss 0.2641\n",
            "epoch 19/100, batch 34/75, loss 0.3102\n",
            "epoch 19/100, batch 35/75, loss 0.2802\n",
            "epoch 19/100, batch 36/75, loss 1.6337\n",
            "epoch 19/100, batch 37/75, loss 0.4119\n",
            "epoch 19/100, batch 38/75, loss 0.2333\n",
            "epoch 19/100, batch 39/75, loss 0.2594\n",
            "epoch 19/100, batch 40/75, loss 0.2557\n",
            "epoch 19/100, batch 41/75, loss 0.3130\n",
            "epoch 19/100, batch 42/75, loss 0.2563\n",
            "epoch 19/100, batch 43/75, loss 0.2417\n",
            "epoch 19/100, batch 44/75, loss 0.2716\n",
            "epoch 19/100, batch 45/75, loss 0.2845\n",
            "epoch 19/100, batch 46/75, loss 0.2385\n",
            "epoch 19/100, batch 47/75, loss 0.2970\n",
            "epoch 19/100, batch 48/75, loss 0.2458\n",
            "epoch 19/100, batch 49/75, loss 0.2795\n",
            "epoch 19/100, batch 50/75, loss 0.2895\n",
            "epoch 19/100, batch 51/75, loss 0.2677\n",
            "epoch 19/100, batch 52/75, loss 0.2306\n",
            "epoch 19/100, batch 53/75, loss 11.9729\n",
            "epoch 19/100, batch 54/75, loss 0.3243\n",
            "epoch 19/100, batch 55/75, loss 0.2799\n",
            "epoch 19/100, batch 56/75, loss 0.2699\n",
            "epoch 19/100, batch 57/75, loss 0.2841\n",
            "epoch 19/100, batch 58/75, loss 0.2757\n",
            "epoch 19/100, batch 59/75, loss 0.2676\n",
            "epoch 19/100, batch 60/75, loss 0.2874\n",
            "epoch 19/100, batch 61/75, loss 0.2877\n",
            "epoch 19/100, batch 62/75, loss 0.3371\n",
            "epoch 19/100, batch 63/75, loss 0.2928\n",
            "epoch 19/100, batch 64/75, loss 0.3071\n",
            "epoch 19/100, batch 65/75, loss 0.3565\n",
            "epoch 19/100, batch 66/75, loss 0.2926\n",
            "epoch 19/100, batch 67/75, loss 0.2933\n",
            "epoch 19/100, batch 68/75, loss 0.3112\n",
            "epoch 19/100, batch 69/75, loss 0.3063\n",
            "epoch 19/100, batch 70/75, loss 0.2706\n",
            "epoch 19/100, batch 71/75, loss 0.2758\n",
            "epoch 19/100, batch 72/75, loss 0.2564\n",
            "epoch 19/100, batch 73/75, loss 0.2637\n",
            "epoch 19/100, batch 74/75, loss 0.2910\n",
            "epoch 19/100, batch 75/75, loss 0.2388\n",
            "epoch 19/100, training roc_auc_score 0.9264\n",
            "EarlyStopping counter: 9 out of 10\n",
            "epoch 19/100, validation roc_auc_score 0.8006, best validation roc_auc_score 0.8547\n",
            "epoch 20/100, batch 1/75, loss 0.4012\n",
            "epoch 20/100, batch 2/75, loss 0.3479\n",
            "epoch 20/100, batch 3/75, loss 0.2699\n",
            "epoch 20/100, batch 4/75, loss 0.3534\n",
            "epoch 20/100, batch 5/75, loss 0.2427\n",
            "epoch 20/100, batch 6/75, loss 0.2256\n",
            "epoch 20/100, batch 7/75, loss 0.2512\n",
            "epoch 20/100, batch 8/75, loss 0.5776\n",
            "epoch 20/100, batch 9/75, loss 0.1966\n",
            "epoch 20/100, batch 10/75, loss 0.4269\n",
            "epoch 20/100, batch 11/75, loss 0.3225\n",
            "epoch 20/100, batch 12/75, loss 0.2143\n",
            "epoch 20/100, batch 13/75, loss 0.3070\n",
            "epoch 20/100, batch 14/75, loss 0.2425\n",
            "epoch 20/100, batch 15/75, loss 1.3239\n",
            "epoch 20/100, batch 16/75, loss 0.2756\n",
            "epoch 20/100, batch 17/75, loss 0.2738\n",
            "epoch 20/100, batch 18/75, loss 0.2998\n",
            "epoch 20/100, batch 19/75, loss 0.2332\n",
            "epoch 20/100, batch 20/75, loss 0.7349\n",
            "epoch 20/100, batch 21/75, loss 0.1996\n",
            "epoch 20/100, batch 22/75, loss 0.2211\n",
            "epoch 20/100, batch 23/75, loss 0.2054\n",
            "epoch 20/100, batch 24/75, loss 0.2285\n",
            "epoch 20/100, batch 25/75, loss 0.2310\n",
            "epoch 20/100, batch 26/75, loss 1.0888\n",
            "epoch 20/100, batch 27/75, loss 0.2234\n",
            "epoch 20/100, batch 28/75, loss 0.2858\n",
            "epoch 20/100, batch 29/75, loss 0.2283\n",
            "epoch 20/100, batch 30/75, loss 0.2523\n",
            "epoch 20/100, batch 31/75, loss 0.2333\n",
            "epoch 20/100, batch 32/75, loss 0.2379\n",
            "epoch 20/100, batch 33/75, loss 0.2463\n",
            "epoch 20/100, batch 34/75, loss 0.2864\n",
            "epoch 20/100, batch 35/75, loss 0.2587\n",
            "epoch 20/100, batch 36/75, loss 1.5111\n",
            "epoch 20/100, batch 37/75, loss 0.3248\n",
            "epoch 20/100, batch 38/75, loss 0.2169\n",
            "epoch 20/100, batch 39/75, loss 0.2325\n",
            "epoch 20/100, batch 40/75, loss 0.2469\n",
            "epoch 20/100, batch 41/75, loss 0.2701\n",
            "epoch 20/100, batch 42/75, loss 0.2463\n",
            "epoch 20/100, batch 43/75, loss 0.2299\n",
            "epoch 20/100, batch 44/75, loss 0.2631\n",
            "epoch 20/100, batch 45/75, loss 0.2618\n",
            "epoch 20/100, batch 46/75, loss 0.2269\n",
            "epoch 20/100, batch 47/75, loss 0.2834\n",
            "epoch 20/100, batch 48/75, loss 0.2257\n",
            "epoch 20/100, batch 49/75, loss 0.2515\n",
            "epoch 20/100, batch 50/75, loss 0.2849\n",
            "epoch 20/100, batch 51/75, loss 0.2472\n",
            "epoch 20/100, batch 52/75, loss 0.2176\n",
            "epoch 20/100, batch 53/75, loss 11.1524\n",
            "epoch 20/100, batch 54/75, loss 0.2704\n",
            "epoch 20/100, batch 55/75, loss 0.2708\n",
            "epoch 20/100, batch 56/75, loss 0.2532\n",
            "epoch 20/100, batch 57/75, loss 0.2599\n",
            "epoch 20/100, batch 58/75, loss 0.2635\n",
            "epoch 20/100, batch 59/75, loss 0.2596\n",
            "epoch 20/100, batch 60/75, loss 0.3020\n",
            "epoch 20/100, batch 61/75, loss 0.2910\n",
            "epoch 20/100, batch 62/75, loss 0.3437\n",
            "epoch 20/100, batch 63/75, loss 0.2984\n",
            "epoch 20/100, batch 64/75, loss 0.2776\n",
            "epoch 20/100, batch 65/75, loss 0.3693\n",
            "epoch 20/100, batch 66/75, loss 0.3028\n",
            "epoch 20/100, batch 67/75, loss 0.2959\n",
            "epoch 20/100, batch 68/75, loss 0.3165\n",
            "epoch 20/100, batch 69/75, loss 0.2994\n",
            "epoch 20/100, batch 70/75, loss 0.2827\n",
            "epoch 20/100, batch 71/75, loss 0.2876\n",
            "epoch 20/100, batch 72/75, loss 0.2520\n",
            "epoch 20/100, batch 73/75, loss 0.2589\n",
            "epoch 20/100, batch 74/75, loss 0.3011\n",
            "epoch 20/100, batch 75/75, loss 0.2568\n",
            "epoch 20/100, training roc_auc_score 0.9414\n",
            "EarlyStopping counter: 10 out of 10\n",
            "epoch 20/100, validation roc_auc_score 0.7662, best validation roc_auc_score 0.8547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YaWpHl2Gnr-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f7f2b78c-40f5-4d09-e621-cc3ebd9939f9"
      },
      "source": [
        "stopper.load_checkpoint(model)\n",
        "test_score = run_an_eval_epoch(args, model, test_loader)\n",
        "print('test {} {:.4f}'.format(args['metric_name'], test_score))\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test roc_auc_score 0.7141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLkl062SLtaF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}