{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAT_ncov.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1W00oQcjsYEfS0u9NKwrtNDY4ESZal1k_",
      "authorship_tag": "ABX9TyM4fsf4KNw4ysw1tPxiHQiq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seyonechithrananda/ncov-ligand-protein/blob/master/GAT_ncov.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnhZ7HBLDf73",
        "colab_type": "code",
        "outputId": "c833aa3b-c0e0-43e4-8936-d7135570e211",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!wget -c https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!conda install -q -y -c conda-forge rdkit\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-05 07:32:14--  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8303, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 85055499 (81M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-latest-Linux-x86_64.sh’\n",
            "\n",
            "\r          Miniconda   0%[                    ]       0  --.-KB/s               \r         Miniconda3  44%[=======>            ]  36.43M   182MB/s               \r        Miniconda3-  88%[================>   ]  71.91M   180MB/s               \rMiniconda3-latest-L 100%[===================>]  81.12M   180MB/s    in 0.4s    \n",
            "\n",
            "2020-05-05 07:32:14 (180 MB/s) - ‘Miniconda3-latest-Linux-x86_64.sh’ saved [85055499/85055499]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - _libgcc_mutex==0.1=main\n",
            "    - asn1crypto==1.3.0=py37_0\n",
            "    - ca-certificates==2020.1.1=0\n",
            "    - certifi==2019.11.28=py37_0\n",
            "    - cffi==1.14.0=py37h2e261b9_0\n",
            "    - chardet==3.0.4=py37_1003\n",
            "    - conda-package-handling==1.6.0=py37h7b6447c_0\n",
            "    - conda==4.8.2=py37_0\n",
            "    - cryptography==2.8=py37h1ba5d50_0\n",
            "    - idna==2.8=py37_0\n",
            "    - ld_impl_linux-64==2.33.1=h53a641e_7\n",
            "    - libedit==3.1.20181209=hc058e9b_0\n",
            "    - libffi==3.2.1=hd88cf55_4\n",
            "    - libgcc-ng==9.1.0=hdf63c60_0\n",
            "    - libstdcxx-ng==9.1.0=hdf63c60_0\n",
            "    - ncurses==6.2=he6710b0_0\n",
            "    - openssl==1.1.1d=h7b6447c_4\n",
            "    - pip==20.0.2=py37_1\n",
            "    - pycosat==0.6.3=py37h7b6447c_0\n",
            "    - pycparser==2.19=py37_0\n",
            "    - pyopenssl==19.1.0=py37_0\n",
            "    - pysocks==1.7.1=py37_0\n",
            "    - python==3.7.6=h0371630_2\n",
            "    - readline==7.0=h7b6447c_5\n",
            "    - requests==2.22.0=py37_1\n",
            "    - ruamel_yaml==0.15.87=py37h7b6447c_0\n",
            "    - setuptools==45.2.0=py37_0\n",
            "    - six==1.14.0=py37_0\n",
            "    - sqlite==3.31.1=h7b6447c_0\n",
            "    - tk==8.6.8=hbc83047_0\n",
            "    - tqdm==4.42.1=py_0\n",
            "    - urllib3==1.25.8=py37_0\n",
            "    - wheel==0.34.2=py37_0\n",
            "    - xz==5.2.4=h14c3975_4\n",
            "    - yaml==0.1.7=had09818_2\n",
            "    - zlib==1.2.11=h7b6447c_3\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
            "  asn1crypto         pkgs/main/linux-64::asn1crypto-1.3.0-py37_0\n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2020.1.1-0\n",
            "  certifi            pkgs/main/linux-64::certifi-2019.11.28-py37_0\n",
            "  cffi               pkgs/main/linux-64::cffi-1.14.0-py37h2e261b9_0\n",
            "  chardet            pkgs/main/linux-64::chardet-3.0.4-py37_1003\n",
            "  conda              pkgs/main/linux-64::conda-4.8.2-py37_0\n",
            "  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1.6.0-py37h7b6447c_0\n",
            "  cryptography       pkgs/main/linux-64::cryptography-2.8-py37h1ba5d50_0\n",
            "  idna               pkgs/main/linux-64::idna-2.8-py37_0\n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.33.1-h53a641e_7\n",
            "  libedit            pkgs/main/linux-64::libedit-3.1.20181209-hc058e9b_0\n",
            "  libffi             pkgs/main/linux-64::libffi-3.2.1-hd88cf55_4\n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-9.1.0-hdf63c60_0\n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-9.1.0-hdf63c60_0\n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.2-he6710b0_0\n",
            "  openssl            pkgs/main/linux-64::openssl-1.1.1d-h7b6447c_4\n",
            "  pip                pkgs/main/linux-64::pip-20.0.2-py37_1\n",
            "  pycosat            pkgs/main/linux-64::pycosat-0.6.3-py37h7b6447c_0\n",
            "  pycparser          pkgs/main/linux-64::pycparser-2.19-py37_0\n",
            "  pyopenssl          pkgs/main/linux-64::pyopenssl-19.1.0-py37_0\n",
            "  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py37_0\n",
            "  python             pkgs/main/linux-64::python-3.7.6-h0371630_2\n",
            "  readline           pkgs/main/linux-64::readline-7.0-h7b6447c_5\n",
            "  requests           pkgs/main/linux-64::requests-2.22.0-py37_1\n",
            "  ruamel_yaml        pkgs/main/linux-64::ruamel_yaml-0.15.87-py37h7b6447c_0\n",
            "  setuptools         pkgs/main/linux-64::setuptools-45.2.0-py37_0\n",
            "  six                pkgs/main/linux-64::six-1.14.0-py37_0\n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.31.1-h7b6447c_0\n",
            "  tk                 pkgs/main/linux-64::tk-8.6.8-hbc83047_0\n",
            "  tqdm               pkgs/main/noarch::tqdm-4.42.1-py_0\n",
            "  urllib3            pkgs/main/linux-64::urllib3-1.25.8-py37_0\n",
            "  wheel              pkgs/main/linux-64::wheel-0.34.2-py37_0\n",
            "  xz                 pkgs/main/linux-64::xz-5.2.4-h14c3975_4\n",
            "  yaml               pkgs/main/linux-64::yaml-0.1.7-had09818_2\n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.11-h7b6447c_3\n",
            "\n",
            "\n",
            "Preparing transaction: | \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - rdkit\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    boost-1.72.0               |   py37h9de70de_0         316 KB  conda-forge\n",
            "    boost-cpp-1.72.0           |       h8e57a91_0        21.8 MB  conda-forge\n",
            "    bzip2-1.0.8                |       h516909a_2         396 KB  conda-forge\n",
            "    ca-certificates-2020.4.5.1 |       hecc5488_0         146 KB  conda-forge\n",
            "    cairo-1.16.0               |    hcf35c78_1003         1.5 MB  conda-forge\n",
            "    certifi-2020.4.5.1         |   py37hc8dfbb8_0         151 KB  conda-forge\n",
            "    conda-4.8.3                |   py37hc8dfbb8_1         3.0 MB  conda-forge\n",
            "    fontconfig-2.13.1          |    h86ecdb6_1001         340 KB  conda-forge\n",
            "    freetype-2.10.1            |       he06d7ca_0         877 KB  conda-forge\n",
            "    gettext-0.19.8.1           |    hc5be6a0_1002         3.6 MB  conda-forge\n",
            "    glib-2.64.2                |       h6f030ca_0         3.4 MB  conda-forge\n",
            "    icu-64.2                   |       he1b5a44_1        12.6 MB  conda-forge\n",
            "    jpeg-9c                    |    h14c3975_1001         251 KB  conda-forge\n",
            "    libblas-3.8.0              |      14_openblas          10 KB  conda-forge\n",
            "    libcblas-3.8.0             |      14_openblas          10 KB  conda-forge\n",
            "    libgfortran-ng-7.3.0       |       hdf63c60_5         1.7 MB  conda-forge\n",
            "    libiconv-1.15              |    h516909a_1006         2.0 MB  conda-forge\n",
            "    liblapack-3.8.0            |      14_openblas          10 KB  conda-forge\n",
            "    libopenblas-0.3.7          |       h5ec1e0e_6         7.6 MB  conda-forge\n",
            "    libpng-1.6.37              |       hed695b0_1         308 KB  conda-forge\n",
            "    libtiff-4.1.0              |       hc7e4089_6         668 KB  conda-forge\n",
            "    libuuid-2.32.1             |    h14c3975_1000          26 KB  conda-forge\n",
            "    libwebp-base-1.1.0         |       h516909a_3         845 KB  conda-forge\n",
            "    libxcb-1.13                |    h14c3975_1002         396 KB  conda-forge\n",
            "    libxml2-2.9.10             |       hee79883_0         1.3 MB  conda-forge\n",
            "    lz4-c-1.8.3                |    he1b5a44_1001         187 KB  conda-forge\n",
            "    numpy-1.18.4               |   py37h8960a57_0         5.2 MB  conda-forge\n",
            "    olefile-0.46               |             py_0          31 KB  conda-forge\n",
            "    openssl-1.1.1g             |       h516909a_0         2.1 MB  conda-forge\n",
            "    pandas-1.0.3               |   py37h0da4684_1        11.1 MB  conda-forge\n",
            "    pcre-8.44                  |       he1b5a44_0         261 KB  conda-forge\n",
            "    pillow-7.1.2               |   py37hb39fc2d_0         603 KB\n",
            "    pixman-0.38.0              |    h516909a_1003         594 KB  conda-forge\n",
            "    pthread-stubs-0.4          |    h14c3975_1001           5 KB  conda-forge\n",
            "    pycairo-1.19.1             |   py37h01af8b0_3          77 KB  conda-forge\n",
            "    python-dateutil-2.8.1      |             py_0         220 KB  conda-forge\n",
            "    python_abi-3.7             |          1_cp37m           4 KB  conda-forge\n",
            "    pytz-2020.1                |     pyh9f0ad1d_0         227 KB  conda-forge\n",
            "    rdkit-2020.03.1            |   py37hdd87690_3        24.6 MB  conda-forge\n",
            "    xorg-kbproto-1.0.7         |    h14c3975_1002          26 KB  conda-forge\n",
            "    xorg-libice-1.0.10         |       h516909a_0          57 KB  conda-forge\n",
            "    xorg-libsm-1.2.3           |    h84519dc_1000          25 KB  conda-forge\n",
            "    xorg-libx11-1.6.9          |       h516909a_0         918 KB  conda-forge\n",
            "    xorg-libxau-1.0.9          |       h14c3975_0          13 KB  conda-forge\n",
            "    xorg-libxdmcp-1.1.3        |       h516909a_0          18 KB  conda-forge\n",
            "    xorg-libxext-1.3.4         |       h516909a_0          51 KB  conda-forge\n",
            "    xorg-libxrender-0.9.10     |    h516909a_1002          31 KB  conda-forge\n",
            "    xorg-renderproto-0.11.1    |    h14c3975_1002           8 KB  conda-forge\n",
            "    xorg-xextproto-7.3.0       |    h14c3975_1002          27 KB  conda-forge\n",
            "    xorg-xproto-7.0.31         |    h14c3975_1007          72 KB  conda-forge\n",
            "    zstd-1.4.4                 |       h3b9ef0a_2         982 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       110.7 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  boost              conda-forge/linux-64::boost-1.72.0-py37h9de70de_0\n",
            "  boost-cpp          conda-forge/linux-64::boost-cpp-1.72.0-h8e57a91_0\n",
            "  bzip2              conda-forge/linux-64::bzip2-1.0.8-h516909a_2\n",
            "  cairo              conda-forge/linux-64::cairo-1.16.0-hcf35c78_1003\n",
            "  fontconfig         conda-forge/linux-64::fontconfig-2.13.1-h86ecdb6_1001\n",
            "  freetype           conda-forge/linux-64::freetype-2.10.1-he06d7ca_0\n",
            "  gettext            conda-forge/linux-64::gettext-0.19.8.1-hc5be6a0_1002\n",
            "  glib               conda-forge/linux-64::glib-2.64.2-h6f030ca_0\n",
            "  icu                conda-forge/linux-64::icu-64.2-he1b5a44_1\n",
            "  jpeg               conda-forge/linux-64::jpeg-9c-h14c3975_1001\n",
            "  libblas            conda-forge/linux-64::libblas-3.8.0-14_openblas\n",
            "  libcblas           conda-forge/linux-64::libcblas-3.8.0-14_openblas\n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-7.3.0-hdf63c60_5\n",
            "  libiconv           conda-forge/linux-64::libiconv-1.15-h516909a_1006\n",
            "  liblapack          conda-forge/linux-64::liblapack-3.8.0-14_openblas\n",
            "  libopenblas        conda-forge/linux-64::libopenblas-0.3.7-h5ec1e0e_6\n",
            "  libpng             conda-forge/linux-64::libpng-1.6.37-hed695b0_1\n",
            "  libtiff            conda-forge/linux-64::libtiff-4.1.0-hc7e4089_6\n",
            "  libuuid            conda-forge/linux-64::libuuid-2.32.1-h14c3975_1000\n",
            "  libwebp-base       conda-forge/linux-64::libwebp-base-1.1.0-h516909a_3\n",
            "  libxcb             conda-forge/linux-64::libxcb-1.13-h14c3975_1002\n",
            "  libxml2            conda-forge/linux-64::libxml2-2.9.10-hee79883_0\n",
            "  lz4-c              conda-forge/linux-64::lz4-c-1.8.3-he1b5a44_1001\n",
            "  numpy              conda-forge/linux-64::numpy-1.18.4-py37h8960a57_0\n",
            "  olefile            conda-forge/noarch::olefile-0.46-py_0\n",
            "  pandas             conda-forge/linux-64::pandas-1.0.3-py37h0da4684_1\n",
            "  pcre               conda-forge/linux-64::pcre-8.44-he1b5a44_0\n",
            "  pillow             pkgs/main/linux-64::pillow-7.1.2-py37hb39fc2d_0\n",
            "  pixman             conda-forge/linux-64::pixman-0.38.0-h516909a_1003\n",
            "  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-h14c3975_1001\n",
            "  pycairo            conda-forge/linux-64::pycairo-1.19.1-py37h01af8b0_3\n",
            "  python-dateutil    conda-forge/noarch::python-dateutil-2.8.1-py_0\n",
            "  python_abi         conda-forge/linux-64::python_abi-3.7-1_cp37m\n",
            "  pytz               conda-forge/noarch::pytz-2020.1-pyh9f0ad1d_0\n",
            "  rdkit              conda-forge/linux-64::rdkit-2020.03.1-py37hdd87690_3\n",
            "  xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-h14c3975_1002\n",
            "  xorg-libice        conda-forge/linux-64::xorg-libice-1.0.10-h516909a_0\n",
            "  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.3-h84519dc_1000\n",
            "  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.6.9-h516909a_0\n",
            "  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.9-h14c3975_0\n",
            "  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.3-h516909a_0\n",
            "  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.4-h516909a_0\n",
            "  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.10-h516909a_1002\n",
            "  xorg-renderproto   conda-forge/linux-64::xorg-renderproto-0.11.1-h14c3975_1002\n",
            "  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h14c3975_1002\n",
            "  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h14c3975_1007\n",
            "  zstd               conda-forge/linux-64::zstd-1.4.4-h3b9ef0a_2\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates     pkgs/main::ca-certificates-2020.1.1-0 --> conda-forge::ca-certificates-2020.4.5.1-hecc5488_0\n",
            "  certifi              pkgs/main::certifi-2019.11.28-py37_0 --> conda-forge::certifi-2020.4.5.1-py37hc8dfbb8_0\n",
            "  conda                       pkgs/main::conda-4.8.2-py37_0 --> conda-forge::conda-4.8.3-py37hc8dfbb8_1\n",
            "  openssl              pkgs/main::openssl-1.1.1d-h7b6447c_4 --> conda-forge::openssl-1.1.1g-h516909a_0\n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxPjaPjsGNGb",
        "colab_type": "code",
        "outputId": "6cdbee22-bf41-4eaf-c088-a52962f76e32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        }
      },
      "source": [
        "!conda install -c dglteam dgl-cuda10.1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - dgl-cuda10.1\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    blas-1.0                   |         openblas          46 KB\n",
            "    certifi-2020.4.5.1         |           py37_0         155 KB\n",
            "    decorator-4.4.2            |             py_0          14 KB\n",
            "    dgl-cuda10.1-0.4.3post2    |           py37_0        11.2 MB  dglteam\n",
            "    networkx-2.4               |             py_0         1.2 MB\n",
            "    scipy-1.4.1                |   py37habc2bb6_0        14.6 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        27.1 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  blas               pkgs/main/linux-64::blas-1.0-openblas\n",
            "  decorator          pkgs/main/noarch::decorator-4.4.2-py_0\n",
            "  dgl-cuda10.1       dglteam/linux-64::dgl-cuda10.1-0.4.3post2-py37_0\n",
            "  networkx           pkgs/main/noarch::networkx-2.4-py_0\n",
            "  scipy              pkgs/main/linux-64::scipy-1.4.1-py37habc2bb6_0\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  certifi            conda-forge::certifi-2020.4.5.1-py37h~ --> pkgs/main::certifi-2020.4.5.1-py37_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "certifi-2020.4.5.1   | 155 KB    | : 100% 1.0/1 [00:00<00:00,  9.59it/s]\n",
            "decorator-4.4.2      | 14 KB     | : 100% 1.0/1 [00:00<00:00, 17.29it/s]\n",
            "dgl-cuda10.1-0.4.3po | 11.2 MB   | : 100% 1.0/1 [00:04<00:00,  4.96s/it]                \n",
            "blas-1.0             | 46 KB     | : 100% 1.0/1 [00:00<00:00, 25.45it/s]\n",
            "scipy-1.4.1          | 14.6 MB   | : 100% 1.0/1 [00:00<00:00,  2.22it/s]               \n",
            "networkx-2.4         | 1.2 MB    | : 100% 1.0/1 [00:00<00:00,  3.59it/s]\n",
            "Preparing transaction: | \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ba7Nw5eGUTr",
        "colab_type": "code",
        "outputId": "0af51342-1494-41e7-d4bb-69b2bd0a7814",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        }
      },
      "source": [
        "!conda install -c dglteam dgllife\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - dgllife\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    dgllife-0.2.1              |           py37_0         132 KB  dglteam\n",
            "    joblib-0.14.1              |             py_0         201 KB\n",
            "    scikit-learn-0.22.1        |   py37h22eb022_0         5.3 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         5.6 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  dgllife            dglteam/linux-64::dgllife-0.2.1-py37_0\n",
            "  joblib             pkgs/main/noarch::joblib-0.14.1-py_0\n",
            "  scikit-learn       pkgs/main/linux-64::scikit-learn-0.22.1-py37h22eb022_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "joblib-0.14.1        | 201 KB    | : 100% 1.0/1 [00:00<00:00,  7.19it/s]               \n",
            "scikit-learn-0.22.1  | 5.3 MB    | : 100% 1.0/1 [00:00<00:00,  1.61it/s]               \n",
            "dgllife-0.2.1        | 132 KB    | : 100% 1.0/1 [00:00<00:00,  1.42it/s]                \n",
            "Preparing transaction: | \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqRn8KexGdiL",
        "colab_type": "code",
        "outputId": "75d77ef0-a743-4b80-83b7-e6386f081ad2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        }
      },
      "source": [
        "!conda install pandas"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Solving environment: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - pandas\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    conda-4.8.3                |           py37_0         2.8 MB\n",
            "    openssl-1.1.1g             |       h7b6447c_0         2.5 MB\n",
            "    pandas-1.0.3               |   py37h0573a6f_0         8.6 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        13.9 MB\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  ca-certificates    conda-forge::ca-certificates-2020.4.5~ --> pkgs/main::ca-certificates-2020.1.1-0\n",
            "  conda              conda-forge::conda-4.8.3-py37hc8dfbb8~ --> pkgs/main::conda-4.8.3-py37_0\n",
            "  openssl            conda-forge::openssl-1.1.1g-h516909a_0 --> pkgs/main::openssl-1.1.1g-h7b6447c_0\n",
            "  pandas             conda-forge::pandas-1.0.3-py37h0da468~ --> pkgs/main::pandas-1.0.3-py37h0573a6f_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "conda-4.8.3          | 2.8 MB    | : 100% 1.0/1 [00:00<00:00,  1.86s/it]                 \n",
            "openssl-1.1.1g       | 2.5 MB    | : 100% 1.0/1 [00:00<00:00,  9.41it/s]\n",
            "pandas-1.0.3         | 8.6 MB    | : 100% 1.0/1 [00:00<00:00,  2.33it/s]\n",
            "Preparing transaction: - \b\b\\ \b\bdone\n",
            "Verifying transaction: / \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\bdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVKD7kwtHDUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "import sys \n",
        "import pandas as pd\n",
        "\n",
        "# train --> balanced dataset\n",
        "dataset_train_file = \"/content/drive/My Drive/Project De Novo/AID1706_binarized_sars_full_eval_actives_12k_samples.csv\"\n",
        "dataset_eval_file = \"/content/drive/My Drive/Project De Novo/mpro_xchem.csv\"\n",
        "dataset_train = pd.read_csv(dataset_train_file)\n",
        "dataset_eval = pd.read_csv(dataset_eval_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy7_zDz9LCq2",
        "colab_type": "code",
        "outputId": "b4ef8b81-c1ab-49b0-ef1b-e97c7bcc89d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "dataset_train.head"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                                   smiles  activity\n",
              "0      C1CC(C1)C(=O)NC2=CC=C(C=C2)N(C(C3=CC(=CC=C3)F)...         1\n",
              "1      CC(C)(C)C1=CC=C(C=C1)N(C(C2=CN=NC=C2)C(=O)NC(C...         1\n",
              "2      CC(C)(C)NC(=O)C(C1=CSC=C1)N(C2=CC=C(C=C2)N)C(=...         1\n",
              "3      CC(C)C(=O)NC1=CC=C(C=C1)N(C(C2=CSC=C2)C(=O)NC(...         1\n",
              "4      CC(C)C(=O)NC1=CC=C(C=C1)N(CC2=CSC=C2)C(=O)CN3C...         1\n",
              "...                                                  ...       ...\n",
              "11994                               C1=CC2=C(C=C1N)NN=C2         0\n",
              "11995  CC(=O)[C@H]1CC[C@@H]2[C@@]1(CC(=O)[C@H]3[C@H]2...         0\n",
              "11996                       C1CN(CCN1CC(CO)O)C2=CC=CC=C2         0\n",
              "11997  CCOC(=O)N1CCC(=C2C3=C(CCC4=C2N=CC=C4)C=C(C=C3)...         0\n",
              "11998                    C1=CC2=C(C=C1OC(F)(F)F)SC(=N2)N         0\n",
              "\n",
              "[11999 rows x 2 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBnPl4isM1FQ",
        "colab_type": "code",
        "outputId": "a7076ce6-4ef3-403e-e9b9-4687d529ce60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "dataset_eval.head"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                 smiles  activity\n",
              "0      OC=1C=CC=CC1CNC2=NC=3C=CC=CC3N2         1\n",
              "1        CC(=O)NCCC1=CNC=2C=CC(F)=CC12         1\n",
              "2    O=C([C@@H]1[C@H](C2=CSC=C2)CCC1)N         1\n",
              "3       CN1CCCC=2C=CC(=CC12)S(=O)(=O)N         1\n",
              "4     CC(=O)NC=1C=CC(OC=2N=CC=CN2)=CC1         1\n",
              "..                                 ...       ...\n",
              "875   CC(C)C=1C=CC(NC(=O)N2CCOCC2)=CC1         0\n",
              "876        CN(CC(=O)O)C(=O)C=1C=CC=CN1         0\n",
              "877  CN1CCN(CC1)C(=O)C=2C=CC(F)=C(F)C2         0\n",
              "878      FC=1C=CC=C(F)C1C(=O)N2CCCCCC2         0\n",
              "879             FC=1C=CC=NC1NCC2CCOCC2         0\n",
              "\n",
              "[880 rows x 2 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVrUoqtTHGy8",
        "colab_type": "code",
        "outputId": "49c02cfa-f130-42b0-e4b5-1dad7c27ef61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "from dgllife.data import MoleculeCSVDataset\n",
        "from dgllife.data.csv_dataset import *\n",
        "from dgllife.utils.featurizers import *\n",
        "from dgllife.utils.mol_to_graph import *\n",
        "\n",
        "# featurize bigraph/molecular graph set for train (SARS-COV-1) set\n",
        "train_set = MoleculeCSVDataset(dataset_train, smiles_to_graph=smiles_to_bigraph, node_featurizer=CanonicalAtomFeaturizer(),\n",
        "                               edge_featurizer=CanonicalBondFeaturizer(), smiles_column='smiles', cache_file_path='/content/drive/My Drive/Project De Novo/graph_featurizer/train.bin', task_names=['activity'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Using backend: pytorch\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "Loading previously saved dgl graphs...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofwpNYUXJASi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f39f6bf7-e0c4-4cf3-f17e-8b882a24e4c4"
      },
      "source": [
        "# featurize bigraph/molecular graph set for test (SARS-COV-2) set\n",
        "test_set = MoleculeCSVDataset(dataset_eval, smiles_to_graph=smiles_to_bigraph, node_featurizer=CanonicalAtomFeaturizer(),\n",
        "                               edge_featurizer=CanonicalBondFeaturizer(), smiles_column='smiles', cache_file_path='/content/drive/My Drive/Project De Novo/graph_featurizer/test.bin', task_names=['activity'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading previously saved dgl graphs...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1TtgCu8U26K",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "9f729a9d-9245-4cb9-ea5c-e667f4a009ba"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b482d203-804b-40d6-8b0a-5c982c6e1b63\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-b482d203-804b-40d6-8b0a-5c982c6e1b63\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving utils.py to utils.py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'utils.py': b'import dgl\\nimport numpy as np\\nimport random\\nimport torch\\n\\nfrom dgllife.utils.featurizers import one_hot_encoding\\nfrom dgllife.utils.splitters import RandomSplitter\\n\\ndef set_random_seed(seed=0):\\n    \"\"\"Set random seed.\\n    Parameters\\n    ----------\\n    seed : int\\n        Random seed to use\\n    \"\"\"\\n    random.seed(seed)\\n    np.random.seed(seed)\\n    torch.manual_seed(seed)\\n    if torch.cuda.is_available():\\n        torch.cuda.manual_seed(seed)\\n\\n\\ndef load_dataset_for_classification(args):\\n    \"\"\"Load dataset for classification tasks.\\n    Parameters\\n    ----------\\n    args : dict\\n        Configurations.\\n    Returns\\n    -------\\n    dataset\\n        The whole dataset.\\n    train_set\\n        Subset for training.\\n    val_set\\n        Subset for validation.\\n    test_set\\n        Subset for test.\\n    \"\"\"\\n    assert args[\\'dataset\\'] in [\\'Tox21\\']\\n    if args[\\'dataset\\'] == \\'Tox21\\':\\n        from dgllife.data import Tox21\\n        dataset = Tox21(smiles_to_graph=args[\\'smiles_to_graph\\'],\\n                        node_featurizer=args.get(\\'node_featurizer\\', None),\\n                        edge_featurizer=args.get(\\'edge_featurizer\\', None))\\n        train_set, val_set, test_set = RandomSplitter.train_val_test_split(\\n            dataset, frac_train=args[\\'frac_train\\'], frac_val=args[\\'frac_val\\'],\\n            frac_test=args[\\'frac_test\\'], random_state=args[\\'random_seed\\'])\\n\\n    return dataset, train_set, val_set, test_set\\n\\n\\ndef load_dataset_for_regression(args):\\n    \"\"\"Load dataset for regression tasks.\\n    Parameters\\n    ----------\\n    args : dict\\n        Configurations.\\n    Returns\\n    -------\\n    train_set\\n        Subset for training.\\n    val_set\\n        Subset for validation.\\n    test_set\\n        Subset for test.\\n    \"\"\"\\n    assert args[\\'dataset\\'] in [\\'Alchemy\\', \\'Aromaticity\\']\\n\\n    if args[\\'dataset\\'] == \\'Alchemy\\':\\n        from dgllife.data import TencentAlchemyDataset\\n        train_set = TencentAlchemyDataset(mode=\\'dev\\')\\n        val_set = TencentAlchemyDataset(mode=\\'valid\\')\\n        test_set = None\\n\\n    if args[\\'dataset\\'] == \\'Aromaticity\\':\\n        from dgllife.data import PubChemBioAssayAromaticity\\n        dataset = PubChemBioAssayAromaticity(smiles_to_graph=args[\\'smiles_to_graph\\'],\\n                                             node_featurizer=args.get(\\'node_featurizer\\', None),\\n                                             edge_featurizer=args.get(\\'edge_featurizer\\', None))\\n        train_set, val_set, test_set = RandomSplitter.train_val_test_split(\\n            dataset, frac_train=args[\\'frac_train\\'], frac_val=args[\\'frac_val\\'],\\n            frac_test=args[\\'frac_test\\'], random_state=args[\\'random_seed\\'])\\n\\n    return train_set, val_set, test_set\\n\\n\\ndef collate_molgraphs(data):\\n    \"\"\"Batching a list of datapoints for dataloader.\\n    Parameters\\n    ----------\\n    data : list of 3-tuples or 4-tuples.\\n        Each tuple is for a single datapoint, consisting of\\n        a SMILES, a DGLGraph, all-task labels and optionally\\n        a binary mask indicating the existence of labels.\\n    Returns\\n    -------\\n    smiles : list\\n        List of smiles\\n    bg : DGLGraph\\n        The batched DGLGraph.\\n    labels : Tensor of dtype float32 and shape (B, T)\\n        Batched datapoint labels. B is len(data) and\\n        T is the number of total tasks.\\n    masks : Tensor of dtype float32 and shape (B, T)\\n        Batched datapoint binary mask, indicating the\\n        existence of labels. If binary masks are not\\n        provided, return a tensor with ones.\\n    \"\"\"\\n    assert len(data[0]) in [3, 4], \\\\\\n        \\'Expect the tuple to be of length 3 or 4, got {:d}\\'.format(len(data[0]))\\n    if len(data[0]) == 3:\\n        smiles, graphs, labels = map(list, zip(*data))\\n        masks = None\\n    else:\\n        smiles, graphs, labels, masks = map(list, zip(*data))\\n\\n    bg = dgl.batch(graphs)\\n    bg.set_n_initializer(dgl.init.zero_initializer)\\n    bg.set_e_initializer(dgl.init.zero_initializer)\\n    labels = torch.stack(labels, dim=0)\\n\\n    if masks is None:\\n        masks = torch.ones(labels.shape)\\n    else:\\n        masks = torch.stack(masks, dim=0)\\n    return smiles, bg, labels, masks\\n\\n\\ndef load_model(args):\\n    if args[\\'model\\'] == \\'GCN\\':\\n        from dgllife.model import GCNPredictor\\n        model = GCNPredictor(in_feats=args[\\'node_featurizer\\'].feat_size(),\\n                             hidden_feats=args[\\'gcn_hidden_feats\\'],\\n                             classifier_hidden_feats=args[\\'classifier_hidden_feats\\'],\\n                             n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'GAT\\':\\n        from dgllife.model import GATPredictor\\n        model = GATPredictor(in_feats=args[\\'node_featurizer\\'].feat_size(),\\n                             hidden_feats=args[\\'gat_hidden_feats\\'],\\n                             num_heads=args[\\'num_heads\\'],\\n                             classifier_hidden_feats=args[\\'classifier_hidden_feats\\'],\\n                             n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'Weave\\':\\n        from dgllife.model import WeavePredictor\\n        model = WeavePredictor(node_in_feats=args[\\'node_featurizer\\'].feat_size(),\\n                               edge_in_feats=args[\\'edge_featurizer\\'].feat_size(),\\n                               num_gnn_layers=args[\\'num_gnn_layers\\'],\\n                               gnn_hidden_feats=args[\\'gnn_hidden_feats\\'],\\n                               graph_feats=args[\\'graph_feats\\'],\\n                               n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'AttentiveFP\\':\\n        from dgllife.model import AttentiveFPPredictor\\n        model = AttentiveFPPredictor(node_feat_size=args[\\'node_featurizer\\'].feat_size(),\\n                                     edge_feat_size=args[\\'edge_featurizer\\'].feat_size(),\\n                                     num_layers=args[\\'num_layers\\'],\\n                                     num_timesteps=args[\\'num_timesteps\\'],\\n                                     graph_feat_size=args[\\'graph_feat_size\\'],\\n                                     n_tasks=args[\\'n_tasks\\'],\\n                                     dropout=args[\\'dropout\\'])\\n\\n    if args[\\'model\\'] == \\'SchNet\\':\\n        from dgllife.model import SchNetPredictor\\n        model = SchNetPredictor(node_feats=args[\\'node_feats\\'],\\n                                hidden_feats=args[\\'hidden_feats\\'],\\n                                classifier_hidden_feats=args[\\'classifier_hidden_feats\\'],\\n                                n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'MGCN\\':\\n        from dgllife.model import MGCNPredictor\\n        model = MGCNPredictor(feats=args[\\'feats\\'],\\n                              n_layers=args[\\'n_layers\\'],\\n                              classifier_hidden_feats=args[\\'classifier_hidden_feats\\'],\\n                              n_tasks=args[\\'n_tasks\\'])\\n\\n    if args[\\'model\\'] == \\'MPNN\\':\\n        from dgllife.model import MPNNPredictor\\n        model = MPNNPredictor(node_in_feats=args[\\'node_in_feats\\'],\\n                              edge_in_feats=args[\\'edge_in_feats\\'],\\n                              node_out_feats=args[\\'node_out_feats\\'],\\n                              edge_hidden_feats=args[\\'edge_hidden_feats\\'],\\n                              n_tasks=args[\\'n_tasks\\'])\\n\\n    return model\\n\\n\\ndef chirality(atom):\\n    try:\\n        return one_hot_encoding(atom.GetProp(\\'_CIPCode\\'), [\\'R\\', \\'S\\']) + \\\\\\n               [atom.HasProp(\\'_ChiralityPossible\\')]\\n    except:\\n        return [False, False] + [atom.HasProp(\\'_ChiralityPossible\\')]\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKMgchzflhiW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = {\n",
        "    'random_seed': 2,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-3,\n",
        "    'num_epochs': 100,\n",
        "    'node_data_field': 'h',\n",
        "    'frac_train': 0.8,\n",
        "    'frac_val': 0.1,\n",
        "    'frac_test': 0.1,\n",
        "    'in_feats': 74,\n",
        "    'gat_hidden_feats': [32, 32],\n",
        "    'classifier_hidden_feats': 64,\n",
        "    'num_heads': [4, 4],\n",
        "    'patience': 10,\n",
        "    'smiles_to_graph': smiles_to_bigraph,\n",
        "    'node_featurizer': CanonicalAtomFeaturizer(),\n",
        "    'metric_name': 'roc_auc_score',\n",
        "    'model': 'GAT'\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuZkFAz-PDvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import utils\n",
        "\n",
        "from dgllife.model import load_pretrained\n",
        "from dgllife.utils import EarlyStopping, Meter\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "from utils import set_random_seed, load_dataset_for_classification, collate_molgraphs, load_model\n",
        "\n",
        "from dgllife.model import GATPredictor\n",
        "\n",
        "args['device'] = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "set_random_seed(args['random_seed'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYfHiYl0T8V4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_set,  batch_size=args['batch_size'],\n",
        "                              collate_fn=collate_molgraphs)\n",
        "\n",
        "test_loader = DataLoader(test_set,  batch_size=args['batch_size'],\n",
        "                          collate_fn=collate_molgraphs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGAUIdpRUAUu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 725
        },
        "outputId": "14fcc7b8-4107-499f-f3a2-ff0d8996e2fa"
      },
      "source": [
        "args['n_tasks'] = 1\n",
        "model = GATPredictor(in_feats=74,\n",
        "                             hidden_feats=args['gat_hidden_feats'],\n",
        "                             num_heads=args['num_heads'],\n",
        "                             classifier_hidden_feats=args['classifier_hidden_feats'],\n",
        "                             n_tasks=args['n_tasks'])\n",
        "\n",
        "import dgl.backend as F\n",
        "\n",
        "train_num_pos = F.sum(train_set.labels, dim=0)\n",
        "train_num_indices = F.sum(train_set.mask, dim=0)\n",
        "train_task_pos_weights = (train_num_indices - train_num_pos) / train_num_pos\n",
        "\n",
        "\n",
        "\n",
        "loss_criterion = BCEWithLogitsLoss(pos_weight=train_task_pos_weights.to(args['device']),\n",
        "                                    reduction='none')\n",
        "optimizer = Adam(model.parameters(), lr=args['lr'])\n",
        "stopper = EarlyStopping(patience=args['patience'])\n",
        "model.to(args['device'])\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GATPredictor(\n",
              "  (gnn): GAT(\n",
              "    (gnn_layers): ModuleList(\n",
              "      (0): GATLayer(\n",
              "        (gat_conv): GATConv(\n",
              "          (fc): Linear(in_features=74, out_features=128, bias=False)\n",
              "          (feat_drop): Dropout(p=0.0, inplace=False)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
              "          (res_fc): Linear(in_features=74, out_features=128, bias=False)\n",
              "        )\n",
              "      )\n",
              "      (1): GATLayer(\n",
              "        (gat_conv): GATConv(\n",
              "          (fc): Linear(in_features=128, out_features=128, bias=False)\n",
              "          (feat_drop): Dropout(p=0.0, inplace=False)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
              "          (res_fc): Linear(in_features=128, out_features=128, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (readout): WeightedSumAndMax(\n",
              "    (weight_and_sum): WeightAndSum(\n",
              "      (atom_weighting): Sequential(\n",
              "        (0): Linear(in_features=32, out_features=1, bias=True)\n",
              "        (1): Sigmoid()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (predict): MLPPredictor(\n",
              "    (predict): Sequential(\n",
              "      (0): Dropout(p=0.0, inplace=False)\n",
              "      (1): Linear(in_features=64, out_features=64, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (4): Linear(in_features=64, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C941OCzRuXH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def predict(args, model, bg):\n",
        "    node_feats = bg.ndata.pop(args['node_data_field']).to(args['device'])\n",
        "    if args.get('edge_featurizer', None) is not None:\n",
        "        edge_feats = bg.edata.pop(args['edge_data_field']).to(args['device'])\n",
        "        return model(bg, node_feats, edge_feats)\n",
        "    else:\n",
        "        return model(bg, node_feats)\n",
        "\n",
        "def run_a_train_epoch(args, epoch, model, data_loader, loss_criterion, optimizer):\n",
        "    model.train()\n",
        "    train_meter = Meter()\n",
        "    for batch_id, batch_data in enumerate(data_loader):\n",
        "        smiles, bg, labels, masks = batch_data\n",
        "        labels, masks = labels.to(args['device']), masks.to(args['device'])\n",
        "        logits = predict(args, model, bg)\n",
        "        # Mask non-existing labels\n",
        "        loss = (loss_criterion(logits, labels) * (masks != 0).float()).mean()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print('epoch {:d}/{:d}, batch {:d}/{:d}, loss {:.4f}'.format(\n",
        "            epoch + 1, args['num_epochs'], batch_id + 1, len(data_loader), loss.item()))\n",
        "        train_meter.update(logits, labels, masks)\n",
        "    train_score = np.mean(train_meter.compute_metric(args['metric_name']))\n",
        "    print('epoch {:d}/{:d}, training {} {:.4f}'.format(\n",
        "        epoch + 1, args['num_epochs'], args['metric_name'], train_score))\n",
        "\n",
        "def run_an_eval_epoch(args, model, data_loader):\n",
        "    model.eval()\n",
        "    eval_meter = Meter()\n",
        "    with torch.no_grad():\n",
        "        for batch_id, batch_data in enumerate(data_loader):\n",
        "            smiles, bg, labels, masks = batch_data\n",
        "            labels = labels.to(args['device'])\n",
        "            logits = predict(args, model, bg)\n",
        "            eval_meter.update(logits, labels, masks)\n",
        "    return np.mean(eval_meter.compute_metric(args['metric_name']))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73ZBLfQxuHQA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "62169df2-8f07-4cf7-f5c8-3a1604a6efdf"
      },
      "source": [
        "for epoch in range(args['num_epochs']):\n",
        "        # Train\n",
        "        run_a_train_epoch(args, epoch, model, train_loader, loss_criterion, optimizer)\n",
        "\n",
        "        # Validation and early stop\n",
        "        val_score = run_an_eval_epoch(args, model, test_loader)\n",
        "        early_stop = stopper.step(val_score, model)\n",
        "        print('epoch {:d}/{:d}, validation {} {:.4f}, best validation {} {:.4f}'.format(\n",
        "            epoch + 1, args['num_epochs'], args['metric_name'],\n",
        "            val_score, args['metric_name'], stopper.best_score))\n",
        "        if early_stop:\n",
        "            break"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1/100, batch 1/94, loss 17.8308\n",
            "epoch 1/100, batch 2/94, loss 17.7902\n",
            "epoch 1/100, batch 3/94, loss 17.7452\n",
            "epoch 1/100, batch 4/94, loss 8.6491\n",
            "epoch 1/100, batch 5/94, loss 0.7117\n",
            "epoch 1/100, batch 6/94, loss 0.7206\n",
            "epoch 1/100, batch 7/94, loss 0.7271\n",
            "epoch 1/100, batch 8/94, loss 0.7349\n",
            "epoch 1/100, batch 9/94, loss 0.7423\n",
            "epoch 1/100, batch 10/94, loss 0.7461\n",
            "epoch 1/100, batch 11/94, loss 0.7503\n",
            "epoch 1/100, batch 12/94, loss 0.7553\n",
            "epoch 1/100, batch 13/94, loss 0.7549\n",
            "epoch 1/100, batch 14/94, loss 0.7550\n",
            "epoch 1/100, batch 15/94, loss 0.7607\n",
            "epoch 1/100, batch 16/94, loss 0.7600\n",
            "epoch 1/100, batch 17/94, loss 0.7559\n",
            "epoch 1/100, batch 18/94, loss 0.7589\n",
            "epoch 1/100, batch 19/94, loss 0.7568\n",
            "epoch 1/100, batch 20/94, loss 0.7538\n",
            "epoch 1/100, batch 21/94, loss 0.7497\n",
            "epoch 1/100, batch 22/94, loss 0.7510\n",
            "epoch 1/100, batch 23/94, loss 0.7478\n",
            "epoch 1/100, batch 24/94, loss 0.7484\n",
            "epoch 1/100, batch 25/94, loss 0.7441\n",
            "epoch 1/100, batch 26/94, loss 0.7433\n",
            "epoch 1/100, batch 27/94, loss 0.7401\n",
            "epoch 1/100, batch 28/94, loss 0.7378\n",
            "epoch 1/100, batch 29/94, loss 0.7335\n",
            "epoch 1/100, batch 30/94, loss 0.7324\n",
            "epoch 1/100, batch 31/94, loss 0.7310\n",
            "epoch 1/100, batch 32/94, loss 0.7274\n",
            "epoch 1/100, batch 33/94, loss 0.7274\n",
            "epoch 1/100, batch 34/94, loss 0.7249\n",
            "epoch 1/100, batch 35/94, loss 0.7246\n",
            "epoch 1/100, batch 36/94, loss 0.7227\n",
            "epoch 1/100, batch 37/94, loss 0.7224\n",
            "epoch 1/100, batch 38/94, loss 0.7221\n",
            "epoch 1/100, batch 39/94, loss 0.7224\n",
            "epoch 1/100, batch 40/94, loss 0.7207\n",
            "epoch 1/100, batch 41/94, loss 0.7213\n",
            "epoch 1/100, batch 42/94, loss 0.7201\n",
            "epoch 1/100, batch 43/94, loss 0.7193\n",
            "epoch 1/100, batch 44/94, loss 0.7189\n",
            "epoch 1/100, batch 45/94, loss 0.7182\n",
            "epoch 1/100, batch 46/94, loss 0.7180\n",
            "epoch 1/100, batch 47/94, loss 0.7176\n",
            "epoch 1/100, batch 48/94, loss 0.7174\n",
            "epoch 1/100, batch 49/94, loss 0.7158\n",
            "epoch 1/100, batch 50/94, loss 0.7160\n",
            "epoch 1/100, batch 51/94, loss 0.7157\n",
            "epoch 1/100, batch 52/94, loss 0.7153\n",
            "epoch 1/100, batch 53/94, loss 0.7145\n",
            "epoch 1/100, batch 54/94, loss 0.7147\n",
            "epoch 1/100, batch 55/94, loss 0.7141\n",
            "epoch 1/100, batch 56/94, loss 0.7136\n",
            "epoch 1/100, batch 57/94, loss 0.7132\n",
            "epoch 1/100, batch 58/94, loss 0.7125\n",
            "epoch 1/100, batch 59/94, loss 0.7120\n",
            "epoch 1/100, batch 60/94, loss 0.7121\n",
            "epoch 1/100, batch 61/94, loss 0.7113\n",
            "epoch 1/100, batch 62/94, loss 0.7105\n",
            "epoch 1/100, batch 63/94, loss 0.7103\n",
            "epoch 1/100, batch 64/94, loss 0.7099\n",
            "epoch 1/100, batch 65/94, loss 0.7096\n",
            "epoch 1/100, batch 66/94, loss 0.7088\n",
            "epoch 1/100, batch 67/94, loss 0.7089\n",
            "epoch 1/100, batch 68/94, loss 0.7085\n",
            "epoch 1/100, batch 69/94, loss 0.7077\n",
            "epoch 1/100, batch 70/94, loss 0.7072\n",
            "epoch 1/100, batch 71/94, loss 0.7072\n",
            "epoch 1/100, batch 72/94, loss 0.7065\n",
            "epoch 1/100, batch 73/94, loss 0.7062\n",
            "epoch 1/100, batch 74/94, loss 0.7057\n",
            "epoch 1/100, batch 75/94, loss 0.7054\n",
            "epoch 1/100, batch 76/94, loss 0.7046\n",
            "epoch 1/100, batch 77/94, loss 0.7041\n",
            "epoch 1/100, batch 78/94, loss 0.7036\n",
            "epoch 1/100, batch 79/94, loss 0.7031\n",
            "epoch 1/100, batch 80/94, loss 0.7030\n",
            "epoch 1/100, batch 81/94, loss 0.7022\n",
            "epoch 1/100, batch 82/94, loss 0.7019\n",
            "epoch 1/100, batch 83/94, loss 0.7011\n",
            "epoch 1/100, batch 84/94, loss 0.7011\n",
            "epoch 1/100, batch 85/94, loss 0.7003\n",
            "epoch 1/100, batch 86/94, loss 0.6999\n",
            "epoch 1/100, batch 87/94, loss 0.6994\n",
            "epoch 1/100, batch 88/94, loss 0.6991\n",
            "epoch 1/100, batch 89/94, loss 0.6986\n",
            "epoch 1/100, batch 90/94, loss 0.6981\n",
            "epoch 1/100, batch 91/94, loss 0.6976\n",
            "epoch 1/100, batch 92/94, loss 0.6972\n",
            "epoch 1/100, batch 93/94, loss 0.6969\n",
            "epoch 1/100, batch 94/94, loss 0.6964\n",
            "epoch 1/100, training roc_auc_score 0.4919\n",
            "epoch 1/100, validation roc_auc_score 0.4654, best validation roc_auc_score 0.4654\n",
            "epoch 2/100, batch 1/94, loss 17.9285\n",
            "epoch 2/100, batch 2/94, loss 17.9121\n",
            "epoch 2/100, batch 3/94, loss 17.8566\n",
            "epoch 2/100, batch 4/94, loss 8.7794\n",
            "epoch 2/100, batch 5/94, loss 0.7028\n",
            "epoch 2/100, batch 6/94, loss 0.7066\n",
            "epoch 2/100, batch 7/94, loss 0.7098\n",
            "epoch 2/100, batch 8/94, loss 0.7136\n",
            "epoch 2/100, batch 9/94, loss 0.7168\n",
            "epoch 2/100, batch 10/94, loss 0.7190\n",
            "epoch 2/100, batch 11/94, loss 0.7236\n",
            "epoch 2/100, batch 12/94, loss 0.7243\n",
            "epoch 2/100, batch 13/94, loss 0.7270\n",
            "epoch 2/100, batch 14/94, loss 0.7266\n",
            "epoch 2/100, batch 15/94, loss 0.7296\n",
            "epoch 2/100, batch 16/94, loss 0.7305\n",
            "epoch 2/100, batch 17/94, loss 0.7318\n",
            "epoch 2/100, batch 18/94, loss 0.7325\n",
            "epoch 2/100, batch 19/94, loss 0.7330\n",
            "epoch 2/100, batch 20/94, loss 0.7313\n",
            "epoch 2/100, batch 21/94, loss 0.7299\n",
            "epoch 2/100, batch 22/94, loss 0.7309\n",
            "epoch 2/100, batch 23/94, loss 0.7310\n",
            "epoch 2/100, batch 24/94, loss 0.7304\n",
            "epoch 2/100, batch 25/94, loss 0.7292\n",
            "epoch 2/100, batch 26/94, loss 0.7295\n",
            "epoch 2/100, batch 27/94, loss 0.7286\n",
            "epoch 2/100, batch 28/94, loss 0.7278\n",
            "epoch 2/100, batch 29/94, loss 0.7259\n",
            "epoch 2/100, batch 30/94, loss 0.7264\n",
            "epoch 2/100, batch 31/94, loss 0.7252\n",
            "epoch 2/100, batch 32/94, loss 0.7240\n",
            "epoch 2/100, batch 33/94, loss 0.7240\n",
            "epoch 2/100, batch 34/94, loss 0.7222\n",
            "epoch 2/100, batch 35/94, loss 0.7221\n",
            "epoch 2/100, batch 36/94, loss 0.7205\n",
            "epoch 2/100, batch 37/94, loss 0.7192\n",
            "epoch 2/100, batch 38/94, loss 0.7206\n",
            "epoch 2/100, batch 39/94, loss 0.7191\n",
            "epoch 2/100, batch 40/94, loss 0.7183\n",
            "epoch 2/100, batch 41/94, loss 0.7174\n",
            "epoch 2/100, batch 42/94, loss 0.7164\n",
            "epoch 2/100, batch 43/94, loss 0.7155\n",
            "epoch 2/100, batch 44/94, loss 0.7157\n",
            "epoch 2/100, batch 45/94, loss 0.7145\n",
            "epoch 2/100, batch 46/94, loss 0.7143\n",
            "epoch 2/100, batch 47/94, loss 0.7138\n",
            "epoch 2/100, batch 48/94, loss 0.7133\n",
            "epoch 2/100, batch 49/94, loss 0.7128\n",
            "epoch 2/100, batch 50/94, loss 0.7124\n",
            "epoch 2/100, batch 51/94, loss 0.7117\n",
            "epoch 2/100, batch 52/94, loss 0.7111\n",
            "epoch 2/100, batch 53/94, loss 0.7107\n",
            "epoch 2/100, batch 54/94, loss 0.7101\n",
            "epoch 2/100, batch 55/94, loss 0.7101\n",
            "epoch 2/100, batch 56/94, loss 0.7093\n",
            "epoch 2/100, batch 57/94, loss 0.7090\n",
            "epoch 2/100, batch 58/94, loss 0.7083\n",
            "epoch 2/100, batch 59/94, loss 0.7076\n",
            "epoch 2/100, batch 60/94, loss 0.7078\n",
            "epoch 2/100, batch 61/94, loss 0.7073\n",
            "epoch 2/100, batch 62/94, loss 0.7066\n",
            "epoch 2/100, batch 63/94, loss 0.7065\n",
            "epoch 2/100, batch 64/94, loss 0.7059\n",
            "epoch 2/100, batch 65/94, loss 0.7053\n",
            "epoch 2/100, batch 66/94, loss 0.7050\n",
            "epoch 2/100, batch 67/94, loss 0.7047\n",
            "epoch 2/100, batch 68/94, loss 0.7045\n",
            "epoch 2/100, batch 69/94, loss 0.7039\n",
            "epoch 2/100, batch 70/94, loss 0.7036\n",
            "epoch 2/100, batch 71/94, loss 0.7029\n",
            "epoch 2/100, batch 72/94, loss 0.7026\n",
            "epoch 2/100, batch 73/94, loss 0.7022\n",
            "epoch 2/100, batch 74/94, loss 0.7019\n",
            "epoch 2/100, batch 75/94, loss 0.7015\n",
            "epoch 2/100, batch 76/94, loss 0.7010\n",
            "epoch 2/100, batch 77/94, loss 0.7006\n",
            "epoch 2/100, batch 78/94, loss 0.7001\n",
            "epoch 2/100, batch 79/94, loss 0.6997\n",
            "epoch 2/100, batch 80/94, loss 0.6994\n",
            "epoch 2/100, batch 81/94, loss 0.6991\n",
            "epoch 2/100, batch 82/94, loss 0.6986\n",
            "epoch 2/100, batch 83/94, loss 0.6981\n",
            "epoch 2/100, batch 84/94, loss 0.6979\n",
            "epoch 2/100, batch 85/94, loss 0.6974\n",
            "epoch 2/100, batch 86/94, loss 0.6968\n",
            "epoch 2/100, batch 87/94, loss 0.6965\n",
            "epoch 2/100, batch 88/94, loss 0.6960\n",
            "epoch 2/100, batch 89/94, loss 0.6960\n",
            "epoch 2/100, batch 90/94, loss 0.6955\n",
            "epoch 2/100, batch 91/94, loss 0.6949\n",
            "epoch 2/100, batch 92/94, loss 0.6948\n",
            "epoch 2/100, batch 93/94, loss 0.6945\n",
            "epoch 2/100, batch 94/94, loss 0.6940\n",
            "epoch 2/100, training roc_auc_score 0.4741\n",
            "epoch 2/100, validation roc_auc_score 0.5917, best validation roc_auc_score 0.5917\n",
            "epoch 3/100, batch 1/94, loss 17.9876\n",
            "epoch 3/100, batch 2/94, loss 17.9706\n",
            "epoch 3/100, batch 3/94, loss 17.9185\n",
            "epoch 3/100, batch 4/94, loss 8.7781\n",
            "epoch 3/100, batch 5/94, loss 0.6999\n",
            "epoch 3/100, batch 6/94, loss 0.7031\n",
            "epoch 3/100, batch 7/94, loss 0.7055\n",
            "epoch 3/100, batch 8/94, loss 0.7083\n",
            "epoch 3/100, batch 9/94, loss 0.7107\n",
            "epoch 3/100, batch 10/94, loss 0.7129\n",
            "epoch 3/100, batch 11/94, loss 0.7159\n",
            "epoch 3/100, batch 12/94, loss 0.7166\n",
            "epoch 3/100, batch 13/94, loss 0.7183\n",
            "epoch 3/100, batch 14/94, loss 0.7188\n",
            "epoch 3/100, batch 15/94, loss 0.7210\n",
            "epoch 3/100, batch 16/94, loss 0.7220\n",
            "epoch 3/100, batch 17/94, loss 0.7224\n",
            "epoch 3/100, batch 18/94, loss 0.7239\n",
            "epoch 3/100, batch 19/94, loss 0.7243\n",
            "epoch 3/100, batch 20/94, loss 0.7236\n",
            "epoch 3/100, batch 21/94, loss 0.7228\n",
            "epoch 3/100, batch 22/94, loss 0.7242\n",
            "epoch 3/100, batch 23/94, loss 0.7243\n",
            "epoch 3/100, batch 24/94, loss 0.7239\n",
            "epoch 3/100, batch 25/94, loss 0.7236\n",
            "epoch 3/100, batch 26/94, loss 0.7237\n",
            "epoch 3/100, batch 27/94, loss 0.7231\n",
            "epoch 3/100, batch 28/94, loss 0.7222\n",
            "epoch 3/100, batch 29/94, loss 0.7216\n",
            "epoch 3/100, batch 30/94, loss 0.7214\n",
            "epoch 3/100, batch 31/94, loss 0.7209\n",
            "epoch 3/100, batch 32/94, loss 0.7205\n",
            "epoch 3/100, batch 33/94, loss 0.7204\n",
            "epoch 3/100, batch 34/94, loss 0.7188\n",
            "epoch 3/100, batch 35/94, loss 0.7193\n",
            "epoch 3/100, batch 36/94, loss 0.7167\n",
            "epoch 3/100, batch 37/94, loss 0.7176\n",
            "epoch 3/100, batch 38/94, loss 0.7160\n",
            "epoch 3/100, batch 39/94, loss 0.7154\n",
            "epoch 3/100, batch 40/94, loss 0.7155\n",
            "epoch 3/100, batch 41/94, loss 0.7152\n",
            "epoch 3/100, batch 42/94, loss 0.7146\n",
            "epoch 3/100, batch 43/94, loss 0.7125\n",
            "epoch 3/100, batch 44/94, loss 0.7138\n",
            "epoch 3/100, batch 45/94, loss 0.7122\n",
            "epoch 3/100, batch 46/94, loss 0.7117\n",
            "epoch 3/100, batch 47/94, loss 0.7116\n",
            "epoch 3/100, batch 48/94, loss 0.7111\n",
            "epoch 3/100, batch 49/94, loss 0.7100\n",
            "epoch 3/100, batch 50/94, loss 0.7096\n",
            "epoch 3/100, batch 51/94, loss 0.7093\n",
            "epoch 3/100, batch 52/94, loss 0.7088\n",
            "epoch 3/100, batch 53/94, loss 0.7080\n",
            "epoch 3/100, batch 54/94, loss 0.7077\n",
            "epoch 3/100, batch 55/94, loss 0.7073\n",
            "epoch 3/100, batch 56/94, loss 0.7068\n",
            "epoch 3/100, batch 57/94, loss 0.7064\n",
            "epoch 3/100, batch 58/94, loss 0.7060\n",
            "epoch 3/100, batch 59/94, loss 0.7053\n",
            "epoch 3/100, batch 60/94, loss 0.7055\n",
            "epoch 3/100, batch 61/94, loss 0.7048\n",
            "epoch 3/100, batch 62/94, loss 0.7046\n",
            "epoch 3/100, batch 63/94, loss 0.7040\n",
            "epoch 3/100, batch 64/94, loss 0.7035\n",
            "epoch 3/100, batch 65/94, loss 0.7032\n",
            "epoch 3/100, batch 66/94, loss 0.7029\n",
            "epoch 3/100, batch 67/94, loss 0.7026\n",
            "epoch 3/100, batch 68/94, loss 0.7022\n",
            "epoch 3/100, batch 69/94, loss 0.7017\n",
            "epoch 3/100, batch 70/94, loss 0.7013\n",
            "epoch 3/100, batch 71/94, loss 0.7010\n",
            "epoch 3/100, batch 72/94, loss 0.7007\n",
            "epoch 3/100, batch 73/94, loss 0.7003\n",
            "epoch 3/100, batch 74/94, loss 0.7001\n",
            "epoch 3/100, batch 75/94, loss 0.6995\n",
            "epoch 3/100, batch 76/94, loss 0.6990\n",
            "epoch 3/100, batch 77/94, loss 0.6986\n",
            "epoch 3/100, batch 78/94, loss 0.6980\n",
            "epoch 3/100, batch 79/94, loss 0.6978\n",
            "epoch 3/100, batch 80/94, loss 0.6973\n",
            "epoch 3/100, batch 81/94, loss 0.6969\n",
            "epoch 3/100, batch 82/94, loss 0.6966\n",
            "epoch 3/100, batch 83/94, loss 0.6962\n",
            "epoch 3/100, batch 84/94, loss 0.6958\n",
            "epoch 3/100, batch 85/94, loss 0.6955\n",
            "epoch 3/100, batch 86/94, loss 0.6950\n",
            "epoch 3/100, batch 87/94, loss 0.6947\n",
            "epoch 3/100, batch 88/94, loss 0.6943\n",
            "epoch 3/100, batch 89/94, loss 0.6939\n",
            "epoch 3/100, batch 90/94, loss 0.6935\n",
            "epoch 3/100, batch 91/94, loss 0.6932\n",
            "epoch 3/100, batch 92/94, loss 0.6934\n",
            "epoch 3/100, batch 93/94, loss 0.6927\n",
            "epoch 3/100, batch 94/94, loss 0.6922\n",
            "epoch 3/100, training roc_auc_score 0.4557\n",
            "epoch 3/100, validation roc_auc_score 0.6290, best validation roc_auc_score 0.6290\n",
            "epoch 4/100, batch 1/94, loss 18.0242\n",
            "epoch 4/100, batch 2/94, loss 18.0141\n",
            "epoch 4/100, batch 3/94, loss 17.9610\n",
            "epoch 4/100, batch 4/94, loss 8.7407\n",
            "epoch 4/100, batch 5/94, loss 0.6985\n",
            "epoch 4/100, batch 6/94, loss 0.7018\n",
            "epoch 4/100, batch 7/94, loss 0.7046\n",
            "epoch 4/100, batch 8/94, loss 0.7075\n",
            "epoch 4/100, batch 9/94, loss 0.7105\n",
            "epoch 4/100, batch 10/94, loss 0.7130\n",
            "epoch 4/100, batch 11/94, loss 0.7158\n",
            "epoch 4/100, batch 12/94, loss 0.7168\n",
            "epoch 4/100, batch 13/94, loss 0.7190\n",
            "epoch 4/100, batch 14/94, loss 0.7191\n",
            "epoch 4/100, batch 15/94, loss 0.7219\n",
            "epoch 4/100, batch 16/94, loss 0.7225\n",
            "epoch 4/100, batch 17/94, loss 0.7233\n",
            "epoch 4/100, batch 18/94, loss 0.7248\n",
            "epoch 4/100, batch 19/94, loss 0.7253\n",
            "epoch 4/100, batch 20/94, loss 0.7248\n",
            "epoch 4/100, batch 21/94, loss 0.7248\n",
            "epoch 4/100, batch 22/94, loss 0.7250\n",
            "epoch 4/100, batch 23/94, loss 0.7256\n",
            "epoch 4/100, batch 24/94, loss 0.7248\n",
            "epoch 4/100, batch 25/94, loss 0.7245\n",
            "epoch 4/100, batch 26/94, loss 0.7240\n",
            "epoch 4/100, batch 27/94, loss 0.7238\n",
            "epoch 4/100, batch 28/94, loss 0.7232\n",
            "epoch 4/100, batch 29/94, loss 0.7221\n",
            "epoch 4/100, batch 30/94, loss 0.7223\n",
            "epoch 4/100, batch 31/94, loss 0.7212\n",
            "epoch 4/100, batch 32/94, loss 0.7208\n",
            "epoch 4/100, batch 33/94, loss 0.7207\n",
            "epoch 4/100, batch 34/94, loss 0.7187\n",
            "epoch 4/100, batch 35/94, loss 0.7185\n",
            "epoch 4/100, batch 36/94, loss 0.7174\n",
            "epoch 4/100, batch 37/94, loss 0.7160\n",
            "epoch 4/100, batch 38/94, loss 0.7174\n",
            "epoch 4/100, batch 39/94, loss 0.7162\n",
            "epoch 4/100, batch 40/94, loss 0.7156\n",
            "epoch 4/100, batch 41/94, loss 0.7149\n",
            "epoch 4/100, batch 42/94, loss 0.7139\n",
            "epoch 4/100, batch 43/94, loss 0.7124\n",
            "epoch 4/100, batch 44/94, loss 0.7123\n",
            "epoch 4/100, batch 45/94, loss 0.7116\n",
            "epoch 4/100, batch 46/94, loss 0.7110\n",
            "epoch 4/100, batch 47/94, loss 0.7102\n",
            "epoch 4/100, batch 48/94, loss 0.7101\n",
            "epoch 4/100, batch 49/94, loss 0.7098\n",
            "epoch 4/100, batch 50/94, loss 0.7094\n",
            "epoch 4/100, batch 51/94, loss 0.7086\n",
            "epoch 4/100, batch 52/94, loss 0.7079\n",
            "epoch 4/100, batch 53/94, loss 0.7073\n",
            "epoch 4/100, batch 54/94, loss 0.7069\n",
            "epoch 4/100, batch 55/94, loss 0.7062\n",
            "epoch 4/100, batch 56/94, loss 0.7058\n",
            "epoch 4/100, batch 57/94, loss 0.7057\n",
            "epoch 4/100, batch 58/94, loss 0.7050\n",
            "epoch 4/100, batch 59/94, loss 0.7044\n",
            "epoch 4/100, batch 60/94, loss 0.7043\n",
            "epoch 4/100, batch 61/94, loss 0.7037\n",
            "epoch 4/100, batch 62/94, loss 0.7031\n",
            "epoch 4/100, batch 63/94, loss 0.7031\n",
            "epoch 4/100, batch 64/94, loss 0.7022\n",
            "epoch 4/100, batch 65/94, loss 0.7021\n",
            "epoch 4/100, batch 66/94, loss 0.7015\n",
            "epoch 4/100, batch 67/94, loss 0.7014\n",
            "epoch 4/100, batch 68/94, loss 0.7010\n",
            "epoch 4/100, batch 69/94, loss 0.7006\n",
            "epoch 4/100, batch 70/94, loss 0.7002\n",
            "epoch 4/100, batch 71/94, loss 0.6998\n",
            "epoch 4/100, batch 72/94, loss 0.6993\n",
            "epoch 4/100, batch 73/94, loss 0.6991\n",
            "epoch 4/100, batch 74/94, loss 0.6985\n",
            "epoch 4/100, batch 75/94, loss 0.6984\n",
            "epoch 4/100, batch 76/94, loss 0.6979\n",
            "epoch 4/100, batch 77/94, loss 0.6974\n",
            "epoch 4/100, batch 78/94, loss 0.6969\n",
            "epoch 4/100, batch 79/94, loss 0.6967\n",
            "epoch 4/100, batch 80/94, loss 0.6962\n",
            "epoch 4/100, batch 81/94, loss 0.6958\n",
            "epoch 4/100, batch 82/94, loss 0.6955\n",
            "epoch 4/100, batch 83/94, loss 0.6951\n",
            "epoch 4/100, batch 84/94, loss 0.6948\n",
            "epoch 4/100, batch 85/94, loss 0.6944\n",
            "epoch 4/100, batch 86/94, loss 0.6940\n",
            "epoch 4/100, batch 87/94, loss 0.6936\n",
            "epoch 4/100, batch 88/94, loss 0.6932\n",
            "epoch 4/100, batch 89/94, loss 0.6928\n",
            "epoch 4/100, batch 90/94, loss 0.6924\n",
            "epoch 4/100, batch 91/94, loss 0.6920\n",
            "epoch 4/100, batch 92/94, loss 0.6928\n",
            "epoch 4/100, batch 93/94, loss 0.6918\n",
            "epoch 4/100, batch 94/94, loss 0.6912\n",
            "epoch 4/100, training roc_auc_score 0.4746\n",
            "EarlyStopping counter: 1 out of 10\n",
            "epoch 4/100, validation roc_auc_score 0.5552, best validation roc_auc_score 0.6290\n",
            "epoch 5/100, batch 1/94, loss 18.0547\n",
            "epoch 5/100, batch 2/94, loss 18.0356\n",
            "epoch 5/100, batch 3/94, loss 17.9878\n",
            "epoch 5/100, batch 4/94, loss 8.8527\n",
            "epoch 5/100, batch 5/94, loss 0.6974\n",
            "epoch 5/100, batch 6/94, loss 0.7019\n",
            "epoch 5/100, batch 7/94, loss 0.7052\n",
            "epoch 5/100, batch 8/94, loss 0.7093\n",
            "epoch 5/100, batch 9/94, loss 0.7132\n",
            "epoch 5/100, batch 10/94, loss 0.7163\n",
            "epoch 5/100, batch 11/94, loss 0.7208\n",
            "epoch 5/100, batch 12/94, loss 0.7218\n",
            "epoch 5/100, batch 13/94, loss 0.7240\n",
            "epoch 5/100, batch 14/94, loss 0.7226\n",
            "epoch 5/100, batch 15/94, loss 0.7261\n",
            "epoch 5/100, batch 16/94, loss 0.7250\n",
            "epoch 5/100, batch 17/94, loss 0.7258\n",
            "epoch 5/100, batch 18/94, loss 0.7259\n",
            "epoch 5/100, batch 19/94, loss 0.7264\n",
            "epoch 5/100, batch 20/94, loss 0.7262\n",
            "epoch 5/100, batch 21/94, loss 0.7254\n",
            "epoch 5/100, batch 22/94, loss 0.7260\n",
            "epoch 5/100, batch 23/94, loss 0.7263\n",
            "epoch 5/100, batch 24/94, loss 0.7247\n",
            "epoch 5/100, batch 25/94, loss 0.7244\n",
            "epoch 5/100, batch 26/94, loss 0.7238\n",
            "epoch 5/100, batch 27/94, loss 0.7226\n",
            "epoch 5/100, batch 28/94, loss 0.7216\n",
            "epoch 5/100, batch 29/94, loss 0.7204\n",
            "epoch 5/100, batch 30/94, loss 0.7198\n",
            "epoch 5/100, batch 31/94, loss 0.7188\n",
            "epoch 5/100, batch 32/94, loss 0.7186\n",
            "epoch 5/100, batch 33/94, loss 0.7178\n",
            "epoch 5/100, batch 34/94, loss 0.7164\n",
            "epoch 5/100, batch 35/94, loss 0.7162\n",
            "epoch 5/100, batch 36/94, loss 0.7154\n",
            "epoch 5/100, batch 37/94, loss 0.7139\n",
            "epoch 5/100, batch 38/94, loss 0.7150\n",
            "epoch 5/100, batch 39/94, loss 0.7131\n",
            "epoch 5/100, batch 40/94, loss 0.7130\n",
            "epoch 5/100, batch 41/94, loss 0.7120\n",
            "epoch 5/100, batch 42/94, loss 0.7111\n",
            "epoch 5/100, batch 43/94, loss 0.7101\n",
            "epoch 5/100, batch 44/94, loss 0.7098\n",
            "epoch 5/100, batch 45/94, loss 0.7092\n",
            "epoch 5/100, batch 46/94, loss 0.7086\n",
            "epoch 5/100, batch 47/94, loss 0.7079\n",
            "epoch 5/100, batch 48/94, loss 0.7076\n",
            "epoch 5/100, batch 49/94, loss 0.7073\n",
            "epoch 5/100, batch 50/94, loss 0.7070\n",
            "epoch 5/100, batch 51/94, loss 0.7061\n",
            "epoch 5/100, batch 52/94, loss 0.7055\n",
            "epoch 5/100, batch 53/94, loss 0.7049\n",
            "epoch 5/100, batch 54/94, loss 0.7047\n",
            "epoch 5/100, batch 55/94, loss 0.7040\n",
            "epoch 5/100, batch 56/94, loss 0.7034\n",
            "epoch 5/100, batch 57/94, loss 0.7032\n",
            "epoch 5/100, batch 58/94, loss 0.7026\n",
            "epoch 5/100, batch 59/94, loss 0.7021\n",
            "epoch 5/100, batch 60/94, loss 0.7018\n",
            "epoch 5/100, batch 61/94, loss 0.7015\n",
            "epoch 5/100, batch 62/94, loss 0.7009\n",
            "epoch 5/100, batch 63/94, loss 0.7005\n",
            "epoch 5/100, batch 64/94, loss 0.7000\n",
            "epoch 5/100, batch 65/94, loss 0.6995\n",
            "epoch 5/100, batch 66/94, loss 0.6990\n",
            "epoch 5/100, batch 67/94, loss 0.6988\n",
            "epoch 5/100, batch 68/94, loss 0.6983\n",
            "epoch 5/100, batch 69/94, loss 0.6979\n",
            "epoch 5/100, batch 70/94, loss 0.6976\n",
            "epoch 5/100, batch 71/94, loss 0.6971\n",
            "epoch 5/100, batch 72/94, loss 0.6965\n",
            "epoch 5/100, batch 73/94, loss 0.6963\n",
            "epoch 5/100, batch 74/94, loss 0.6957\n",
            "epoch 5/100, batch 75/94, loss 0.6954\n",
            "epoch 5/100, batch 76/94, loss 0.6950\n",
            "epoch 5/100, batch 77/94, loss 0.6945\n",
            "epoch 5/100, batch 78/94, loss 0.6940\n",
            "epoch 5/100, batch 79/94, loss 0.6936\n",
            "epoch 5/100, batch 80/94, loss 0.6931\n",
            "epoch 5/100, batch 81/94, loss 0.6927\n",
            "epoch 5/100, batch 82/94, loss 0.6924\n",
            "epoch 5/100, batch 83/94, loss 0.6919\n",
            "epoch 5/100, batch 84/94, loss 0.6914\n",
            "epoch 5/100, batch 85/94, loss 0.6910\n",
            "epoch 5/100, batch 86/94, loss 0.6905\n",
            "epoch 5/100, batch 87/94, loss 0.6901\n",
            "epoch 5/100, batch 88/94, loss 0.6896\n",
            "epoch 5/100, batch 89/94, loss 0.6892\n",
            "epoch 5/100, batch 90/94, loss 0.6887\n",
            "epoch 5/100, batch 91/94, loss 0.6883\n",
            "epoch 5/100, batch 92/94, loss 0.6879\n",
            "epoch 5/100, batch 93/94, loss 0.6874\n",
            "epoch 5/100, batch 94/94, loss 0.6870\n",
            "epoch 5/100, training roc_auc_score 0.4734\n",
            "EarlyStopping counter: 2 out of 10\n",
            "epoch 5/100, validation roc_auc_score 0.5864, best validation roc_auc_score 0.6290\n",
            "epoch 6/100, batch 1/94, loss 18.1429\n",
            "epoch 6/100, batch 2/94, loss 18.1279\n",
            "epoch 6/100, batch 3/94, loss 18.0799\n",
            "epoch 6/100, batch 4/94, loss 8.9038\n",
            "epoch 6/100, batch 5/94, loss 0.6937\n",
            "epoch 6/100, batch 6/94, loss 0.6975\n",
            "epoch 6/100, batch 7/94, loss 0.7005\n",
            "epoch 6/100, batch 8/94, loss 0.7038\n",
            "epoch 6/100, batch 9/94, loss 0.7073\n",
            "epoch 6/100, batch 10/94, loss 0.7103\n",
            "epoch 6/100, batch 11/94, loss 0.7136\n",
            "epoch 6/100, batch 12/94, loss 0.7143\n",
            "epoch 6/100, batch 13/94, loss 0.7172\n",
            "epoch 6/100, batch 14/94, loss 0.7169\n",
            "epoch 6/100, batch 15/94, loss 0.7194\n",
            "epoch 6/100, batch 16/94, loss 0.7198\n",
            "epoch 6/100, batch 17/94, loss 0.7216\n",
            "epoch 6/100, batch 18/94, loss 0.7230\n",
            "epoch 6/100, batch 19/94, loss 0.7239\n",
            "epoch 6/100, batch 20/94, loss 0.7239\n",
            "epoch 6/100, batch 21/94, loss 0.7236\n",
            "epoch 6/100, batch 22/94, loss 0.7239\n",
            "epoch 6/100, batch 23/94, loss 0.7250\n",
            "epoch 6/100, batch 24/94, loss 0.7231\n",
            "epoch 6/100, batch 25/94, loss 0.7233\n",
            "epoch 6/100, batch 26/94, loss 0.7233\n",
            "epoch 6/100, batch 27/94, loss 0.7221\n",
            "epoch 6/100, batch 28/94, loss 0.7216\n",
            "epoch 6/100, batch 29/94, loss 0.7207\n",
            "epoch 6/100, batch 30/94, loss 0.7203\n",
            "epoch 6/100, batch 31/94, loss 0.7194\n",
            "epoch 6/100, batch 32/94, loss 0.7195\n",
            "epoch 6/100, batch 33/94, loss 0.7186\n",
            "epoch 6/100, batch 34/94, loss 0.7167\n",
            "epoch 6/100, batch 35/94, loss 0.7165\n",
            "epoch 6/100, batch 36/94, loss 0.7161\n",
            "epoch 6/100, batch 37/94, loss 0.7142\n",
            "epoch 6/100, batch 38/94, loss 0.7157\n",
            "epoch 6/100, batch 39/94, loss 0.7129\n",
            "epoch 6/100, batch 40/94, loss 0.7129\n",
            "epoch 6/100, batch 41/94, loss 0.7118\n",
            "epoch 6/100, batch 42/94, loss 0.7111\n",
            "epoch 6/100, batch 43/94, loss 0.7094\n",
            "epoch 6/100, batch 44/94, loss 0.7088\n",
            "epoch 6/100, batch 45/94, loss 0.7082\n",
            "epoch 6/100, batch 46/94, loss 0.7075\n",
            "epoch 6/100, batch 47/94, loss 0.7069\n",
            "epoch 6/100, batch 48/94, loss 0.7068\n",
            "epoch 6/100, batch 49/94, loss 0.7070\n",
            "epoch 6/100, batch 50/94, loss 0.7061\n",
            "epoch 6/100, batch 51/94, loss 0.7056\n",
            "epoch 6/100, batch 52/94, loss 0.7052\n",
            "epoch 6/100, batch 53/94, loss 0.7042\n",
            "epoch 6/100, batch 54/94, loss 0.7033\n",
            "epoch 6/100, batch 55/94, loss 0.7027\n",
            "epoch 6/100, batch 56/94, loss 0.7023\n",
            "epoch 6/100, batch 57/94, loss 0.7025\n",
            "epoch 6/100, batch 58/94, loss 0.7018\n",
            "epoch 6/100, batch 59/94, loss 0.7010\n",
            "epoch 6/100, batch 60/94, loss 0.7009\n",
            "epoch 6/100, batch 61/94, loss 0.7004\n",
            "epoch 6/100, batch 62/94, loss 0.6995\n",
            "epoch 6/100, batch 63/94, loss 0.6992\n",
            "epoch 6/100, batch 64/94, loss 0.6987\n",
            "epoch 6/100, batch 65/94, loss 0.6983\n",
            "epoch 6/100, batch 66/94, loss 0.6980\n",
            "epoch 6/100, batch 67/94, loss 0.6978\n",
            "epoch 6/100, batch 68/94, loss 0.6972\n",
            "epoch 6/100, batch 69/94, loss 0.6968\n",
            "epoch 6/100, batch 70/94, loss 0.6965\n",
            "epoch 6/100, batch 71/94, loss 0.6962\n",
            "epoch 6/100, batch 72/94, loss 0.6957\n",
            "epoch 6/100, batch 73/94, loss 0.6955\n",
            "epoch 6/100, batch 74/94, loss 0.6950\n",
            "epoch 6/100, batch 75/94, loss 0.6948\n",
            "epoch 6/100, batch 76/94, loss 0.6944\n",
            "epoch 6/100, batch 77/94, loss 0.6938\n",
            "epoch 6/100, batch 78/94, loss 0.6935\n",
            "epoch 6/100, batch 79/94, loss 0.6930\n",
            "epoch 6/100, batch 80/94, loss 0.6926\n",
            "epoch 6/100, batch 81/94, loss 0.6922\n",
            "epoch 6/100, batch 82/94, loss 0.6920\n",
            "epoch 6/100, batch 83/94, loss 0.6916\n",
            "epoch 6/100, batch 84/94, loss 0.6912\n",
            "epoch 6/100, batch 85/94, loss 0.6911\n",
            "epoch 6/100, batch 86/94, loss 0.6905\n",
            "epoch 6/100, batch 87/94, loss 0.6900\n",
            "epoch 6/100, batch 88/94, loss 0.6896\n",
            "epoch 6/100, batch 89/94, loss 0.6893\n",
            "epoch 6/100, batch 90/94, loss 0.6888\n",
            "epoch 6/100, batch 91/94, loss 0.6885\n",
            "epoch 6/100, batch 92/94, loss 0.6890\n",
            "epoch 6/100, batch 93/94, loss 0.6882\n",
            "epoch 6/100, batch 94/94, loss 0.6876\n",
            "epoch 6/100, training roc_auc_score 0.4635\n",
            "EarlyStopping counter: 3 out of 10\n",
            "epoch 6/100, validation roc_auc_score 0.5768, best validation roc_auc_score 0.6290\n",
            "epoch 7/100, batch 1/94, loss 18.1451\n",
            "epoch 7/100, batch 2/94, loss 18.1354\n",
            "epoch 7/100, batch 3/94, loss 18.0798\n",
            "epoch 7/100, batch 4/94, loss 8.7843\n",
            "epoch 7/100, batch 5/94, loss 0.6942\n",
            "epoch 7/100, batch 6/94, loss 0.6985\n",
            "epoch 7/100, batch 7/94, loss 0.7016\n",
            "epoch 7/100, batch 8/94, loss 0.7050\n",
            "epoch 7/100, batch 9/94, loss 0.7088\n",
            "epoch 7/100, batch 10/94, loss 0.7121\n",
            "epoch 7/100, batch 11/94, loss 0.7151\n",
            "epoch 7/100, batch 12/94, loss 0.7163\n",
            "epoch 7/100, batch 13/94, loss 0.7190\n",
            "epoch 7/100, batch 14/94, loss 0.7212\n",
            "epoch 7/100, batch 15/94, loss 0.7228\n",
            "epoch 7/100, batch 16/94, loss 0.7241\n",
            "epoch 7/100, batch 17/94, loss 0.7226\n",
            "epoch 7/100, batch 18/94, loss 0.7249\n",
            "epoch 7/100, batch 19/94, loss 0.7245\n",
            "epoch 7/100, batch 20/94, loss 0.7258\n",
            "epoch 7/100, batch 21/94, loss 0.7249\n",
            "epoch 7/100, batch 22/94, loss 0.7237\n",
            "epoch 7/100, batch 23/94, loss 0.7256\n",
            "epoch 7/100, batch 24/94, loss 0.7241\n",
            "epoch 7/100, batch 25/94, loss 0.7233\n",
            "epoch 7/100, batch 26/94, loss 0.7232\n",
            "epoch 7/100, batch 27/94, loss 0.7214\n",
            "epoch 7/100, batch 28/94, loss 0.7203\n",
            "epoch 7/100, batch 29/94, loss 0.7196\n",
            "epoch 7/100, batch 30/94, loss 0.7193\n",
            "epoch 7/100, batch 31/94, loss 0.7173\n",
            "epoch 7/100, batch 32/94, loss 0.7181\n",
            "epoch 7/100, batch 33/94, loss 0.7165\n",
            "epoch 7/100, batch 34/94, loss 0.7159\n",
            "epoch 7/100, batch 35/94, loss 0.7151\n",
            "epoch 7/100, batch 36/94, loss 0.7149\n",
            "epoch 7/100, batch 37/94, loss 0.7135\n",
            "epoch 7/100, batch 38/94, loss 0.7140\n",
            "epoch 7/100, batch 39/94, loss 0.7120\n",
            "epoch 7/100, batch 40/94, loss 0.7120\n",
            "epoch 7/100, batch 41/94, loss 0.7105\n",
            "epoch 7/100, batch 42/94, loss 0.7098\n",
            "epoch 7/100, batch 43/94, loss 0.7094\n",
            "epoch 7/100, batch 44/94, loss 0.7092\n",
            "epoch 7/100, batch 45/94, loss 0.7084\n",
            "epoch 7/100, batch 46/94, loss 0.7072\n",
            "epoch 7/100, batch 47/94, loss 0.7069\n",
            "epoch 7/100, batch 48/94, loss 0.7066\n",
            "epoch 7/100, batch 49/94, loss 0.7063\n",
            "epoch 7/100, batch 50/94, loss 0.7063\n",
            "epoch 7/100, batch 51/94, loss 0.7052\n",
            "epoch 7/100, batch 52/94, loss 0.7046\n",
            "epoch 7/100, batch 53/94, loss 0.7044\n",
            "epoch 7/100, batch 54/94, loss 0.7034\n",
            "epoch 7/100, batch 55/94, loss 0.7029\n",
            "epoch 7/100, batch 56/94, loss 0.7022\n",
            "epoch 7/100, batch 57/94, loss 0.7026\n",
            "epoch 7/100, batch 58/94, loss 0.7018\n",
            "epoch 7/100, batch 59/94, loss 0.7008\n",
            "epoch 7/100, batch 60/94, loss 0.7010\n",
            "epoch 7/100, batch 61/94, loss 0.7003\n",
            "epoch 7/100, batch 62/94, loss 0.6997\n",
            "epoch 7/100, batch 63/94, loss 0.6996\n",
            "epoch 7/100, batch 64/94, loss 0.6989\n",
            "epoch 7/100, batch 65/94, loss 0.6984\n",
            "epoch 7/100, batch 66/94, loss 0.6978\n",
            "epoch 7/100, batch 67/94, loss 0.6980\n",
            "epoch 7/100, batch 68/94, loss 0.6975\n",
            "epoch 7/100, batch 69/94, loss 0.6973\n",
            "epoch 7/100, batch 70/94, loss 0.6969\n",
            "epoch 7/100, batch 71/94, loss 0.6964\n",
            "epoch 7/100, batch 72/94, loss 0.6957\n",
            "epoch 7/100, batch 73/94, loss 0.6958\n",
            "epoch 7/100, batch 74/94, loss 0.6951\n",
            "epoch 7/100, batch 75/94, loss 0.6950\n",
            "epoch 7/100, batch 76/94, loss 0.6945\n",
            "epoch 7/100, batch 77/94, loss 0.6941\n",
            "epoch 7/100, batch 78/94, loss 0.6935\n",
            "epoch 7/100, batch 79/94, loss 0.6932\n",
            "epoch 7/100, batch 80/94, loss 0.6926\n",
            "epoch 7/100, batch 81/94, loss 0.6924\n",
            "epoch 7/100, batch 82/94, loss 0.6921\n",
            "epoch 7/100, batch 83/94, loss 0.6917\n",
            "epoch 7/100, batch 84/94, loss 0.6913\n",
            "epoch 7/100, batch 85/94, loss 0.6911\n",
            "epoch 7/100, batch 86/94, loss 0.6907\n",
            "epoch 7/100, batch 87/94, loss 0.6903\n",
            "epoch 7/100, batch 88/94, loss 0.6900\n",
            "epoch 7/100, batch 89/94, loss 0.6895\n",
            "epoch 7/100, batch 90/94, loss 0.6892\n",
            "epoch 7/100, batch 91/94, loss 0.6888\n",
            "epoch 7/100, batch 92/94, loss 0.6890\n",
            "epoch 7/100, batch 93/94, loss 0.6885\n",
            "epoch 7/100, batch 94/94, loss 0.6882\n",
            "epoch 7/100, training roc_auc_score 0.4835\n",
            "epoch 7/100, validation roc_auc_score 0.6601, best validation roc_auc_score 0.6601\n",
            "epoch 8/100, batch 1/94, loss 18.1497\n",
            "epoch 8/100, batch 2/94, loss 18.1335\n",
            "epoch 8/100, batch 3/94, loss 18.0824\n",
            "epoch 8/100, batch 4/94, loss 8.6180\n",
            "epoch 8/100, batch 5/94, loss 0.6943\n",
            "epoch 8/100, batch 6/94, loss 0.6979\n",
            "epoch 8/100, batch 7/94, loss 0.7012\n",
            "epoch 8/100, batch 8/94, loss 0.7039\n",
            "epoch 8/100, batch 9/94, loss 0.7069\n",
            "epoch 8/100, batch 10/94, loss 0.7101\n",
            "epoch 8/100, batch 11/94, loss 0.7131\n",
            "epoch 8/100, batch 12/94, loss 0.7139\n",
            "epoch 8/100, batch 13/94, loss 0.7162\n",
            "epoch 8/100, batch 14/94, loss 0.7187\n",
            "epoch 8/100, batch 15/94, loss 0.7197\n",
            "epoch 8/100, batch 16/94, loss 0.7201\n",
            "epoch 8/100, batch 17/94, loss 0.7193\n",
            "epoch 8/100, batch 18/94, loss 0.7208\n",
            "epoch 8/100, batch 19/94, loss 0.7213\n",
            "epoch 8/100, batch 20/94, loss 0.7222\n",
            "epoch 8/100, batch 21/94, loss 0.7204\n",
            "epoch 8/100, batch 22/94, loss 0.7208\n",
            "epoch 8/100, batch 23/94, loss 0.7218\n",
            "epoch 8/100, batch 24/94, loss 0.7203\n",
            "epoch 8/100, batch 25/94, loss 0.7208\n",
            "epoch 8/100, batch 26/94, loss 0.7200\n",
            "epoch 8/100, batch 27/94, loss 0.7186\n",
            "epoch 8/100, batch 28/94, loss 0.7177\n",
            "epoch 8/100, batch 29/94, loss 0.7165\n",
            "epoch 8/100, batch 30/94, loss 0.7157\n",
            "epoch 8/100, batch 31/94, loss 0.7149\n",
            "epoch 8/100, batch 32/94, loss 0.7147\n",
            "epoch 8/100, batch 33/94, loss 0.7140\n",
            "epoch 8/100, batch 34/94, loss 0.7117\n",
            "epoch 8/100, batch 35/94, loss 0.7121\n",
            "epoch 8/100, batch 36/94, loss 0.7111\n",
            "epoch 8/100, batch 37/94, loss 0.7098\n",
            "epoch 8/100, batch 38/94, loss 0.7105\n",
            "epoch 8/100, batch 39/94, loss 0.7082\n",
            "epoch 8/100, batch 40/94, loss 0.7081\n",
            "epoch 8/100, batch 41/94, loss 0.7077\n",
            "epoch 8/100, batch 42/94, loss 0.7072\n",
            "epoch 8/100, batch 43/94, loss 0.7064\n",
            "epoch 8/100, batch 44/94, loss 0.7061\n",
            "epoch 8/100, batch 45/94, loss 0.7055\n",
            "epoch 8/100, batch 46/94, loss 0.7051\n",
            "epoch 8/100, batch 47/94, loss 0.7044\n",
            "epoch 8/100, batch 48/94, loss 0.7048\n",
            "epoch 8/100, batch 49/94, loss 0.7044\n",
            "epoch 8/100, batch 50/94, loss 0.7039\n",
            "epoch 8/100, batch 51/94, loss 0.7035\n",
            "epoch 8/100, batch 52/94, loss 0.7032\n",
            "epoch 8/100, batch 53/94, loss 0.7027\n",
            "epoch 8/100, batch 54/94, loss 0.7020\n",
            "epoch 8/100, batch 55/94, loss 0.7007\n",
            "epoch 8/100, batch 56/94, loss 0.7011\n",
            "epoch 8/100, batch 57/94, loss 0.7005\n",
            "epoch 8/100, batch 58/94, loss 0.6998\n",
            "epoch 8/100, batch 59/94, loss 0.6992\n",
            "epoch 8/100, batch 60/94, loss 0.6992\n",
            "epoch 8/100, batch 61/94, loss 0.6987\n",
            "epoch 8/100, batch 62/94, loss 0.6982\n",
            "epoch 8/100, batch 63/94, loss 0.6981\n",
            "epoch 8/100, batch 64/94, loss 0.6976\n",
            "epoch 8/100, batch 65/94, loss 0.6972\n",
            "epoch 8/100, batch 66/94, loss 0.6969\n",
            "epoch 8/100, batch 67/94, loss 0.6968\n",
            "epoch 8/100, batch 68/94, loss 0.6964\n",
            "epoch 8/100, batch 69/94, loss 0.6961\n",
            "epoch 8/100, batch 70/94, loss 0.6960\n",
            "epoch 8/100, batch 71/94, loss 0.6955\n",
            "epoch 8/100, batch 72/94, loss 0.6950\n",
            "epoch 8/100, batch 73/94, loss 0.6949\n",
            "epoch 8/100, batch 74/94, loss 0.6944\n",
            "epoch 8/100, batch 75/94, loss 0.6941\n",
            "epoch 8/100, batch 76/94, loss 0.6939\n",
            "epoch 8/100, batch 77/94, loss 0.6935\n",
            "epoch 8/100, batch 78/94, loss 0.6931\n",
            "epoch 8/100, batch 79/94, loss 0.6928\n",
            "epoch 8/100, batch 80/94, loss 0.6923\n",
            "epoch 8/100, batch 81/94, loss 0.6921\n",
            "epoch 8/100, batch 82/94, loss 0.6918\n",
            "epoch 8/100, batch 83/94, loss 0.6915\n",
            "epoch 8/100, batch 84/94, loss 0.6910\n",
            "epoch 8/100, batch 85/94, loss 0.6908\n",
            "epoch 8/100, batch 86/94, loss 0.6904\n",
            "epoch 8/100, batch 87/94, loss 0.6902\n",
            "epoch 8/100, batch 88/94, loss 0.6898\n",
            "epoch 8/100, batch 89/94, loss 0.6895\n",
            "epoch 8/100, batch 90/94, loss 0.6892\n",
            "epoch 8/100, batch 91/94, loss 0.6887\n",
            "epoch 8/100, batch 92/94, loss 0.6898\n",
            "epoch 8/100, batch 93/94, loss 0.6885\n",
            "epoch 8/100, batch 94/94, loss 0.6880\n",
            "epoch 8/100, training roc_auc_score 0.4757\n",
            "EarlyStopping counter: 1 out of 10\n",
            "epoch 8/100, validation roc_auc_score 0.6402, best validation roc_auc_score 0.6601\n",
            "epoch 9/100, batch 1/94, loss 18.1409\n",
            "epoch 9/100, batch 2/94, loss 18.1309\n",
            "epoch 9/100, batch 3/94, loss 18.0797\n",
            "epoch 9/100, batch 4/94, loss 8.6394\n",
            "epoch 9/100, batch 5/94, loss 0.6936\n",
            "epoch 9/100, batch 6/94, loss 0.6959\n",
            "epoch 9/100, batch 7/94, loss 0.6979\n",
            "epoch 9/100, batch 8/94, loss 0.6996\n",
            "epoch 9/100, batch 9/94, loss 0.7016\n",
            "epoch 9/100, batch 10/94, loss 0.7034\n",
            "epoch 9/100, batch 11/94, loss 0.7052\n",
            "epoch 9/100, batch 12/94, loss 0.7058\n",
            "epoch 9/100, batch 13/94, loss 0.7073\n",
            "epoch 9/100, batch 14/94, loss 0.7094\n",
            "epoch 9/100, batch 15/94, loss 0.7095\n",
            "epoch 9/100, batch 16/94, loss 0.7108\n",
            "epoch 9/100, batch 17/94, loss 0.7097\n",
            "epoch 9/100, batch 18/94, loss 0.7112\n",
            "epoch 9/100, batch 19/94, loss 0.7112\n",
            "epoch 9/100, batch 20/94, loss 0.7124\n",
            "epoch 9/100, batch 21/94, loss 0.7111\n",
            "epoch 9/100, batch 22/94, loss 0.7116\n",
            "epoch 9/100, batch 23/94, loss 0.7124\n",
            "epoch 9/100, batch 24/94, loss 0.7122\n",
            "epoch 9/100, batch 25/94, loss 0.7123\n",
            "epoch 9/100, batch 26/94, loss 0.7125\n",
            "epoch 9/100, batch 27/94, loss 0.7119\n",
            "epoch 9/100, batch 28/94, loss 0.7111\n",
            "epoch 9/100, batch 29/94, loss 0.7111\n",
            "epoch 9/100, batch 30/94, loss 0.7109\n",
            "epoch 9/100, batch 31/94, loss 0.7102\n",
            "epoch 9/100, batch 32/94, loss 0.7104\n",
            "epoch 9/100, batch 33/94, loss 0.7102\n",
            "epoch 9/100, batch 34/94, loss 0.7099\n",
            "epoch 9/100, batch 35/94, loss 0.7095\n",
            "epoch 9/100, batch 36/94, loss 0.7088\n",
            "epoch 9/100, batch 37/94, loss 0.7080\n",
            "epoch 9/100, batch 38/94, loss 0.7093\n",
            "epoch 9/100, batch 39/94, loss 0.7079\n",
            "epoch 9/100, batch 40/94, loss 0.7078\n",
            "epoch 9/100, batch 41/94, loss 0.7069\n",
            "epoch 9/100, batch 42/94, loss 0.7064\n",
            "epoch 9/100, batch 43/94, loss 0.7057\n",
            "epoch 9/100, batch 44/94, loss 0.7061\n",
            "epoch 9/100, batch 45/94, loss 0.7056\n",
            "epoch 9/100, batch 46/94, loss 0.7052\n",
            "epoch 9/100, batch 47/94, loss 0.7043\n",
            "epoch 9/100, batch 48/94, loss 0.7044\n",
            "epoch 9/100, batch 49/94, loss 0.7035\n",
            "epoch 9/100, batch 50/94, loss 0.7038\n",
            "epoch 9/100, batch 51/94, loss 0.7031\n",
            "epoch 9/100, batch 52/94, loss 0.7026\n",
            "epoch 9/100, batch 53/94, loss 0.7023\n",
            "epoch 9/100, batch 54/94, loss 0.7022\n",
            "epoch 9/100, batch 55/94, loss 0.7014\n",
            "epoch 9/100, batch 56/94, loss 0.7010\n",
            "epoch 9/100, batch 57/94, loss 0.7011\n",
            "epoch 9/100, batch 58/94, loss 0.7004\n",
            "epoch 9/100, batch 59/94, loss 0.6996\n",
            "epoch 9/100, batch 60/94, loss 0.6999\n",
            "epoch 9/100, batch 61/94, loss 0.6992\n",
            "epoch 9/100, batch 62/94, loss 0.6987\n",
            "epoch 9/100, batch 63/94, loss 0.6987\n",
            "epoch 9/100, batch 64/94, loss 0.6980\n",
            "epoch 9/100, batch 65/94, loss 0.6977\n",
            "epoch 9/100, batch 66/94, loss 0.6972\n",
            "epoch 9/100, batch 67/94, loss 0.6972\n",
            "epoch 9/100, batch 68/94, loss 0.6970\n",
            "epoch 9/100, batch 69/94, loss 0.6967\n",
            "epoch 9/100, batch 70/94, loss 0.6963\n",
            "epoch 9/100, batch 71/94, loss 0.6958\n",
            "epoch 9/100, batch 72/94, loss 0.6952\n",
            "epoch 9/100, batch 73/94, loss 0.6954\n",
            "epoch 9/100, batch 74/94, loss 0.6948\n",
            "epoch 9/100, batch 75/94, loss 0.6943\n",
            "epoch 9/100, batch 76/94, loss 0.6942\n",
            "epoch 9/100, batch 77/94, loss 0.6938\n",
            "epoch 9/100, batch 78/94, loss 0.6933\n",
            "epoch 9/100, batch 79/94, loss 0.6932\n",
            "epoch 9/100, batch 80/94, loss 0.6926\n",
            "epoch 9/100, batch 81/94, loss 0.6925\n",
            "epoch 9/100, batch 82/94, loss 0.6921\n",
            "epoch 9/100, batch 83/94, loss 0.6918\n",
            "epoch 9/100, batch 84/94, loss 0.6913\n",
            "epoch 9/100, batch 85/94, loss 0.6911\n",
            "epoch 9/100, batch 86/94, loss 0.6908\n",
            "epoch 9/100, batch 87/94, loss 0.6905\n",
            "epoch 9/100, batch 88/94, loss 0.6902\n",
            "epoch 9/100, batch 89/94, loss 0.6898\n",
            "epoch 9/100, batch 90/94, loss 0.6895\n",
            "epoch 9/100, batch 91/94, loss 0.6890\n",
            "epoch 9/100, batch 92/94, loss 0.6891\n",
            "epoch 9/100, batch 93/94, loss 0.6889\n",
            "epoch 9/100, batch 94/94, loss 0.6884\n",
            "epoch 9/100, training roc_auc_score 0.4708\n",
            "EarlyStopping counter: 2 out of 10\n",
            "epoch 9/100, validation roc_auc_score 0.6089, best validation roc_auc_score 0.6601\n",
            "epoch 10/100, batch 1/94, loss 18.1396\n",
            "epoch 10/100, batch 2/94, loss 18.1327\n",
            "epoch 10/100, batch 3/94, loss 18.0855\n",
            "epoch 10/100, batch 4/94, loss 8.4636\n",
            "epoch 10/100, batch 5/94, loss 0.6936\n",
            "epoch 10/100, batch 6/94, loss 0.6958\n",
            "epoch 10/100, batch 7/94, loss 0.6975\n",
            "epoch 10/100, batch 8/94, loss 0.6993\n",
            "epoch 10/100, batch 9/94, loss 0.7012\n",
            "epoch 10/100, batch 10/94, loss 0.7026\n",
            "epoch 10/100, batch 11/94, loss 0.7042\n",
            "epoch 10/100, batch 12/94, loss 0.7047\n",
            "epoch 10/100, batch 13/94, loss 0.7063\n",
            "epoch 10/100, batch 14/94, loss 0.7075\n",
            "epoch 10/100, batch 15/94, loss 0.7082\n",
            "epoch 10/100, batch 16/94, loss 0.7094\n",
            "epoch 10/100, batch 17/94, loss 0.7085\n",
            "epoch 10/100, batch 18/94, loss 0.7101\n",
            "epoch 10/100, batch 19/94, loss 0.7101\n",
            "epoch 10/100, batch 20/94, loss 0.7111\n",
            "epoch 10/100, batch 21/94, loss 0.7098\n",
            "epoch 10/100, batch 22/94, loss 0.7106\n",
            "epoch 10/100, batch 23/94, loss 0.7111\n",
            "epoch 10/100, batch 24/94, loss 0.7110\n",
            "epoch 10/100, batch 25/94, loss 0.7111\n",
            "epoch 10/100, batch 26/94, loss 0.7116\n",
            "epoch 10/100, batch 27/94, loss 0.7110\n",
            "epoch 10/100, batch 28/94, loss 0.7099\n",
            "epoch 10/100, batch 29/94, loss 0.7103\n",
            "epoch 10/100, batch 30/94, loss 0.7103\n",
            "epoch 10/100, batch 31/94, loss 0.7093\n",
            "epoch 10/100, batch 32/94, loss 0.7097\n",
            "epoch 10/100, batch 33/94, loss 0.7093\n",
            "epoch 10/100, batch 34/94, loss 0.7090\n",
            "epoch 10/100, batch 35/94, loss 0.7087\n",
            "epoch 10/100, batch 36/94, loss 0.7080\n",
            "epoch 10/100, batch 37/94, loss 0.7071\n",
            "epoch 10/100, batch 38/94, loss 0.7087\n",
            "epoch 10/100, batch 39/94, loss 0.7072\n",
            "epoch 10/100, batch 40/94, loss 0.7072\n",
            "epoch 10/100, batch 41/94, loss 0.7063\n",
            "epoch 10/100, batch 42/94, loss 0.7059\n",
            "epoch 10/100, batch 43/94, loss 0.7051\n",
            "epoch 10/100, batch 44/94, loss 0.7055\n",
            "epoch 10/100, batch 45/94, loss 0.7052\n",
            "epoch 10/100, batch 46/94, loss 0.7049\n",
            "epoch 10/100, batch 47/94, loss 0.7038\n",
            "epoch 10/100, batch 48/94, loss 0.7040\n",
            "epoch 10/100, batch 49/94, loss 0.7031\n",
            "epoch 10/100, batch 50/94, loss 0.7033\n",
            "epoch 10/100, batch 51/94, loss 0.7027\n",
            "epoch 10/100, batch 52/94, loss 0.7021\n",
            "epoch 10/100, batch 53/94, loss 0.7019\n",
            "epoch 10/100, batch 54/94, loss 0.7018\n",
            "epoch 10/100, batch 55/94, loss 0.7010\n",
            "epoch 10/100, batch 56/94, loss 0.7006\n",
            "epoch 10/100, batch 57/94, loss 0.7012\n",
            "epoch 10/100, batch 58/94, loss 0.7001\n",
            "epoch 10/100, batch 59/94, loss 0.6994\n",
            "epoch 10/100, batch 60/94, loss 0.6996\n",
            "epoch 10/100, batch 61/94, loss 0.6990\n",
            "epoch 10/100, batch 62/94, loss 0.6984\n",
            "epoch 10/100, batch 63/94, loss 0.6985\n",
            "epoch 10/100, batch 64/94, loss 0.6978\n",
            "epoch 10/100, batch 65/94, loss 0.6974\n",
            "epoch 10/100, batch 66/94, loss 0.6969\n",
            "epoch 10/100, batch 67/94, loss 0.6970\n",
            "epoch 10/100, batch 68/94, loss 0.6967\n",
            "epoch 10/100, batch 69/94, loss 0.6964\n",
            "epoch 10/100, batch 70/94, loss 0.6962\n",
            "epoch 10/100, batch 71/94, loss 0.6956\n",
            "epoch 10/100, batch 72/94, loss 0.6950\n",
            "epoch 10/100, batch 73/94, loss 0.6951\n",
            "epoch 10/100, batch 74/94, loss 0.6946\n",
            "epoch 10/100, batch 75/94, loss 0.6942\n",
            "epoch 10/100, batch 76/94, loss 0.6940\n",
            "epoch 10/100, batch 77/94, loss 0.6936\n",
            "epoch 10/100, batch 78/94, loss 0.6933\n",
            "epoch 10/100, batch 79/94, loss 0.6932\n",
            "epoch 10/100, batch 80/94, loss 0.6924\n",
            "epoch 10/100, batch 81/94, loss 0.6924\n",
            "epoch 10/100, batch 82/94, loss 0.6919\n",
            "epoch 10/100, batch 83/94, loss 0.6916\n",
            "epoch 10/100, batch 84/94, loss 0.6912\n",
            "epoch 10/100, batch 85/94, loss 0.6910\n",
            "epoch 10/100, batch 86/94, loss 0.6907\n",
            "epoch 10/100, batch 87/94, loss 0.6905\n",
            "epoch 10/100, batch 88/94, loss 0.6901\n",
            "epoch 10/100, batch 89/94, loss 0.6897\n",
            "epoch 10/100, batch 90/94, loss 0.6895\n",
            "epoch 10/100, batch 91/94, loss 0.6889\n",
            "epoch 10/100, batch 92/94, loss 0.6891\n",
            "epoch 10/100, batch 93/94, loss 0.6892\n",
            "epoch 10/100, batch 94/94, loss 0.6884\n",
            "epoch 10/100, training roc_auc_score 0.4824\n",
            "EarlyStopping counter: 3 out of 10\n",
            "epoch 10/100, validation roc_auc_score 0.5646, best validation roc_auc_score 0.6601\n",
            "epoch 11/100, batch 1/94, loss 18.1577\n",
            "epoch 11/100, batch 2/94, loss 18.1593\n",
            "epoch 11/100, batch 3/94, loss 18.0950\n",
            "epoch 11/100, batch 4/94, loss 8.3598\n",
            "epoch 11/100, batch 5/94, loss 0.6935\n",
            "epoch 11/100, batch 6/94, loss 0.6957\n",
            "epoch 11/100, batch 7/94, loss 0.6973\n",
            "epoch 11/100, batch 8/94, loss 0.6995\n",
            "epoch 11/100, batch 9/94, loss 0.7015\n",
            "epoch 11/100, batch 10/94, loss 0.7030\n",
            "epoch 11/100, batch 11/94, loss 0.7051\n",
            "epoch 11/100, batch 12/94, loss 0.7051\n",
            "epoch 11/100, batch 13/94, loss 0.7069\n",
            "epoch 11/100, batch 14/94, loss 0.7082\n",
            "epoch 11/100, batch 15/94, loss 0.7081\n",
            "epoch 11/100, batch 16/94, loss 0.7095\n",
            "epoch 11/100, batch 17/94, loss 0.7086\n",
            "epoch 11/100, batch 18/94, loss 0.7099\n",
            "epoch 11/100, batch 19/94, loss 0.7096\n",
            "epoch 11/100, batch 20/94, loss 0.7106\n",
            "epoch 11/100, batch 21/94, loss 0.7093\n",
            "epoch 11/100, batch 22/94, loss 0.7100\n",
            "epoch 11/100, batch 23/94, loss 0.7102\n",
            "epoch 11/100, batch 24/94, loss 0.7104\n",
            "epoch 11/100, batch 25/94, loss 0.7100\n",
            "epoch 11/100, batch 26/94, loss 0.7111\n",
            "epoch 11/100, batch 27/94, loss 0.7098\n",
            "epoch 11/100, batch 28/94, loss 0.7088\n",
            "epoch 11/100, batch 29/94, loss 0.7094\n",
            "epoch 11/100, batch 30/94, loss 0.7093\n",
            "epoch 11/100, batch 31/94, loss 0.7082\n",
            "epoch 11/100, batch 32/94, loss 0.7089\n",
            "epoch 11/100, batch 33/94, loss 0.7085\n",
            "epoch 11/100, batch 34/94, loss 0.7083\n",
            "epoch 11/100, batch 35/94, loss 0.7080\n",
            "epoch 11/100, batch 36/94, loss 0.7070\n",
            "epoch 11/100, batch 37/94, loss 0.7064\n",
            "epoch 11/100, batch 38/94, loss 0.7077\n",
            "epoch 11/100, batch 39/94, loss 0.7065\n",
            "epoch 11/100, batch 40/94, loss 0.7062\n",
            "epoch 11/100, batch 41/94, loss 0.7055\n",
            "epoch 11/100, batch 42/94, loss 0.7051\n",
            "epoch 11/100, batch 43/94, loss 0.7047\n",
            "epoch 11/100, batch 44/94, loss 0.7050\n",
            "epoch 11/100, batch 45/94, loss 0.7048\n",
            "epoch 11/100, batch 46/94, loss 0.7043\n",
            "epoch 11/100, batch 47/94, loss 0.7034\n",
            "epoch 11/100, batch 48/94, loss 0.7037\n",
            "epoch 11/100, batch 49/94, loss 0.7024\n",
            "epoch 11/100, batch 50/94, loss 0.7027\n",
            "epoch 11/100, batch 51/94, loss 0.7022\n",
            "epoch 11/100, batch 52/94, loss 0.7016\n",
            "epoch 11/100, batch 53/94, loss 0.7016\n",
            "epoch 11/100, batch 54/94, loss 0.7015\n",
            "epoch 11/100, batch 55/94, loss 0.7007\n",
            "epoch 11/100, batch 56/94, loss 0.7003\n",
            "epoch 11/100, batch 57/94, loss 0.7007\n",
            "epoch 11/100, batch 58/94, loss 0.6998\n",
            "epoch 11/100, batch 59/94, loss 0.6990\n",
            "epoch 11/100, batch 60/94, loss 0.6994\n",
            "epoch 11/100, batch 61/94, loss 0.6986\n",
            "epoch 11/100, batch 62/94, loss 0.6981\n",
            "epoch 11/100, batch 63/94, loss 0.6982\n",
            "epoch 11/100, batch 64/94, loss 0.6975\n",
            "epoch 11/100, batch 65/94, loss 0.6972\n",
            "epoch 11/100, batch 66/94, loss 0.6968\n",
            "epoch 11/100, batch 67/94, loss 0.6969\n",
            "epoch 11/100, batch 68/94, loss 0.6966\n",
            "epoch 11/100, batch 69/94, loss 0.6965\n",
            "epoch 11/100, batch 70/94, loss 0.6959\n",
            "epoch 11/100, batch 71/94, loss 0.6954\n",
            "epoch 11/100, batch 72/94, loss 0.6948\n",
            "epoch 11/100, batch 73/94, loss 0.6953\n",
            "epoch 11/100, batch 74/94, loss 0.6946\n",
            "epoch 11/100, batch 75/94, loss 0.6940\n",
            "epoch 11/100, batch 76/94, loss 0.6940\n",
            "epoch 11/100, batch 77/94, loss 0.6937\n",
            "epoch 11/100, batch 78/94, loss 0.6932\n",
            "epoch 11/100, batch 79/94, loss 0.6931\n",
            "epoch 11/100, batch 80/94, loss 0.6925\n",
            "epoch 11/100, batch 81/94, loss 0.6925\n",
            "epoch 11/100, batch 82/94, loss 0.6921\n",
            "epoch 11/100, batch 83/94, loss 0.6917\n",
            "epoch 11/100, batch 84/94, loss 0.6914\n",
            "epoch 11/100, batch 85/94, loss 0.6911\n",
            "epoch 11/100, batch 86/94, loss 0.6908\n",
            "epoch 11/100, batch 87/94, loss 0.6906\n",
            "epoch 11/100, batch 88/94, loss 0.6902\n",
            "epoch 11/100, batch 89/94, loss 0.6899\n",
            "epoch 11/100, batch 90/94, loss 0.6897\n",
            "epoch 11/100, batch 91/94, loss 0.6891\n",
            "epoch 11/100, batch 92/94, loss 0.6890\n",
            "epoch 11/100, batch 93/94, loss 0.6889\n",
            "epoch 11/100, batch 94/94, loss 0.6883\n",
            "epoch 11/100, training roc_auc_score 0.4865\n",
            "EarlyStopping counter: 4 out of 10\n",
            "epoch 11/100, validation roc_auc_score 0.5756, best validation roc_auc_score 0.6601\n",
            "epoch 12/100, batch 1/94, loss 18.1396\n",
            "epoch 12/100, batch 2/94, loss 18.1408\n",
            "epoch 12/100, batch 3/94, loss 18.0956\n",
            "epoch 12/100, batch 4/94, loss 8.2976\n",
            "epoch 12/100, batch 5/94, loss 0.6935\n",
            "epoch 12/100, batch 6/94, loss 0.6955\n",
            "epoch 12/100, batch 7/94, loss 0.6970\n",
            "epoch 12/100, batch 8/94, loss 0.6989\n",
            "epoch 12/100, batch 9/94, loss 0.7008\n",
            "epoch 12/100, batch 10/94, loss 0.7019\n",
            "epoch 12/100, batch 11/94, loss 0.7036\n",
            "epoch 12/100, batch 12/94, loss 0.7039\n",
            "epoch 12/100, batch 13/94, loss 0.7054\n",
            "epoch 12/100, batch 14/94, loss 0.7067\n",
            "epoch 12/100, batch 15/94, loss 0.7069\n",
            "epoch 12/100, batch 16/94, loss 0.7086\n",
            "epoch 12/100, batch 17/94, loss 0.7076\n",
            "epoch 12/100, batch 18/94, loss 0.7092\n",
            "epoch 12/100, batch 19/94, loss 0.7087\n",
            "epoch 12/100, batch 20/94, loss 0.7099\n",
            "epoch 12/100, batch 21/94, loss 0.7085\n",
            "epoch 12/100, batch 22/94, loss 0.7093\n",
            "epoch 12/100, batch 23/94, loss 0.7097\n",
            "epoch 12/100, batch 24/94, loss 0.7096\n",
            "epoch 12/100, batch 25/94, loss 0.7092\n",
            "epoch 12/100, batch 26/94, loss 0.7108\n",
            "epoch 12/100, batch 27/94, loss 0.7094\n",
            "epoch 12/100, batch 28/94, loss 0.7083\n",
            "epoch 12/100, batch 29/94, loss 0.7087\n",
            "epoch 12/100, batch 30/94, loss 0.7090\n",
            "epoch 12/100, batch 31/94, loss 0.7077\n",
            "epoch 12/100, batch 32/94, loss 0.7085\n",
            "epoch 12/100, batch 33/94, loss 0.7077\n",
            "epoch 12/100, batch 34/94, loss 0.7075\n",
            "epoch 12/100, batch 35/94, loss 0.7070\n",
            "epoch 12/100, batch 36/94, loss 0.7063\n",
            "epoch 12/100, batch 37/94, loss 0.7053\n",
            "epoch 12/100, batch 38/94, loss 0.7069\n",
            "epoch 12/100, batch 39/94, loss 0.7055\n",
            "epoch 12/100, batch 40/94, loss 0.7050\n",
            "epoch 12/100, batch 41/94, loss 0.7045\n",
            "epoch 12/100, batch 42/94, loss 0.7041\n",
            "epoch 12/100, batch 43/94, loss 0.7036\n",
            "epoch 12/100, batch 44/94, loss 0.7037\n",
            "epoch 12/100, batch 45/94, loss 0.7034\n",
            "epoch 12/100, batch 46/94, loss 0.7033\n",
            "epoch 12/100, batch 47/94, loss 0.7022\n",
            "epoch 12/100, batch 48/94, loss 0.7023\n",
            "epoch 12/100, batch 49/94, loss 0.7014\n",
            "epoch 12/100, batch 50/94, loss 0.7011\n",
            "epoch 12/100, batch 51/94, loss 0.7011\n",
            "epoch 12/100, batch 52/94, loss 0.7004\n",
            "epoch 12/100, batch 53/94, loss 0.7003\n",
            "epoch 12/100, batch 54/94, loss 0.7000\n",
            "epoch 12/100, batch 55/94, loss 0.6994\n",
            "epoch 12/100, batch 56/94, loss 0.6994\n",
            "epoch 12/100, batch 57/94, loss 0.7002\n",
            "epoch 12/100, batch 58/94, loss 0.6987\n",
            "epoch 12/100, batch 59/94, loss 0.6981\n",
            "epoch 12/100, batch 60/94, loss 0.6983\n",
            "epoch 12/100, batch 61/94, loss 0.6976\n",
            "epoch 12/100, batch 62/94, loss 0.6969\n",
            "epoch 12/100, batch 63/94, loss 0.6970\n",
            "epoch 12/100, batch 64/94, loss 0.6964\n",
            "epoch 12/100, batch 65/94, loss 0.6959\n",
            "epoch 12/100, batch 66/94, loss 0.6957\n",
            "epoch 12/100, batch 67/94, loss 0.6957\n",
            "epoch 12/100, batch 68/94, loss 0.6954\n",
            "epoch 12/100, batch 69/94, loss 0.6953\n",
            "epoch 12/100, batch 70/94, loss 0.6946\n",
            "epoch 12/100, batch 71/94, loss 0.6940\n",
            "epoch 12/100, batch 72/94, loss 0.6936\n",
            "epoch 12/100, batch 73/94, loss 0.6937\n",
            "epoch 12/100, batch 74/94, loss 0.6932\n",
            "epoch 12/100, batch 75/94, loss 0.6932\n",
            "epoch 12/100, batch 76/94, loss 0.6926\n",
            "epoch 12/100, batch 77/94, loss 0.6922\n",
            "epoch 12/100, batch 78/94, loss 0.6920\n",
            "epoch 12/100, batch 79/94, loss 0.6916\n",
            "epoch 12/100, batch 80/94, loss 0.6909\n",
            "epoch 12/100, batch 81/94, loss 0.6910\n",
            "epoch 12/100, batch 82/94, loss 0.6904\n",
            "epoch 12/100, batch 83/94, loss 0.6900\n",
            "epoch 12/100, batch 84/94, loss 0.6896\n",
            "epoch 12/100, batch 85/94, loss 0.6894\n",
            "epoch 12/100, batch 86/94, loss 0.6890\n",
            "epoch 12/100, batch 87/94, loss 0.6887\n",
            "epoch 12/100, batch 88/94, loss 0.6882\n",
            "epoch 12/100, batch 89/94, loss 0.6878\n",
            "epoch 12/100, batch 90/94, loss 0.6875\n",
            "epoch 12/100, batch 91/94, loss 0.6870\n",
            "epoch 12/100, batch 92/94, loss 0.6872\n",
            "epoch 12/100, batch 93/94, loss 0.6871\n",
            "epoch 12/100, batch 94/94, loss 0.6862\n",
            "epoch 12/100, training roc_auc_score 0.5034\n",
            "EarlyStopping counter: 5 out of 10\n",
            "epoch 12/100, validation roc_auc_score 0.5279, best validation roc_auc_score 0.6601\n",
            "epoch 13/100, batch 1/94, loss 18.1964\n",
            "epoch 13/100, batch 2/94, loss 18.2145\n",
            "epoch 13/100, batch 3/94, loss 18.1418\n",
            "epoch 13/100, batch 4/94, loss 8.2793\n",
            "epoch 13/100, batch 5/94, loss 0.6922\n",
            "epoch 13/100, batch 6/94, loss 0.6949\n",
            "epoch 13/100, batch 7/94, loss 0.6964\n",
            "epoch 13/100, batch 8/94, loss 0.6988\n",
            "epoch 13/100, batch 9/94, loss 0.7010\n",
            "epoch 13/100, batch 10/94, loss 0.7020\n",
            "epoch 13/100, batch 11/94, loss 0.7037\n",
            "epoch 13/100, batch 12/94, loss 0.7043\n",
            "epoch 13/100, batch 13/94, loss 0.7060\n",
            "epoch 13/100, batch 14/94, loss 0.7079\n",
            "epoch 13/100, batch 15/94, loss 0.7078\n",
            "epoch 13/100, batch 16/94, loss 0.7103\n",
            "epoch 13/100, batch 17/94, loss 0.7083\n",
            "epoch 13/100, batch 18/94, loss 0.7104\n",
            "epoch 13/100, batch 19/94, loss 0.7097\n",
            "epoch 13/100, batch 20/94, loss 0.7115\n",
            "epoch 13/100, batch 21/94, loss 0.7093\n",
            "epoch 13/100, batch 22/94, loss 0.7108\n",
            "epoch 13/100, batch 23/94, loss 0.7109\n",
            "epoch 13/100, batch 24/94, loss 0.7113\n",
            "epoch 13/100, batch 25/94, loss 0.7107\n",
            "epoch 13/100, batch 26/94, loss 0.7129\n",
            "epoch 13/100, batch 27/94, loss 0.7119\n",
            "epoch 13/100, batch 28/94, loss 0.7094\n",
            "epoch 13/100, batch 29/94, loss 0.7103\n",
            "epoch 13/100, batch 30/94, loss 0.7103\n",
            "epoch 13/100, batch 31/94, loss 0.7085\n",
            "epoch 13/100, batch 32/94, loss 0.7098\n",
            "epoch 13/100, batch 33/94, loss 0.7091\n",
            "epoch 13/100, batch 34/94, loss 0.7088\n",
            "epoch 13/100, batch 35/94, loss 0.7083\n",
            "epoch 13/100, batch 36/94, loss 0.7069\n",
            "epoch 13/100, batch 37/94, loss 0.7063\n",
            "epoch 13/100, batch 38/94, loss 0.7082\n",
            "epoch 13/100, batch 39/94, loss 0.7066\n",
            "epoch 13/100, batch 40/94, loss 0.7061\n",
            "epoch 13/100, batch 41/94, loss 0.7052\n",
            "epoch 13/100, batch 42/94, loss 0.7051\n",
            "epoch 13/100, batch 43/94, loss 0.7044\n",
            "epoch 13/100, batch 44/94, loss 0.7050\n",
            "epoch 13/100, batch 45/94, loss 0.7047\n",
            "epoch 13/100, batch 46/94, loss 0.7039\n",
            "epoch 13/100, batch 47/94, loss 0.7031\n",
            "epoch 13/100, batch 48/94, loss 0.7036\n",
            "epoch 13/100, batch 49/94, loss 0.7020\n",
            "epoch 13/100, batch 50/94, loss 0.7023\n",
            "epoch 13/100, batch 51/94, loss 0.7020\n",
            "epoch 13/100, batch 52/94, loss 0.7013\n",
            "epoch 13/100, batch 53/94, loss 0.7014\n",
            "epoch 13/100, batch 54/94, loss 0.7010\n",
            "epoch 13/100, batch 55/94, loss 0.7001\n",
            "epoch 13/100, batch 56/94, loss 0.6997\n",
            "epoch 13/100, batch 57/94, loss 0.7007\n",
            "epoch 13/100, batch 58/94, loss 0.6995\n",
            "epoch 13/100, batch 59/94, loss 0.6984\n",
            "epoch 13/100, batch 60/94, loss 0.6989\n",
            "epoch 13/100, batch 61/94, loss 0.6981\n",
            "epoch 13/100, batch 62/94, loss 0.6973\n",
            "epoch 13/100, batch 63/94, loss 0.6976\n",
            "epoch 13/100, batch 64/94, loss 0.6967\n",
            "epoch 13/100, batch 65/94, loss 0.6963\n",
            "epoch 13/100, batch 66/94, loss 0.6960\n",
            "epoch 13/100, batch 67/94, loss 0.6965\n",
            "epoch 13/100, batch 68/94, loss 0.6960\n",
            "epoch 13/100, batch 69/94, loss 0.6960\n",
            "epoch 13/100, batch 70/94, loss 0.6953\n",
            "epoch 13/100, batch 71/94, loss 0.6946\n",
            "epoch 13/100, batch 72/94, loss 0.6941\n",
            "epoch 13/100, batch 73/94, loss 0.6947\n",
            "epoch 13/100, batch 74/94, loss 0.6939\n",
            "epoch 13/100, batch 75/94, loss 0.6934\n",
            "epoch 13/100, batch 76/94, loss 0.6936\n",
            "epoch 13/100, batch 77/94, loss 0.6932\n",
            "epoch 13/100, batch 78/94, loss 0.6926\n",
            "epoch 13/100, batch 79/94, loss 0.6925\n",
            "epoch 13/100, batch 80/94, loss 0.6917\n",
            "epoch 13/100, batch 81/94, loss 0.6918\n",
            "epoch 13/100, batch 82/94, loss 0.6915\n",
            "epoch 13/100, batch 83/94, loss 0.6909\n",
            "epoch 13/100, batch 84/94, loss 0.6907\n",
            "epoch 13/100, batch 85/94, loss 0.6905\n",
            "epoch 13/100, batch 86/94, loss 0.6901\n",
            "epoch 13/100, batch 87/94, loss 0.6898\n",
            "epoch 13/100, batch 88/94, loss 0.6895\n",
            "epoch 13/100, batch 89/94, loss 0.6892\n",
            "epoch 13/100, batch 90/94, loss 0.6889\n",
            "epoch 13/100, batch 91/94, loss 0.6883\n",
            "epoch 13/100, batch 92/94, loss 0.6895\n",
            "epoch 13/100, batch 93/94, loss 0.6891\n",
            "epoch 13/100, batch 94/94, loss 0.6879\n",
            "epoch 13/100, training roc_auc_score 0.4873\n",
            "EarlyStopping counter: 6 out of 10\n",
            "epoch 13/100, validation roc_auc_score 0.5738, best validation roc_auc_score 0.6601\n",
            "epoch 14/100, batch 1/94, loss 18.1786\n",
            "epoch 14/100, batch 2/94, loss 18.1882\n",
            "epoch 14/100, batch 3/94, loss 18.1336\n",
            "epoch 14/100, batch 4/94, loss 8.1195\n",
            "epoch 14/100, batch 5/94, loss 0.6927\n",
            "epoch 14/100, batch 6/94, loss 0.6948\n",
            "epoch 14/100, batch 7/94, loss 0.6963\n",
            "epoch 14/100, batch 8/94, loss 0.6982\n",
            "epoch 14/100, batch 9/94, loss 0.7003\n",
            "epoch 14/100, batch 10/94, loss 0.7015\n",
            "epoch 14/100, batch 11/94, loss 0.7035\n",
            "epoch 14/100, batch 12/94, loss 0.7035\n",
            "epoch 14/100, batch 13/94, loss 0.7050\n",
            "epoch 14/100, batch 14/94, loss 0.7065\n",
            "epoch 14/100, batch 15/94, loss 0.7060\n",
            "epoch 14/100, batch 16/94, loss 0.7081\n",
            "epoch 14/100, batch 17/94, loss 0.7068\n",
            "epoch 14/100, batch 18/94, loss 0.7084\n",
            "epoch 14/100, batch 19/94, loss 0.7073\n",
            "epoch 14/100, batch 20/94, loss 0.7087\n",
            "epoch 14/100, batch 21/94, loss 0.7075\n",
            "epoch 14/100, batch 22/94, loss 0.7086\n",
            "epoch 14/100, batch 23/94, loss 0.7085\n",
            "epoch 14/100, batch 24/94, loss 0.7089\n",
            "epoch 14/100, batch 25/94, loss 0.7082\n",
            "epoch 14/100, batch 26/94, loss 0.7102\n",
            "epoch 14/100, batch 27/94, loss 0.7084\n",
            "epoch 14/100, batch 28/94, loss 0.7070\n",
            "epoch 14/100, batch 29/94, loss 0.7079\n",
            "epoch 14/100, batch 30/94, loss 0.7083\n",
            "epoch 14/100, batch 31/94, loss 0.7067\n",
            "epoch 14/100, batch 32/94, loss 0.7079\n",
            "epoch 14/100, batch 33/94, loss 0.7065\n",
            "epoch 14/100, batch 34/94, loss 0.7069\n",
            "epoch 14/100, batch 35/94, loss 0.7065\n",
            "epoch 14/100, batch 36/94, loss 0.7056\n",
            "epoch 14/100, batch 37/94, loss 0.7046\n",
            "epoch 14/100, batch 38/94, loss 0.7064\n",
            "epoch 14/100, batch 39/94, loss 0.7047\n",
            "epoch 14/100, batch 40/94, loss 0.7045\n",
            "epoch 14/100, batch 41/94, loss 0.7037\n",
            "epoch 14/100, batch 42/94, loss 0.7040\n",
            "epoch 14/100, batch 43/94, loss 0.7031\n",
            "epoch 14/100, batch 44/94, loss 0.7037\n",
            "epoch 14/100, batch 45/94, loss 0.7035\n",
            "epoch 14/100, batch 46/94, loss 0.7034\n",
            "epoch 14/100, batch 47/94, loss 0.7021\n",
            "epoch 14/100, batch 48/94, loss 0.7022\n",
            "epoch 14/100, batch 49/94, loss 0.7011\n",
            "epoch 14/100, batch 50/94, loss 0.7014\n",
            "epoch 14/100, batch 51/94, loss 0.7011\n",
            "epoch 14/100, batch 52/94, loss 0.7007\n",
            "epoch 14/100, batch 53/94, loss 0.7005\n",
            "epoch 14/100, batch 54/94, loss 0.7002\n",
            "epoch 14/100, batch 55/94, loss 0.6994\n",
            "epoch 14/100, batch 56/94, loss 0.6993\n",
            "epoch 14/100, batch 57/94, loss 0.7005\n",
            "epoch 14/100, batch 58/94, loss 0.6989\n",
            "epoch 14/100, batch 59/94, loss 0.6980\n",
            "epoch 14/100, batch 60/94, loss 0.6984\n",
            "epoch 14/100, batch 61/94, loss 0.6979\n",
            "epoch 14/100, batch 62/94, loss 0.6970\n",
            "epoch 14/100, batch 63/94, loss 0.6973\n",
            "epoch 14/100, batch 64/94, loss 0.6965\n",
            "epoch 14/100, batch 65/94, loss 0.6960\n",
            "epoch 14/100, batch 66/94, loss 0.6958\n",
            "epoch 14/100, batch 67/94, loss 0.6961\n",
            "epoch 14/100, batch 68/94, loss 0.6957\n",
            "epoch 14/100, batch 69/94, loss 0.6959\n",
            "epoch 14/100, batch 70/94, loss 0.6952\n",
            "epoch 14/100, batch 71/94, loss 0.6944\n",
            "epoch 14/100, batch 72/94, loss 0.6938\n",
            "epoch 14/100, batch 73/94, loss 0.6943\n",
            "epoch 14/100, batch 74/94, loss 0.6937\n",
            "epoch 14/100, batch 75/94, loss 0.6934\n",
            "epoch 14/100, batch 76/94, loss 0.6933\n",
            "epoch 14/100, batch 77/94, loss 0.6930\n",
            "epoch 14/100, batch 78/94, loss 0.6928\n",
            "epoch 14/100, batch 79/94, loss 0.6924\n",
            "epoch 14/100, batch 80/94, loss 0.6918\n",
            "epoch 14/100, batch 81/94, loss 0.6919\n",
            "epoch 14/100, batch 82/94, loss 0.6914\n",
            "epoch 14/100, batch 83/94, loss 0.6908\n",
            "epoch 14/100, batch 84/94, loss 0.6906\n",
            "epoch 14/100, batch 85/94, loss 0.6905\n",
            "epoch 14/100, batch 86/94, loss 0.6902\n",
            "epoch 14/100, batch 87/94, loss 0.6899\n",
            "epoch 14/100, batch 88/94, loss 0.6894\n",
            "epoch 14/100, batch 89/94, loss 0.6893\n",
            "epoch 14/100, batch 90/94, loss 0.6892\n",
            "epoch 14/100, batch 91/94, loss 0.6882\n",
            "epoch 14/100, batch 92/94, loss 0.6887\n",
            "epoch 14/100, batch 93/94, loss 0.6887\n",
            "epoch 14/100, batch 94/94, loss 0.6878\n",
            "epoch 14/100, training roc_auc_score 0.5053\n",
            "EarlyStopping counter: 7 out of 10\n",
            "epoch 14/100, validation roc_auc_score 0.5219, best validation roc_auc_score 0.6601\n",
            "epoch 15/100, batch 1/94, loss 18.1932\n",
            "epoch 15/100, batch 2/94, loss 18.2173\n",
            "epoch 15/100, batch 3/94, loss 18.1407\n",
            "epoch 15/100, batch 4/94, loss 7.9779\n",
            "epoch 15/100, batch 5/94, loss 0.6928\n",
            "epoch 15/100, batch 6/94, loss 0.6949\n",
            "epoch 15/100, batch 7/94, loss 0.6963\n",
            "epoch 15/100, batch 8/94, loss 0.6980\n",
            "epoch 15/100, batch 9/94, loss 0.7000\n",
            "epoch 15/100, batch 10/94, loss 0.7007\n",
            "epoch 15/100, batch 11/94, loss 0.7025\n",
            "epoch 15/100, batch 12/94, loss 0.7027\n",
            "epoch 15/100, batch 13/94, loss 0.7044\n",
            "epoch 15/100, batch 14/94, loss 0.7060\n",
            "epoch 15/100, batch 15/94, loss 0.7053\n",
            "epoch 15/100, batch 16/94, loss 0.7080\n",
            "epoch 15/100, batch 17/94, loss 0.7058\n",
            "epoch 15/100, batch 18/94, loss 0.7078\n",
            "epoch 15/100, batch 19/94, loss 0.7067\n",
            "epoch 15/100, batch 20/94, loss 0.7091\n",
            "epoch 15/100, batch 21/94, loss 0.7072\n",
            "epoch 15/100, batch 22/94, loss 0.7081\n",
            "epoch 15/100, batch 23/94, loss 0.7085\n",
            "epoch 15/100, batch 24/94, loss 0.7084\n",
            "epoch 15/100, batch 25/94, loss 0.7080\n",
            "epoch 15/100, batch 26/94, loss 0.7107\n",
            "epoch 15/100, batch 27/94, loss 0.7080\n",
            "epoch 15/100, batch 28/94, loss 0.7068\n",
            "epoch 15/100, batch 29/94, loss 0.7078\n",
            "epoch 15/100, batch 30/94, loss 0.7078\n",
            "epoch 15/100, batch 31/94, loss 0.7067\n",
            "epoch 15/100, batch 32/94, loss 0.7078\n",
            "epoch 15/100, batch 33/94, loss 0.7069\n",
            "epoch 15/100, batch 34/94, loss 0.7063\n",
            "epoch 15/100, batch 35/94, loss 0.7065\n",
            "epoch 15/100, batch 36/94, loss 0.7052\n",
            "epoch 15/100, batch 37/94, loss 0.7044\n",
            "epoch 15/100, batch 38/94, loss 0.7062\n",
            "epoch 15/100, batch 39/94, loss 0.7047\n",
            "epoch 15/100, batch 40/94, loss 0.7042\n",
            "epoch 15/100, batch 41/94, loss 0.7034\n",
            "epoch 15/100, batch 42/94, loss 0.7036\n",
            "epoch 15/100, batch 43/94, loss 0.7031\n",
            "epoch 15/100, batch 44/94, loss 0.7034\n",
            "epoch 15/100, batch 45/94, loss 0.7032\n",
            "epoch 15/100, batch 46/94, loss 0.7027\n",
            "epoch 15/100, batch 47/94, loss 0.7021\n",
            "epoch 15/100, batch 48/94, loss 0.7022\n",
            "epoch 15/100, batch 49/94, loss 0.7010\n",
            "epoch 15/100, batch 50/94, loss 0.7015\n",
            "epoch 15/100, batch 51/94, loss 0.7010\n",
            "epoch 15/100, batch 52/94, loss 0.7003\n",
            "epoch 15/100, batch 53/94, loss 0.7004\n",
            "epoch 15/100, batch 54/94, loss 0.7000\n",
            "epoch 15/100, batch 55/94, loss 0.6992\n",
            "epoch 15/100, batch 56/94, loss 0.6990\n",
            "epoch 15/100, batch 57/94, loss 0.6997\n",
            "epoch 15/100, batch 58/94, loss 0.6986\n",
            "epoch 15/100, batch 59/94, loss 0.6976\n",
            "epoch 15/100, batch 60/94, loss 0.6984\n",
            "epoch 15/100, batch 61/94, loss 0.6976\n",
            "epoch 15/100, batch 62/94, loss 0.6968\n",
            "epoch 15/100, batch 63/94, loss 0.6970\n",
            "epoch 15/100, batch 64/94, loss 0.6961\n",
            "epoch 15/100, batch 65/94, loss 0.6959\n",
            "epoch 15/100, batch 66/94, loss 0.6956\n",
            "epoch 15/100, batch 67/94, loss 0.6962\n",
            "epoch 15/100, batch 68/94, loss 0.6955\n",
            "epoch 15/100, batch 69/94, loss 0.6957\n",
            "epoch 15/100, batch 70/94, loss 0.6951\n",
            "epoch 15/100, batch 71/94, loss 0.6945\n",
            "epoch 15/100, batch 72/94, loss 0.6937\n",
            "epoch 15/100, batch 73/94, loss 0.6945\n",
            "epoch 15/100, batch 74/94, loss 0.6936\n",
            "epoch 15/100, batch 75/94, loss 0.6929\n",
            "epoch 15/100, batch 76/94, loss 0.6931\n",
            "epoch 15/100, batch 77/94, loss 0.6930\n",
            "epoch 15/100, batch 78/94, loss 0.6924\n",
            "epoch 15/100, batch 79/94, loss 0.6924\n",
            "epoch 15/100, batch 80/94, loss 0.6915\n",
            "epoch 15/100, batch 81/94, loss 0.6917\n",
            "epoch 15/100, batch 82/94, loss 0.6916\n",
            "epoch 15/100, batch 83/94, loss 0.6908\n",
            "epoch 15/100, batch 84/94, loss 0.6907\n",
            "epoch 15/100, batch 85/94, loss 0.6905\n",
            "epoch 15/100, batch 86/94, loss 0.6902\n",
            "epoch 15/100, batch 87/94, loss 0.6898\n",
            "epoch 15/100, batch 88/94, loss 0.6896\n",
            "epoch 15/100, batch 89/94, loss 0.6893\n",
            "epoch 15/100, batch 90/94, loss 0.6890\n",
            "epoch 15/100, batch 91/94, loss 0.6883\n",
            "epoch 15/100, batch 92/94, loss 0.6883\n",
            "epoch 15/100, batch 93/94, loss 0.6884\n",
            "epoch 15/100, batch 94/94, loss 0.6876\n",
            "epoch 15/100, training roc_auc_score 0.4978\n",
            "EarlyStopping counter: 8 out of 10\n",
            "epoch 15/100, validation roc_auc_score 0.5717, best validation roc_auc_score 0.6601\n",
            "epoch 16/100, batch 1/94, loss 18.1845\n",
            "epoch 16/100, batch 2/94, loss 18.1894\n",
            "epoch 16/100, batch 3/94, loss 18.1396\n",
            "epoch 16/100, batch 4/94, loss 7.9698\n",
            "epoch 16/100, batch 5/94, loss 0.6926\n",
            "epoch 16/100, batch 6/94, loss 0.6946\n",
            "epoch 16/100, batch 7/94, loss 0.6960\n",
            "epoch 16/100, batch 8/94, loss 0.6981\n",
            "epoch 16/100, batch 9/94, loss 0.7000\n",
            "epoch 16/100, batch 10/94, loss 0.7008\n",
            "epoch 16/100, batch 11/94, loss 0.7030\n",
            "epoch 16/100, batch 12/94, loss 0.7030\n",
            "epoch 16/100, batch 13/94, loss 0.7049\n",
            "epoch 16/100, batch 14/94, loss 0.7064\n",
            "epoch 16/100, batch 15/94, loss 0.7055\n",
            "epoch 16/100, batch 16/94, loss 0.7080\n",
            "epoch 16/100, batch 17/94, loss 0.7078\n",
            "epoch 16/100, batch 18/94, loss 0.7085\n",
            "epoch 16/100, batch 19/94, loss 0.7078\n",
            "epoch 16/100, batch 20/94, loss 0.7087\n",
            "epoch 16/100, batch 21/94, loss 0.7082\n",
            "epoch 16/100, batch 22/94, loss 0.7096\n",
            "epoch 16/100, batch 23/94, loss 0.7088\n",
            "epoch 16/100, batch 24/94, loss 0.7097\n",
            "epoch 16/100, batch 25/94, loss 0.7082\n",
            "epoch 16/100, batch 26/94, loss 0.7113\n",
            "epoch 16/100, batch 27/94, loss 0.7079\n",
            "epoch 16/100, batch 28/94, loss 0.7071\n",
            "epoch 16/100, batch 29/94, loss 0.7077\n",
            "epoch 16/100, batch 30/94, loss 0.7084\n",
            "epoch 16/100, batch 31/94, loss 0.7064\n",
            "epoch 16/100, batch 32/94, loss 0.7080\n",
            "epoch 16/100, batch 33/94, loss 0.7061\n",
            "epoch 16/100, batch 34/94, loss 0.7066\n",
            "epoch 16/100, batch 35/94, loss 0.7059\n",
            "epoch 16/100, batch 36/94, loss 0.7057\n",
            "epoch 16/100, batch 37/94, loss 0.7043\n",
            "epoch 16/100, batch 38/94, loss 0.7066\n",
            "epoch 16/100, batch 39/94, loss 0.7043\n",
            "epoch 16/100, batch 40/94, loss 0.7036\n",
            "epoch 16/100, batch 41/94, loss 0.7030\n",
            "epoch 16/100, batch 42/94, loss 0.7037\n",
            "epoch 16/100, batch 43/94, loss 0.7025\n",
            "epoch 16/100, batch 44/94, loss 0.7032\n",
            "epoch 16/100, batch 45/94, loss 0.7033\n",
            "epoch 16/100, batch 46/94, loss 0.7030\n",
            "epoch 16/100, batch 47/94, loss 0.7015\n",
            "epoch 16/100, batch 48/94, loss 0.7017\n",
            "epoch 16/100, batch 49/94, loss 0.7007\n",
            "epoch 16/100, batch 50/94, loss 0.7006\n",
            "epoch 16/100, batch 51/94, loss 0.7007\n",
            "epoch 16/100, batch 52/94, loss 0.7004\n",
            "epoch 16/100, batch 53/94, loss 0.7002\n",
            "epoch 16/100, batch 54/94, loss 0.6996\n",
            "epoch 16/100, batch 55/94, loss 0.6986\n",
            "epoch 16/100, batch 56/94, loss 0.6987\n",
            "epoch 16/100, batch 57/94, loss 0.7001\n",
            "epoch 16/100, batch 58/94, loss 0.6984\n",
            "epoch 16/100, batch 59/94, loss 0.6975\n",
            "epoch 16/100, batch 60/94, loss 0.6977\n",
            "epoch 16/100, batch 61/94, loss 0.6975\n",
            "epoch 16/100, batch 62/94, loss 0.6962\n",
            "epoch 16/100, batch 63/94, loss 0.6968\n",
            "epoch 16/100, batch 64/94, loss 0.6958\n",
            "epoch 16/100, batch 65/94, loss 0.6953\n",
            "epoch 16/100, batch 66/94, loss 0.6952\n",
            "epoch 16/100, batch 67/94, loss 0.6956\n",
            "epoch 16/100, batch 68/94, loss 0.6953\n",
            "epoch 16/100, batch 69/94, loss 0.6952\n",
            "epoch 16/100, batch 70/94, loss 0.6946\n",
            "epoch 16/100, batch 71/94, loss 0.6939\n",
            "epoch 16/100, batch 72/94, loss 0.6932\n",
            "epoch 16/100, batch 73/94, loss 0.6936\n",
            "epoch 16/100, batch 74/94, loss 0.6929\n",
            "epoch 16/100, batch 75/94, loss 0.6928\n",
            "epoch 16/100, batch 76/94, loss 0.6925\n",
            "epoch 16/100, batch 77/94, loss 0.6921\n",
            "epoch 16/100, batch 78/94, loss 0.6920\n",
            "epoch 16/100, batch 79/94, loss 0.6917\n",
            "epoch 16/100, batch 80/94, loss 0.6908\n",
            "epoch 16/100, batch 81/94, loss 0.6910\n",
            "epoch 16/100, batch 82/94, loss 0.6904\n",
            "epoch 16/100, batch 83/94, loss 0.6897\n",
            "epoch 16/100, batch 84/94, loss 0.6898\n",
            "epoch 16/100, batch 85/94, loss 0.6896\n",
            "epoch 16/100, batch 86/94, loss 0.6892\n",
            "epoch 16/100, batch 87/94, loss 0.6889\n",
            "epoch 16/100, batch 88/94, loss 0.6883\n",
            "epoch 16/100, batch 89/94, loss 0.6883\n",
            "epoch 16/100, batch 90/94, loss 0.6880\n",
            "epoch 16/100, batch 91/94, loss 0.6870\n",
            "epoch 16/100, batch 92/94, loss 0.6873\n",
            "epoch 16/100, batch 93/94, loss 0.6875\n",
            "epoch 16/100, batch 94/94, loss 0.6863\n",
            "epoch 16/100, training roc_auc_score 0.5132\n",
            "EarlyStopping counter: 9 out of 10\n",
            "epoch 16/100, validation roc_auc_score 0.5521, best validation roc_auc_score 0.6601\n",
            "epoch 17/100, batch 1/94, loss 18.2457\n",
            "epoch 17/100, batch 2/94, loss 18.2737\n",
            "epoch 17/100, batch 3/94, loss 18.1814\n",
            "epoch 17/100, batch 4/94, loss 7.8329\n",
            "epoch 17/100, batch 5/94, loss 0.6918\n",
            "epoch 17/100, batch 6/94, loss 0.6941\n",
            "epoch 17/100, batch 7/94, loss 0.6956\n",
            "epoch 17/100, batch 8/94, loss 0.6977\n",
            "epoch 17/100, batch 9/94, loss 0.7002\n",
            "epoch 17/100, batch 10/94, loss 0.7008\n",
            "epoch 17/100, batch 11/94, loss 0.7033\n",
            "epoch 17/100, batch 12/94, loss 0.7032\n",
            "epoch 17/100, batch 13/94, loss 0.7051\n",
            "epoch 17/100, batch 14/94, loss 0.7065\n",
            "epoch 17/100, batch 15/94, loss 0.7052\n",
            "epoch 17/100, batch 16/94, loss 0.7079\n",
            "epoch 17/100, batch 17/94, loss 0.7072\n",
            "epoch 17/100, batch 18/94, loss 0.7083\n",
            "epoch 17/100, batch 19/94, loss 0.7075\n",
            "epoch 17/100, batch 20/94, loss 0.7089\n",
            "epoch 17/100, batch 21/94, loss 0.7076\n",
            "epoch 17/100, batch 22/94, loss 0.7093\n",
            "epoch 17/100, batch 23/94, loss 0.7084\n",
            "epoch 17/100, batch 24/94, loss 0.7095\n",
            "epoch 17/100, batch 25/94, loss 0.7083\n",
            "epoch 17/100, batch 26/94, loss 0.7111\n",
            "epoch 17/100, batch 27/94, loss 0.7071\n",
            "epoch 17/100, batch 28/94, loss 0.7065\n",
            "epoch 17/100, batch 29/94, loss 0.7079\n",
            "epoch 17/100, batch 30/94, loss 0.7074\n",
            "epoch 17/100, batch 31/94, loss 0.7055\n",
            "epoch 17/100, batch 32/94, loss 0.7074\n",
            "epoch 17/100, batch 33/94, loss 0.7061\n",
            "epoch 17/100, batch 34/94, loss 0.7056\n",
            "epoch 17/100, batch 35/94, loss 0.7058\n",
            "epoch 17/100, batch 36/94, loss 0.7044\n",
            "epoch 17/100, batch 37/94, loss 0.7038\n",
            "epoch 17/100, batch 38/94, loss 0.7054\n",
            "epoch 17/100, batch 39/94, loss 0.7042\n",
            "epoch 17/100, batch 40/94, loss 0.7033\n",
            "epoch 17/100, batch 41/94, loss 0.7025\n",
            "epoch 17/100, batch 42/94, loss 0.7027\n",
            "epoch 17/100, batch 43/94, loss 0.7020\n",
            "epoch 17/100, batch 44/94, loss 0.7026\n",
            "epoch 17/100, batch 45/94, loss 0.7024\n",
            "epoch 17/100, batch 46/94, loss 0.7018\n",
            "epoch 17/100, batch 47/94, loss 0.7012\n",
            "epoch 17/100, batch 48/94, loss 0.7015\n",
            "epoch 17/100, batch 49/94, loss 0.7002\n",
            "epoch 17/100, batch 50/94, loss 0.7004\n",
            "epoch 17/100, batch 51/94, loss 0.7003\n",
            "epoch 17/100, batch 52/94, loss 0.6995\n",
            "epoch 17/100, batch 53/94, loss 0.6995\n",
            "epoch 17/100, batch 54/94, loss 0.6993\n",
            "epoch 17/100, batch 55/94, loss 0.6986\n",
            "epoch 17/100, batch 56/94, loss 0.6983\n",
            "epoch 17/100, batch 57/94, loss 0.6992\n",
            "epoch 17/100, batch 58/94, loss 0.6979\n",
            "epoch 17/100, batch 59/94, loss 0.6967\n",
            "epoch 17/100, batch 60/94, loss 0.6978\n",
            "epoch 17/100, batch 61/94, loss 0.6969\n",
            "epoch 17/100, batch 62/94, loss 0.6961\n",
            "epoch 17/100, batch 63/94, loss 0.6964\n",
            "epoch 17/100, batch 64/94, loss 0.6955\n",
            "epoch 17/100, batch 65/94, loss 0.6949\n",
            "epoch 17/100, batch 66/94, loss 0.6950\n",
            "epoch 17/100, batch 67/94, loss 0.6957\n",
            "epoch 17/100, batch 68/94, loss 0.6951\n",
            "epoch 17/100, batch 69/94, loss 0.6953\n",
            "epoch 17/100, batch 70/94, loss 0.6948\n",
            "epoch 17/100, batch 71/94, loss 0.6940\n",
            "epoch 17/100, batch 72/94, loss 0.6931\n",
            "epoch 17/100, batch 73/94, loss 0.6939\n",
            "epoch 17/100, batch 74/94, loss 0.6931\n",
            "epoch 17/100, batch 75/94, loss 0.6925\n",
            "epoch 17/100, batch 76/94, loss 0.6926\n",
            "epoch 17/100, batch 77/94, loss 0.6925\n",
            "epoch 17/100, batch 78/94, loss 0.6919\n",
            "epoch 17/100, batch 79/94, loss 0.6918\n",
            "epoch 17/100, batch 80/94, loss 0.6910\n",
            "epoch 17/100, batch 81/94, loss 0.6913\n",
            "epoch 17/100, batch 82/94, loss 0.6909\n",
            "epoch 17/100, batch 83/94, loss 0.6903\n",
            "epoch 17/100, batch 84/94, loss 0.6899\n",
            "epoch 17/100, batch 85/94, loss 0.6900\n",
            "epoch 17/100, batch 86/94, loss 0.6895\n",
            "epoch 17/100, batch 87/94, loss 0.6889\n",
            "epoch 17/100, batch 88/94, loss 0.6889\n",
            "epoch 17/100, batch 89/94, loss 0.6886\n",
            "epoch 17/100, batch 90/94, loss 0.6884\n",
            "epoch 17/100, batch 91/94, loss 0.6875\n",
            "epoch 17/100, batch 92/94, loss 0.6872\n",
            "epoch 17/100, batch 93/94, loss 0.6874\n",
            "epoch 17/100, batch 94/94, loss 0.6867\n",
            "epoch 17/100, training roc_auc_score 0.5026\n",
            "EarlyStopping counter: 10 out of 10\n",
            "epoch 17/100, validation roc_auc_score 0.5161, best validation roc_auc_score 0.6601\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoKebqIJymzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}